if ((movedir==1) || (movedir==3))
reward <- -1
else if (movedir==2) # right
next_state <- state("(1,2)")
else if (movedir==4)  # down
next_state <- state("(2,1)")
}
if (state==state("(1,2)")) {
if (movedir==3) #up
reward <- -1
else if (movedir==4) { #down
next_state <- state("(2,2)")
reward <- trap_reward
} else if (movedir==1) #left
next_state <- state("(1,1)")
else if (movedir==2) #right
next_state <- state("(1,3)")
}
if (state==state("(1,3)")) {
if (movedir==3) #up
reward <- -1
if (movedir==2)
next_state <- state("(1,4)")
else if (movedir==4)
next_state <- state("(2,3)")
else if (movedir==1)
next_state <- state("(1,2)")
}
if (state==state("(1,4)")) {
if ((movedir==3) || (movedir==2)) #up or right
reward <- -1
if (movedir==1)
next_state <- state("(1,3)")
else if (movedir==4) {
next_state <- state("(2,4)")
reward <- trap_reward
}
}
if (state==state("(2,1)")) {
if (movedir==1)
reward <- -1
else if (movedir==3)
next_state <- state("(1,1)")
else if (movedir==2) {
next_state <- state("(2,2)")
reward <- trap_reward
} else if (movedir==4)
next_state <- state("(3,1)")
}
if (state==state("(2,3)")) {
if (movedir==1) {
next_state <- state("(2,2)")
reward <- trap_reward
} else if (movedir==2) {
next_state <- state("(2,4)")
reward <- trap_reward
} else if (movedir==3)
next_state <- state("(1,3)")
else if (movedir==4)
next_state <- state("(3,3)")
}
if (state==state("(3,1)")) {
if (movedir==1)
reward <- -1
else if (movedir==4) {
next_state <- state("(4,1)")
reward <- trap_reward
} else if (movedir==2)
next_state <- state("(3,2)")
else if (movedir==3)
next_state <- state("(2,1)")
}
if (state==state("(3,2)")) {
if (movedir==1)
next_state <- state("(3,1)")
else if (movedir==2)
next_state <- state("(3,3)")
else if (movedir==4)
next_state <- state("(4,2)")
else if (movedir==3) {
reward <- trap_reward
next_state <- state("(2,2)")
}
}
if (state==state("(3,3)")) {
if (movedir==1)
next_state <- state("(3,2)")
else if (movedir==2) {
next_state <- state("(3,4)")
reward <- trap_reward
} else if (movedir==3)
next_state <- state("(3,2)")
else if (movedir==4)
next_state <- state("(4,3)")
}
if (state==state("(4,2)")) {
if (movedir==1) {
next_state <- state("(4,1)")
reward <- trap_reward
} else if (movedir==2)
next_state <- state("(4,3)")
else if (movedir==3)
next_state <- state("(3,2)")
else if (movedir==4)
reward <- -1
}
if (state==state("(4,3)")) {
if (movedir==1)
next_state <- state("(4,2)")
else if (movedir==2) {
next_state <- state("(4,4)")
reward <- 10       # Bingo!
} else if (movedir==3)
next_state <- state("(3,3)")
else if (movedir==4)
reward <- -1
}
return(list(NextState=next_state, Reward=reward))
}
robot_data2 <- sampleExperience(N      =1000,
env    =gridenv2,
states =states,
actions=new_actions)
set.seed(12)
robot_data2 <- sampleExperience(N      =1000,
env    =gridenv2,
states =states,
actions=new_actions)
head(robot_data2,10)
# Did we sample all rewards and all states?
robot_data2 %>% group_by(Reward) %>% summarize(count=n())
robot_data2 %>% group_by(State) %>% summarize(count=n())
control <- list(alpha=0.1, gamma=0.1, epsilon=0.1)
rl2 <- ReinforcementLearning(data=robot_data2,
s="State",
a="Action",
r="Reward",
s_new="NextState",
control=control)
rl2
summary(rl2)
round(rl2$Q,4)
data <- sampleGridSequence(1000)
?sampleGridSequence
head(data)
# Setting reinforcement learning parameters
control <- list(alpha = 0.1, gamma = 0.1, epsilon = 0.1)
# Performing reinforcement learning
model <- ReinforcementLearning(data,
s = "State",
a = "Action",
r = "Reward",
s_new = "NextState",
control = control,
iter=1)
print(model)
library(lattice)
apples <- read.csv("../data/S_and_P/apples.csv")
?xyplot()
apples <- read.csv("../datasets/apples.csv")
apples <- read.csv("../../ADS 5064/data/S_and_P/apples.csv")
?pwd
?cwd
cwd
pwd
getwd()
apples <- read.csv("../ADS 5064/data/S_and_P/apples.csv")
xyplot(diameter ~ measurement | as.factor(Tree),
data=apples,
xlab="Measurement time",
ylab="Diameter (inches)",
layout=c(4,3,1))
xyplot(diameter ~ measurement | as.factor(Tree),
data=apples,
xlab="Measurement time",
ylab="Diameter (inches)",
as.table=TRUE,
layout=c(4,3,1))
?glm()
options(width=80)
library(ISLR2)
library(MASS)
library(dplyr)
library(knitr)
library(caret)
library(keras)
reticulate::use_condaenv(condaenv = "r-tensorflow")
knitr::opts_chunk$set(echo=TRUE, warning=F, message=F, fig.asp=0.7, cache=FALSE)
knit_hooks$set(no.main = function(before, options, envir) {
if (before) par(mar = c(4.1, 4.1, 1.1, 1.1))  # smaller margin on top
})
dat <- data.frame(x1=c(0,0,1,1), x2=c(0,1,0,1), y=c(0,0,0,1))
dat
dat
summary(lm(y ~ x1:x2 - 1, data=dat))
(
a1 <- c(0,1,1)
a2 <- c(-1,1,1)
X %*% a1
X <- cbind(rep(1,4),dat[,1],dat[,2])
X
X %*% a1
a1 <- c(0,1,1)
a2 <- c(-1,1,1)
X %*% a1
relu(X %*% a1)
relu <- function(x) {ifelse(x > 0,x,0)}
relu(X %*% a1)
X %*% a2
relu(X %*% a2)
A1 <- relu(X %*% a1)
A2 <- relu(X %*% a2)
A1
A2
b <- c(-1,3)
T <- cbind(A1,A2) %*% b
relu(T)
options(width=80)
library(ISLR2)
library(MASS)
library(dplyr)
library(knitr)
library(caret)
library(keras)
reticulate::use_condaenv(condaenv = "r-tensorflow")
knitr::opts_chunk$set(echo=TRUE, warning=F, message=F, fig.asp=0.7, cache=FALSE)
knit_hooks$set(no.main = function(before, options, envir) {
if (before) par(mar = c(4.1, 4.1, 1.1, 1.1))  # smaller margin on top
})
dat <- data.frame(x1=c(0,0,1,1), x2=c(0,1,0,1), y=c(0,0,0,1))
dat
relu <- function(x) {ifelse(x > 0,x,0)}
X <- cbind(rep(1,4),dat[,1],dat[,2])
solve_and <- function(a1,a2,b) {
X2 <- cbind(rep(1,4),
relu(X %*% a1),
relu(X %*% a2))
T <- X2 %*% b
print(X2)
print(relu(T))
}
# Grace Magdamo
solve_and(c(-1, 1, 1),
c(-1, 1, 1),
c(1,0,0))
# Grace Magdamo
solve_and(c(-1, 1, 1),
c(-1, 1, 1),
c(0,1,0))
# Grace Magdamo
solve_and(c(-1, 1, 1),
c(-1, 1, 1),
c(0,0,1))
# Grace Magdamo
solve_and(c(-1, 1, 1),
c(-1, 1, 1),
c(0,0,1))
# Shuhan Wang
solve_and(c( 0, 1, 1),
c(-1, 1, 1),
c( 0, 0, 1))
100-26
# Rutvij
solve_and(c(1, 1, 1),
c(1, 1, 1),
c(-5, 1, 1))
# Rutvij
solve_and(c(1, 1, 1),
c(1, 1, 1),
c(-5, 1, 1))
# Yasaman
solve_and(c(-0.9, 0, 1),
c(-0.9, 1, 0),
c(-1.9, 1, 1))
# Yasaman
solve_and(c(-0.9, 1, 0),
c(-0.9, 0, 1),
c(-1.9, 1, 1))
#
solve_and(c(-1, 1, 1),
c(-1, 1, 1),
c(-1, 1, 1))
#
solve_and(c(-1, 1, 1),
c(-1, 1, 1),
c(-1, 1, 1))
# 1.1.
# Define ReLU function
relu <- function(x) {
return(max(0, x))
}
# Corrected assumptions for weights and biases
w1 <- c(1, 1)
w2 <- c(1, 1)
b1 <- -1
b2 <- -2
wo <- c(1, 1)
bo <- -1.5
# Test
input <- expand.grid(X1 = c(0, 1), X2 = c(0, 1))
inpit
input
# Hidden layer outputs
hidden1 <- relu(input$X1 * w1[1] + input$X2 * w1[2] + b1)
hidden2 <- relu(input$X1 * w2[1] + input$X2 * w2[2] + b2)
hidden1
# Output layer output
output <- relu(hidden1 * wo[1] + hidden2 * wo[2] + bo)
# Combine inputs and output
result <- data.frame(input, Y_Predicted = output)
print(result)
output
solve_and(c(-1, 1, 1),
c(-2, 1, 1),
c(-1.5, 1, 1))
relu <- function(x) {ifelse(x > 0,x,0)}
X <- cbind(rep(1,4),dat[,1],dat[,2])
X
solve_and(c(-1, 1, 1),
c(-2, 1, 1),
c(-1.5, 1, 1))
(
(
ban_train <- read.csv(file="../../data/banana_train.csv",stringsAsFactors=TRUE)
ban_test <- read.csv(file="../../data/banana_test.csv",stringsAsFactors=TRUE)
)
ban_train <- read.csv(file="../../data/banana_train.csv",stringsAsFactors=TRUE)
ban_test <- read.csv(file="../../data/banana_test.csv",stringsAsFactors=TRUE)
x_train <- scale(ban_train[, 1:7])
x_test <- scale(ban_test[, 1:7])
y_train <- ifelse(ban_train$Quality == "Good", 1, 0)
y_test <- ifelse(ban_test$Quality == "Good", 1, 0)
yc_train <- to_categorical(y_train, 2)
yc_test <- to_categorical(y_test, 2)
# Define and compile the model
model_2_1 <- keras_model_sequential() %>%
layer_dense(units = 2, input_shape = c(7), activation = 'softmax') %>%
compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
model_2_1
# Train
history_2_1 <- model_2_1 %>% fit(
x_train, yc_train,
epochs = 50,
batch_size = 128,
validation_data = list(x_test, yc_test)
)
# Plot
plot(history_2_1)
# Predict and calculate confusion matrix
predictions_proba <- model_2_1 %>% predict(x_test)
predictions_2_1 <- k_argmax(predictions_proba) %>% as.array()
conf_matrix <- table(Predicted = predictions_2_1, Actual = y_test)
print(conf_matrix)
model_2_2 <- keras_model_sequential() %>%
layer_dense(units = 98, input_shape = c(7), activation = 'tanh') %>%
layer_dense(units = 2, activation = 'softmax')
model_2_2 %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
summary(model_2_2)
history_2_2 <- model_2_2 %>% fit(
x_train, yc_train,
epochs = 50,
batch_size = 128,
validation_data = list(x_test, yc_test)
)
plot(history_2_2)
# Predict probabilities
predictions_proba_2_2 <- model_2_2 %>% predict(x_test)
predictions_2_2 <- k_argmax(predictions_proba_2_2) %>% as.array()
# Create and print the confusion matrix - 2.2
conf_matrix_2_2 <- table(Predicted = predictions_2_2, Actual = y_test)
print(conf_matrix_2_2)
# Plotting
history_data_2_2 <- data.frame(
epoch = seq_along(history_2_2$metrics$accuracy),
accuracy = history_2_2$metrics$accuracy,
val_accuracy = history_2_2$metrics$val_accuracy,
loss = history_2_2$metrics$loss,
val_loss = history_2_2$metrics$val_loss
)
ggplot(history_data_2_2, aes(x = epoch)) +
geom_line(aes(y = accuracy, color = "Training")) +
geom_line(aes(y = val_accuracy, color = "Validation")) +
labs(title = "Model 2_2 Accuracy", y = "Accuracy", x = "Epoch") +
scale_color_manual(values = c("Training" = "blue", "Validation" = "red")) +
theme_minimal()
model_2_3 <- keras_model_sequential() %>%
layer_dense(units = 32, input_shape = c(7), activation = 'tanh') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 16, activation = 'tanh') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 8, activation = 'tanh') %>%
layer_dense(units = 2, activation = 'softmax')
model_2_3 %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
summary(model_2_3)
history_2_3 <- model_2_3 %>% fit(
x_train, yc_train,
epochs = 50,
batch_size = 128,
validation_data = list(x_test, yc_test)
)
plot(history_2_3)
# Predict probabilities and convert to class labels - 2.3
predictions_proba_2_3 <- model_2_3 %>% predict(x_test)
predictions_2_3 <- k_argmax(predictions_proba_2_3) %>% as.array()
# Create and print the confusion matrix
conf_matrix_2_3 <- table(Predicted = predictions_2_3, Actual = y_test)
print(conf_matrix_2_3)
model_3 <- keras_model_sequential()
# Add layers
model_3 %>%
layer_dense(units = 24, activation = 'tanh', input_shape = c(15)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 12, activation = 'tanh') %>%
layer_dense(units = 6, activation = 'softmax')
model_3
rm(list=ls())
98 / 12
71/12
8*12
8*12 +7
rep(1,6)
df <- data.frame(y=c(1,2,3,4,5,6),x=c(1,0.5,0.25,1,0.5,0.25),A=c(1,1,1,2,2,2))
df$A <- as.factor()
df$A <- as.factor(df$A)
str(df)
summary(lm(y ~ x + A, data=df))
?reference()
?as.reference()
?as.factor()
?relevel()
df$A <- relevel(as.factor(df$A),level=2)
df$A <- relevel(as.factor(df$A),ref=2)
str(df$A)
str(df)
df <- data.frame(y=c(1,2,3,4,5,6),x=c(1,0.5,0.25,1,0.5,0.25),A=c(1,1,1,2,2,2))
df$A <- relevel(as.factor(df$A),ref=2)
summary(lm(y ~ x + A, data=df))
df2 <- data.frame(y=c(1,2,3,4,5,6),
x=c(1,0.5,0.25,1,0.5,0.25),
A1=c(1,1,1,0,0,0))
summary(lm(y ~ x + A1, data=df2))
lm(y ~ x + A, data=df)$coefficienbts
lm(y ~ x + A, data=df)$coefficients
summary((lm(y ~ x + A, data=df))$coefficients
)
(summary(lm(y ~ x + A, data=df))$coefficients
)
summary(lm(y ~ x + A, data=df)$coefficients
)
coef(summary(lm(y ~ x + A, data=df))
)
coef(lm(y ~ x + A, data=df))
coef(summary(lm(y ~ x + A, data=df)))
df$A <- relevel(as.factor(df$A),ref=1)
coef(summary(lm(y ~ x + A, data=df)))
df <- data.frame(y=c(1,2,3,4,5,6),
x=c(1,0.5,0.25,1,0.5,0.25),
A=c(1,1,1,2,2,2))
df$A <- relevel(as.factor(df$A),ref=2)
coef(summary(lm(y ~ x + A, data=df)))
df <- data.frame(y=c(1,2,3,4,5,6),
x=c(1,0.5,0.25,1,0.5,0.25),
A=c(1,1,1,2,2,2))
df$A <- relevel(as.factor(df$A),ref=1)
coef(summary(lm(y ~ x + A, data=df)))
df <- data.frame(y=c(1,2,3,4,5,6),
x=c(1,0.5,0.25,1,0.5,0.25),
A=c(1,1,1,2,2,2))
df$A <- relevel(as.factor(df$A),ref=2)
coef(summary(lm(y ~ x + A, data=df)))
df <- data.frame(y=c(1,2,3,4,5,6),
x=c(1,0.5,0.25,1,0.5,0.25),
A=c(1,1,1,2,2,2))
df$A <- relevel(as.factor(df$A),ref=2)
coef(summary(lm(y ~ x + A, data=df)))
coef(summary(lm(y ~ x + relevel(A,ref=1), data=df)))
df$A <- relevel(as.factor(df$A),ref=1)
coef(summary(lm(y ~ x + relevel(A,ref=1), data=df)))
df <- data.frame(y=c(1,2,3,4,5,6),
x=c(1,0.5,0.25,1,0.5,0.25),
A=c(1,1,1,2,2,2))
df$A <- relevel(as.factor(df$A),ref=1)
coef(summary(lm(y ~ x + relevel(A,ref=1), data=df)))
coef(summary(lm(y ~ x + A, data=df)))
reticulate::repl_python()
?rnorm
set.seed(45)
xn <- rnorm(1000,mean=2,sd=1.5)
F_xn <- pnorm(xn)
hist(F_xn)
hist(x_n)
hist(xn)
F_xn <- pnorm(xn,mean=2,sd=1.5)
hist(xn)
hist(F_xn)
?runif
?qnorm
norm_rv <- qnorm(runif(1000),mean=2,sd=1.5)
hist(norm_rv)
reticulate::repl_python()
