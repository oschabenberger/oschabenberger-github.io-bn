# History and Evolution of Data Science {#sec-history}

## What is Data Science?

It is revealing that there is no single agreed-upon definition of data science. 
The lack of a clear definition is understandable if one considers that,

-   data science is a young discipline, its boundaries—what is in and what is out—are still evolving.

-   data science is a cross-functional discipline that combines domain expertise 
with foundations in statistics, mathematics, and computer science. Views of data 
science differ based on how these components are weighted.

-   data science is defined by what data scientists do. That encompasses everything 
from finding data, handling data, processing data at scale, to applying statistics 
and machine learning, to writing code and implementing algorithms to interpreting, 
visualizing, and communicating data and results.

We can begin to develop an understanding of data science by considering the three
foundational disciplines: statistics, mathematics, and computer science. What problems 
associated with data required their combination and could not be solved by the 
discipline in isolation? Maybe we should put the question slightly differently: 
What problems are we able to solve at the intersection of these disciplines?

First, data is information collected about the real world. Data science then is 
concerned with real-world problems and phenomena. We test hypotheses about 
real-world problems, but we do not solve hypothetical problems or prove theorems. 
Typical questions addressed by data scientists based on data are:

-   **Description**: What is and what has been?
-   **Prediction**: What will be?
-   **Classification**: What category does this item belong to?
-   **Hypothesis Testing**: What can I say about X?
-   **Prescription**: What should I do?
-   **Clustering**: Which things are similar?
-   **Association**: Which things occur together?
-   **Optimization**: What is the best way to do something?
-   **Generation**: What novel content is there?

**Statistics** is concerned with describing the world and with drawing conclusions 
about the world using incomplete information. The information—data—is incomplete 
for various reasons. We might have drawn a sample from a population rather than 
observed every entity. We might have assigned treatments at random to subjects 
in an experiment in order to balance out the influence of variables not controlled 
in the experiment. We have incomplete information about the input variables that 
relate to an output variable. We have incomplete information about the true 
relationship between inputs and outputs.

**Computer Science** is concerned with computation, information, and automation. 
Key to computer science are data structures—efficient ways to store and process 
information—and algorithms—specifications to solve a specific problem through 
computation. Because today data is almost always stored in digital form it is now 
immediately accessible to processing via computation. A remarkable contribution 
of computer science to data science over the last decades is to broaden its view 
from methods and algorithms to organize and store data to methods and algorithms 
that draw conclusions from the data through computation. Computer science has 
discovered data as a source of learning, not just as a medium of storage and 
processing.

The algorithmic approach to understanding data rather than a probabilistic approach 
is one of the great contributions of computer science to data science. Another 
major contribution is software engineering and software development. Data science 
projects are software projects, they involve the use of software tools and create 
code written in languages such as Python, SQL, R, Julia, Scala, Java, JavaScript, 
and others.

**Mathematics** is concerned with the study of numbers, formulas (algebra), 
shapes and structures (geometry), and patterns (analysis). Statistics is often 
considered a form of applied mathematics. Relationships between inputs and outputs 
in data science are often modeled through continuous functions, their properties 
are studied through linear algebra, and they are related to data through differentiation, 
integration, and numerical analysis.

:::{.definition}
::::{.definition-header}
Definition: Data Science
::::
::::{.definition-container}
At the intersection of the foundation disciplines, performing data science means 
drawing conclusions from data about real-world problems using computation and 
automation in the presence of uncertainty.
::::
:::

Rather than thinking of data science as a new domain of knowledge, we think of it 
as a set of skills that are applied to a subject matter domain (=area of expertise). 
This view is informed less by a scientific discipline around data than the 
recognition that today all subject matter domains, including the sciences, are 
using data to answer questions.

---

Which skills are most important? Is it the hard technical skills in statistics, 
machine learning, software development? Or is it the ability to communicate across 
organizational functions in a team-based environment? Or is it the ability to 
understand and analyze real-world problems in a specific domain? A data scientist
will eventually need to acquire all those skills, but they are not interchangeable. 
Subject matter skills can be acquired by working in a particular domain. Working 
on data science problems in financial services, for example, you will pick up the 
specifics and idiosyncrasies of credit and debit card transactions, anti-money 
laundering, electronic payments, and so on.

Domain-specific knowledge takes the least time to learn to contribute to solving 
data science problems. Learning the statistical and mathematical foundations and 
how to develop good software is the more difficult task. When you join an organization 
as a data scientist, you will be surrounded by people who understand the domain 
inside and out---it is what they do. You, however, might be the only person who 
understands how to apply machine learning to forecast data and who knows how to 
write analytical software that works.

## The Sexiest Job and John W. Tukey

In 2009, Hal Varian, Chief Economist at Google declared that the sexy job in the 
next ten years would be statisticians. As a statistician, I agreed of course. In 
2012, Thomas Davenport and DJ Patil published “Data Scientist: Sexiest Job of the 
21^st^ Century” in Harvard Business Review [@DavenportPatil_2012]. These articles 
were not describing the work of the typical statistician, but a more general 
approach to extracting information from large data sets and presenting the insights 
to others. A new kind of profession was emerging in response to a greater abundance 
of data in the world that did not fit neatly into existing categories like 
mathematician, statistician, business analyst, or “quant”. Varian said,

> *The ability to take data—to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it—that’s going to be a hugely important skill in the next decades…*

The approach seemed fresh in placing the data at the center of the investigation 
rather than the traditional approach of choosing the problem first and collecting 
data second to answer the problem. It emphasized learning from data through 
visualization and communicating about data---data literacy. What can we learn 
from the data we have, what do the data tell us? Alas, this shift in focus was 
not new, it was first proposed by a famous statistician, John W. Tukey.

:::{.callout-tip title="Data and Datum" collapse="true"}
You might have noticed that data is used in the plural form, "data are...". This 
is grammatically correct since data is the plural form of datum. It is common to 
use data as a singular noun, "data is...". I might fall into that trap every now 
and then but prefer to use the plural form. After all, using data as singular 
would be wasting a perfectly good noun: datum.
:::


Tukey is known to statisticians as the founder of **exploratory** data analysis (EDA), 
as compared to **confirmatory** data analysis. His famous book “Exploratory Data 
Analysis” [@TukeyEDA] established the idea that much can be learned by using the 
data itself and that data can suggest hypotheses to test. In confirmatory analysis, 
on the other hand, you start with a hypothesis and then collect data to verify 
whether the hypothesis might be true.

Much of the statistical work leading up to this point had been confirmatory, based 
on the concept that data are the realization of a data-generating mechanism—usually 
a random process. Hypotheses are tested by capturing this mechanism in a statistical 
model, deriving estimates for the parameters of the model. Once the statistical 
model is accepted as the abstraction of the data-generating mechanism it becomes 
the lens through which the problem of interest is viewed. The model is applied 
to test hypotheses and to calculate predictions along with measures of uncertainty.

In 1962, John W. Tukey published a pre-cursor to EDA, “[The Future of Data Analysis](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-33/issue-1/The-Future-of-Data-Analysis/10.1214/aoms/1177704711.full)”, in which he laid the foundation of modern 
data science, he called it data analysis, and argued why this is a scientific 
discipline [@TukeyDA].

> *For a long time I have thought I was a statistician, interested in the inference from the particular to the general. But as I have watched mathematical statistics evolve, I have had cause to wonder and to doubt.*
>
> *All in all, I have come to feel that my central interest is in data analysis, which I take to include, among other things: procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise and more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.*

Tukey asked that we give up familiar constraints and look for stories and meaning 
in data sets. Data analysis can precede probability models; for example, one can 
identify an interesting function of the data and make progress by asking what the 
function might reasonably be estimating.

This was a liberating view that contrasted against the rigor of probability models 
and the search for optimal estimators in favor of putting the data first, accepting 
“good enough”, giving advice when there is reasonable evidence for the advice to 
be sound, and being prepared that in a reasonable fraction of cases that advice 
will be wrong.

> *Data analysis must progress by approximate answers, at best, since its knowledge of what the problem really is will at best be approximate. Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.*

## Data Mining and Data Science

Data mining is the process of searching large data sets for trends and patterns 
using computers and automation. We can see a direct link to data analysis in the 
sense of Tukey, working with a data set to extract interesting patterns in data, 
to discover new information in the data, to classify observations, identify 
associations, detect outliers or anomalies, and to identify groups of similar 
observations (clustering).

With the dramatic growth of data sets in rows and columns, it became impossible 
to conduct this type of exploratory discovery (in the sense of Tukey) manually. 
There was also growing interest in using special tools and algorithms to discover 
hidden nuggets of information and relationships in data sets that would otherwise 
go undetected. Data mining relies on computing power and automation to find 
patterns in data at scale.

The overall process of discovering knowledge from data was formalized as Knowledge 
Discovery in Databases ([KDD](https://www.techopedia.com/definition/25827/knowledge-discovery-in-databases-kdd)). 
Data mining is the part of the KDD process that uses algorithms to discover patterns.

This approach is not without problems or critics. The statistical viewpoint likened 
the approach to “[data dredging](https://en.wikipedia.org/wiki/Data_dredging)” or 
“data fishing”: looking for relationships even if they are meaningless and then 
forming hypotheses about why the relationships exist. For example, associations 
and correlations can be spurious, caused by latent or mediating variables. A strong 
correlation is not evidence of a causal relationship and might lead to the formulation 
of bad hypotheses or poor decisions.

::: example
::: example-header
Example: Chocolate Consumption and Nobel Laureates
:::

::: example-container
@Messerli_2012 published in 2012 in the New England Journal of Medicine an article 
that relates the chocolate consumption per capita to the number of Nobel prize 
winners in various countries, a highly significant statistical relationship that 
explains almost 2/3 of the country-to-country variation in Nobel laureates (@fig-proj-chocolate).

```{r, fig.align="center"}
#| label: fig-proj-chocolate
#| echo: false
#| fig-cap: |
#|   Relationship between chocolate consumption and number of Nobel laureates from @Messerli_2012.
#| fig-alt: |
#|   A diagram displaying the relationship between chocolate consumption and
#|   number of Nobel laureates in countries.
#| lightbox:
#| out.width: 75%

knitr::include_graphics("images/ChocolateNobel.jpg")
```

If the relationship were causal—which it is not—an increase of 0.4 kg per year per 
capita would produce one additional Nobel laureate. In the U.S. that amounts to 
an extra 125 million kg of chocolate per year. If the relationship between chocolate 
consumption and number of Nobel laureates is not causal, how can we then explain 
the obvious relationship seen in the figure? Could it be explained by chocolate 
consumption improving cognition which creates a fertile ground from which Nobel 
laureates sprout? A look at how the data was collected sheds light and casts 
doubt on the study: only four years of chocolate consumption were considered on a l
imited number of chocolate products and no data prior to 2002 was used. The number 
of laureates is a cumulative measure that spans a much longer time frame.

It appears that the data were organized in such a way as to suggest a relationship 
between the variables.
:::
:::

Another criticism of the data mining approach is that traditional statistical 
decisioning based on $p$-values is not adequate. Many statistical tests are overpowered 
by very large data sizes, making even small differences statistically significant. 
The repeated application of statistical tests across hundreds or thousands of 
variables leads to inflated error rates unless multiplicity adjustments are made.

::: example
::: example-header
Example: Market Basket Analysis
:::

::: example-container
Market basket analysis uses Association Rule Mining (ARM) to find associations 
between items that occur in databases. An application is to identify items that 
are purchased together, for example, customers who buy whole milk might be more 
likely to also purchase yogurt and cereals as compared to a completely random 
choice of products. 

The name market basket analysis stems from this application, 
but ARM has many other use cases. 
Association rules can be used in diagnosing medical conditions based on co-occurrence 
of symptoms, in text analytics to extract meaning of documents based on the 
co-occurrence of words, in survey analysis to find associations between answers 
to different questions. 

A rule is represented as a logical $\{A\}\Rightarrow\{B\}$, 
where $\{A\}$ is a set of items called the antecedent (head) of the rule and $\{B\}$
is a set of items called the consequent (body) of the rule. ARM discovers rules 
such as $\{\text{whole milk}\}\Rightarrow\{\text{yogurt, cereals}\}$ and arranges 
them by measures of rule quality such as support, confidence, and lift. 

The **support** of an item set is the frequency with which its items appear, the 
**confidence** is a measure of predictability of the rule, and the **lift** measures 
how much more likely the item set appears compared to a random allocation of items. 

When applied to large databases the number of possible association rules is astronomical. 
Suppose there are 500 items in a store. There are more than 62 million rules with 
just two items in the antecedent and a single item in the consequent. If all 
associations between items are completely random, at a 1% Type-I error rate we 
would declare over 600,000 associations as “significant”.
:::
:::

This criticism is valid, of course, if the results of data mining are used in a 
confirmatory fashion. But when one puts data first, not a probability model that 
might have created the data, then data mining techniques are incredibly useful 
and necessary to help us learn about data, to formulate hypotheses, and to plot 
the path of inquiry—rather than to confirm a result. Data mining is part of the 
data science methodology and not a separate discipline. The Data Science Process 
Alliance, concerned with project management in data science, uses the cross industry 
standard process for data mining (CRISP-DM) as the foundation for the data science 
process—data mining and data science today are intertwined.

## 2001: An Odyssey

Two influential papers appeared in 2001 that paved the way from statistics to data science.

In “Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics”, 
William S. Cleveland makes the case to enlarge the major focus areas in statistics 
to a field called data science [@Cleveland2001]. The new discipline places more 
emphasis on

-   **Multidisciplinary projects**. Collaboration with domain experts is a source of innovation. Data is the engine for invention in data science.
-   **Model building**. Effort should increase toward methods for building models compared to formal, mathematical-statistical inference in models.
-   **Computational methods** and computing with data that includes databases, networks, and analytical software.
-   **Communication and pedagogy**.

In “[Statistical Modeling: The Two Cultures](https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.full)”, Leo Breiman contrasts **statistical 
(data) modeling** and **algorithmic modeling** [@BreimanTwoCultures]. The former assumes 
that the data are generated by a stochastic data model. According to Breiman, 98% 
of statisticians subscribe to this approach. Algorithmic modeling, on the other 
hand, makes no assumption about the underlying data model, treats the data mechanism 
as unknown, and is more common in fields outside of statistics. In Breiman’s words

> *Perhaps the damaging consequence of the insistence on data models is that statisticians have ruled themselves out of some of the most interesting and challenging statistical problems that have arisen out of the rapidly increasing ability of computers to store and manipulate data. These problems are increasingly present in many fields, both scientific and commercial, and solutions are being found by nonstatisticians*.

The goal of algorithmic models is more predictive accuracy than confirmatory 
inference and hypothesis testing. The model is supposed to approximate an unknown 
relationship between inputs and outputs well enough to provide satisfactory 
accuracy in predicting outputs of previously unseen inputs. Neural networks and 
decision trees are examples of algorithmic tools that found rapid adoption outside 
of statistics. Machine learning as it emerged from computer science is a manifestation 
of algorithmic modeling.

In data modeling, theory focuses on the probabilistic properties of the model and 
of quantities derived from it. In algorithmic modeling, the focus is on the 
properties of the algorithm itself: starting values, optimization, convergence 
behavior, parallelization, hyperparameter tuning, and so on.

Breiman’s article was widely discussed—the invited comments by leading statisticians 
at the end of the paper give a sample of opinions.

Meanwhile, computer scientists had realized that data is not just an abstract 
concept, that data is more than information to be structured, stored, secured, 
and transmitted. They realized that data had intrinsic value; processing data can 
derive insight from data. That knowledge filled a void left by statisticians with 
limited knowledge of computing environments. Computer scientists, on the other 
hand, had limited knowledge about how to approach the analysis of data. The fields 
were ripe for a merger.

## The Big Data Era

An early use of the term Big Data was in a 1997 paper by Michael Cox and David Ellsworth 
in the Proceedings of the IEEE 8^th^ Conference on Visualization [@CoxEllsworth]. 
The authors noted that data set sizes have grown bigger to the point that they do 
not fit in main memory and termed the problem *big data*.

Did we really need another term to describe increasing data size? Whether data 
fits into main memory of a computer depends on the size of the data and the size 
of the memory. And since the invention of digital computers both have constantly 
increased. There had to be more to it than just an increase in the volume of data.

Indeed, in the 2000s a new class of applications and uses of data emerged as 
several developments in data and computing coalesced:

-   More and more data were now captured in digital form, the amount of data generated in digital form increased sharply.
-   A growing interest in the analysis of rich data types such as text, documents, audio, and video.
-   A continuum of data in motion; from data at rest in the cloud or data center to streaming data.
-   Large-scale data collection enabled through the internet.
-   A shorter shelf life of new kinds of data such as behavioral data (online shopping, social media activity, web browsing, behavioral data) compared to more stable demographic data (age, ZIP code).
-   The beginnings of data-driven application where the data defines how the system operates.
-   Greater automation of data-processing.

The Big Data phenomenon was never just about the size of the data alone, or even 
the “3Vs”, volume, velocity, and variety. Big Data was about doing something different 
with data than had been done before, using different data types, different model 
types, different algorithms, and different computing environments. Building a 
recommendation system at Netflix or Amazon from data on millions of customers and 
items viewed or bought is an example of this new type of applications.

Big Data [was]{.underline} about reaching the limits of the ability to store, access, 
and process data. The volume of data can be a limiting factor, of course and call 
for specialized approaches. Traditional statistical modeling with hypothesis testing 
might be useful when deciding about 20 possible input variables. What happens when 
there are 30,000 potential input variables? Computing $p$-values in a random sample 
of size 100 makes sense, but when the data set contains 10 million observations 
the statistical test is so powerful that nearly any hypothesis can be rejected. 
What should be used instead? How do you engage multiple computers to solve massive 
analytic problems in parallel?

If the definition of Big Data is “whatever does not fit on a single machine” and 
needs some form of distributed computing, then the frontier is continuously receding. 
Today (2023), a storage-optimized instance in the Amazon Elastic Compute Cloud 
(AWS EC2, i4g.16xlarge), features 64 cores (vCPUs) and 512 GB of memory. AWS launched 
EC2 in 2006 with single-core machines that featured 2 GB of RAM. That is an 
increase of more than two orders of magnitude. And, in case that is not powerful 
enough, p5.48xlarge instances feature 192 cores and 2 TB of memory. A lot of 
Big Data work can be done on a single machine today.

Jordan Tigani, CEO and founder of database company MotherDuck and founding engineer 
of Google BigQuery has captured this development in the blog “[Big Data Is Dead](https://motherduck.com/blog/big-data-is-dead/)” [@Tigani_BigData]:

> *The world in 2023 looks different from when the Big Data alarm bells started going off. The data cataclysm that had been predicted hasn’t come to pass. Data sizes may have gotten marginally larger, but hardware has gotten bigger at an even faster rate. Vendors are still pushing their ability to scale, but practitioners are starting to wonder how any of that relates to their real world problems.\
\
Of course, just because the amount of data being generated is increasing doesn’t mean that it becomes a problem for everyone; data is not distributed equally. Most applications do not need to process massive amounts of data. This has led to a resurgence in data management systems with traditional architectures.*

The amount of data an organization stores is typically much greater than the amount 
of data being analyzed: because more recent data is more important, because data 
are being processed for analytics, and so on.

Whatever we think of the term Big Data, the era has contributed to the rise of 
data science as a discipline. New data-driven applications with new types of data 
challenged us to approach data analytics in a new way. Algorithmic approaches 
that put the problem first (I need to predict customer churn) are winning in this 
environment over data modeling approaches that first build an abstraction of the
data mechanism based on probability principles.

## Data Science Today

The people who identify as data scientists often have unusual career paths; they 
do not necessarily come to data science through one of the foundational disciplines. 
They turn out to be problem solvers with an acumen for technology, programming, 
and communication.

Big Data created an epic wave and data scientists are the people who know how to 
surf. Thanks to the Big Data wave, tools and techniques to process and analyze 
large and complex datasets are now standard in data science. Knowledge in data 
storage, parallel computing, distributed systems, scalable data processing frameworks, 
and cloud computing is part and parcel of data science.

Ten years after they published “Data Scientist: Sexiest Job of the 21^st^ Century”, 
Thomas Davenport and DJ Patil revisited in a [2022 Harvard Business Review article](https://hbr.org/2022/07/is-data-scientist-still-the-sexiest-job-of-the-21st-century) 
whether the assessment still holds [@DavenportPatil_2022]. They conclude that over 
the last 10 years there has been continuity in the developments but also important changes.

The job of the data scientist continues to gain importance as the amount of data, 
analytics, and AI in business, government, and society is bound to only increase. 
A decade ago, the data scientist was a *unicorn* who combined rare skills in 
statistics, machine learning, and coding to wrangle information from large datas ets. 
Thanks to many graduate and undergraduate programs, online courses and certifications, 
there is today a more structured approach to acquire data science skills. While 
these skills continue to range widely, there is also more differentiation of the 
data scientist role against other professions. In response to specialization and 
the need to fill a gap in managing data in the data science project lifecycle, AI 
engineers, ML engineers, and data engineers are on the rise. In the words of 
Davenport and Patil:

> *We expect to see continued differentiation of responsibilities and roles that all once fell under the data scientist category. Companies will need detailed skill classification and certification processes for these diverse jobs, and must ensure that all of the needed roles are present on large-scale data science projects. Professional data scientists themselves will focus on algorithmic innovation, but will also need to be responsible for ensuring that amateurs don’t get in over their heads. Most importantly, data scientists must contribute towards appropriate collection of data, responsible analysis, fully-deployed models, and successful business outcomes.*

Among the important changes in technology in the past decade the authors list

-   Cloud computing and cloud data storage (data lake, data warehouse, lakehouse)
-   Auto machine learning and citizen data science
-   Large language models
-   Model operations
-   Ethics of data science

Twenty years after the arrival of Big Data, data science continues to evade a precise 
definition because it is defined by those who practice it, rather than by a list of 
activities. Hacker’s art, statistics, programming, visualization, modeling, 
subject-matter experience, and communication to solve problems with data through 
computation are key ingredients. Data science will continue to evolve as its 
ingredients change and with the emergence of new methods and tools. Most recently, 
since late 2022, large language models based on transformer technology created a 
sea change in natural language understanding that is believed to disrupt many 
occupations. Data scientists embrace such disruptions to draw better conclusions 
from data.

## Data Science versus Data in Science

The majority of data science projects and applications take place in non-academic 
settings, often lumped together under the generic term *industry*. Most data 
science jobs are thus found in industry.

There is a difference between data science in research settings and commercial settings. 
The boundaries typically align with academia and industry, but it is not unusual 
for commercial enterprises to have Research & Development divisions that engage 
in scientific research in support of the enterprise’s mission and products.

Rather than pitting industry against academia, consider how data science projects 
differ between research and commercial settings. There is a lot of hype about 
how organizations become more data driven, replacing decisions made on gut feeling
with hard evidence based on data---the data science team is at the heart of that 
digital transformation, disrupting the old ways of doing business with novel 
approaches based on machine learning and AI. In most situations, it is just that: hype.

In research and commercial settings, the goal is to provide value through using 
data. How that value is measured is fundamentally different. Research adds value 
by increasing our body of knowledge. Commercial enterprises are interested in 
increasing the value of the company.

Most data science jobs in commercial settings exist not to disrupt the business
model but to support an existing business by solving the current data-related 
problems. The problems tend to be narrow and specific with result metrics that 
are tied to the objectives of the enterprise. Objectives such as “increase revenue”, 
“retain customers”, “increase productivity”, “reduce cost” are universal and boring. 
Because questions are not open ended and the time to find a solution is of the 
essence, there is a tendency to reuse rather than to develop new approaches and/or 
algorithms. Solutions have merit if they meet the goals and can be implemented; 
they must make a difference in the practice of the business.

In research settings, on the other hand, questions are more open-ended without a 
clear solution path. The goal of research is to find answers to new questions. 
Time is allotted to deep thinking and to uncovering mechanisms. This often involves 
novel approaches, custom algorithms implemented by the research team in specialized 
software. Solutions are developed only as necessary to prove the result of the 
research. They do not have to be implemented or practical. A research team might 
find a better active ingredient to fight disease and leave the task of productizing 
a drug to someone else.

Research projects have carefully designed protocols about data, what data to use, 
how to measure attributes, how to sample observations. Experiments are designed 
to collect data under controlled conditions in order to draw cause-and-effect 
conclusions.

In commercial settings you are more likely to work with the data that is collected 
as a by-product of the normal business operations. True experimentation with data 
collected under controlled conditions is rare, sometimes confined to A/B testing 
of one data product against another. It is thus common in commercial situations 
to be working with data collected for a different purpose. The data in the enterprise 
CRM (customer relationship management) system was collected to track interactions 
with customers, the data in the Salesforce instance tracks revenue opportunities, 
and ServiceNow records IT assets and tickets. Is that the best data to optimize 
digital marketing campaigns? Weak correlations, low signal-to-noise ratios, poor 
data quality are common problems.

The goal of this section is not to put a damper on working as a data scientist in 
commercial enterprises. It is a good idea to ask what role data and software play 
in the organization. If the core business and the core product is built on data 
and software, then it is likely that data science contributes directly to the mission 
and the bottom line of the company—it is essential to the business. In many organizations 
the core business is not built on data and technology—at least not yet. A bank’s 
core business is to deposit and lend money. A retailer’s core business is to sell 
goods. An insurance’s core business is risk assessment and underwriting. Data science 
supports the core functions through traditional data functions such as reporting, 
data analysis, forecasting—it is important but not essential. Increasingly, however, 
these core functions are becoming the focus of data science itself providing a fertile
ground for interesting and novel approaches: micro-lending (banks), recommendation 
systems (retailers), reinforcement learning for underwriting (insurance). At this 
point data science helps transform a traditional business model as its products 
become essential to operating the business.

## A Case Study in Data Science History: The 1854 Cholera Outbreak in Soho, London {#sec-history-cholera}

### Two Snows

In 1854, a severe outbreak of cholera occurred near Broad Street in Soho, London,
killing over 600 people. The outbreak was studied by John Snow, considered one 
of the founders of modern epidemiology. No, not the Jon Snow you might be thinking 
of, Lord Commander of the Night Watch (@fig-jon-snow), but the John Snow in
@fig-john-snow.

::: {#fig-the-snows layout-ncol=2}

![John Snow. Source: [Wikipedia](https://en.wikipedia.org/wiki/John_Snow)](images/John_Snow.jpg){#fig-john-snow fig-align="center"}

![Jon Snow. Source: [Wikipedia](https://en.wikipedia.org/wiki/Jon_Snow_(character))](images/Jon_Snow.png){#fig-jon-snow fig-align="center"}

Famous John/Jon Snows
:::

### Visualization {#sec-cholera-viz}

@fig-cholera-map shows the map drawn by John Snow, recording the number of
cholera cases with stacked bars at the location where cholera cases occurred (click on the 
map to zoom in). Also shown on the map as black circles and annotated as "PUMP" 
are the public water pumps throughout the city. The high number of cholera cases 
on Broad Street stands out, and they seem to be clustered near the location of 
the Broad Street Pump [@Snow1855, p.46].


![John Snow's original cholera map from Soho, London in 1854.](images/Snow_cholera_map_1.jpg){#fig-cholera-map fig-align="center" width=90% .lightbox}

Cholera had been a major problem in the city, thousands had died during previous 
outbreaks. The prevailing theories of the cause of cholera were (i), airborne particles,
called *miasma* that rose from decomposing organic material and (ii), an as of
yet unidentified germ. According to the miasma theory, cholera is contracted by
breathing bad air. John Snow adhered to the germ theory and believed that it
was transmitted through contaminated water.

### Data Validation {#sec-cholera-data}

Talking to residents, Snow identified the public water pump on Broad Street to 
be the source of the outbreak. He failed to identify *the germ* under the microscope
but came to the conclusion based on the pattern in the data and conversations
with residents. Investigating on the ground, he found that nearly all deaths 
were in the vicinity of the Broad Street Pump or by people who had consumed water 
from the pump [@Snow1855, p.47]:

>*It will be observed that the deaths either very much diminished, or ceased altogether, at every point where it becomes decidedly nearer to send to another pump than to the one in Broad street. It may also be noticed that the deaths are most numerous near to the pump where the water could be more readily obtained.*

He persuaded local authorities to remove the handle from the pump to prevent access
to the water. The mortality rate declined after that, but it is believed that the
outbreak was already in decline as people had fled the area. 

:::{.callout-note title="Removing the handle"}
"Removing the handle" is now a term in epidemiology for the removal of a harmful 
agent from the environment. When epidemiologists look for simple answers to
questions about epidemics, they ask "Where is the handle to this pump?" [@CompThinking].
:::

There were some data points (outliers?) that did not agree with the hypothesis that
proximity to the Broad Street pump resulted in more cholera incidences. At the
intersection of Broad Street and New Street was the Lion Brewery; there were no
cholera cases at the brewery although it used water from the Broad Street pump. It turns
out that the workers there were protected from cholera by virtue of a daily
beer allowance. The cholera bacteria is killed in the brewing process making the
beer safe to drink. Drinking beer instead of the contaminated water saved the 
workers from cholera. What appears as an outlier to the model actually reinforces
it.

@CompThinking discuss other data points that appeared initially as anomalies and
ended up implicating the public pump on Broad Street:

- There were deaths in houses closer to the Rupert Street pump than the Broad Street pump. 
It was more convenient for those residents to use the Broad Street pump due to 
the street layout.

- Deaths in houses several blocks away from the Broad Street pump were linked to
children who drank from the Broad Street pump on their way to school.

- John Snow was initially puzzled by two isolated deaths in the Hampstead area, 
far from Soho. He learned that the deceased had once lived in the Broad Street area. 
Because they liked the taste, they had water from the Broad Street pump delivered
to Hampstead every day. 

The source of the contamination was ultimately determined to be a leak from a
nearby cesspit into the well from which the Broad Street pump drew water. 
Clothing from a cholera patient had been discarded into the pit and leakage from 
the pit had contaminated the well.

### Toward Causality {#sec-cholera-deeper}

The evidence that the contaminated well water at the Broad Street pump caused
the high rate of cholera in that neighborhood and among those who consumed the
water is strong. Is it conclusive, however? Have we ruled out any other explanation
*beyond a reasonable doubt*? 

There could be other explanations for the higher cholera incidence rate in the
Broad Street neighborhood compared to other areas of London. Maybe the diet is
different among the residents of that poorer area. Maybe their occupations expose
them to harmful agents at work. Maybe there is something different in the way
their houses were constructed. 

While we know today that the bacterium *Vibrio cholerae* causes cholera, that 
discovery was not made until 1883 and John Snow had failed to identify a "germ"
when he studied the Broad Street pump water. The prevailing miasma theory of
infection from airborne particles also did not support Snow's findings. While
his data, visualization, and analysis showed a strong association between 
cholera and proximity to the Broad Street pump, a deeper analysis was necessary
to convince his contemporaries.

To establish **cause and effect** and prove that a variable causes an outcome, 
modern science would design and run an experiment, provided it is ethically 
and technically possible. In such an experiment one would control for all other
factors except the one hypothesized to cause the outcome. One method of controlling
these **confounding** factors is by randomly assigning the conditions of interest
to people and to observe what happens. Exposing folks deliberately to contaminated 
water that could harm or even kill them is not justified. Fortunately, John Snow
found a real-life experiment with perfect conditions to establish cause and 
effect between cholera and water contamination.

He studied the cholera incidences among recipients of water from two water supply
companies. The Lambeth company used water from the River Thames drawn upriver from 
sewage discharge and the Southwark \& Vauxhall company drew water below the 
discharge. Snow also established that for all intents and purposes the households
receiving water from either company were indistinguishable; in statistical terms
they were comparable. The only thing that differentiated the two groups was the
water supplier. @Snow1855 [p. 75] wrote 

>*In many cases a single house has a supply different from that on either side. Each company supplies both rich and poor, both large houses and small; there is no difference in the condition or occupation of the persons receiving the water of the different companies...\
\
As there is no difference whatever either in the houses or the people receiving the supply of the two Water Companies, or in any of the physical conditions with which they are surrounded, it is obvious that no experiment could have been devised which would more thoroughly test the effect of water supply on the progress of Cholera than this, which circumstances placed ready made before the observer.*

@tbl-chol-rate is based on @Snow1855 [p. 80] and covers the period from 
January 1 to December 12, 1853 leading up to the 1854 outbreak. The cholera death 
rate on a per 10,000 house basis is almost 14 times higher in households supplied by 
the Southwark \& Vauxhall water company compared to those who received their water 
from the Lambeth company.

| Water Supplier | No. of Houses | Cholera Deaths | Deaths per 10,000 Houses |
|:---:|:---:|:---:|:---:|
| Southwark \& Vauxhall | 40,046 | 286 | 71 |
| Lambeth | 26,107 | 14 | 5 |

: Cholera incidences and rates for two water supply companies leading up to the 1854 outbreak. {#tbl-chol-rate .striped}

In all of London there were 563 deaths from cholera in the same period. In other
words, 50\% of the deaths took place among customers of the Southwark \& Vauxhall
company. Ouch.

For the first seven week period of the 1854 outbreak, @Snow1855 [p. 86] recorded
the death rates in @tbl-chol-rate2.

| Water Supplier | No. of Houses | Cholera Deaths | Deaths per 10,000 Houses |
|:---:|:---:|:---:|:---:|
| Southwark \& Vauxhall | 40,046 | 1,263 | 315 |
| Lambeth | 26,107 | 98 | 37 |
| Rest of London | 256,423 | 1,422 | $55^*$ |

: Cholera incidences and rates during the first seven weeks of the outbreak. The death rate in the rest of London was reported as 59 in Table IX of @Snow1855, but calculates to 55 deaths per 10,000. {#tbl-chol-rate2 .striped}

In statistical terms, such a difference in the death rates is **highly significant**,
meaning that if there is no difference in the water quality between the suppliers,
such a discrepancy would virtually never happen. The only reasonable explanation
for the higher death rate, since differences between the groups receiving the 
water have been ruled out, is the quality (contamination) of the Southwark \&
Vauxhall water.

Case closed!

