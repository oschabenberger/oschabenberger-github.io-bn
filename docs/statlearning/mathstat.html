<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.552">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Foundations of Data Science - 13&nbsp; Probability and Statistics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../statlearning/linalg.html" rel="next">
<link href="../statlearning/general.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../statlearning/general.html">Part III. Statistical Learning</a></li><li class="breadcrumb-item"><a href="../statlearning/mathstat.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Probability and Statistics</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Foundations of Data Science</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Part I. Data Science and Data Science Projects</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../proj/history.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">History and Evolution of Data Science</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../proj/lifecycle.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Data Science Project Lifecycle</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../proj/teams.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Data Science Teams</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Part II. Data Preparation and Understanding</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../data/sources_and_files.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Sources and File Formats</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../data/data_access.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Access</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../data/quality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Data Quality</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../data/summarization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Data Summarization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../data/visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Data Visualization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../data/sqlbasics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">SQL Basics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../data/integration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Data Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Part III. Statistical Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../statlearning/general.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">General Topics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../statlearning/mathstat.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Probability and Statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../statlearning/linalg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Linear Algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../statlearning/featproc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Feature and Target Processing</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Part IV. Applied Ethics in Data Science</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ethics/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ethics/gonewrong.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">How Things Go Wrong</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ethics/bias_harm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Bias and Harm in Algorithms</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ethics/privacy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Personal Information and Personal Data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Part V. Integration of Data Science Solutions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../integ/coding_practices.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Coding Best Practices</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../integ/tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Data Science Tools</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-prob" id="toc-sec-prob" class="nav-link active" data-scroll-target="#sec-prob"><span class="header-section-number">13.1</span> Probability</a>
  <ul>
  <li><a href="#sample-space-and-events" id="toc-sample-space-and-events" class="nav-link" data-scroll-target="#sample-space-and-events">Sample Space and Events</a></li>
  <li><a href="#probability-measures" id="toc-probability-measures" class="nav-link" data-scroll-target="#probability-measures">Probability Measures</a>
  <ul class="collapse">
  <li><a href="#properties" id="toc-properties" class="nav-link" data-scroll-target="#properties">Properties</a></li>
  <li><a href="#conditional-probability" id="toc-conditional-probability" class="nav-link" data-scroll-target="#conditional-probability">Conditional probability</a></li>
  <li><a href="#law-of-total-probability" id="toc-law-of-total-probability" class="nav-link" data-scroll-target="#law-of-total-probability">Law of total probability</a></li>
  <li><a href="#bayes-rule" id="toc-bayes-rule" class="nav-link" data-scroll-target="#bayes-rule">Bayes’ rule</a></li>
  <li><a href="#independence" id="toc-independence" class="nav-link" data-scroll-target="#independence">Independence</a></li>
  </ul></li>
  <li><a href="#sec-prob-random-variables" id="toc-sec-prob-random-variables" class="nav-link" data-scroll-target="#sec-prob-random-variables">Random Variables</a>
  <ul class="collapse">
  <li><a href="#distribution-functions" id="toc-distribution-functions" class="nav-link" data-scroll-target="#distribution-functions">Distribution functions</a></li>
  <li><a href="#expected-value" id="toc-expected-value" class="nav-link" data-scroll-target="#expected-value">Expected value</a></li>
  <li><a href="#variance-and-standard-deviation" id="toc-variance-and-standard-deviation" class="nav-link" data-scroll-target="#variance-and-standard-deviation">Variance and standard deviation</a></li>
  <li><a href="#centering-and-scaling" id="toc-centering-and-scaling" class="nav-link" data-scroll-target="#centering-and-scaling">Centering and Scaling</a></li>
  <li><a href="#covariance-and-correlation" id="toc-covariance-and-correlation" class="nav-link" data-scroll-target="#covariance-and-correlation">Covariance and correlation</a></li>
  <li><a href="#independence-1" id="toc-independence-1" class="nav-link" data-scroll-target="#independence-1">Independence</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-prob-discrete-dist" id="toc-sec-prob-discrete-dist" class="nav-link" data-scroll-target="#sec-prob-discrete-dist"><span class="header-section-number">13.2</span> Discrete (Univariate) Distributions</a>
  <ul>
  <li><a href="#bernoulli-binary-bernoullipi" id="toc-bernoulli-binary-bernoullipi" class="nav-link" data-scroll-target="#bernoulli-binary-bernoullipi">Bernoulli (Binary), Bernoulli<span class="math inline">\((\pi)\)</span></a></li>
  <li><a href="#binomial-binomialnpi" id="toc-binomial-binomialnpi" class="nav-link" data-scroll-target="#binomial-binomialnpi">Binomial, Binomial<span class="math inline">\((n,\pi)\)</span></a></li>
  <li><a href="#geometric-geometricpi" id="toc-geometric-geometricpi" class="nav-link" data-scroll-target="#geometric-geometricpi">Geometric, Geometric<span class="math inline">\((\pi)\)</span></a></li>
  <li><a href="#negative-binomial-negbinkpi" id="toc-negative-binomial-negbinkpi" class="nav-link" data-scroll-target="#negative-binomial-negbinkpi">Negative Binomial, NegBin<span class="math inline">\((k,\pi)\)</span></a></li>
  <li><a href="#poisson-poissonlambda" id="toc-poisson-poissonlambda" class="nav-link" data-scroll-target="#poisson-poissonlambda">Poisson, Poisson<span class="math inline">\((\lambda)\)</span></a></li>
  </ul></li>
  <li><a href="#sec-prob-cont-dist" id="toc-sec-prob-cont-dist" class="nav-link" data-scroll-target="#sec-prob-cont-dist"><span class="header-section-number">13.3</span> Continuous (Univariate) Distributions</a>
  <ul>
  <li><a href="#uniform-uab" id="toc-uniform-uab" class="nav-link" data-scroll-target="#uniform-uab">Uniform, U<span class="math inline">\((a,b)\)</span></a></li>
  <li><a href="#exponential-expolambda" id="toc-exponential-expolambda" class="nav-link" data-scroll-target="#exponential-expolambda">Exponential, Expo<span class="math inline">\((\lambda)\)</span></a></li>
  <li><a href="#gamma-gammaalphabeta" id="toc-gamma-gammaalphabeta" class="nav-link" data-scroll-target="#gamma-gammaalphabeta">Gamma, Gamma<span class="math inline">\((\alpha,\beta)\)</span></a></li>
  <li><a href="#beta-betaalphabeta" id="toc-beta-betaalphabeta" class="nav-link" data-scroll-target="#beta-betaalphabeta">Beta, Beta<span class="math inline">\((\alpha,\beta)\)</span></a></li>
  <li><a href="#gaussian-normal-gmusigma2" id="toc-gaussian-normal-gmusigma2" class="nav-link" data-scroll-target="#gaussian-normal-gmusigma2">Gaussian (Normal), G<span class="math inline">\((\mu,\sigma^2)\)</span></a></li>
  </ul></li>
  <li><a href="#sec-prob-sampling-dist" id="toc-sec-prob-sampling-dist" class="nav-link" data-scroll-target="#sec-prob-sampling-dist"><span class="header-section-number">13.4</span> Sampling Distributions</a>
  <ul>
  <li><a href="#chi-square-chi2_nu" id="toc-chi-square-chi2_nu" class="nav-link" data-scroll-target="#chi-square-chi2_nu">Chi-Square, <span class="math inline">\(\chi^2_\nu\)</span></a></li>
  <li><a href="#students-t-t_nu" id="toc-students-t-t_nu" class="nav-link" data-scroll-target="#students-t-t_nu">Students’ t, <span class="math inline">\(t_\nu\)</span></a></li>
  <li><a href="#f-f_nu_1-nu_2" id="toc-f-f_nu_1-nu_2" class="nav-link" data-scroll-target="#f-f_nu_1-nu_2">F, <span class="math inline">\(F_{\nu_1, \nu_2}\)</span></a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../statlearning/general.html">Part III. Statistical Learning</a></li><li class="breadcrumb-item"><a href="../statlearning/mathstat.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Probability and Statistics</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-prob-stats" class="quarto-section-identifier"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Probability and Statistics</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This chapter covers some basic topics in mathematical statistics that are relevant to understanding data science methodology related to modeling data. The topics are considered more review than exposition and are brought together here as a reference.</p>
<section id="sec-prob" class="level2" data-number="13.1">
<h2 data-number="13.1" class="anchored" data-anchor-id="sec-prob"><span class="header-section-number">13.1</span> Probability</h2>
<p>Data are not deterministic; they have an element of uncertainty. Measurement errors, sampling variability, random assignment and selection, incomplete observation, are just some sources of random variability found in data. A solid understanding of probability concepts is necessary for any data professional. To separate signal from noise in data means separating systematic from random effects; to make statements about data requires the quantification of uncertainty.</p>
<p>Situations in which the outcomes occur randomly are called generically <em>experiments</em> and probability theory is used to model such experiments.</p>
<section id="sample-space-and-events" class="level3">
<h3 class="anchored" data-anchor-id="sample-space-and-events">Sample Space and Events</h3>
<p>The <strong>sample space</strong>, denoted <span class="math inline">\(\Omega\)</span>, is the set of all possible outcomes of the experiment. If we model the number of calls queued in a customer service hotline as random, the sample space is the set of non-negative integers, <span class="math inline">\(\Omega = \{ 0,\ 1,\ 2,\cdots,n\}\)</span>. On my way to work I pass through two traffic lights that are either red <span class="math inline">\((r)\)</span>, yellow <span class="math inline">\((y)\)</span>, or green <span class="math inline">\((g)\)</span>at the time I reach the light. The sample space is the set of all possible light combinations:</p>
<p><span class="math display">\[\Omega = \left\{ rr,ry,rg,yr,yy,yg,gr,gy,gg \right\}\]</span></p>
<p>An <strong>event</strong> is a subset of a sample space, denoted with uppercase letters. The event that less than 3 calls are queued is <span class="math inline">\(A = \{ 0,\ 1,\ 2\}\)</span>. The event that both lights are green is <span class="math inline">\(A = \left\{ gg \right\}\)</span>.</p>
<p>Consider two events, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. We can construct other events from <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. The <strong>union</strong> of the events, <span class="math inline">\(A \cup B\)</span>, is the event that <span class="math inline">\(A\)</span> occurs or <span class="math inline">\(B\)</span> occurs or both occur. The <strong>intersection</strong> of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, denoted <span class="math inline">\(A \cap B\)</span>, is the event that both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occur. The complement of event <span class="math inline">\(A\)</span>, denoted <span class="math inline">\(A^{c}\)</span>, consists of all events in <span class="math inline">\(\Omega\)</span> that are not in <span class="math inline">\(A\)</span>.</p>
<p>Suppose <span class="math inline">\(B\)</span> is the event that exactly one light is green in the traffic example, <span class="math inline">\(B = \left\{ rg,yg,gr,gy \right\}\)</span>. Then</p>
<p><span class="math display">\[A \cup B = \left\{ gg,rg,yg,gr,gy \right\}\]</span></p>
<p><span class="math display">\[A \cap B = \varnothing\]</span></p>
<p><span class="math display">\[B^{c} = \left\{ rr,ry,yr,yy,gg \right\}\]</span></p>
<p>The intersection of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> in this example is the empty set <span class="math inline">\(\varnothing\)</span>, the set without elements. <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are said to be <strong>disjoint</strong> events.</p>
<p>Set theory teaches us about laws involving events.</p>
<table class="table">
<caption>Laws of set theory. These are useful in deriving probabilities.</caption>
<thead>
<tr class="header">
<th>Law</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Commutative</strong></td>
<td><span class="math inline">\(A \cup B = B \cup A\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\(A \cap B = B \cap A\)</span></td>
</tr>
<tr class="odd">
<td><strong>Associative</strong></td>
<td><span class="math inline">\((A \cup B) \cup C = A \cup (B \cup C)\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\((A \cap B) \cap C = A \cap (B \cap C)\)</span></td>
</tr>
<tr class="odd">
<td><strong>Distributive</strong></td>
<td><span class="math inline">\((A \cup B) \cap C = (A \cap C) \cup (B \cap C)\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\((A \cap B) \cup C = (A \cup C) \cap (B \cup C)\)</span></td>
</tr>
<tr class="odd">
<td><strong>De Morgan’s</strong></td>
<td><span class="math inline">\((A \cup B)^{c} = A^{c} \cap B^{c}\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\((A \cap B)^{c} = A^{c} \cup B^{c}\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="probability-measures" class="level3">
<h3 class="anchored" data-anchor-id="probability-measures">Probability Measures</h3>
<section id="properties" class="level4">
<h4 class="anchored" data-anchor-id="properties">Properties</h4>
<p>A probability measure is a function <span class="math inline">\(\Pr( \cdot )\)</span> that maps from subsets of a sample space <span class="math inline">\(\Omega\)</span> (from events) to real numbers between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. We have the following axioms and properties:</p>
<ol type="1">
<li><p><span class="math inline">\(\Pr{(\Omega) = 1}\)</span></p></li>
<li><p><span class="math inline">\(\Pr{\left( A^{c}\  \right) = 1 - \Pr(A)}\)</span></p></li>
<li><p><span class="math inline">\(\Pr{(A \cup B) = \Pr{(A) + \Pr{(B) + \Pr(A \cap B)}}}\)</span></p></li>
<li><p>If <span class="math inline">\(A\)</span> is a subset of <span class="math inline">\(\Omega\)</span>, denoted <span class="math inline">\(A \subset \Omega\)</span>, then <span class="math inline">\(\Pr{(A) \geq 0}\)</span></p></li>
<li><p>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are disjoint, then <span class="math inline">\(\Pr{(A \cup B) = \Pr{(A) + \Pr(B)}}\)</span></p></li>
<li><p>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are disjoint, then <span class="math inline">\(\Pr{(A \cap B) = 0}\)</span></p></li>
<li><p>If <span class="math inline">\(A_{1},\cdots,A_{n}\)</span> are mutually disjoint events, then <span class="math inline">\(\Pr{\left( \bigcup_{i = 1}^{n}A_{i} \right) = \sum_{i = 1}^{n}{\Pr\left( A_{i} \right)}}\)</span></p></li>
<li><p><span class="math inline">\(\Pr{(\varnothing) = 0}\)</span></p></li>
</ol>
</section>
<section id="conditional-probability" class="level4">
<h4 class="anchored" data-anchor-id="conditional-probability">Conditional probability</h4>
<p>The <strong>conditional</strong> probability that event <span class="math inline">\(A\)</span> occurs given that event <span class="math inline">\(B\)</span> has occurred is</p>
<p><span class="math display">\[\Pr{\left( A|B \right) = \frac{\Pr(A \cap B)}{\Pr(B)}}\]</span></p>
<p>This requires that <span class="math inline">\(\Pr{(B) &gt; 0}\)</span>, we cannot condition on something that has zero probability of happening. The idea of the conditional probability is that for purpose of conditioning on <span class="math inline">\(B\)</span>, we change the relevant sample space from <span class="math inline">\(\Omega\)</span> to <span class="math inline">\(B\)</span>.</p>
<p>Rearranging the probabilities in this expression, we find that the probability that two events occur together (the events intersect) can be written as</p>
<p><span class="math display">\[\Pr(A \cap B) = \Pr{\left( A|B \right) \times \Pr(B)}\]</span></p>
<p>This is known as the <strong>multiplication law</strong> of probabilities. Another way of putting this result is that the <strong>joint</strong> probability of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is the product of the conditional probability given <span class="math inline">\(B\)</span> and the marginal probability of <span class="math inline">\(B\)</span>. If <span class="math inline">\(\Pr{(A) &gt; 0}\)</span>, we can also use <span class="math inline">\(A\)</span> to condition the calculation:</p>
<p><span class="math display">\[\Pr(A \cap B) = \Pr{\left( B|A \right) \times \Pr(A)}\]</span></p>
<p>Suppose that the probability of rain <span class="math inline">\((A)\)</span> on a cloudy <span class="math inline">\((B)\)</span> day is <span class="math inline">\(\Pr{\left( A|B \right) = 0.3}\)</span>. The probability that it is cloudy is <span class="math inline">\(\Pr(B) = 0.2\)</span>. The probability that it is cloudy and raining is <span class="math inline">\(\Pr{\left( A|B \right) \times \Pr(B)} = 0.3 \times 0.2 = 0.06\)</span>. Notice the difference between the event <code>cloudy and raining</code> and the event <code>raining given that it is cloudy</code>.</p>
</section>
<section id="law-of-total-probability" class="level4">
<h4 class="anchored" data-anchor-id="law-of-total-probability">Law of total probability</h4>
<p>Suppose we divide the sample space into two disjoint events, <span class="math inline">\(B\)</span> and <span class="math inline">\(B^{c}\)</span>. Then the probability that <span class="math inline">\(A\)</span> occurs can be decomposed as</p>
<p><span class="math display">\[\Pr{(A) = \Pr{(A \cap B) + \Pr\left( A \cap B^{c} \right)}}\]</span></p>
<p>Substituting the conditional and marginal probabilities this can be rewritten as</p>
<p><span class="math display">\[\Pr{(A) = \Pr{\left( A|B \right)\Pr(B) + \Pr{\left( A|B^{c} \right)\Pr\left( B^{c} \right)}}}\]</span></p>
<p>Each of the products on the right-hand side conditions on a different sample space, but since <span class="math inline">\(B\)</span> and <span class="math inline">\(B^{c}\)</span> are disjoint, the entire space <span class="math inline">\(\Omega\)</span> is covered. For example, the probability that it snows in Virginia is the sum of the probabilities that it snows in Montgomery County and that it snows in the other counties of the state.</p>
<p>We can extend the decomposition from two disjoint sets to any number of disjoint sets. If <span class="math inline">\(B_{1},\cdots,\ B_{n}\)</span> are disjoint sets and <span class="math inline">\(\bigcup_{i = 1}^{n}B_{i} = \Omega\)</span>, and all $\Pr{\left( B_{i} \right) &gt; 0},\ $then</p>
<p><span class="math display">\[\Pr(A) = \sum_{i = 1}^{n}{\Pr{\left( A|B_{i} \right)\Pr\left( B_{i} \right)}}\]</span></p>
</section>
<section id="bayes-rule" class="level4">
<h4 class="anchored" data-anchor-id="bayes-rule">Bayes’ rule</h4>
<p>What is known as the Bayes rule, named after English mathematician Thomas Bayes, is on the surface another way of expressing conditional probabilities. Recall the definition of the conditional probability,</p>
<p><span class="math display">\[\Pr{\left( A|B \right) = \frac{\Pr(A \cap B)}{\Pr(B)}}\]</span></p>
<p>The numerator, the probability that events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occur together, can be written in terms of a conditional probability as well, <span class="math inline">\(\Pr(A \cap B) = \Pr{\left( B|A \right)\Pr(A)}\)</span>. Combining the two results yields Bayes’ rule:</p>
<p><span class="math display">\[\Pr\left( A|B \right) = \frac{\Pr{\left( B|A \right)\Pr(A)}}{\Pr(B)}\]</span></p>
<p>The conditional probability on the left-hand side, <span class="math inline">\(\Pr\left( A|B \right)\)</span>, is called the <strong>posterior} probability. The marginal probabilities</strong> <span class="math inline">\(\Pr(A)\)</span> and <span class="math inline">\(\Pr(B)\)</span> are also called the prior** probabilities. The Bayes rule allows us to express the probability of <span class="math inline">\(A|B\)</span> as a function of the probability of <span class="math inline">\(B|A\)</span> and vice versa.</p>
<p><span class="math display">\[\Pr\left( A|B \right) = \frac{\Pr{\left( B|A \right)\Pr(A)}}{\Pr(B)}\]</span></p>
<p><span class="math display">\[\Pr\left( B|A \right) = \frac{\Pr{\left( A|B\  \right)\Pr(B)}}{\Pr(A)}\]</span></p>
<p>You can combine Bayes’ formula as given here with the law of total probability to compute the marginal probability in the denominator. This makes it even more evident how Bayes’ rule allows us to reverse the conditioning:</p>
<p><span class="math display">\[\Pr{\left( B_{j}|A \right) =}\frac{\Pr\left( A|B_{j} \right)\Pr\left( B_{j} \right)}{\sum_{i = 1}^{n}{\Pr\left( A|B_{i} \right)\Pr\left( B_{i} \right)}}\]</span></p>
<div class="example">
<div class="example-header">
<p>Example: Lie Detector Test</p>
</div>
<div class="example-container">
<p>A lie detector test returns two possible readings, a positive reading <span class="math inline">\(( \oplus )\)</span> that the subject is lying, or a negative reading <span class="math inline">\(( \ominus )\)</span> that the subject is telling the truth. The subject of the test is indeed lying <span class="math inline">\((L)\)</span> or is indeed telling the truth <span class="math inline">\((T)\)</span>.</p>
<p>We can construct from this a number of events:</p>
<p><span class="math inline">\(\oplus |\ L\)</span>: the polygraph indicates the subject is lying and they are indeed lying.</p>
<p><span class="math inline">\(\oplus |T\)</span>: the polygraph indicates the subject is lying and they are telling the truth.</p>
<p><span class="math inline">\(\ominus |L\)</span>: the polygraph indicates the subject is telling the truth when they are lying.</p>
<p><span class="math inline">\(T\)</span>: the test subject is telling the truth.</p>
<p><span class="math inline">\(L\)</span>: the test subject is lying.</p>
<p>Suppose that we know <span class="math inline">\(\Pr\left( \oplus |L \right) = 0.88\)</span> and thus <span class="math inline">\(\Pr\left( \ominus |\ L \right) = 0.12\)</span>. The first probability is the true positive rate of the device. If a person is lying, the probability that the polygraph detects it is 0.88. Similarly, assume we know that the true negative rate, the probability that the lie detector indicates someone is telling the truth when they are indeed truthful, is <span class="math inline">\(\Pr{\left( \ominus |T \right) = 0.86}.\)</span></p>
<p>If we know the marginal (prior) probabilities that someone is telling the truth on a particular question, say, <span class="math inline">\(\Pr{(T) = 0.99}\)</span>, then we can use Bayes’ rule to ask the question: What is the probability the person is telling the truth when the polygraph says that they are lying, <span class="math inline">\(\Pr{(T| \oplus )}\)</span>:</p>
<p><span class="math display">\[\Pr{\left( T \middle| \oplus \right) = \frac{\Pr{\left( \oplus |T \right)\Pr(T)}}{\Pr{\left( \oplus |T \right)\Pr(T)} + \Pr{\left( \oplus |L \right)\Pr(L)}}} = \frac{0.14 \times \ 0.99}{0.14 \times 0.99 + 0.88 \times .01} = 0.94\]</span></p>
<p>Despite the relatively large true positive and true negative rates, 94% of all positive readings will be incorrect—the subject answered truthfully. If the probability that someone lies on a particular question is <span class="math inline">\(\Pr{(T) = 0.5}\)</span>, the chance of a polygraph to indict the innocent is much smaller:</p>
<p><span class="math display">\[\Pr\left( T \middle| \oplus \right) = \frac{0.14 \times \ 0.5}{0.14 \times 0.5 + 0.88 \times 0.5} = 0.13\]</span></p>
</div>
</div>
</section>
<section id="independence" class="level4">
<h4 class="anchored" data-anchor-id="independence">Independence</h4>
<p>Independent and disjoint events are different concepts. Events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are disjoint if they cannot occur together. Their intersection is the empty set and thus <span class="math inline">\(\Pr{(A \cap B) = \Pr{(\varnothing) = 0}}\)</span>. Independent events are <strong>unrelated</strong> events, that means the outcome of one event does not affect the outcome of the other event—the events can occur together, however.</p>
<p>Being a sophomore, junior, or senior student are disjoint events; you cannot be simultaneously a junior and a senior. On the other hand, the events <code>rain in Virginia'' and</code>winning the lottery ticket’’ are independent; they can occur together but whether it rains has no bearing on whether you win the lottery that day and winning the lottery has no effect on the weather.</p>
<p>Formally, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent events if <span class="math inline">\(\Pr(A \cap B) = \Pr(A){\times Pr}(B)\)</span>. This should make intuitive sense: if knowing <span class="math inline">\(A\)</span> carries no information about the occurrence of <span class="math inline">\(B\)</span>, then the probability that they occur together is the product of the probabilities that they occur separately.</p>
<p>Applying this to the definition of the conditional probability, the following holds for two independent events:</p>
<p><span class="math display">\[\Pr\left( A|B \right) = \frac{\Pr(A \cap B)}{\Pr(B)} = \frac{\Pr{(A)\Pr(B)}}{\Pr(B)} = \Pr(A)\]</span></p>
<p>In other words, when <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent, the conditional probability equals the marginal probability—the occurrence of <span class="math inline">\(B\)</span> does not alter the probability of <span class="math inline">\(A\)</span>.</p>
</section>
</section>
<section id="sec-prob-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="sec-prob-random-variables">Random Variables</h3>
<p>Random variables are real-valued functions defined on sample spaces. Recall the sample space of the traffic lights encountered on the way to work:</p>
<p><span class="math display">\[\Omega = \left\{ rr,ry,rg,yr,yy,yg,gr,gy,gg \right\}\]</span></p>
<p><span class="math inline">\(X\)</span>, the number of green lights is a random variable. <span class="math inline">\(X\)</span> is a <strong>discrete</strong> random variable since it can take on a countable number of values, <span class="math inline">\(X = 0,\ 1,\ 2\)</span>. If the nine traffic light configurations in <span class="math inline">\(\Omega\)</span> are equally likely, then the values of <span class="math inline">\(X\)</span> occur with probabilities</p>
<p><span class="math display">\[\Pr(X = 0) = \frac{4}{9}\]</span></p>
<p><span class="math display">\[\Pr(X = 1) = \frac{4}{9}\]</span></p>
<p><span class="math display">\[\Pr(X = 2) = \frac{1}{9}\]</span></p>
<section id="distribution-functions" class="level4">
<h4 class="anchored" data-anchor-id="distribution-functions">Distribution functions</h4>
<p>The function <span class="math inline">\(p(x) = \Pr(X = x)\)</span>, that assigns probabilities to the discrete values of the random variable, is known as the <strong>probability mass function</strong> (p.m.f.). The possible values the random variable can take on are called its <strong>support</strong>. Discrete random variables can have infinitely large support when there is no limit, for example the number of coin tosses until 5 heads are observed has support <span class="math inline">\(5,\ 6,\ 7,\ \cdots\)</span>. The number of fish caught in a day at a lake has infinite support <span class="math inline">\(0,\ 1,\ 2,\ \cdots\)</span>; it might be highly unlikely to catch 1,000 fish per day, but it is not impossible.</p>
<p>If the number of possible values is not countable, the random variable is called <strong>continuous</strong>. The concept of a probability mass function then does not make sense. Instead, continuous random variables are characterized by their <strong>probability density function</strong> (p.d.f., <span class="math inline">\(f(x)\)</span>). Probabilities for continuous random variables are calculated by integrating the p.d.f.:</p>
<p><span class="math display">\[\Pr(a &lt; X &lt; b) = \int_{a}^{b}{f(x)\ dx}\]</span></p>
<p>The <strong>cumulative distribution function</strong> (c.d.f., <span class="math inline">\(F(x)\)</span> is defined for any random variable as</p>
<p><span class="math display">\[F(x) = \Pr{(X \leq x)}\]</span></p>
<p>In the case of a discrete random variable, this means summing the probabilities on the support up to and including <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[F(x) = \sum_{X:X \leq x}^{}{p(x)}\]</span></p>
<p>For a continuous random variable, the c.d.f. is the integral up to <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[F(x) = \int_{- \infty}^{x}{f(x)\ dx}\]</span></p>
<p>The <span class="math inline">\(p\)</span>th <strong>quantile</strong> of a distribution is the value <span class="math inline">\(x_{p}\)</span> for which <span class="math inline">\(F\left( x_{p} \right) = p\)</span>. For example, the 0.85<sup>th</sup> quantile—also called the 85<sup>th</sup> percentile—is <span class="math inline">\(x_{.85}\)</span> and satisfies <span class="math inline">\(F\left( x_{.85} \right) = 0.85\)</span>. Special quantiles are obtained for <span class="math inline">\(p = 0.25\)</span>, the first quartile, <span class="math inline">\(p = 0.5\)</span>, the median (second quartile), and <span class="math inline">\(p = 0.75\)</span>, the third quartile.</p>
</section>
<section id="expected-value" class="level4">
<h4 class="anchored" data-anchor-id="expected-value">Expected value</h4>
<p>The <strong>expected value</strong> of a random variable, <span class="math inline">\(\text{E}\lbrack X\rbrack\)</span>, also known as the <strong>mean</strong> of <span class="math inline">\(X\)</span>, is the weighted average of the values of the random variable weighted by the mass or density. For a discrete random variable,</p>
<p><span class="math display">\[\text{E}\lbrack X\rbrack = \sum_{}^{}{x\ p(x)}\]</span></p>
<p>and for a continuous random variable,</p>
<p><span class="math display">\[\text{E}\lbrack X\rbrack = \int_{}^{}{x\ f(x)\ dx}\]</span></p>
<p>Technically, we need the conditions <span class="math inline">\(\sum |x|p(x) &lt; \infty\)</span> and <span class="math inline">\(\int|x|f(x)dx &lt; \infty\)</span>, respectively, for the expected values to be defined. Almost all random variables satisfy this condition. A famous example to the contrary is the ratio of two normal distributions with mean zero, known as the Cauchy distribution—its mean (and variance) are not defined.</p>
<p>We can think of the expected value as the center of mass of the distribution function (density or mass function), the point on which the distribution balances.</p>
<p>The Greek symbol <span class="math inline">\(\mu\)</span> is often used to denote the expected value of a random variable.</p>
<p>Randomness is contagious—a function of a random variable is a random variable. So, if <span class="math inline">\(X\)</span> is a random variable, the function <span class="math inline">\(h(X)\)</span> is a random variable as well. We can find the expected value of <span class="math inline">\(h(X)\)</span> through the mass or density function of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[\text{E}\left\lbrack h(X) \right\rbrack = \sum_{x}^{}{h(x)p(x)}\]</span></p>
<p><span class="math display">\[\text{E}\left\lbrack h(X) \right\rbrack = \int_{- \infty}^{\infty}{h(x)f(x)dx}\]</span></p>
<p>Note that the sum and interval are taken over the support of <span class="math inline">\(X\)</span>, rather than <span class="math inline">\(h(X)\)</span>. You can also compute the expected value over the support of <span class="math inline">\(h(X)\)</span>, but then values of <span class="math inline">\(h(x)\)</span> need to be weighted with the probabilities (or densities) of <span class="math inline">\(h(X)\)</span>.</p>
<p>An important case of the expectation of a function is <span class="math inline">\(Y\)</span> as a linear combination of <span class="math inline">\(X\)</span>: <span class="math inline">\(Y = aX + b\)</span>:</p>
<p><span class="math display">\[\text{E}\lbrack Y\rbrack = \text{E}\lbrack aX + b\rbrack = a\text{E}\lbrack X\rbrack + b\]</span></p>
<p>The expected value is a linear operator. For <span class="math inline">\(k\)</span> random variables <span class="math inline">\(X_{1},\cdots,\ X_{k}\)</span> and constants <span class="math inline">\(a_{1},\cdots,a_{k}\)</span>,</p>
<p><span class="math display">\[\text{E}\left\lbrack a_{1}X_{1} + a_{2}X_{2} + \cdots + a_{k}X_{k} \right\rbrack = a_{1}\text{E}\left\lbrack X_{1} \right\rbrack + a_{2}\text{E}\left\lbrack X_{2} \right\rbrack + \cdots + a_{k}\text{E}\lbrack X_{k}\rbrack\]</span></p>
<div class="example">
<div class="example-header">
<p>Example: Mean of Binomial Distribution</p>
</div>
<div class="example-container">
<p>A random variable <span class="math inline">\(X\)</span> with binomial distribution has p.m.f.</p>
<p><span class="math display">\[\Pr(X = x) = \begin{pmatrix} n \\ x \end{pmatrix}\pi^{x}(1 - \pi)^{n - x}\]</span></p>
<p>The support of <span class="math inline">\(X\)</span> is <span class="math inline">\(0,\ 1,\ 2,\ \cdots,\ n\)</span>. The term <span class="math inline">\(\begin{pmatrix} n \\ x \end{pmatrix}\)</span> is known as the binomial coefficient,</p>
<p><span class="math display">\[\begin{pmatrix} n \\ x \end{pmatrix} = \frac{n!}{x!(n - x)!}\]</span></p>
<p>The binomial random variable is the sum of <span class="math inline">\(n\)</span> independent Bernoulli experiments. A Bernoulli experiment can result in only two possible outcomes that occur with probabilities <span class="math inline">\(\pi\)</span> and <span class="math inline">\(1 - \pi\)</span>.</p>
<p>Computing the expected value of the binomial random variable from the p.m.f. is messy, it requires evaluation of</p>
<p><span class="math display">\[\text{E}\lbrack X\rbrack = \sum_{x = 0}^{n}{\begin{pmatrix} n \\ x \end{pmatrix}x{\ \pi}^{x}}\ (1 - \pi)^{n - x}\]</span></p>
<p>If we recognize the binomial random variable <span class="math inline">\(X\)</span> as the sum of <span class="math inline">\(n\)</span> Bernoulli variables <span class="math inline">\(Y_{1},\cdots,Y_{n}\)</span> with probability <span class="math inline">\(\pi\)</span>, the expected value follows as</p>
<p><span class="math display">\[\text{E}\lbrack X\rbrack = \text{E}\left\lbrack \sum Y_{i} \right\rbrack = \sum \text{E}\left\lbrack Y_{i} \right\rbrack = n\pi\]</span></p>
</div>
</div>
</section>
<section id="variance-and-standard-deviation" class="level4">
<h4 class="anchored" data-anchor-id="variance-and-standard-deviation">Variance and standard deviation</h4>
<p>Next to the mean, the most important expected value of a random variable, is the <strong>variance</strong>, the expected value of the squared deviation of a random variable from its mean:</p>
<p><span class="math display">\[\text{Var}\lbrack X\rbrack = \text{E}\left\lbrack \left( X - \text{E}\lbrack X\rbrack \right)^{2} \right\rbrack = \text{E}\left\lbrack X^{2} \right\rbrack -\text{E}\lbrack X\rbrack^{2}\]</span></p>
<p>The second expression states that the variance can also be written as the difference of the mean of the square of the random variable and the square of the mean of the random variable. The Greek symbol <span class="math inline">\(\sigma^{2}\)</span> is commonly used to denote the variance. The square is useful to remind us that the variance is in squared units. If the random variable <span class="math inline">\(X\)</span> is a length in feet, the variance represents an area in square feet.</p>
<p>The square root of the variance is called the <strong>standard deviation</strong> of the random variable, frequently denoted <span class="math inline">\(\sigma\)</span>. The standard deviation is measured in the same units as <span class="math inline">\(X\)</span>.</p>
<p>From the definition of expected values and functions of random values, the variance can be calculated from first principles as the expected value of <span class="math inline">\((X - \mu)^{2}\)</span>, where <span class="math inline">\(\mu = E\lbrack X\rbrack:\)</span></p>
<p><span class="math display">\[\text{Var}\lbrack X\rbrack = \sum_{x}^{}(x - \mu)^{2}p(x)\]</span></p>
<p><span class="math display">\[\text{Var}\lbrack X\rbrack = \int_{- \infty}^{\infty}{(x - \mu)^2 \, f(x)dx} \]</span></p>
<p>It is often easier to derive the variance of a random variable from the following properties. Suppose <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants and <span class="math inline">\(X\)</span> is a random variable with variance <span class="math inline">\(\sigma^{2}\)</span></p>
<ol type="1">
<li><p><span class="math inline">\(\text{Var}\lbrack a\rbrack = 0\)</span></p></li>
<li><p><span class="math inline">\(\text{Var}\lbrack X + b\rbrack = \sigma^2\)</span></p></li>
<li><p><span class="math inline">\(\text{Var}\lbrack aX\rbrack = a^2\sigma^2\)</span></p></li>
<li><p><span class="math inline">\(\text{Var}\lbrack aX + b\rbrack = a^2\sigma^2\)</span></p></li>
</ol>
<p>The first property states that constants have no variability—this should make sense. A constant is equal to its expected value, <span class="math inline">\(\text{E}\lbrack a\rbrack = a\)</span>, so that deviations between value and mean are always 0.</p>
<p>The second property states that shifting a random variable by a fixed amount does not change its dispersion. That also should make intuitive sense: adding (or subtracting) a constant amount from every value does not change the shape of the distribution, it simply moves the distribution to a different location—the mean changes but the variance does not.</p>
<p>The third property states that scaling a random variable with constant <span class="math inline">\(a\)</span> has a squared effect on the variance. This is also intuitive since the variance is in squared units. The effect of scaling on the standard deviation is linear: If you multiply every value with the constant <span class="math inline">\(a\)</span>, the standard deviation of the product increases by factor <span class="math inline">\(a\)</span>.</p>
<p>The fourth property simply combines properties 1 and 3.</p>
<p>The following result plays an important role in deriving the variance of linear functions of random variables. Many statistical estimators are of this form, from the sample mean to least-squares regression estimators.</p>
<p>If <span class="math inline">\(X_{1},\cdots,X_{k}\)</span> are independent random variables and <span class="math inline">\(a_{1},\cdots,a_{k}\)</span> are constants, then the variance of <span class="math inline">\(a_{1}X_{1} + \cdots + a_{k}X_{k}\)</span> is given by</p>
<p><span class="math display">\[\text{Var}\left\lbrack a_{1}X_{1} + \cdots + a_{k}X_{k} \right\rbrack = a_{1}^{2}\text{Var}\left\lbrack X_{1} \right\rbrack + a_{2}^{2}\text{Var}\left\lbrack X_{2} \right\rbrack + \cdots + a_{k}^{2}\text{Var}\lbrack X_{k}\rbrack\]</span></p>
<p>For this result to hold it is sufficient that the <span class="math inline">\(X_i\)</span> are <strong>uncorrelated</strong>, they do not have to be independent.</p>
<div class="example">
<div class="example-header">
<p>Example: Variance of Binomial Distribution</p>
</div>
<div class="example-container">
<p>We saw earlier that the sum of <span class="math inline">\(n\)</span> independent Bernoulli random variables with probability <span class="math inline">\(\pi\)</span> is a Binomial random variable. A Bernoulli(<span class="math inline">\(\pi\)</span>) random variable <span class="math inline">\(Y_i\)</span> takes on values 1 and 0 with p.m.f.</p>
<p><span class="math display">\[\Pr\left( Y_{i} = 1 \right) = \pi\]</span></p>
<p><span class="math display">\[\Pr\left( Y_{i} = 0 \right) = 1 - \pi\]</span></p>
<p>The mean and variance of <span class="math inline">\(Y\)</span> are <span class="math inline">\(E\left\lbrack Y_{i} \right\rbrack = \pi\)</span> and <span class="math inline">\(Var\left\lbrack Y_{i} \right\rbrack = \pi(1 - \pi)\)</span>, respectively.</p>
<p>Since <span class="math inline">\(X = \sum_{i = 1}^{n}Y_{i}\)</span> and the <span class="math inline">\(Y_{i}\)</span>s are independent,</p>
<p><span class="math display">\[\text{Var}\lbrack X\rbrack = \sum_{i = 1}^{n}{\text{Var}\left\lbrack Y_{i} \right\rbrack} = n\pi(1 - \pi)\]</span></p>
</div>
</div>
<p>We can also apply these results to derive the variance of the sample mean.</p>
<div class="example">
<div class="example-header">
<p>Example: Variance of the Sample Mean</p>
</div>
<div class="example-container">
<p>Suppose that <span class="math inline">\(Y_1, \cdots, Y_n\)</span> are a random sample from a distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. What is the variance of the sample mean <span class="math inline">\(\overline{Y} = \frac{1}{n}\sum_{i=1}^n Y_i\)</span>?</p>
<p>The sample mean is the sum of the <span class="math inline">\(Y_i\)</span>s, divided by the sample size <span class="math inline">\(n\)</span>. By virtue of drawing a random sample, independence of the <span class="math inline">\(Y_i\)</span> is guaranteed. We can thus apply the results about the variance of a sum of independent random variables and about the variance of a scaled random variable: <span class="math display">\[
\begin{align*}
\text{Var}[\overline{Y}] &amp;= \text{Var}\left[ \frac{1}{n}\sum_{i=1}^n Y_i\right] \\
&amp;= \frac{1}{n^2}\text{Var}\left[\sum_{i=1}^n Y_i\right] \\
&amp;= \frac{1}{n^2}\sum_{i=1}^n \text{Var}[Y_i] \\
&amp;= \frac{1}{n^2} n \sigma^2 = \frac{\sigma^2}{n}
\end{align*}
\]</span></p>
</div>
</div>
<p>It is sufficient for this result to hold that the random variables are mutually uncorrelated, a weaker condition than mutual independence. We are introducing independence below after discussing the concept of joint distribution functions.</p>
</section>
<section id="centering-and-scaling" class="level4">
<h4 class="anchored" data-anchor-id="centering-and-scaling">Centering and Scaling</h4>
<p>A random variable <span class="math inline">\(X\)</span> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span> is <strong>centered</strong> by subtracting its mean. A random variable is <strong>scaled</strong> by multiplying or dividing it by a factor. A random variable is <strong>standardized</strong> by centering and dividing by its standard deviation:</p>
<p><span class="math display">\[Y = \frac{X - \mu}{\sigma}\]</span></p>
<p>If <span class="math inline">\(X\)</span> has mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then <span class="math inline">\(Y\)</span> has mean 0 and variance 1. This follows by applying the properties of the expecation and variance operators: <span class="math display">\[
\begin{align*}
\text{E}[Y] &amp;= \text{E}\left[\frac{X-\mu}{\sigma}\right] = \frac{1}{\sigma}\text{E}[X-\mu] =\frac{1}{\sigma}(\mu-\mu) = 0\\
\text{Var}[Y]
&amp;= \text{Var}\left[\frac{X-\mu}{\sigma}\right] = \frac{1}{\sigma^2}\text{Var}[X-\mu] =
\frac{1}{\sigma^2}\text{Var}[X] = \frac{\sigma^2}{\sigma^2} =1
\end{align*}
\]</span></p>
<p>It is also called the centered-and-scaled version of <span class="math inline">\(X\)</span>.</p>
<p>A special type of scaling is <strong>range scaling</strong>. This form of scaling of the data transforms the data so it falls between a known lower and upper bound, often 0 and 1. Suppose that <span class="math inline">\(\text{min}(X) \le X \le \text{max}(X)\)</span> and we want to create a variable <span class="math inline">\(z_\text{min} \le Z \le z_\text{max}\)</span> from <span class="math inline">\(X\)</span>. <span class="math inline">\(Z\)</span> can be computed by scaling and shifting a standardized form of <span class="math inline">\(X\)</span>: <span class="math display">\[
\begin{align*}
  x^* &amp;= \frac{x-\min(x)}{\max(x)-\min(x)} \\
  z &amp;= z_{\text{min}} + x^* \times (z_{\text{max}} - z_{\text{min}})
\end{align*}
\]</span> If the bounds are <span class="math inline">\(z_{\text{min}} = 0\)</span> and <span class="math inline">\(z_{\text{max}} = 1\)</span>, then <span class="math inline">\(z = x^*\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Technically, scaling refers to multiplication of a variable with a factor. Because standardization involves centering and scaling, and standardization is used frequently, you will find the term <em>scaling</em> applied to standardization. In fact, the <code>scale</code> function in <code>R</code> performs standardization (centering and scaling) by default.</p>
</div>
</div>
</section>
<section id="covariance-and-correlation" class="level4">
<h4 class="anchored" data-anchor-id="covariance-and-correlation">Covariance and correlation</h4>
<p>Whereas the variance measures the dispersion of a random variable, the <strong>covariance</strong> measures how two random variables vary <strong>together</strong>. The covariance of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is the expected value of the cross-product of two variables centered around their respective means, <span class="math inline">\(\mu_{X}\)</span> and <span class="math inline">\(\mu_{Y}\)</span>:</p>
<p><span class="math display">\[\text{Cov}\lbrack X,Y\rbrack = \text{E}\left\lbrack \left( X - \mu_{X} \right)\left( Y - \mu_{Y} \right) \right\rbrack = \text{E}\lbrack XY\rbrack - \mu_{X}\mu_{Y}\]</span></p>
<p>Similar to the variance, the covariance has properties that come in handy when working with random variables:</p>
<ol type="1">
<li><p><span class="math inline">\(\text{Cov}\lbrack X,a\rbrack = 0\)</span></p></li>
<li><p><span class="math inline">\(\text{Cov}\lbrack X,X\rbrack = \text{Var}\lbrack X\rbrack\)</span></p></li>
<li><p><span class="math inline">\(\text{Cov}\lbrack aX,bY\rbrack = ab \text{Cov}\lbrack X,Y\rbrack\)</span></p></li>
<li><p><span class="math inline">\(\text{Cov}\lbrack aY + bU,cW + dV\rbrack = ac \text{Cov}\lbrack Y,W\rbrack + bc \text{Cov}\lbrack U,W\rbrack + ad \text{Cov}\lbrack Y,V\rbrack + bd \text{Cov}\lbrack U,V\rbrack\)</span></p></li>
</ol>
<p>Since the variance is a special case of the covariance of a random variable with itself, the properties of the covariance operator are general cases of the properties of the variance operator discussed earlier.</p>
<p>Earlier we gave an expression for the variance of a linear combination of independent random variables. We can now generalize the result to linear combinations of correlated random variables.</p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables with variance <span class="math inline">\(\sigma_{X}^{2}\)</span> and <span class="math inline">\(\sigma_{Y}^{2}\)</span>, respectively, and <span class="math inline">\(a, b\)</span> are constants, then</p>
<p><span class="math display">\[
\begin{align*}
\text{Var}\lbrack aX + bY\rbrack &amp;= \text{Var}\lbrack aX\rbrack + \text{Var}\lbrack bY\rbrack + \text{Cov}\lbrack aX,bY\rbrack \\
     &amp;= a^{2}\sigma_{X}^{2} + b^{2}\sigma_{Y}^{2} + ab \text{Cov}\lbrack X,Y\rbrack
\end{align*}
\]</span></p>
<p>If <span class="math inline">\(\text{Cov}\lbrack X,Y\rbrack = 0\)</span>, the random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>uncorrelated</strong>. The <strong>correlation</strong> between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is defined as</p>
<p><span class="math display">\[\text{Corr}\lbrack X,Y\rbrack = \frac{\text{Cov}\lbrack X,Y\rbrack}{\sigma_{X}\sigma_{Y}}\]</span></p>
<p>The correlation is often denoted <span class="math inline">\(\rho_{XY}\)</span> and takes on values <span class="math inline">\(-1 \leq \rho_{XY} \leq 1\)</span>. A zero correlation is a weaker condition than <strong>independence</strong> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. A positive value of <span class="math inline">\(\rho_{XY}\)</span> means that an increase in the value of <span class="math inline">\(X\)</span> is associated with an increase in the value of <span class="math inline">\(Y\)</span>. Note that we are not stating that the increase in <span class="math inline">\(X\)</span> is caused by the increase in <span class="math inline">\(Y\)</span>. Correlation is a measure of association and does not imply causation.</p>
<p>If <span class="math inline">\(\text{Corr}[X,Y] = 1\)</span> or <span class="math inline">\(\text{Corr}[X,Y] = -1\)</span>, then <span class="math inline">\(X\)</span> can be predicted perfectly from <span class="math inline">\(Y\)</span>; knowing <span class="math inline">\(Y\)</span> is equal to knowing <span class="math inline">\(X\)</span>. An example of a perfect positive correlation is when <span class="math inline">\(X\)</span> is a length measured in inches and <span class="math inline">\(Y\)</span> is measured in cm. When <span class="math inline">\(\text{Corr}[X,Y] = 0\)</span>, then <span class="math inline">\(X\)</span> carries no information about <span class="math inline">\(Y\)</span>, and vice versa. A person’s shoe size and tomorrow’s weather is an example of uncorrelated variables.</p>
</section>
<section id="independence-1" class="level4">
<h4 class="anchored" data-anchor-id="independence-1">Independence</h4>
<p>The joint behavior of random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is described by their <strong>joint</strong> cumulative distribution function,</p>
<p><span class="math display">\[F(x,y) = \Pr(X \leq x,Y \leq y)\]</span></p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are continuous, this probability is calculated as</p>
<p><span class="math display">\[F(x,y) = \int_{- \infty}^{x}{\int_{- \infty}^{y}{f(u,v)\ dvdu}}\]</span></p>
<p>The bivariate probability density is derived from <span class="math inline">\(F(x,y)\)</span> by differentiating,</p>
<p><span class="math display">\[f(x,y) = \frac{\partial^{2}}{\partial x\partial y}F(x,y)\]</span></p>
<p>Two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent if their joint c.d.f.s (or joint p.d.f.s) factor into the <strong>marginal</strong> distributions. The marginal density function of <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> can be derived from the joint density by integrating over the other random variable:</p>
<p><span class="math display">\[f_{X}(x) = \int_{- \infty}^{\infty}{f(x,y)dy}\]</span></p>
<p><span class="math display">\[f_{Y}(y) = \int_{- \infty}^{\infty}{f(x,y)dx}\]</span></p>
<p>Finally, we can state that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent if <span class="math inline">\(f(x,y) = f_{X}(x)f_{Y}(y)\)</span> (or <span class="math inline">\(F(x,y) = F_{X}(x)F_{Y}(y)\)</span>).</p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then</p>
<p><span class="math display">\[\text{E}\left\lbrack g(X)h(Y) \right\rbrack = \text{E}\left\lbrack g(X) \right\rbrack \text{E}\left\lbrack h(Y) \right\rbrack\]</span></p>
<p>For example, <span class="math inline">\(\text{E}\lbrack XY\rbrack = \text{E}\lbrack X\rbrack \text{E}\lbrack Y\rbrack\)</span> and it follows that the covariance between independent random variables is zero. That is, independent random variables are uncorrelated. The reverse does not have to be true. Lack of correlation (a zero covariance) does not imply independence.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Probability Playground">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Probability Playground
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before we dive into some important discrete and continuous distributions in statistics, I want to point you at a remarkable website, called the <a href="https://www.acsu.buffalo.edu/~adamcunn/probability/probability.html">Probability Playground</a>, created by Adam Cunningham at the University of Buffalo. The site was featured in an article in the September 2024 issue of Amstat News <span class="citation" data-cites="Cunningham24">(<a href="../references.html#ref-Cunningham24" role="doc-biblioref">Cunningham 2024</a>)</span>.</p>
<p>This is an interactive site where you can see the relationships between probability distributions (on the Map) and explore the distributions. You can study visually their properties such as convergence to other distributions, their shapes, the effects of their parameters, and even simulate them.</p>
<p>Spending a few minutes on the Probability Playground will deepen your understanding of any distribution and see how distributions are connected with each other. And you can see how means and variances are derived from first principles.</p>
</div>
</div>
</section>
</section>
</section>
<section id="sec-prob-discrete-dist" class="level2" data-number="13.2">
<h2 data-number="13.2" class="anchored" data-anchor-id="sec-prob-discrete-dist"><span class="header-section-number">13.2</span> Discrete (Univariate) Distributions</h2>
<section id="bernoulli-binary-bernoullipi" class="level3">
<h3 class="anchored" data-anchor-id="bernoulli-binary-bernoullipi">Bernoulli (Binary), Bernoulli<span class="math inline">\((\pi)\)</span></h3>
<p>A Bernoulli (or binary) experiment has two possible outcomes that occur with probabilities <span class="math inline">\(\pi\)</span> and <span class="math inline">\(1 - \pi\)</span>, respectively. The outcomes are coded numerically as <span class="math inline">\(Y = 1\)</span> (with probability <span class="math inline">\(\pi\)</span>) and <span class="math inline">\(Y = 0\)</span>, the subset of the sample space <span class="math inline">\(\Omega\)</span> that maps to <span class="math inline">\(Y = 1\)</span> is often called the “event” or the “success” outcome of the binary distribution, the complement is called the “non-event” or the “failure” outcome.</p>
<p>For example, in the traffic light example the sample space is</p>
<p><span class="math display">\[\Omega = \left\{ rr,ry,rg,yr,yy,yg,gr,gy,gg \right\}\]</span></p>
<p>A binary random variable can be defined as <span class="math inline">\(Y = 1\)</span> if the first light is green. In other words, the event <span class="math inline">\(A = \{ gr,gy,gg\}\)</span> maps to <span class="math inline">\(Y = 1\)</span> and the complement <span class="math inline">\(A^{c}\)</span> maps to <span class="math inline">\(Y = 0\)</span>. The p.m.f of the random variable is then given by the Bernoulli(<span class="math inline">\(\pi\)</span>) distribution as</p>
<p><span class="math display">\[p(y) = \left\{ \begin{matrix}
\pi &amp; Y = 1 \\
1 - \pi &amp; Y = 0
\end{matrix} \right.\ \]</span></p>
<p>Since binary data are often found in studies where events are detrimental—e.g., disease, fraud, death, disapproval—the “event” vernacular is preferred over the “success” vernacular.</p>
<p>The mean and variance of the Bernoulli(<span class="math inline">\(\pi\)</span>) random variable are <span class="math inline">\(\pi\)</span> and <span class="math inline">\(\pi(1 - \pi)\)</span>, respectively. Notice that the variance is largest at <span class="math inline">\(\pi = 0.5\)</span>, when there is greatest uncertainty about which of the two events will occur.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mathstat/Bernoulli.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption>Probability mass function of the Bernoulli(0.7) random variable.</figcaption>
</figure>
</div>
</section>
<section id="binomial-binomialnpi" class="level3">
<h3 class="anchored" data-anchor-id="binomial-binomialnpi">Binomial, Binomial<span class="math inline">\((n,\pi)\)</span></h3>
<p>Let <span class="math inline">\(Y_{1},\cdots,Y_{n}\)</span> be independent binary experiments with the same event probability <span class="math inline">\(\pi\)</span>. Then <span class="math inline">\(X = \sum_{i = 1}^{n}Y_{i}\)</span> has a Binomial(<span class="math inline">\(n,\pi\)</span>) distribution with p.m.f.</p>
<p><span class="math display">\[\Pr(X = x) = \begin{pmatrix}
n \\
x
\end{pmatrix}\pi^{x}(1 - \pi)^{n - x},\ \ \ \ \ x = 0,1,\cdots,n\]</span></p>
<p>The mean and variance of the Binomial(<span class="math inline">\(n,\pi\)</span>) variable can be found easily from its definition as a sum of independent Bernoulli(<span class="math inline">\(\pi\)</span>) variables:</p>
<p><span class="math display">\[\text{E}\lbrack X\rbrack = n\pi\ \ \ \ \ \text{Var}\lbrack X\rbrack = n\pi(1 - \pi)\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mathstat/Binomial.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption>Probability mass function of a Binomial(8,0.3) random variable.</figcaption>
</figure>
</div>
<p>The Bernoulli(<span class="math inline">\(\pi\)</span>) distribution is the special case of the Binomial(<span class="math inline">\(1,\pi\)</span>) distribution.</p>
</section>
<section id="geometric-geometricpi" class="level3">
<h3 class="anchored" data-anchor-id="geometric-geometricpi">Geometric, Geometric<span class="math inline">\((\pi)\)</span></h3>
<p>The Binomial(<span class="math inline">\(n,\pi\)</span>) distribution is the number of events in a series of <span class="math inline">\(n\)</span> Bernoulli(<span class="math inline">\(\pi\)</span>) experiments. The Geometric(<span class="math inline">\(\pi\)</span>) distribution also can be defined in terms of independent Bernoulli(<span class="math inline">\(\pi\)</span>) experiments as the number of trials needed to obtain the first event. There is no theoretical upper bound for the support, as you might need infinitely many binary experiments to realize one event.</p>
<p>The p.m.f. of a Geometric(<span class="math inline">\(\pi\)</span>) random variable, and its mean and variance, are given by</p>
<p><span class="math display">\[p(x) = \pi(1 - \pi)^{x - 1}\ \ \ \ \ \ x = 1,2,\cdots\]</span></p>
<p><span class="math display">\[\text{E}\lbrack X\rbrack = \frac{1}{\pi}\ \ \ \ \ \text{Var}\lbrack X\rbrack = \frac{1 - \pi}{\pi^{2}}\]</span></p>
<p>The following figure shows the p.m.f. of a Geometric(0.5) distribution. The probability to observe an event on the first try is 1/2, on the second try is 1/4, on the third try is 1/8, and so forth.</p>
<p>An interesting property of Geometric(<span class="math inline">\(\pi\)</span>) random variables is their lack of memory:</p>
<p><span class="math display">\[\Pr\left( X &gt; s + t|X &gt; t \right) = \Pr(X &gt; s)\]</span></p>
<p>The probability that we have to try <span class="math inline">\(s\)</span> more times to see the first event is independent of how many times we have tried before (<span class="math inline">\(t\)</span>). To prove this note that <span class="math inline">\(\Pr(X &gt; s)\)</span> means the first event occurs after the <span class="math inline">\(s\)</span><sup>th</sup> try which implies that the first <span class="math inline">\(s\)</span> tries were all non-events: <span class="math inline">\(\Pr{(X &gt; s) = (1 - \pi)^{s}}\)</span>. The conditional probability in question becomes</p>
<p><span class="math display">\[\Pr\left( X &gt; s + t|X &gt; t \right) = \frac{\Pr{(X &gt; s + t,\ X &gt; t)}}{\Pr{(X &gt; t)}} = \frac{\Pr{(X &gt; s + t)}}{\Pr{(X &gt; t)}} = \frac{(1 - \pi)^{s + t}}{(1 - \pi)^{t}} = (1 - \pi)^{s}\]</span></p>
<p>But the last expression is just <span class="math inline">\(\Pr{(X &gt; s)}\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mathstat/Geometric.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption>Probability mass function of the Geometric(0.5) distribution.</figcaption>
</figure>
</div>
<p>Please note that there is a second definition of the Geometric(<span class="math inline">\(\pi\)</span>) distribution in terms of independent Bernoulli(<span class="math inline">\(\pi\)</span>) experiments, namely as the number of non-events before the first event occurs. The p.m.f. of this random variable is</p>
<p><span class="math display">\[p(y) = \pi(1 - \pi)^{y}\ \ \ \ \ y = 0,1,2,\cdots\]</span></p>
<p>With mean <span class="math inline">\(\text{E}\lbrack Y\rbrack = \frac{(1 - \pi)}{\pi}\)</span> and variance <span class="math inline">\(\text{Var}\lbrack Y\rbrack = \frac{(1 - \pi)}{\pi}^{2}\)</span>.</p>
</section>
<section id="negative-binomial-negbinkpi" class="level3">
<h3 class="anchored" data-anchor-id="negative-binomial-negbinkpi">Negative Binomial, NegBin<span class="math inline">\((k,\pi)\)</span></h3>
<p>An extension of the Geometric(<span class="math inline">\(\pi\)</span>) distribution is the Negative Binomial (NegBin(<span class="math inline">\(k,\pi\)</span>)) distribution. In a series of independent Bernoulli(<span class="math inline">\(\pi\)</span>) trials, the number of experiments until the <span class="math inline">\(k\)</span><sup>th</sup> event is observed is a NegBin(<span class="math inline">\(k,\pi\)</span>) random variable.</p>
<p>A NegBin(<span class="math inline">\(k,\pi\)</span>) random variable is thus the sum of <span class="math inline">\(k\)</span> Geometric(<span class="math inline">\(\pi\)</span>) random variables. The p.m.f., mean, and variance of <span class="math inline">\(X \sim NegBin(k,\pi)\)</span> are</p>
<p><span class="math display">\[p(x) = \begin{pmatrix}
x - 1 \\
k - 1
\end{pmatrix}\pi^{k}(1 - \pi)^{x - k},\ \ \ \ \ x = k,k + 1,\cdots\]</span></p>
<p><span class="math display">\[\text{E}\lbrack X\rbrack = \frac{k}{\pi}\ \ \ \ \ \text{Var}\lbrack X\rbrack = \frac{k(1 - \pi)}{\pi^{2}}\]</span></p>
<p>The negative binomial distribution appears in many parameterizations. A popular form is in terms of the number of non-events before the <span class="math inline">\(k\)</span><sup>th</sup> event occurs. This changes the support of the random variable from <span class="math inline">\(x = k,k + 1,\cdots\)</span> to <span class="math inline">\(y = 0,1,\cdots.\)</span> The p.m.f,. mean, and variance of that random variable are</p>
<p><span class="math display">\[p(y) = \begin{pmatrix}
y + k - 1 \\
k - 1
\end{pmatrix}\pi^{k}(1 - \pi)^{y},\ \ \ \ y = 0,1,\cdots\]</span></p>
<p><span class="math display">\[\text{E}\lbrack Y\rbrack = \frac{k(1 - \pi)}{\pi}\ \ \ \ \text{Var}\lbrack Y\rbrack = \frac{k(1 - \pi)}{\pi^{2}}\]</span></p>
<p>The p.m.f. for a NegBin(5,0.7) in this parameterization is shown in <a href="#fig-sl-negbin" class="quarto-xref">Figure&nbsp;<span>13.1</span></a>.</p>
<div id="fig-sl-negbin" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sl-negbin-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/mathstat/NegativeBinomial.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:75.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sl-negbin-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.1: Probability mass function of a NegBin(5,0.7) random variable.
</figcaption>
</figure>
</div>
<p>An important application of the negative binomial distribution is in mixing models. These are models where the parameters of a distribution are assumed to be random variables rather than constants. This mechanism introduces additional variability into the system and is applied when the observed data appear more dispersed than a distribution permits. This condition is called <strong>overdispersion</strong>. For example, a binomial random variable has variance <span class="math inline">\(n\pi(1 - \pi)\)</span>; the variability is a function of the mean <span class="math inline">\(n\pi\)</span>. When the observed data suggest that this relationship between mean and variance does not hold—and typically the observed variability is greater than the nominal variance—one can treat <span class="math inline">\(n\)</span> or <span class="math inline">\(\pi\)</span> as a random variable. If you assume that <span class="math inline">\(n\)</span> follows a Poisson distribution (see next), then the marginal distribution of the data is also Poisson. If you start with a Poisson distribution and assume that its parameter follows a Gamma distribution, the resulting marginal distribution of the data is negative binomial.</p>
</section>
<section id="poisson-poissonlambda" class="level3">
<h3 class="anchored" data-anchor-id="poisson-poissonlambda">Poisson, Poisson<span class="math inline">\((\lambda)\)</span></h3>
<p>The Poisson distribution is a common probability model for count variables that represent counts per unit, rather than counts that can be converted to proportions. For example, the number of chocolate chips on a cookie, the number of defective parts per day on an assembly line or the number of fish caught per day can be modeled as Poisson random variables.</p>
<p>For the Poisson assumption to be met when modeling event counts over some unit, e.g.&nbsp;time, the rate at which events occur cannot depend on the occurrence of any events and events have to occur independently. For example, if the number of customer calls to a service center increases sharply after the release of a new software product, then the rate of events (calls) per day is not constant across days. This can still be modeled as a Poisson process where <span class="math inline">\(\lambda\)</span> depends on other input variables such as the time since release of the new product. If one customer’s call makes it more likely that another customer calls into the service center, the Poisson assumption is not valid.</p>
<p>The random variable <span class="math inline">\(Y\)</span> has a Poisson(<span class="math inline">\(\lambda\)</span>) distribution if its probability mass function is given by</p>
<p><span class="math display">\[p(y) = \frac{e^{- \lambda\ }\lambda^{y}}{y!},\ \ \ \ y = 0,1,\cdots\]</span></p>
<p>The mean and variance of a Poisson(<span class="math inline">\(\lambda\)</span>) variable are the same, <span class="math inline">\(\text{E}\lbrack Y\rbrack = \text{Var}\lbrack Y\rbrack = \lambda\)</span>. Although the random variable <span class="math inline">\(Y\)</span> takes on only non-negative integer value, the parameter <span class="math inline">\(\lambda\)</span> is a real number.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mathstat/Poisson.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption>Probability mass function of a Poisson(3) random variable.</figcaption>
</figure>
</div>
<p>For larger values of <span class="math inline">\(\lambda\)</span> the distribution shifts to the right and becomes more symmetric. For small values of <span class="math inline">\(\lambda\)</span> the mass function is very asymmetric and concentrated at small values of <span class="math inline">\(Y\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mathstat/Poisson2.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption>Probability mass function of a Poisson(0.2) random variable.</figcaption>
</figure>
</div>
<p>This is the basis for the Poisson approximation to Binomial probabilities. In a Binomial(<span class="math inline">\(n,\pi\)</span>) process, if <span class="math inline">\(n \rightarrow \infty\)</span> and <span class="math inline">\(\pi\)</span> shrinks so that <span class="math inline">\(n\pi\)</span> converges to a constant <span class="math inline">\(\lambda\)</span>, then the Binomial(<span class="math inline">\(n,\pi\)</span>) process can be approximated as a Poisson(<span class="math inline">\(\lambda\)</span>) process. The next figure compares a Binomial(200,0.05) mass function to the p.m.f. of the Poisson(10). At least visually, the histograms are almost indistinguishable.</p>
<div id="fig-sl-poisson-approx-bin" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sl-poisson-approx-bin-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/mathstat/PoissonBinomial.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:75.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sl-poisson-approx-bin-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.2: Poisson approximation to the Binomial, <span class="math inline">\(n\pi \rightarrow \lambda\)</span>.
</figcaption>
</figure>
</div>
<p>For example, the probability that three sixes turn up when three dice are rolled is <span class="math inline">\(\frac{1}{216} = 0.00463\)</span>. If the three dice are rolled 200 times, what is the probability that at least one triple six shows up? If <span class="math inline">\(Y\)</span> is the number of triple sixes out of 200, then the binomial probability is calculated as</p>
<p><span class="math display">\[1 - \begin{pmatrix}
200 \\
0
\end{pmatrix}\left( \frac{1}{216} \right)^{0}\left( \frac{215}{216} \right)^{200} = 0.6046\]</span></p>
<p>and the Poisson approximation is</p>
<p><span class="math display">\[1 - \frac{\left( \frac{200}{216} \right)^{0}}{0!}\exp\left\{ - \frac{200}{216} \right\} = 0.6038\]</span></p>
<p>As <span class="math inline">\(\lambda \rightarrow \infty\)</span>, the Poisson p.m.f. approaches the shape of a Gaussian (normal) distribution. The normal approximation is sometimes made for sufficiently large values of the Poisson parameter, <span class="math inline">\(\lambda &gt; 20\)</span>. <a href="#fig-sl-normal-approx-poi" class="quarto-xref">Figure&nbsp;<span>13.3</span></a> shows the empirical histogram and density for 10,000 random draws from a Poisson(20) distribution and a Gaussian distribution with the same mean and variance as the Poisson. Some folks recommend the Gaussian approximation for the Poisson for <span class="math inline">\(\lambda &gt; 100\)</span> or even <span class="math inline">\(\lambda &gt; 1000\)</span>.</p>
<div id="fig-sl-normal-approx-poi" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sl-normal-approx-poi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/mathstat/PoissonNormal.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:75.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sl-normal-approx-poi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.3: Normal approximation to the Poisson distribution for <span class="math inline">\(\lambda = 20\)</span>.
</figcaption>
</figure>
</div>
<p>Because the events being counted occur independently of each other, a Poisson distribution is divisible. You can think of a Poisson(<span class="math inline">\(\lambda = 5\)</span>) variable as the sum of five Poisson(1) variables. The result of one variable producing on average 5 events per time units or five variables each producing on average one event per unit is the same. More generally, if <span class="math inline">\(Y_{1},\cdots,Y_{n}\)</span> are independent random variables with respective Poisson(<span class="math inline">\(\lambda_{i}\)</span>) distributions, then their sum <span class="math inline">\(\sum_{i}^{}Y_{i}\)</span> follows a Poisson distribution with mean <span class="math inline">\(\lambda = \sum_{i}^{}\lambda_{i}\)</span>.</p>
</section>
</section>
<section id="sec-prob-cont-dist" class="level2" data-number="13.3">
<h2 data-number="13.3" class="anchored" data-anchor-id="sec-prob-cont-dist"><span class="header-section-number">13.3</span> Continuous (Univariate) Distributions</h2>
<p>For continuous random variables, the probability that the variable takes on any particular value is zero. To make meaningful probability statements we consider integration of the probability density function (p.d.f.) over sets, for example, intervals:</p>
<p><span class="math display">\[\Pr{(a \leq X \leq b)} = \int_{a}^{b}{f(x)dx}\]</span></p>
<p>The density function satisfies <span class="math inline">\(\int_{- \infty}^{\infty}{f(x)dx} = 1\)</span> and can be obtained by differentiating the c.d.f <span class="math inline">\(F(x) = \Pr{(X \leq x)}\)</span>; <span class="math inline">\(f(x) = \frac{dF(x)}{dx}\)</span>.</p>
<section id="uniform-uab" class="level3">
<h3 class="anchored" data-anchor-id="uniform-uab">Uniform, U<span class="math inline">\((a,b)\)</span></h3>
<p>If a random variable has a continuous uniform distribution on the interval <span class="math inline">\(\lbrack a,b\rbrack\)</span>, denoted <span class="math inline">\(Y \sim U(a,b)\)</span>, its p.d.f. is given by</p>
<p><span class="math display">\[f(x) = \left\{ \begin{matrix}
\frac{1}{(b - a)} &amp; a \leq x \leq b \\
0 &amp; \text{otherwise}
\end{matrix} \right.\ \]</span></p>
<p>The mean and variance of a <span class="math inline">\(U(a,b)\)</span> random variable are</p>
<p><span class="math display">\[\text{E}\lbrack Y\rbrack = \frac{a + b}{2}\ \ \ \ \ \ \text{Var}\lbrack Y\rbrack = \frac{(b - a)^{2}}{12}\]</span></p>
</section>
<section id="exponential-expolambda" class="level3">
<h3 class="anchored" data-anchor-id="exponential-expolambda">Exponential, Expo<span class="math inline">\((\lambda)\)</span></h3>
<p>The exponential distribution is a useful probability model for modeling continuous lifetimes. It is related to Poisson processes. If events occur continuously and independently at a constant rate <span class="math inline">\(\lambda\)</span>, the number of events is a Poisson random variable. The time between the events is an exponential random variable, denoted <span class="math inline">\(Y \sim\)</span> Expo(<span class="math inline">\(\lambda\)</span>).</p>
<p><span class="math display">\[p(y) = \lambda e^{- \lambda y},\ \ \ \ y \geq 0\]</span></p>
<p><span class="math display">\[F(y) = 1 - e^{- \lambda y},\ \ \ y \geq 0\]</span></p>
<p><span class="math display">\[\text{E}\lbrack Y\rbrack = \frac{1}{\lambda}\ \ \ \ \ \ \text{Var}\lbrack Y\rbrack = \frac{1}{\lambda^{2}}\]</span></p>
<p>Like the discrete Geometric(<span class="math inline">\(\pi\)</span>) distribution, the Expo(<span class="math inline">\(\lambda\)</span>) distribution is forgetful,</p>
<p><span class="math display">\[\Pr{(Y &gt; s + t|Y &gt; t)} = \Pr{(Y &gt; s)}\]</span></p>
<p>and it turns out that no other continuous function has this <strong>memoryless</strong> property. This property is easily proven using <span class="math inline">\(\Pr(Y &gt; y) = 1 - F(y) = e^{- \lambda y}\)</span>:</p>
<p><span class="math display">\[\Pr\left( Y &gt; t + s \middle| Y &gt; t \right) = \frac{\Pr{(Y &gt; t + s,Y &gt; t)}}{\Pr{(Y &gt; t)}} = \frac{Pr(Y &gt; t + s)}{Pr(Y &gt; t)} = \frac{e^{- \lambda(t + s)}}{e^{- \lambda t}} = e^{- \lambda s}\]</span></p>
<p>The memoryless property of the exponential distribution makes it <strong>not</strong> a good model for human lifetimes. The probability that a 20-year-old will live another 10 years is not the same as the probability that a 75-year-old will live another 10 years. The exponential distribution implies that this would be the case. When modeling earthquakes, it might be reasonable that the probability of an earthquake in the next ten years is the same, regardless of when the last earthquake occurred—the exponential distribution would then be reasonable.</p>
<p>You don’t have to worry about whether other distributions have this memoryless property in applications where lack of memory would not be appropriate. The exponential distribution is defined by this property, it is the only continuous distribution with lack of memory.</p>
</section>
<section id="gamma-gammaalphabeta" class="level3">
<h3 class="anchored" data-anchor-id="gamma-gammaalphabeta">Gamma, Gamma<span class="math inline">\((\alpha,\beta)\)</span></h3>
<p>The Expo(<span class="math inline">\(\lambda\)</span>) distribution is a special case of a broader family of distributions, the Gamma(<span class="math inline">\(\alpha,\beta\)</span>) distribution. A random variable <span class="math inline">\(Y\)</span> is said to have a Gamma(<span class="math inline">\(\alpha,\beta\)</span>) distribution if its density function is</p>
<p><span class="math display">\[f(y) = \frac{1}{\beta^{\alpha}\Gamma(\alpha)}y^{\alpha - 1}e^{- y/\beta},\ \ \ \ \ y \geq 0,\ \alpha,\beta &gt; 0\]</span></p>
<p>The mean and variance of a Gamma(<span class="math inline">\(\alpha,\beta\)</span>) random variable are given by</p>
<p><span class="math display">\[\text{E}\lbrack Y\rbrack = \alpha\beta\ \ \ \ \ \text{Var}\lbrack Y\rbrack = \alpha\beta^{2}\]</span></p>
<p><span class="math inline">\(\alpha\)</span> is called the shape parameter of the distribution and <span class="math inline">\(\beta\)</span> is called the scale parameter. Varying <span class="math inline">\(\alpha\)</span> affects the shape and varying <span class="math inline">\(\beta\)</span> affects the units of measurement.</p>
<p>The term <span class="math inline">\(\Gamma(\alpha)\)</span> in the denominator of the density function is called the Gamma function,</p>
<p><span class="math display">\[\Gamma(\alpha) = \int_{0}^{\infty}{y^{\alpha - 1}e^{- y}}dy\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Fun fact: if <span class="math inline">\(\alpha\)</span> is an integer, <span class="math inline">\(\Gamma(\alpha) = (\alpha - 1)!\)</span></p>
</div>
</div>
<p>The exponential random variable introduced earlier is a special case of the Gamma family, the Expo(<span class="math inline">\(1/\beta\)</span>) is the same as the Gamma(1,<span class="math inline">\(\beta\)</span>). Gamma-distributed random variables occur in applications of waiting times. Suppose that cars arrive at an intersection at a rate of one every two minutes. The time you have to wait until the 5th car arrives at the intersection is a Gamma(<span class="math inline">\(\alpha=5,\beta=2\)</span>) random variable. If lightbulbs last 5 years on average and are replaced when they fail, the time a box of six lasts is a Gamma(<span class="math inline">\(\alpha=6, \beta=5\)</span>) random variable <span class="citation" data-cites="Cunningham24">(<a href="../references.html#ref-Cunningham24" role="doc-biblioref">Cunningham 2024</a>)</span>.</p>
<p>Another special case of the gamma-type random variables is the chi-square random variable. A random variable <span class="math inline">\(Y\)</span> is said to have a chi-squared distribution with <span class="math inline">\(\nu\)</span> degrees of freedom, denoted <span class="math inline">\(\chi_{\nu}^{2}\)</span>, if <span class="math inline">\(Y\)</span> is a Gamma(<span class="math inline">\(\frac{\nu}{2},2\)</span>) random variable. More on <span class="math inline">\(\chi^{2}\)</span> random variables below after we introduced sampling from a Gaussian distribution.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mathstat/Gamma.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption>Densities of Gamma(3,1/2) (solid), Gamma(2,2) (dashed) and Gamma(1,1/2) random variables. The Gamma(2,2) is also a <span class="math inline">\(\chi_{4}^{2}\)</span> distribution. The Gamma(1,1/2) is a Expo(2) distribution.</figcaption>
</figure>
</div>
</section>
<section id="beta-betaalphabeta" class="level3">
<h3 class="anchored" data-anchor-id="beta-betaalphabeta">Beta, Beta<span class="math inline">\((\alpha,\beta)\)</span></h3>
<p>A random variable has a Beta distribution with parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, denoted <span class="math inline">\(Y \sim \text{Beta}(\alpha,\beta)\)</span>, if its density function is given by <span class="math display">\[
f(y) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\, y^{\alpha-1}\,(1-y)^{\beta-1}\quad 0 &lt; y &lt; 1
\]</span> The family of beta distributions takes on varied shapes as seen in <a href="#fig-sl-beta" class="quarto-xref">Figure&nbsp;<span>13.4</span></a>.</p>
<div id="fig-sl-beta" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sl-beta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/mathstat/Beta.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:75.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sl-beta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.4: Densities of Beta(<span class="math inline">\(\alpha,\beta\)</span>) distributions
</figcaption>
</figure>
</div>
<p>The ratio of Gamma functions is known as the Beta function and the density can also be written as <span class="math inline">\(f(y) = y^{\alpha-1}(1-y)^{(\beta-1)} / B(\alpha,\beta)\)</span> where <span class="math inline">\(B(\alpha,\beta) = \Gamma(\alpha)\Gamma(\beta)/\Gamma(\alpha+\beta)\)</span>.</p>
<p>The mean of a <span class="math inline">\(\text{Beta}(\alpha,\beta])\)</span> random variable is <span class="math display">\[\text{E}[Y] = \frac{\alpha}{\alpha+\beta}
\]</span> and the variance is <span class="math display">\[
\text{Var}[Y] = \frac{\alpha\beta}{(\alpha+\beta)^2 (\alpha+\beta+1)} = \text{E}[Y]\frac{\beta}{(\alpha+\beta)(\alpha+\beta+1)}
\]</span></p>
<p>The support of a Beta random variable is continuous on [0,1], which makes it an attractive candidate for modeling proportions, for example, the proportion of time a vehicle is in maintenance or the proportion of disposable income spent on rent. <span class="citation" data-cites="Cunningham24">Cunningham (<a href="../references.html#ref-Cunningham24" role="doc-biblioref">2024</a>)</span> gives the following examples of Beta distributions:</p>
<ul>
<li>Five numbers are chosen at random from the interval (0,1) and arranged in order. The middle number has a Beta(3,3) distribution.</li>
<li>The probability that an apple will be unblemished has a Beta(8,0.5) distribution.</li>
<li>Relative humidity for Auckland, New Zealand for the first six months of 2022 has an approximate Beta(5.93, 1.78) distribution.</li>
</ul>
<p>The Beta distribution can also be used for random variables that are defined on a different scale, <span class="math inline">\(a &lt; Y &lt; b\)</span> by transforming to the [0,1] scale: <span class="math inline">\(Y^* = (Y-a)/(b-a)\)</span>.</p>
<p>Here are some interesting properties and relationships for Beta random variables:</p>
<ul>
<li><p>The <span class="math inline">\(\text{Beta}(1,1)\)</span> is a continuous uniform random variable on [0,1].</p></li>
<li><p>If <span class="math inline">\(Y \sim \text{Beta}(\alpha,\beta)\)</span>, then <span class="math inline">\(1 - Y \sim \text{Beta}(\beta,\alpha)\)</span>.</p></li>
<li><p>The relationship between Gamma and Beta functions hints at a relationship between Gamma and Beta random variables. Indeed, there is one. If <span class="math inline">\(X \sim \text{Gamma}(\alpha_1,\beta)\)</span> and <span class="math inline">\(Y \sim \text{Gamma}(\alpha_2,\beta)\)</span>, and <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then the ratio <span class="math inline">\(X/(X+Y)\)</span> follows a Beta<span class="math inline">\((\alpha_1,\alpha_2)\)</span> distribution:</p></li>
</ul>
<p><span class="math display">\[
\frac{X}{X+Y} \sim \text{Beta}(\alpha_1,\alpha_2)
\]</span></p>
<ul>
<li>If <span class="math inline">\(X\)</span> is an Expo(<span class="math inline">\(\lambda\)</span>) random variable then <span class="math inline">\(e^{−X} \sim \text{Beta}(\lambda,1)\)</span>.</li>
</ul>
<p>Since <span class="math inline">\(Y\)</span> is continuous, we can define the support of the Beta random variable as <span class="math inline">\(0 \le y \le 1\)</span> or as <span class="math inline">\(0 &lt; y &lt; 1\)</span>. The probability that the continuous random variable takes on exactly the value 0 or 1 is zero. However, in practice you can observe proportions at the extreme of the support; the proportion of income spent on rent by a homeowner is zero.</p>
</section>
<section id="gaussian-normal-gmusigma2" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-normal-gmusigma2">Gaussian (Normal), G<span class="math inline">\((\mu,\sigma^2)\)</span></h3>
<p>The Gaussian (or Normal) distribution is arguably the most important continuous distribution in all of probability and statistics. A random variable <span class="math inline">\(Y\)</span> has a Gaussian distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span> if its density function is</p>
<p><span class="math display">\[f(y) = \frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{ - \frac{1}{2\sigma^{2}}(y - \mu)^{2} \right\},\ \ \ \ \text{-}\infty &lt; y &lt; \infty\]</span></p>
<p>The notation <span class="math inline">\(Y \sim G\left( \mu,\sigma^{2} \right)\)</span> or <span class="math inline">\(Y \sim N\left( \mu,\sigma^{2} \right)\)</span> is common.</p>
<p>The Gaussian distribution has the famous bell shape, symmetric about the mean $\mu$.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mathstat/Gaussians.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption>Gaussian distributions: <span class="math inline">\(G(4,1)\)</span> (solid), <span class="math inline">\(G(5,4)\)</span> (dashed), and <span class="math inline">\(G(8, {0.75}^{2})\)</span> (dotted).</figcaption>
</figure>
</div>
<p>A special version is the <strong>standard</strong> Gaussian (standard normal) distribution <span class="math inline">\(Z \sim G(0,1)\)</span> with density</p>
<p><span class="math display">\[f(z) = \frac{1}{\sqrt{2\pi}}\exp\left\{ - \frac{1}{2}y^{2} \right\}\]</span></p>
<p>The standard Gaussian is also referred to as the <strong>unit</strong> normal distribution.</p>
<p>Gaussian random variables have some interesting properties. For example, linear combinations of Gaussian random variables are Gaussian distributed. If <span class="math inline">\(Y\sim G\left( \mu,\sigma^{2} \right)\)</span>, then <span class="math inline">\(X = aY + b\)</span> has distribution <span class="math inline">\(G(a\mu + b,a^{2}\sigma^{2})\)</span>. As an example, if <span class="math inline">\(Y\sim G\left( \mu,\sigma^{2} \right)\)</span>, then</p>
<p><span class="math display">\[Z = \frac{Y - \mu}{\sigma}\]</span></p>
<p>has a standard Gaussian distribution. You can express probabilities about <span class="math inline">\(Y\)</span> in terms of probabilities about <span class="math inline">\(Z\)</span>:</p>
<p><span class="math display">\[\Pr{(X \leq x)} = \Pr\left( Z \leq \frac{x - \mu}{\sigma} \right)\]</span></p>
<p>Because linear functions of Gaussian random variables are Gaussian random variables, it is easy to establish the distribution of the sample mean <span class="math inline">\(\overline{Y} = \frac{1}{n}\sum_{i}^{}Y_{i}\)</span> in a random sample from a <span class="math inline">\(G(\mu,\sigma^{2})\)</span> distribution. First, if we take a random sample from <strong>any</strong> distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span>, then the sample mean <span class="math inline">\(\overline{Y}\)</span> has mean and variance</p>
<p><span class="math display">\[\text{E}\left\lbrack \overline{Y} \right\rbrack = \mu\ \ \ \ \ \ \ \ \ \text{Var}\left\lbrack \overline{Y} \right\rbrack = \frac{\sigma^{2}}{n}\]</span></p>
<p>This follows from the linearity of the expectation operator and the independence of the observations in the random sample. If, in addition, the <span class="math inline">\(Y_{i} \sim G\left( \mu,\sigma^{2} \right)\)</span>, then</p>
<p><span class="math display">\[\overline{Y} \sim G\left( \mu,\frac{\sigma^{2}}{n} \right)\]</span></p>
<p>The sample mean of a random sample from a Gaussian distribution also has a Gaussian distribution. Do we know anything about the distribution of <span class="math inline">\(\overline{Y}\)</span> if we randomly sample a non-Gaussian distribution? Yes, we do. That is the domain of the <strong>central limit theorem</strong>.</p>
<div class="definition">
<div class="definition-header">
<p>Central Limit Theorem</p>
</div>
<div class="definition-container">
<p>Let <span class="math inline">\(Y_{1},\cdots,Y_{n}\)</span> be independent and identically distributed random variables with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^{2} &lt; \infty.\)</span> The distribution of</p>
<p><span class="math display">\[\frac{\overline{Y} - \mu}{\sigma/\sqrt{n}}\]</span></p>
<p>converges to that of a standard normal random variable as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
</div>
</div>
<p>These amazing properties contribute to the Gaussian distribution being the most important probability distribution. Most continuous attributes actually do not behave like Gaussian random variables at all, they often have a restricted range or are skewed or have heavier tails than a Gaussian random variable. There is really nothing <em>normal</em> about this distribution, which is one reason why we prefer the name Gaussian over Normal distribution.</p>
<div class="callout callout-style-default callout-note callout-titled" title="What is normal?">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What is normal?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We avoid the use of the term normal to describe the Gaussian distribution for another reason: the connection of the concept of normality and this distribution to eugenics. The Belgian astronomer, statistician and mathematician Adolphe Quetelet (1796–1847) introduced the generalized notion of the normal. He studied the distribution of physical attributes and determined that the normal, the most representative, value of an attribute is its average. Discrepancies above and below the average were considered “errors”. Early applications of the Gaussian distribution were in the study of measurement errors. C.F. Gauss used the distribution to represent errors in the measurement of celestial bodies.</p>
<p>Prior to Quetelet, the view of “norm” and “normality” was associated with carpentry. The carpenter square is also called the norm, and in normal construction everything is at right angles. With Quetelet, the middle of the distribution, the center, became the “new normal.” There is nothing objectionable so far.</p>
<p>However, this view did not sit well with Francis Galton, who introduced the term eugenics. Galton replaced the term “error” with standard deviation and considered variability within a human population as potential for racial progress <span class="citation" data-cites="GrueHeiberg">(<a href="../references.html#ref-GrueHeiberg" role="doc-biblioref">Grue and Heiberg 2006</a>)</span>. The bell shape of the normal distribution was not used to focus our attention on the average, as Quetelet did. Galton introduced quartiles to categorize the area under the normal into sections of greater and lesser genetic worth. That is not normal!</p>
</div>
</div>
</div>
</section>
</section>
<section id="sec-prob-sampling-dist" class="level2" data-number="13.4">
<h2 data-number="13.4" class="anchored" data-anchor-id="sec-prob-sampling-dist"><span class="header-section-number">13.4</span> Sampling Distributions</h2>
<p>Many important probability distributions are related to sampling data from Gaussian processes, <span class="math inline">\(t\)</span>, <span class="math inline">\(\chi^{2}\)</span>, and <span class="math inline">\(F\)</span> distributions most importantly.</p>
<section id="chi-square-chi2_nu" class="level3">
<h3 class="anchored" data-anchor-id="chi-square-chi2_nu">Chi-Square, <span class="math inline">\(\chi^2_\nu\)</span></h3>
<p>Let <span class="math inline">\(Z_{1},\cdots,Z_{k}\)</span> denote independent standard normal random variables <span class="math inline">\(G(0,1)\)</span>. Then</p>
<p><span class="math display">\[X = Z_{1}^{2} + Z_{2}^{2} + \cdots + Z_{k}^{2}\]</span></p>
<p>has p.d.f.</p>
<p><span class="math display">\[f(x) = \frac{1}{2^{\frac{k}{2}}\Gamma\left( \frac{k}{2} \right)}x^{\frac{k}{2}}e^{- x/2},\ \ \ \ \ x \geq 0\]</span></p>
<p>This is known as the Chi-square distribution with <span class="math inline">\(k\)</span> degrees of freedom, abbreviated <span class="math inline">\(\chi_{k}^{2}\)</span>. The mean and variance of a <span class="math inline">\(\chi_{k}^{2}\)</span> random variable is <span class="math inline">\(\text{E}\lbrack X\rbrack = k\)</span> and <span class="math inline">\(\text{Var}\lbrack X\rbrack = 2k\)</span>.</p>
<p>The degrees of freedom can be thought of as the number of independent pieces of information that contribute to the <span class="math inline">\(\chi^{2}\)</span> variable. Here, that is the number of independent <span class="math inline">\(G(0,1)\)</span> variables. Since the <span class="math inline">\(\chi^{2}\)</span> variable is the sum of their squared values, the density shifts more to the right as the degrees of freedom increase.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mathstat/ChiSquare.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption>Probability density function of a <span class="math inline">\(\chi_4^2\)</span> and a <span class="math inline">\(\chi_8^2\)</span> random variable. <span class="math inline">\(\chi^2\)</span> variables are skewed to the right, the density shifts to the right with increasing degrees of freedom.</figcaption>
</figure>
</div>
<p><span class="math inline">\(\chi^{2}\)</span> distributions are important to capture the sample distributions of dispersion statistics. If <span class="math inline">\(Y_{1},\cdots,Y_{n}\)</span> are a random sample from a <span class="math inline">\(G\left( \mu,\sigma^{2} \right)\)</span> distribution, and the sample variance is</p>
<p><span class="math display">\[S^{2} = \frac{1}{n - 1}\sum_{i = 1}^{n}\left( Y_{i} - \overline{Y} \right)^{2}\]</span></p>
<p>Then the random variable <span class="math inline">\(\frac{(n - 1)S^{2}}{\sigma^{2}}\)</span> follows a <span class="math inline">\(\chi_{n - 1}^{2}\)</span> distribution. It follows immediately that <span class="math inline">\(S^{2}\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^{2}\)</span>:</p>
<p><span class="math display">\[\text{E}\left\lbrack \frac{(n - 1)S^{2}}{\sigma^{2}} \right\rbrack = n - 1\]</span></p>
<p><span class="math display">\[\text{E}\left\lbrack S^{2} \right\rbrack = \sigma^{2}\]</span></p>
</section>
<section id="students-t-t_nu" class="level3">
<h3 class="anchored" data-anchor-id="students-t-t_nu">Students’ t, <span class="math inline">\(t_\nu\)</span></h3>
<p>In the Gaussian case you can also show that <span class="math inline">\(\overline{Y}\)</span> and <span class="math inline">\(S^{2}\)</span> are independent random variables. This is important because of another distributional result: if <span class="math inline">\(Z \sim G(0,1)\)</span> and <span class="math inline">\(U \sim \chi_{\nu}^{2}\)</span>, and <span class="math inline">\(Z\)</span> and <span class="math inline">\(U\)</span> are independent, then the ratio</p>
<p><span class="math display">\[T = \frac{Z}{\sqrt{U/\nu}}\]</span></p>
<p>has a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(\nu\)</span> degrees of freedom. In honor of Student, the pseudonym used by William Gosset for publishing statistical research while working at Guinness Breweries, the <span class="math inline">\(t\)</span> distribution is also known as Student’s <span class="math inline">\(t\)</span> distribution or Student’s distribution for short.</p>
<p>As with the <span class="math inline">\(\chi_{\nu}^{2}\)</span> distributions, the shape of the <span class="math inline">\(t_{\nu}\)</span> distribution depends on the degrees of freedom <span class="math inline">\(\nu\)</span>. However, the <span class="math inline">\(t_{\nu}\)</span> distributions are all symmetric about zero and the degrees of freedom affect how heavy the tails are.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mathstat/Tdistributions.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption>Densities for <span class="math inline">\(t\)</span> distributions with 2, 4, 10 degrees of freedom, and a standard normal density.</figcaption>
</figure>
</div>
<p>As the degrees of freedom grow, the <span class="math inline">\(t\)</span> density approaches that of the standard normal distribution.</p>
<p>We can now combine the results about the sampling distributions of <span class="math inline">\(\overline{Y}\)</span> and <span class="math inline">\(S^{2}\)</span> to derive the distribution of</p>
<p><span class="math display">\[\frac{\overline{Y} - \mu}{S/\sqrt{n}}\]</span></p>
<p>We know that <span class="math inline">\(\overline{Y} \sim G\left( \mu,\frac{\sigma^{2}}{n} \right)\)</span> and that <span class="math inline">\(\frac{(n - 1)S^{2}}{\sigma^{2}}\)</span> follows a <span class="math inline">\(\chi_{n - 1}^{2}\)</span> distribution. Furthermore, <span class="math inline">\(\overline{Y}\)</span> and <span class="math inline">\(S^{2}\)</span> are independent. Taking ratios as required by the definition of a <span class="math inline">\(t\)</span> random variable yields</p>
<p><span class="math display">\[\frac{\frac{\overline{Y} - \mu}{\sigma/\sqrt{n}}}{\sqrt{S^{2}{/\sigma}^{2}}} = \frac{\overline{Y} - \mu}{S/\sqrt{n}} \sim t_{n - 1}\]</span></p>
</section>
<section id="f-f_nu_1-nu_2" class="level3">
<h3 class="anchored" data-anchor-id="f-f_nu_1-nu_2">F, <span class="math inline">\(F_{\nu_1, \nu_2}\)</span></h3>
<p>If <span class="math inline">\(\chi_{1}^{2}\)</span> and <span class="math inline">\(\chi_{2}^{2}\)</span> are independent chi-square random variables with <span class="math inline">\(\nu_{1}\)</span> and <span class="math inline">\(\nu_{2}\)</span> degrees of freedom, respectively, then the ratio</p>
<p><span class="math display">\[F = \frac{\chi_{1}^{2}/v_{1}}{\frac{\chi_{2}^{2}}{\nu_{2}}}\]</span></p>
<p>follows an <span class="math inline">\(F\)</span> distribution with <span class="math inline">\(\nu_{1}\)</span> numerator and <span class="math inline">\(\nu_{2}\)</span> denominator degrees of freedom. We denote this fact <span class="math inline">\(F\sim F_{\nu_{1},\nu_{2}}\)</span>. The mean and variance of the <span class="math inline">\(F\)</span> distribution are</p>
<p><span class="math display">\[\text{E}\lbrack F\rbrack = \frac{\nu_{2}}{\nu_{2} - 2}\ \ \ \ \ \ \ \text{Var}\lbrack F\rbrack = \frac{2\nu_{2}^{2}(\nu_{1} + \nu_{2} + 2)}{\nu_{1}\left( \nu_{2} - 2 \right)^{2}(\nu_{2} - 4)}\]</span></p>
<p>The mean exists only if <span class="math inline">\(\nu_{2} &gt; 2\)</span> and the variance exists onlu if <span class="math inline">\(\nu_{2} &gt; 4\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mathstat/Fdistributions.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption>Density functions of an <span class="math inline">\(F_{4,12}\)</span> and an <span class="math inline">\(F_{12,20}\)</span> distribution.</figcaption>
</figure>
</div>
<p>When sampling from a Gaussian distribution we established that <span class="math inline">\(\frac{(n - 1)S^{2}}{\sigma^{2}}\)</span> follows a <span class="math inline">\(\chi_{n - 1}^{2}\)</span> distribution. Suppose that we have two samples, one of size <span class="math inline">\(n_{1}\)</span> from a <span class="math inline">\(G(\mu_{1},\sigma_{1}^{2})\)</span> distribution and one of size <span class="math inline">\(n_{2}\)</span> from a <span class="math inline">\(G(\mu_{2},\sigma_{2}^{2})\)</span> distribution. If the estimators of the sample variances in the two samples are denoted <span class="math inline">\(S_{1}^{2}\)</span> and <span class="math inline">\(S_{2}^{2}\)</span>, then the ratio</p>
<p><span class="math display">\[\frac{\frac{S_{1}^{2}}{\sigma_{1}^{2}}}{S_{2}^{2}/\sigma_{2}^{2}}\]</span></p>
<p>has an <span class="math inline">\(F_{n_{1} - 1,n_{2} - 1}\)</span> distribution.</p>
<p><span class="math inline">\(F\)</span> distributions play an important role in the analysis of variance, where the ratios are ratios of mean squares.</p>
<p>Recall that the <span class="math inline">\(T\)</span> random variable in the Gaussian case can be written as</p>
<p><span class="math display">\[T = \frac{\frac{\overline{Y} - \mu}{\sigma/\sqrt{n}}}{\sqrt{S^{2}{/\sigma}^{2}}}\]</span></p>
<p>The numerator is a <span class="math inline">\(G(0,1)\)</span> variable, so squaring it gives a <span class="math inline">\(\chi_{1}^{2}\)</span> variable. The square of the denominator is a scaled <span class="math inline">\(\chi_{n - 1}^{2}\)</span> variable, <span class="math inline">\(S^{2}/\sigma^{2}\)</span>. Also, because <span class="math inline">\(\overline{Y}\)</span> and <span class="math inline">\(S^{2}\)</span> are independent, the squares of the numerator and denominator are independent. It thus follows that</p>
<p><span class="math display">\[T^{2} = \frac{\left( \overline{Y} - \mu \right)^{2}}{S^{2}/n}\]</span></p>
<p>follows an <span class="math inline">\(F_{1,n - 1}\)</span> distribution.</p>
<p>This fact is used in software packages that might report the result of an <span class="math inline">\(F\)</span>-test instead of the result of a two-sided <span class="math inline">\(t\)</span>-test. The two are equivalent and the <span class="math inline">\(p\)</span>-values are the same.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Cunningham24" class="csl-entry" role="listitem">
Cunningham, Adam. 2024. <span>“Probability Playground.”</span> <em>Amstat News</em>, 26–28.
</div>
<div id="ref-GrueHeiberg" class="csl-entry" role="listitem">
Grue, Lars, and Arvid Heiberg. 2006. <span>“Notes on the History of Normality–Reflections on the Work of Quetelet and Galton.”</span> <em>Scandinavian Journal of Disability Research</em> 8 (4): 232–46.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../statlearning/general.html" class="pagination-link" aria-label="General Topics">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">General Topics</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../statlearning/linalg.html" class="pagination-link" aria-label="Linear Algebra">
        <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Linear Algebra</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Foundations of Data Science by Oliver Schabenberger</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>