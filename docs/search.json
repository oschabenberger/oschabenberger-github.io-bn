[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Data Science",
    "section": "",
    "text": "Preface\nSince data science appeared on the scene some decades ago it has been subject to constant change. In the beginning many were not quite sure what to make of data science. How was it a scientific discipline and how was it different from statistics or data analytics? Was data scientist not yet another description for the business analyst?\nIn some ways the dust has settled by now and we have a much better handle on what data scientists do and on the skills they need. On the other hand the data profession in general has evolved and keeps expanding. At first, a data scientist was the all-round unicorn that was expert in engineering data, modeling data, writing code, and implementing data solutions. Those individuals were difficult to find and difficult to train.\nIn smaller organizations the workload of the data scientist continues to be fluid, simply because the number of folks who can perform data-related tasks is limited. In larger organizations with dedicated data teams you can now find data engineers, data analysts, business analysts, machine learning engineers, AI engineers, statistical programmers, and so on. Where does a data scientist fit into this picture and what are the skills a data scientist should have to be and to remain competitive in this landscape?\nMy own personal journey to data science took me from forestry to forest biometry to statistics to software development. Like me, many will come to data science from a non-statistical and non-computational path. As educators we need to think about how to ensure the foundations of the discipline are being taught. Assuming that everyone who strives for a career in data science will have a solid background in statistics, applied mathematics, and computer science is not helpful. We have to deal with varied backgrounds and strengths. Data science, like no other discipline I encountered in my 30+ year career, combines technical with non-technical skills, combines hard and “soft” (human) skills, and has a focus on solving real-world problems.\nIt is OK to come to data science with strengths and weaknesses in different aspects of statistics, mathematics, computer science and domain knowledge. Those are the foundation disciplines. Your foundation lies in a genuine interest in these disciplines, the curiosity and drive to fill in the gaps, and a desire for lifelong learning. If you hate math and are not willing to learn the linear algebra concepts necessary to manipulate data science models—forget it. If you have not programmed before and think that you can pass that off to some software engineer—forget it. If you are afraid of public speaking and are not willing to work on improving communication skills—forget it.\nIf you are excited about learning principles of data engineering and statistical learning, about communicating data concepts to colleagues, customers and executives, if you are aching to write better software, and love managing complex projects, then welcome! Data science has been waiting for you.\n\nThe origin for this material lies in recognizing how data science as a discipline has evolved and continues to change. The foundational disciplines remain the same, but the demands on data scientists today are much more varied and complex than some courses and certifications would lead you to believe.\nIn my own practical experience I have witnessed the transition from creating data/analytic teams in organizations—because that is what everyone else was doing—to measuring the success and justification of those teams in terms of value created for the business. Many projects are faltering in this respect, study after study reveals that the majority of data science projects do not succeed. There are many reasons for this sobering reality, how we educate and train for data science careers definitely plays a part.\nUnderstanding data science begins with recognizing data science as a team sport. It is not about slinging code and developing a cool model. As someone said,\n\na data science model in a Jupyter notebook has $0.0 value.\n\nLearning all the classical and modern methods of data analysis is table stakes. Your random forest is not going to be any better than your colleague’s random forest. You are probably using the same software and are tuning the same hyperparameters. Everyone uses TensorFlow or CNTK or PyTorch to implement deep learning. You can implement LeNet-5 or AlexNet for image classification with a few lines of R of Python code—if you are not sure how, google it or ask ChatGPT.\nWhat are the differentiating factors in this competitive field?\n\nUnderstanding how to take a business problem, research, or policy question and turn it into a data project, executing the data project as a team and translating it back into a business, research, or policy solution.\nBeing able to communicate with business, information technology, marketing, finance, and legal to define, develop and implement a data-driven solution.\nBeing at times evangelist, programmer, project manager, instructor, presenter, and implementer.\nUnderstanding the ethical, security, and privacy issues associated with data and data-driven decisions.\nCultivating a desire to learn as new tools and methods affect what can be done with data—GPT anyone?\n\n\nLifelong learning is a mindset that has worked in my favor for decades. It is a growth mindset that builds on curiosity to push your boundaries further out. Faced with a formidable task you don’t know how to accomplish you say to yourself “I don’t know how to do this. Not yet!”. Contrast this with a fixed mindset that views one’s abilities as limited and stays within one’s comfort zone, not pushing against the boundaries. With a fixed mindset, “Not yet!” turns into “Not now” or “Not ever”.\nA growth mindset and joy of learning serves you well in data science. Any of the technology revolutions of the last decades had a profound effect on the way we collect, store, analyze, and use data: the internet, social media, cloud computing, artificial intelligence. You cannot just sit out cloud computing, for example, hoping that it will blow over.\nWe might not know what the next disruption looks like, but we know it is coming. A few years ago, large language models were an interesting novelty, today they have changed our view of what is possible with data. At the same time, principal component analysis (PCA), one of the oldest statistical methods, continues to be one of the most important tools for summarization, visualization, and dimension reduction in statistics and machine learning. This tension between old and new is what makes data science such an exciting field.\nLet’s go.\n\nThis material was written in Quarto, because it handles multiple programming languages in the same document, works in RStudio like RMarkdown on steroids, incorporates \\(\\LaTeX\\) beautifully, and creates great-looking and highly functional documents. To learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 How to Use This Material\nThis material is intended for a regular 3-credit hour course at the graduate level. At Virginia Tech, it is a first-semester core course in the Master of Science in Data Science program.\nIf you are self-studying this material, concentrate on the areas where you have a gap. The chapters do not have to be read sequentially. If the terms in the opening chapters are unfamiliar to you, maybe you need to work through Chapter 11 in Part III. If your background is in data engineering, Part II. Data Preparation and Understanding will be very familiar, spend your time elsewhere.\nThe material is organized in parts. Part I discusses the history of data science as a cross-disciplinary activity that draws on mathematics, statistics, and computer science and applies the “mixture” to solve real-world problem in a specific domain.\nThis problem solution is viewed as an end-to-end process that starts with a business, research, or policy question and ends with implementation of a solution that uses data. The data science project life cycle, as we call it, describes the iterative process of working through these projects as a team sport.\nPart II introduces working with data. We discuss various data sources and formats and how to access them. Not everything comes as a CSV file. Then we go on a “first date” with the data using profiling tools that can highlight problems with data quality. Summarizing and visualizing data and analysis results are major activities that follow once the data is properly pre-processed. We would be remiss not covering SQL, the structured querying language that is the backbone of many database management systems. Paraphrasing an employer: “there is much we do not agree on in this field, but we agree that at some point you need to access data in a database and you need to use version control: learn SQL and learn Git.”\nPart III of the material covers foundational topics in statistical learning, an amalgam of statistical modeling and machine learning. We deliberately placed it after basic data preparation and understanding. You can accomplish a lot with data before you need to know the difference between supervised and unsupervised learning or the intricacies of cross-validations.\nIn the first chapter you learn about supervised and unsupervised learning, regression and classification, the bias-variance tradeoff, mean square error, cross-validation and other important concepts. The goal is to introduce these topics and to develop the vernacular of data science. We hope that you have had the prior exposure to probability, statistics, and linear algebra required to conduct data science. If not, or if you need a refresher, this part of the material offers several chapters.\nExploring statistical learning in depth is a separate topic. You can check out this material as an example.\nPart IV dives into an increasingly important topic, working with data in a manner that does not perpetuate stereotypes, avoids harm, and honors privacy concerns. We wrap this up as Applied Ethics in Data Science. The introductory chapter is full of examples where things have gone wrong, due to unintended consequences, focus on the wrong performance metrics, bad data, and other reasons. How algorithms can introduce bias and cause harm is the subject of the following chapter. A separate chapter is dedicated to concerns around personal information and data privacy.\nPart V of the material focuses on the latter parts of the end-to-end data science project lifecycle. How do you turn the project assets such as data and models into assets that can be used by others? Following coding best practices and using the right data science tools are a good beginning.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "proj/history.html",
    "href": "proj/history.html",
    "title": "2  History and Evolution of Data Science",
    "section": "",
    "text": "2.1 What is Data Science?\nIt is revealing that there is no single agreed-upon definition of data science. The lack of a clear definition is understandable if one considers that,\nWe can begin to develop an understanding of data science by considering the three foundational disciplines: statistics, mathematics, and computer science. What problems associated with data required their combination and could not be solved by the discipline in isolation? Maybe we should put the question slightly differently: What problems are we able to solve at the intersection of these disciplines?\nFirst, data is information collected about the real world. Data science then is concerned with real-world problems and phenomena. We test hypotheses about real-world problems, but we do not solve hypothetical problems or prove theorems. Typical questions addressed by data scientists based on data are:\nStatistics is concerned with describing the world and with drawing conclusions about the world using incomplete information. The information—data—is incomplete for various reasons. We might have drawn a sample from a population rather than observed every entity. We might have assigned treatments at random to subjects in an experiment in order to balance out the influence of variables not controlled in the experiment. We have incomplete information about the input variables that relate to an output variable. We have incomplete information about the true relationship between inputs and outputs.\nComputer Science is concerned with computation, information, and automation. Key to computer science are data structures—efficient ways to store and process information—and algorithms—specifications to solve a specific problem through computation. Because today data is almost always stored in digital form it is now immediately accessible to processing via computation. A remarkable contribution of computer science to data science over the last decades is to broaden its view from methods and algorithms to organize and store data to methods and algorithms that draw conclusions from the data through computation. Computer science has discovered data as a source of learning, not just as a medium of storage and processing.\nThe algorithmic approach to understanding data rather than a probabilistic approach is one of the great contributions of computer science to data science. Another major contribution is software engineering and software development. Data science projects are software projects, they involve the use of software tools and create code written in languages such as Python, SQL, R, Julia, Scala, Java, JavaScript, and others.\nMathematics is concerned with the study of numbers, formulas (algebra), shapes and structures (geometry), and patterns (analysis). Statistics is often considered a form of applied mathematics. Relationships between inputs and outputs in data science are often modeled through continuous functions, their properties are studied through linear algebra, and they are related to data through differentiation, integration, and numerical analysis.\nRather than thinking of data science as a new domain of knowledge, we think of it as a set of skills that are applied to a subject matter domain (=area of expertise). This view is informed less by a scientific discipline around data than the recognition that today all subject matter domains, including the sciences, are using data to answer questions.\nWhich skills are most important? Is it the hard technical skills in statistics, machine learning, software development? Or is it the ability to communicate across organizational functions in a team-based environment? Or is it the ability to understand and analyze real-world problems in a specific domain? A data scientist will eventually need to acquire all those skills, but they are not interchangeable. Subject matter skills can be acquired by working in a particular domain. Working on data science problems in financial services, for example, you will pick up the specifics and idiosyncrasies of credit and debit card transactions, anti-money laundering, electronic payments, and so on.\nDomain-specific knowledge takes the least time to learn to contribute to solving data science problems. Learning the statistical and mathematical foundations and how to develop good software is the more difficult task. When you join an organization as a data scientist, you will be surrounded by people who understand the domain inside and out—it is what they do. You, however, might be the only person who understands how to apply machine learning to forecast data and who knows how to write analytical software that works.",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History and Evolution of Data Science</span>"
    ]
  },
  {
    "objectID": "proj/history.html#what-is-data-science",
    "href": "proj/history.html#what-is-data-science",
    "title": "2  History and Evolution of Data Science",
    "section": "",
    "text": "data science is a young discipline, its boundaries—what is in and what is out—are still evolving.\ndata science is a cross-functional discipline that combines domain expertise with foundations in statistics, mathematics, and computer science. Views of data science differ based on how these components are weighted.\ndata science is defined by what data scientists do. That encompasses everything from finding data, handling data, processing data at scale, to applying statistics and machine learning, to writing code and implementing algorithms to interpreting, visualizing, and communicating data and results.\n\n\n\n\nWhat is and what has been (description)\nWhat will be (prediction)\nWhat category does this item belong to (classification)\nWhat can I say about X (hypothesis testing)\nWhat should I do (prescription)\nWhich things are similar (segmentation, clustering)\nWhich things occur together (association)\nWhat is the best way to do something (optimization)\n\n\n\n\n\n\n\nData Science\n\n\nAt the intersection of the foundation disciplines, performing data science means drawing conclusions from data about real-world problems using computation and automation in the presence of uncertainty.",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History and Evolution of Data Science</span>"
    ]
  },
  {
    "objectID": "proj/history.html#the-sexiest-job-and-john-w.-tukey",
    "href": "proj/history.html#the-sexiest-job-and-john-w.-tukey",
    "title": "2  History and Evolution of Data Science",
    "section": "2.2 The Sexiest Job and John W. Tukey",
    "text": "2.2 The Sexiest Job and John W. Tukey\nIn 2009, Hal Varian, Chief Economist at Google declared that the sexy job in the next ten years would be statisticians. As a statistician, I agreed of course. In 2012, Thomas Davenport and DJ Patil published “Data Scientist: Sexiest Job of the 21st Century” in Harvard Business Review (Davenport and Patil 2012). These articles were not describing the work of the typical statistician, but a more general approach to extracting information from large data sets and presenting the insights to others. A new kind of profession was emerging in response to a greater abundance of data in the world that did not fit neatly into existing categories like mathematician, statistician, business analyst, or “quant”. Varian said,\n\nThe ability to take data—to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it—that’s going to be a hugely important skill in the next decades…\n\nThe approach seemed fresh in placing the data at the center of the investigation rather than the traditional approach of choosing the problem first and collecting data second to answer the problem. It emphasized learning from data through visualization and communicating about data—data literacy. What can we learn from the data we have, what do the data tell us? Alas, this shift in focus was not new, it was first proposed by a famous statistician, John W. Tukey.\n\n\n\n\n\n\nData and Datum\n\n\n\n\n\nYou might have noticed that data is used in the plural form, “data are…”. This is grammatically correct since data is the plural form of datum. It is common to use data as a singular noun, “data is…”. I might fall into that trap every now and then but prefer to use the plural form. After all, using data as singular would be wasting a perfectly good noun: datum.\n\n\n\nTukey is known to statisticians as the founder of exploratory data analysis (EDA), as compared to confirmatory data analysis. His famous book “Exploratory Data Analysis” (Tukey 1977) established the idea that much can be learned by using the data itself and that data can suggest hypotheses to test. In confirmatory analysis, on the other hand, you start with a hypothesis and then collect data to verify whether the hypothesis might be true.\nMuch of the statistical work leading up to this point had been confirmatory, based on the concept that data are the realization of a data-generating mechanism—usually a random process. Hypotheses are tested by capturing this mechanism in a statistical model, deriving estimates for the parameters of the model. Once the statistical model is accepted as the abstraction of the data-generating mechanism it becomes the lens through which the problem of interest is viewed. The model is applied to test hypotheses and to calculate predictions along with measures of uncertainty.\nIn 1962, John W. Tukey published a pre-cursor to EDA, “The Future of Data Analysis”, in which he laid the foundation of modern data science, he called it data analysis, and argued why this is a scientific discipline (Tukey 1962).\n\nFor a long time I have thought I was a statistician, interested in the inference from the particular to the general. But as I have watched mathematical statistics evolve, I have had cause to wonder and to doubt.\nAll in all, I have come to feel that my central interest is in data analysis, which I take to include, among other things: procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise and more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.\n\nTukey asked that we give up familiar constraints and look for stories and meaning in data sets. Data analysis can precede probability models; for example, one can identify an interesting function of the data and make progress by asking what the function might reasonably be estimating.\nThis was a liberating view that contrasted against the rigor of probability models and the search for optimal estimators in favor of putting the data first, accepting “good enough”, giving advice when there is reasonable evidence for the advice to be sound, and being prepared that in a reasonable fraction of cases that advice will be wrong.\n\nData analysis must progress by approximate answers, at best, since its knowledge of what the problem really is will at best be approximate. Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History and Evolution of Data Science</span>"
    ]
  },
  {
    "objectID": "proj/history.html#data-mining-and-data-science",
    "href": "proj/history.html#data-mining-and-data-science",
    "title": "2  History and Evolution of Data Science",
    "section": "2.3 Data Mining and Data Science",
    "text": "2.3 Data Mining and Data Science\nData mining is the process of searching large data sets for trends and patterns using computers and automation. We can see a direct link to data analysis in the sense of Tukey, working with a dataset to extract interesting patterns in data, to discover new information in the data, to classify observations, identify associations, detect outliers or anomalies, and to identify groups of similar observations (clustering).\nWith the dramatic growth of datasets in rows and columns, it became impossible to conduct this type of exploratory discovery (in the sense of Tukey) manually. There was also growing interest in using special tools and algorithms to discover hidden nuggets of information and relationships in datasets that would otherwise go undetected. Data mining relies on computing power and automation to find patterns in data at scale.\nThe overall process of discovering knowledge from data was formalized as Knowledge Discovery in Databases (KDD). Data mining is the part of the KDD process that uses algorithms to discover patterns.\nThis approach is not without problems or critics. The statistical viewpoint likened the approach to “data dredging” or “data fishing”: looking for relationships even if they are meaningless and then forming hypotheses about why the relationships exist. For example, associations and correlations can be spurious, caused by latent or mediating variables. A strong correlation is not evidence of a causal relationship and might lead to the formulation of bad hypotheses or poor decisions.\n\n\nExample: Chocolate Consumption and Nobel Laureates\n\n\nThe New England Journal of Medicine published in 2012 an article that relates the chocolate consumption per capita to the number of Nobel prize winners in various countries, a highly significant statistical relationship that explains almost 2/3 of the country-to-country variation in Nobel laureates\n\n\n\n\n\n\n\n\nFigure 2.1: Messerli, F.H. (2012) “Chocolate Consumption, Cognitive Function, and Nobel Laureates”. New England Journal of Medicine, 367:1562-1564\n\n\n\n\n\nIf the relationship were causal—which it is not—an increase of 0.4 kg per year per capita would produce one additional Nobel laureate. In the U.S. that amounts to an extra 125 million kg of chocolate per year. If the relationship between chocolate consumption and number of Nobel laureates is not causal, how can we then explain the obvious relationship seen in the figure? Could it be explained by chocolate consumption improving cognition which creates a fertile ground from which Nobel laureates sprout? A look at how the data was collected sheds light and casts doubt on the study: only four years of chocolate consumption were considered on a limited number of chocolate products and no data prior to 2002 was used. The number of laureates is a cumulative measure that spans a much longer time frame.\nIt appears that the data were organized in such a way as to suggest a relationship between the variables.\n\n\nAnother criticism of the data mining approach is that traditional statistical decisioning based on p-values is not adequate. Many statistical tests get overpowered by very large data sizes, making even small differences statistically significant. The repeated application of statistical tests across hundreds or thousands of variables leads to inflated error rates unless multiplicity adjustments are made.\n\n\nExample: Market Basket Analysis\n\n\nMarket basket analysis uses Association Rule Mining (ARM) to find associations between items that occur in databases. An application is to identify items that are purchased together, for example, customers who buy whole milk might be more likely to also purchase yogurt and cereals as compared to a completely random choice of products. The name market basket analysis stems from this application, but ARM has many other use cases. Association rules can be used in diagnosing medical conditions based on co-occurrence of symptoms, in text analytics to extract meaning of documents based on the co-occurrence of words, in survey analysis to find associations between answers to different questions. A rule is represented as a logical \\(\\{A\\}\\Rightarrow\\{B\\}\\), where \\(\\{A\\}\\) is a set of items called the antecedent (head) of the rule and \\(\\{B\\}\\) is a set of items called the consequent (body) of the rule. ARM discovers rules such as \\(\\{\\text{whole milk}\\}\\Rightarrow\\{\\text{yogurt, cereals}\\}\\) and arranges them by measures of rule quality such as support, confidence, and lift. The support of an item set is the frequency with which its items appear, the confidence is a measure of predictability of the rule, and the lift measures how much more likely the item set appears compared to a random allocation of items. When applied to large databases the number of possible association rules is astronomical. Suppose there are 500 items in a store. There are more than 62 million rules with just two items in the antecedent and a single item in the consequent. If all associations between items are completely random, at a 1% Type-I error rate we would declare over 600,000 associations as “significant”.\n\n\nThis criticism is valid, of course, if the results of data mining are used in a confirmatory fashion. But when one puts data first, not a probability model that might have created the data, then data mining techniques are incredibly useful and necessary to help us learn about data, to formulate hypotheses, and to plot the path of inquiry—rather than to confirm a result. Data mining is part of the data science methodology and not a separate discipline. The Data Science Process Alliance, concerned with project management in data science, uses the cross industry standard process for data mining (CRISP-DM) as the foundation for the data science process—data mining and data science today are intertwined.",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History and Evolution of Data Science</span>"
    ]
  },
  {
    "objectID": "proj/history.html#an-odyssey",
    "href": "proj/history.html#an-odyssey",
    "title": "2  History and Evolution of Data Science",
    "section": "2.4 2001: An Odyssey",
    "text": "2.4 2001: An Odyssey\nTwo influential papers appeared in 2001 that paved the way from statistics to data science.\nIn “Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics” William S. Cleveland makes the case to enlarge the major focus areas in statistics to a field called data science (Cleveland 2001). The new discipline places more emphasis on\n\nMultidisciplinary projects. Collaboration with domain experts is a source of innovation. Data is the engine for invention in data science.\nModel building. Effort should increase toward methods for building models compared to formal, mathematical-statistical inference in models.\nComputational methods and computing with data that includes databases, networks, and analytical software.\nCommunication and pedagogy.\n\nIn “Statistical Modeling: The Two Cultures”, Leo Breiman contrasts statistical (data) modeling and algorithmic modeling (Breiman 2001). The former assumes that the data are generated by a stochastic data model. According to Breiman, 98% of statisticians subscribe to this approach. Algorithmic modeling, on the other hand, makes no assumption about the underlying data model, treats the data mechanism as unknown, and is more common in fields outside of statistics. In Breiman’s words\n\nPerhaps the damaging consequence of the insistence on data models is that statisticians have ruled themselves out of some of the most interesting and challenging statistical problems that have arisen out of the rapidly increasing ability of computers to store and manipulate data. These problems are increasingly present in many fields, both scientific and commercial, and solutions are being found by nonstatisticians.\n\nThe goal of algorithmic models is more predictive accuracy than confirmatory inference and hypothesis testing. The model is supposed to approximate an unknown relationship between inputs and outputs well enough to provide satisfactory accuracy in predicting outputs of previously unseen inputs. Neural networks and decision trees are examples of algorithmic tools that found rapid adoption outside of statistics. Machine learning as it emerged from computer science is a manifestation of algorithmic modeling.\nIn data modeling, theory focuses on the probabilistic properties of the model and of quantities derived from it. In algorithmic modeling, the focus is on the properties of the algorithm itself: starting values, optimization, convergence behavior, parallelization, hyperparameter tuning, and so on.\nBreiman’s article was widely discussed—the invited comments by leading statisticians at the end of the paper give a sample of opinions.\nMeanwhile, computer scientists had realized that data is not just an abstract concept, that data is more than information to be structured, stored, secured, and transmitted. They realized that data had intrinsic value; processing data can derive insight from data. That knowledge filled a void left by statisticians with limited knowledge of computing environments. Computer scientists, on the other hand, had limited knowledge about how to approach the analysis of data. The fields were ripe for a merger.",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History and Evolution of Data Science</span>"
    ]
  },
  {
    "objectID": "proj/history.html#the-big-data-era",
    "href": "proj/history.html#the-big-data-era",
    "title": "2  History and Evolution of Data Science",
    "section": "2.5 The Big Data Era",
    "text": "2.5 The Big Data Era\nAn early use of the term Big Data was in a paper by Michael Cox and David Ellsworth (“Application-controlled demand paging for out-of-core visualization”) in the Proceedings of the IEEE 8th Conference on Visualization. The authors noted that data set sizes have grown bigger to the point that they do not fit in main memory and termed the problem big data.\nDid we really need another term to describe increasing data size? Whether data fits into main memory of a computer depends on the size of the data and the size of the memory. And since the invention of digital computers both have constantly increased. There had to be more to it than just an increase in the volume of data.\nIndeed, in the 2000s a new class of applications and uses of data emerged as several developments in data and computing coalesced:\n\nMore and more data were now captured in digital form, the amount of data generated in digital form increased sharply.\nA growing interest in the analysis of rich data types such as text, documents, audio, and video.\nA continuum of data in motion; from data at rest in the cloud or data center to streaming data.\nLarge-scale data collection enabled through the internet.\nA shorter shelf life of new kinds of data such as behavioral data (online shopping, social media activity, web browsing, behavioral data) compared to more stable demographic data (age, ZIP code).\nThe beginnings of data-driven application where the data defines how the system operates.\nGreater automation of data-processing.\n\nThe Big Data phenomenon was never just about the size of the data alone, or even the “3Vs”, volume, velocity, and variety. Big Data was about doing something different with data than had been done before, using different data types, different model types, different algorithms, and different computing environments. Building a recommendation system at Netflix or Amazon from data on millions of customers and items viewed or bought is an example of this new type of applications.\nBig Data was about reaching the limits of the ability to store, access, and process data. The volume of data can be a limiting factor, of course and call for specialized approaches. Traditional statistical modeling with hypothesis testing might be useful when deciding about 20 possible input variables. What happens when there are 30,000 potential input variables? Computing \\(p\\)-values in a random sample of size 100 makes sense, but when the data set contains 10 million observations the statistical test is so powerful that nearly any hypothesis can be rejected. What should be used instead? How do you engage multiple computers to solve massive analytic problems in parallel?\nIf the definition of Big Data is “whatever does not fit on a single machine” and needs some form of distributed computing, then the frontier is continuously receding. Today (2023), a storage-optimized instance in the Amazon Elastic Compute Cloud (AWS EC2, i4g.16xlarge), features 64 cores (vCPUs) and 512 GB of memory. AWS launched EC2 in 2006 with single-core machines that featured 2 GB of RAM. That is an increase of more than two orders of magnitude. And, in case that is not powerful enough, p5.48xlarge instances feature 192 cores and 2 TB of memory. A lot of Big Data work can be done on a single machine today.\nJordan Tigani, CEO and founder of database company MotherDuck and founding engineer of Google BigQuery has captured this development in the blog “Big Data Is Dead”:\n\nThe world in 2023 looks different from when the Big Data alarm bells started going off. The data cataclysm that had been predicted hasn’t come to pass. Data sizes may have gotten marginally larger, but hardware has gotten bigger at an even faster rate. Vendors are still pushing their ability to scale, but practitioners are starting to wonder how any of that relates to their real world problems.\nOf course, just because the amount of data being generated is increasing doesn’t mean that it becomes a problem for everyone; data is not distributed equally. Most applications do not need to process massive amounts of data. This has led to a resurgence in data management systems with traditional architectures.\n\nThe amount of data an organization stores is typically much greater than the amount of data being analyzed: because more recent data is more important, because data are being processed for analytics, and so on.\nWhatever we think of the term Big Data, the era has contributed to the rise of data science as a discipline. New data-driven applications with new types of data challenged us to approach data analytics in a new way. Algorithmic approaches that put the problem first (I need to predict customer churn) are winning in this environment over data modeling approaches that first build an abstraction of the data mechanism based on probability principles.",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History and Evolution of Data Science</span>"
    ]
  },
  {
    "objectID": "proj/history.html#data-science-today",
    "href": "proj/history.html#data-science-today",
    "title": "2  History and Evolution of Data Science",
    "section": "2.6 Data Science Today",
    "text": "2.6 Data Science Today\nThe people who identify as data scientists often have unusual career paths; they do not necessarily come to data science through one of the foundational disciplines. They turn out to be problem solvers with an acumen for technology, programming, and communication.\nBig Data created an epic wave and data scientists are the people who know how to surf. Thanks to the Big Data wave, tools and techniques to process and analyze large and complex datasets are now standard in data science. Knowledge in data storage, parallel computing, distributed systems, scalable data processing frameworks, and cloud computing is part and parcel of data science.\nTen years after they published “Data Scientist: Sexiest Job of the 21st Century”, Thomas Davenport and DJ Patil revisited in a 2022 Harvard Business Review article whether the assessment still holds (Davenport and Patil 2022). They conclude that over the last 10 years there has been continuity in the developments but also important changes.\nThe job of the data scientist continues to gain importance as the amount of data, analytics, and AI in business, government, and society is bound to only increase. A decade ago, the data scientist was a unicorn who combined rare skills in statistics, machine learning, and coding to wrangle information from large datasets. Thanks to many graduate and undergraduate programs, online courses and certifications, there is today a more structured approach to acquire data science skills. While these skills continue to range widely, there is also more differentiation of the data scientist role against other professions. In response to specialization and the need to fill a gap in managing data in the data science project lifecycle, AI engineers, ML engineers, and data engineers are on the rise. In the words of Davenport and Patil:\n\nWe expect to see continued differentiation of responsibilities and roles that all once fell under the data scientist category. Companies will need detailed skill classification and certification processes for these diverse jobs, and must ensure that all of the needed roles are present on large-scale data science projects. Professional data scientists themselves will focus on algorithmic innovation, but will also need to be responsible for ensuring that amateurs don’t get in over their heads. Most importantly, data scientists must contribute towards appropriate collection of data, responsible analysis, fully-deployed models, and successful business outcomes.\n\nAmong the important changes in technology in the past decade the authors list\n\nCloud computing and cloud data storage (data lake, data warehouse, lakehouse)\nAuto machine learning and citizen data science\nLarge language models\nModel operations\nEthics of data science\n\nTwenty years after the arrival of Big Data, data science continues to evade a precise definition because it is defined by those who practice it, rather than by a list of activities. Hacker’s art, statistics, programming, visualization, modeling, subject-matter experience, and communication to solve problems with data through computation are key ingredients. Data science will continue to evolve as its ingredients change and with the emergence of new methods and tools. Most recently, since late 2022, large language models based on transformer technology created a sea change in natural language understanding that is believed to disrupt many occupations. Data scientists embrace such disruptions to draw better conclusions from data.",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History and Evolution of Data Science</span>"
    ]
  },
  {
    "objectID": "proj/history.html#data-science-versus-data-in-science",
    "href": "proj/history.html#data-science-versus-data-in-science",
    "title": "2  History and Evolution of Data Science",
    "section": "2.7 Data Science versus Data in Science",
    "text": "2.7 Data Science versus Data in Science\nThe majority of data science projects and applications take place in non-academic settings, often lumped together under the generic term industry. Most data science jobs are thus found in industry.\nThere is a difference between data science in research settings and commercial settings. The boundaries typically align with academia and industry, but it is not unusual for commercial enterprises to have Research & Development divisions that engage in scientific research in support of the enterprise’s mission and products.\nRather than pitting industry against academia, consider how data science projects differ between research and commercial settings. There is a lot of hype about how organizations become more data driven, replacing decisions made on gut feeling with hard evidence based on data—the data science team is at the heart of that digital transformation, disrupting the old ways of doing business with novel approaches based on machine learning and AI. In most situations, it is just that: hype.\nIn research and commercial settings, the goal is to provide value through using data. How that value is measured is fundamentally different. Research adds value by increasing our body of knowledge. Commercial enterprises are interested in increasing the value of the company.\nMost data science jobs in commercial settings exist not to disrupt the business model but to support an existing business by solving the current data-related problems. The problems tend to be narrow and specific with result metrics that are tied to the objectives of the enterprise. Objectives such as “increase revenue”, “retain customers”, “increase productivity”, “reduce cost” are universal and boring. Because questions are not open ended and the time to find a solution is of the essence, there is a tendency to reuse rather than to develop new approaches and/or algorithms. Solutions have merit if they meet the goals and can be implemented; they must make a difference in the practice of the business.\nIn research settings, on the other hand, questions are more open-ended without a clear solution path. The goal of research is to find answers to new questions. Time is allotted to deep thinking and to uncovering mechanisms. This often involves novel approaches, custom algorithms implemented by the research team in specialized software. Solutions are developed only as necessary to prove the result of the research. They do not have to be implemented or practical. A research team might find a better active ingredient to fight disease and leave the task of productizing a drug to someone else.\nResearch projects have carefully designed protocols about data, what data to use, how to measure attributes, how to sample observations. Experiments are designed to collect data under controlled conditions in order to draw cause-and-effect conclusions.\nIn commercial settings you are more likely to work with the data that is collected as a by-product of the normal business operations. True experimentation with data collected under controlled conditions is rare, sometimes confined to A/B testing of one data product against another. It is thus common in commercial situations to be working with data collected for a different purpose. The data in the enterprise CRM (customer relationship management) system was collected to track interactions with customers, the data in the Salesforce instance tracks revenue opportunities, and ServiceNow records IT assets and tickets. Is that the best data to optimize digital marketing campaigns? Weak correlations, low signal-to-noise ratios, poor data quality are common problems.\nThe goal of this section is not to put a damper on working as a data scientist in commercial enterprises. It is a good idea to ask what role data and software play in the organization. If the core business and the core product is built on data and software, then it is likely that data science contributes directly to the mission and the bottom line of the company—it is essential to the business. In many organizations the core business is not built on data and technology—at least not yet. A bank’s core business is to deposit and lend money. A retailer’s core business is to sell goods. An insurance’s core business is risk assessment and underwriting. Data science supports the core functions through traditional data functions such as reporting, data analysis, forecasting—it is important but not essential. Increasingly, however, these core functions are becoming the focus of data science itself providing a fertile ground for interesting and novel approaches: micro-lending (banks), recommendation systems (retailers), reinforcement learning for underwriting (insurance). At this point data science helps transform a traditional business model as its products become essential to operating the business.",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History and Evolution of Data Science</span>"
    ]
  },
  {
    "objectID": "proj/history.html#what-is-in-a-model",
    "href": "proj/history.html#what-is-in-a-model",
    "title": "2  History and Evolution of Data Science",
    "section": "2.8 What is in a Model?",
    "text": "2.8 What is in a Model?\nThe term model is pervasive in our field, and we come to this conversation with different notions of what constitutes a model. Before going any further in the discussion, we’ll discuss the concept of a model in the context of data science.\n\n\n\n\n\n\n\n\nFigure 2.2: A simple representation of a model that processes inputs with algorithmic logic and produces output.\n\n\n\n\n\nFrom the 30,000 foot view a model is simply a mechanism to process some input and produce a corresponding output (Figure 2.2).\nThe input to drive the model algorithm is almost always some form of data. The algorithm that processes the inputs can be based on data, but that is not necessarily so. Suppose the problem we are trying to solve is to ascertain an individuals annual federal income tax. The problem is solved with a model that takes as input the individuals financial situation. This information is typically known without error as information about income, property taxes, expenses, etc. is well documented. The algorithm processing this input is a translation of the relevant information in the federal income tax laws into machine instructions. The output is the amount of money owed to the government or expected as a refund.\nNow suppose that for some reason the input data in the tax problem is not known without error. For example, income from tips, medical expenses or charitable contributions might be best guesses rather than exact amounts. Income data could be noisy because foreign income is converted at fluctuating exchange rates. If the input data is the realization of stochastic (random) influences, should we modify the algorithm?\nWhen the input data to an algorithm is the result of observing random variation, we are looking to the algorithms of the model to find the signal in the data, to de-noise it. The signal located in the data is then transformed into the model output. Most models we build in data science are of this kind because the data we work with is inherently noisy. The reasons for the random variations are many: selecting observation from a larger population at random, applying treatments to randomly chosen experimental units, variations in measurement instruments and procedures, variations in the environment in which a phenomenon is observed, and so on. The algorithms we use depend on the goals of the analysis, properties of the data, assumptions we are willing to make, attributes we look for in competitive models, and personal preferences.\nMuch of data science methodology is to select the right approach (algorithm) based on input data, learning methodology (supervised, unsupervised, semi-supervised, self-supervised) and analysis goal (prediction, recommendation, classification, clustering, dimension reduction, sequential decisioning), to train the model, and to deploy the model. Figure 2.3 is an attempt at structuring the input, algorithm, and output components of a model in the data science context. The diagram is complex and woefully incomplete and is intended to give you an idea of the diversity of methods and the many ways we can look at things. For example, in discussing input data we could highlight how data are stored, how fast it is moving, the degree to which the data is structured, the data types, and so forth. There are many other categorizations of data one could have listed.\nThe categorization of algorithms—what many consider the models in the narrow sense—leaves out semi-supervised learning, self-supervised learning, transfer learning, and other learning methods. Volumes of books and papers have been written about every item in the list of algorithms and many algorithms are represented by a simple description. Multilayer networks, for example, include artificial neural networks, deep networks such as convolutional and recurrent networks, and transformer architectures such as GPT.\n\n\n\n\n\n\nFigure 2.3: Structuring and categorizing input, algorithm, and output in data science models.\n\n\n\nGeorge E.P. Box is credited with coining the much-used phrase “all models are wrong, but some are useful” (Box 1976). The phrase appears partially (“all models are wrong”) twice in his 1976 paper on Science and Statistics:\n\nSince all models are wrong the scientist cannot obtain a “correct” one by excessive elaboration.\nSince all models are wrong the scientist must be alert to what is importantly wrong.\n\nThe takeaway is that any model is an abstraction of a phenomenon and we strive to find a useful abstraction. The model does not attempt to reproduce the phenomenon. The tax algorithm converts the essence of the tax code into machine instructions, it is not an electronic copy of the entire law. The purpose is to accurately calculate an entity’s tax, anything else can be stripped away. An algorithm processing noisy data that reproduces the data is uninteresting. The goal is to abstract the data in such a way to allow separating the signal from the noise and to convert the signal into the desired output.\nThe first G.E.P. Box quote instructs us not to overdo it in building models; this translates to the problem of overfitting in data science, crafting a model that follows the training data too closely and as a result does not generalize well to new data points. If the goal is to predict, classify, or cluster the unseen; generalizability of the model is key. A model to forecast stock prices or trading volumes is judged by how well it can predict the future, not by how well it can predict the past. The adequate level of generalization for that model must be wrung from current and past stock prices. Finding the appropriate level of abstraction is resolved by striking the right balance in the bias-and-variance tradeoff.\nJohn von Neumann is said to have remarked\n\nWith four parameters I can fit an elephant and with five I can make him wiggle his trunk.\n\nIf the point of the model is to capture the essence of the elephant, then four parameters would be enough.\nThe second G.E.P. Box quote instructs us that models are abstracting away features of the phenomenon. If these are important features, the model is not useful. In the best case this model does not meet its goal and is revised or abandoned. In the worst case the model leads to bad decisions and harmful outcomes.\nNo matter how complex the model, we need to strive to understand how it works (interpret the model), not just what it does. If a model is not intrinsically interpretable then we need to strive to explain the forces that drive the model, keeping in mind that we are then making statements about the model and not about the underlying phenomenon we have abstracted.\nIt might seem like a daunting task to command the plethora of complexity displayed in the previous figure, understand all the pros and cons, grok the idiosyncrasies of software implementations, write code to train the model(s), communicate the results, and possibly implement a solution within a business context. That is what we are here for; let us get started.\n\n\n\nFigure 2.1: Messerli, F.H. (2012) “Chocolate Consumption, Cognitive Function, and Nobel Laureates”. New England Journal of Medicine, 367:1562-1564\nFigure 2.3: Structuring and categorizing input, algorithm, and output in data science models.\n\n\n\nBox, George E. P. 1976. “Science and Statistics.” Journal of the American Statistical Association 71 (356): 791–99.\n\n\nBreiman, Leo. 2001. “Statistical Modeling: The Two Cultures.” Statistical Science 16 (3): 199–231.\n\n\nCleveland, William S. 2001. “Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics.” International Statistical Review / Revue Internationale de Statistique 69 (1): 21–26. http://www.jstor.org/stable/1403527.\n\n\nDavenport, Thomas H., and D. J. Patil. 2012. “Data Scientist: The Sexiest Job of the 21st Century?” Harvard Business Review. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century.\n\n\n———. 2022. “Is Data Scientist Still the Sexiest Job of the 21st Century?” Harvard Business Review. https://hbr.org/2022/07/is-data-scientist-still-the-sexiest-job-of-the-21st-century.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67.\n\n\n———. 1977. Exploratory Data Analysis. Pearson.",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History and Evolution of Data Science</span>"
    ]
  },
  {
    "objectID": "proj/lifecycle.html",
    "href": "proj/lifecycle.html",
    "title": "3  The Data Science Project Lifecycle",
    "section": "",
    "text": "3.1 Introduction\nA data scientist finds an interesting set of data, builds a clever analytical model, and changes the world.\nThis sounds great; unfortunately, data science projects do not unfold this way in the real world. At least not most of them; almost none of them. In this section we examine the typical structure of data science projects in non-research settings—that is, a project to solve a specific problem in an organization. Let’s call it the business problem, acknowledging that such problems arise in industry, government, and non-profit organizations alike. The next chapter takes a closer look at data science in research settings.\nData science models play a big part in this, but it is not where we start. Building a model is just one step in the lifecycle of a data science project.\nProjects involve many steps, diverse teams, and projects can fail at any step. The model building phase is not where most unsuccessful projects go off the rails. Industry analyst firm Gartner estimated in 2016 that about 60% of data science projects are failing and admitted two years later that was an underestimate; the real failure rate was closer to 85%. That is a sobering statistic, and many reasons are cited for the failures:\nThe most important reason for failure of data science projects is called the “last-mile problem” of data science: the struggle to deploy the result of the data analysis (models) into processes and applications where they are used by the organization and are serving the intended end user. It is one thing to build from historical data a model that can predict customer churn accurately. If the goal is to provide a customer service representative with a real-time prediction of a customer’s tendency to cancel a service and to recommend an action, e.g., to give a discount, the project is not complete until the model is incorporated into the system through which customers and representatives interact.\nIt is best to recognize the reality that data science projects will likely fail unless they are carefully planned and executed. Successful projects combine technical skills with soft skills such as communication, trust building, transparency, and empathy. Successful projects are a team sport that incorporate collaboration and communication throughout the project cycle with data scientists taking on multiple roles. The data scientist is at times evangelist, planner, presenter, data wrangler, modeler, software developer, engineer, facilitator, implementer, storyteller, auditor.\nIt is often said that data scientists spend 80% of their time wrangling and cleaning data. That is not true, but the narrative reveals that a data scientists’ job entails much more than the technical aspects of building models on prepared data sets. A good portion of the job is to meet people, interact about project requirements, communicate insights, manage projects, etc. In the terminology of people management, data scientists are “T-shaped people” who combine depth of expertise with broad cross-functional competencies (Figure 3.1).\nThis allows data scientists to understand and communicate about many disciplines and systems (the horizontal bar of the generalist) combined with deep expertise in at least one discipline or system, e.g., cloud-based machine learning (the vertical bar of the expert). As a data scientist you work both independently and collaboratively.\nThe associated skill sets are exercised during the key stages in the data science project lifecycle.\nOur methodology calls out seven phases in the project: discovery, data engineering, model planning, model building, communication, deployment, and monitoring (Figure 3.2).\nOther process models for data science exist, for example CRISP-DM, the cross-industry standard process for data mining is the foundation of the data science process model according to the Data Science Process Alliance. CRISP-DM is shown in Figure 3.3.\nThe process models use similar terms and stages, what matters is not the labels but that there is a general flow that results in a data-driven solution to a real-world problem, that the stages are iterative, and that a project does not start with modeling data. To deliver a production data project that is used repeatably by an organization requires many steps before and after the data modeling phase.",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Science Project Lifecycle</span>"
    ]
  },
  {
    "objectID": "proj/lifecycle.html#introduction",
    "href": "proj/lifecycle.html#introduction",
    "title": "3  The Data Science Project Lifecycle",
    "section": "",
    "text": "Difficulties inherent in integrating data-driven solutions with existing processes and applications\nManagement resistance and internal politics about project ownership, budget, etc.\nLack of skills\nLack of quality data\nSecurity and governance challenges\n\n\n\n\n\n\n\n\n\n\nFigure 3.1: The concept of “T-shaped” competencies, combining deep expertise in one area with breadth in multiple disciplines and systems.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: The stages of the data science project lifecycle.\n\n\n\n\n\n\n\n\n\n\nFigure 3.3: The CRISP-DM model for the data science process.",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Science Project Lifecycle</span>"
    ]
  },
  {
    "objectID": "proj/lifecycle.html#discovery",
    "href": "proj/lifecycle.html#discovery",
    "title": "3  The Data Science Project Lifecycle",
    "section": "3.2 Discovery",
    "text": "3.2 Discovery\nDuring the Discovery phase the business problem is identified and defined with enough specificity to facilitate answering the question: “Does the problem translate to data science?” A problem statement such as “We need to increase revenue” is not specific enough. The question “does a modified user interface for the checkout process increase online sales?” is more specific; it provides information about the target (online sales) and how it is related to measurable changes.\nThe question “Can wildlife species be identified with at least 80% accuracy from drone video?” is very specific, it identifies the goal (wildlife identification), the methodology (drone video), and what success looks like (accuracy greater than 80%). Many questions still need to be addressed before this results in a successful data science project. How will the video be taken (geographic location, flight height, type of camera, time of day, etc.)? Will wildlife be identified based on tracks, signs of habitation, animal images? Are there existing models for object detection that can be used directly or enhanced via transfer learning?\nThe data scientist can help to increase specificity of the problem formulation by asking the right questions. Domain experts think and communicate in terms of subject matter. Data scientists can link the problem to data and analytic methods where that is possible. To measure the level of financial risk exposure the Chief Risk Officer of a bank is interested in forecasting VaR, the value at risk in their investment portfolio for the next two weeks. The data scientist can translate this into a simulation study: based on historical data of investment position values states of the market are simulated and the portfolio is evaluated against the simulated future market states. VaR is calculated as the 95th percentile of the distribution of simulated values.\nData science is powerful but not all questions are solved by analyzing data. The best approach might be to build a handcrafted expert system by translating existing knowledge and logic into software (think “TurboTax”) or to develop a piece of hardware. We need to be honest and transparent if data science cannot provide what is asked for. The adage that if you have only a hammer then everything looks like a nail applies here.\nThe results of a data science project need to be relevant and solve a real-world problem. A client once conveyed the following anecdote: he was approached by the data science team that proudly demonstrated a new algorithm to predict a customer’s age based on their online behavior. The executive responded “That’s cool. But we do know their date of birth.”\n\n\n\nThe recommender system no one needs.\n\n\nDuring the Discovery phase we are beginning to deepen our data understanding. What type of data might be needed to solve the problem, and what data sources are available to us? Exploratory data analysis and visualizations inform us about the shape of the data, the distribution of the variables and possible relationships. Data quality issues will surface during this phase.\nIt is important to carefully document what you find and the decisions that are being made. Why are certain data included or excluded? How are variables defined, what metadata is available? What data quality issues have surfaced (outliers, missing values, inconsistent variable definitions, etc.)? This information is important during the next stage when we use data engineering to prepare data for analysis. Just because two tables have a field customer_id does not imply that we can join the tables on that field. Missing values can be replaced by actual or computed values, but such data imputation is not without pitfalls either.\nDuring the Discovery phase it is also not too soon to consider the ethical concerns and implications of the project. If data will be used or collected from individuals, do we have the consent to do so? Are there potential biases that could lead to poor outcomes by perpetuating stereotypes or by withholding resources? As algorithms and models are becoming more complex, less interpretable, and more easily automated, our need for transparency increases. Data scientists play an important role in recognizing and avoiding the introduction or continuation of bias in data-based decision making. We are devoting an entire chapter to the important topic of ethical decisioning with data.",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Science Project Lifecycle</span>"
    ]
  },
  {
    "objectID": "proj/lifecycle.html#data-engineering",
    "href": "proj/lifecycle.html#data-engineering",
    "title": "3  The Data Science Project Lifecycle",
    "section": "3.3 Data Engineering",
    "text": "3.3 Data Engineering\nIf the real-world problem translates into a data science activity, we progress to the next stage.\nDuring the Data Engineering stage these considerations are made concrete. Are the data available to us? What is their size and quality? Are additional attributes required, if so, where do they come from? Do we need to collect more data, or can we obtain data from a 3rd party source (for free, for fee)?\nHow is raw data converted into usable form? How is data from multiple sources integrated, cleansed, and processed? It is not uncommon these days to have data stored in multiple clouds as well as on-premises systems—how do these data come together for analysis in one place?\nIn data science projects that go beyond reporting and building dashboards, the projects that involve data scientists building models to cluster, predict or classify, it is unusual to work directly with the raw data. Instead, data will be organized, cleansed, and stored to fit the subsequent modeling needs. How will the data be prepared for analysis and where is that data stored? Traditionally, analytic tables are two-dimensional arrays of rows and columns, where columns have simple data types such as integers, doubles, or strings. Increasingly, data can be served to analytics as JSON structures, in Parquet format, or in feature stores designed for machine learning. Data engineers and data scientists need to agree on the “best” format considering efficiency, tooling, cost, etc.\nThe answer and approach to many of these questions depends on what you want to do with the data. We are looking down the road to the modeling phases and it is a great time to emphasize that the data science project cycle is not a single clockwise motion but an iterative process.\nWhat data sources you consider is informed by the types of models you are imagining (Model Planning and Model Building stages). On the other hand, the types of data that can be made available at sufficient quantity and quality will inform the types of models you can plan for.\n\n\n\n\n\n\n\nThe data science project cycle is iterative. You might have to move back to an earlier stage based on information discovered at a later stage.\n\n\nIt is often stated that 80% of the data science project is data engineering or data preparation. That is not true. What is true is that you should spend effort here because not having the right data prepared in the right way can invalidate the subsequent steps and jeopardize the project. It is also true that in smaller organizations you might not find dedicated data engineering teams that could do the heavy lifting in preparing high-quality data for analysis; some of that work then falls to the data scientist.\nImagine that you find a very strong relationship between patient well-being and a health metric in a long-term study only to find out when you communicate the results that the definition of the health metric was changed half-way through the study.\nImagine that you fail to find evidence for the effects of climate change in a multi-national study and discover later that during merging of data from different countries measurements in imperial and metric units were intermingled and that temperatures in degrees Fahrenheit were not converted to degrees Celsius.\nWhen data are missing, we must ask questions about why the data points are unobserved. Are entire records missing, for example, when a group of the population was not included in the data collection? Were measurements unobservable because of limited precision of the instruments? Did subjects drop out of the study, if so, is the dropout related to the purpose of the study? Is the reason why survey recipients did not answer questions related to the study itself?\nSome typical tasks during the Data Engineering stage involve\n\nData Sources: Final selection of the data sources.\nData Quality: The result of this is clean and consistent data with known properties. Modifications to raw data are documented. Any remaining data quality issues are documented.\nData Integration: Merging and joining of data sets, creation of new attributes (feature engineering), re-formatting of data, renaming of columns.\nData Delivery: The result of data preparation is made available in an agreed-upon format, for example, as Parquet files stored in S3 or as tables in a database.",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Science Project Lifecycle</span>"
    ]
  },
  {
    "objectID": "proj/lifecycle.html#model-planning",
    "href": "proj/lifecycle.html#model-planning",
    "title": "3  The Data Science Project Lifecycle",
    "section": "3.4 Model Planning",
    "text": "3.4 Model Planning\nA model is a generic term for a system that takes inputs, processes them according to algorithmic logic and produces outputs (Figure 2.2). Chat GPT is a model (a large language model) that takes input in the form of text prompts, passes the prompts through a pre-trained transformer algorithm, and generates text responses. The Chat GPT model contains parameters, unknown quantities for which we need to find values before the model can be used. This is known as training the model. Once trained, the algorithmic logic of the model can be applied—this step is often referred to as model inference in machine learning or scoring in statistics.\n\nTypes of Algorithms\nStatistical models and machine learning models are trained on data. The training process shapes the capabilities of the model based on the training data. As a rule, more complex models require larger training data sets. The following table shows the increasing complexity (as measured by the number of parameters) of Open AI’s GPT foundation models. GPT-3.5 has 175 billion parameters and essentially was trained on all text data available on the internet.\n\nEvolution of number of parameters in Open AI’s generative pre-trained transformer foundation models.\n\n\n\n\n\n\n\nVersion of GPT\nReleased\nNumber of Parameters\n\n\n\n\nGPT-1\nJune 2018\n\n\n\nGPT-2\nFebruary 2019\n117 million\n\n\nGPT-3.5\nNovember 2022\n1.5 billion\n\n\nGPT-4\nMarch 2023\nRumored to consist of 8 sub-models with 220 billion parameters each, 1.7 trillion total\n\n\n\nIn general, models do not have to rely on data to craft their algorithms. Expert systems, also known as hand-crafted knowledge systems, are translating through software the logic applied by experts to make decisions into computer instructions. Tax preparation software is an example of such an expert system. The input to the system is data about your financial situation, the algorithm represents the logic of the tax code translated into machine instructions, the output is your tax liability or refund. You would not want to use a model that is trained on data to predict your tax liability. The training data would be past tax returns, including fraudulent ones and returns that are not at all like yours. The algorithm would not attempt to reproduce the tax code, but to predict the average tax liability of returns similar to those used in training. That might be useful for government planning purposes to estimate budgets but not to ascertain an individual’s precise tax liability.\nThe algorithms in models can also derive from rules based on operating an organization; these are called business rules. They often take the form of “when this, do that” logic. For example,\n\nIf a job completes more than 5 business days after the request date, apply a 10% discount.\nA manager is defined as an employee to whom more than two people report directly.\nCustomers are grouped based on their payment behavior: those who pay monthly balances in full, those who pay in installments, and those who have late payments.\n\nA real-world problem can involve one or more model types, not all of which depend on data science. The solution to the problem can require incorporating, blending (an ensemble), or switching between models.\nDuring the Model Planning stage, we examine alternative approaches to capture the key decision in a data science project. What kind of model can produce a solution: a mathematical model, physical model, statistical model, machine learning model, expert system, etc.? What alternatives are available within the relevant classes? For example, if the goal is to classify a two-level target variable, we might consider logistic regression, classification trees, random forests, gradient boosted trees, support vector machines, regularized regression and so on. What are their pros and cons?\n\n\nPre-built and Pre-trained Models\nNot all models have to be built from scratch with an organization’s own data sources. A pre-built model is a model that was trained by someone else and is ready to use. Deployment options might be to invoke the model as a cloud service through an API (application programming interface) or to run the model in a container on premises. A sentiment model that classifies the writer’s opinion in a piece of text as positive, neutral, or negative, is an example of a pre-built model. Amazon Web Services (AWS) makes this capability available through the Amazon Comprehend API.\nThe terms pre-trained and pre-built models are often used interchangeably; AWS promotes pre-trained SageMaker models in a marketplace where vendors offer access to what we would call pre-built models. A pre-built model cannot be altered and is ready to use. You rely on someone else to train the model and to package it for use. A pre-built model is pre-trained in the sense that the model builder trained the model on some data. It is important that the training data are representative for your application. A pre-built model for sentiment analysis that was trained on data from a different language will not perform well.\nA pre-trained model is a statistical or machine learning model that was trained on data and the model is made available in that state with the ability to continue training. GPT (Generative Pre-trained Transformer) models are an example of pre-training large language models by Open AI. Each model in the GPT family has been pre-trained to a certain point. For example, text-davinci-003 in the GPT-3.5 family was trained on text data found on the internet through June 2021. These large models are also called foundation models, because they are used to build more specific applications with additional training. Chat-GPT, for example, is a question-answer application built on top of the GPT foundation models.\nYou can accept the pre-trained model as the final model and build applications with it—the pre-trained model has become the pre-built model. Or you can continue training the model with additional data. For example, you can continue to train text-davinci-003 by adding more recent data than June 2021. Or you can add data from sources more relevant to your domain than what can be scraped from the internet. OpenAI calls this process fine tuning of models. You will also see the term transfer learning in this context: transferring the capabilities of a model from one domain to another.\nPre-training is common in computer vision and natural language applications where many capable models already exist, building a model from scratch is difficult because of the massive amounts of data and computing resources required, and transferring capabilities by training on additional data has shown to be effective. Convolutional neural networks such as AlexNet, VGG, or GoogLeNet, ResNet trained on an existing corpus of images serve as the starting point\nDuring Model Planning you should decide which models are trained from scratch, pre-trained and fine-tuned, and pre-built. Beware of the Not Invented Here (NIH) attitude: the tendency to avoid using or buying other products and instead to build everything from scratch. There is strong bias against ideas or capabilities from the outside. If a model does not fall into the core specialty or competency of an organization, then it is best to look outside for pre-built or pre-trained models (if you have additional data to train for your context).\nDuring Model Planning we need to also consider the infrastructure available and/or necessary to train, test, and deploy models. Failure to consider deployment constraints at this stage will come back to haunt you during the Deployment stage. Imagine training a deep learning model with millions of parameters using PyTorch to find out later that it needs to be deployed on a small medical device with limiting processing capabilities.",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Science Project Lifecycle</span>"
    ]
  },
  {
    "objectID": "proj/lifecycle.html#model-building",
    "href": "proj/lifecycle.html#model-building",
    "title": "3  The Data Science Project Lifecycle",
    "section": "3.5 Model Building",
    "text": "3.5 Model Building\nThe Model Building stage is where many data scientists and ML engineers prefer to spend their time. Here you train candidate models on data. You evaluate and validate them and determine the optimal values for tuning parameters (bandwidths, number of inputs, learning rate, depth of trees, regularization penalties, …). You use train:test data splits or cross-validation to choose between competing approaches and to balance the bias—variance tradeoff. You use bagging to improve the predictive accuracy of a weak learning technique and bootstrapping to estimate the variability of quantities. It is the fun part of data science; it is where we nerd out.\nMany programs teaching data science focus on the model building stage; it is a rich field that can fill semesters with material. New techniques increase the list of candidates all the time. Fifteen years ago, deep learning models were not a consideration, we lacked affordable compute power and sufficiently large data sets to train deep neural networks well. In the 2000s recurrent neural networks such as long short-term memory models became all the rage in natural language understanding and convolutional neural networks were the go-to architectures for computer vision applications. In the early 2020s, both network types have been mostly replaced by networks based on transformer architectures. How quickly things change.\nWith greater automation (AutoML, for example), tasks in the Model Building stage will be done more efficiently and at greater scale. Effort will shift from building and evaluating the models to applying the models, to building applications with them, and to helping others to reap the benefits of AI. There will always be room for algorithmic innovation in data science. But the goal post moves with advances in technology. Spending all your time building models at the expense of other stages of the project cycle does not reduce the failure rate of data science projects.\nDuring Model Planning and Model Building we consider the tradeoffs between model choices. The model with the highest accuracy or lowest prediction error is not necessarily the best solution. Interpretability of a model can tip the scale toward a less accurate model. A single decision tree, trained and pruned, is easy to visualize and to explain. A random forest of 500 trees cannot be visualized and is more difficult to explain, although it will likely perform better than the single tree. A neural network with thousands (or millions) of parameters is an inscrutable black box and is not intrinsically interpretable. Trying to explain how the network comes up with a decision, which factors drive it, is the best we can hope for. How do you convince stakeholders of the quality of your data-analytical work if they cannot understand it?\n\n\nExample: Concentration of a drug in patient’s body over time\n\n\nThe data in the following graph show the concentration of a drug in the body of patients over time. The drug is administered at time 0, followed by a period of absorption by the body, and a period of elimination from the body. Three candidate models are fit to the data, represented as different lines: A simple linear regression model (dashed line) A local 2nd degree polynomial smoothing model (LOESS, blue line) A nonlinear, first-order compartmental, model (black line) that expresses concentration as follows: \\[\n    C= \\frac{D k_a k_e}{C(k_a-k_e)}  (\\exp⁡(-tk_e)  -\\exp⁡( tk_a))\n    \\] The quantities in the compartmental model represent \\(D\\): the dose of the drug\n\n\\(k_a\\): the rate of absorption by the body\n\\(k_e\\): the rate of elimination from the body\n\\(C\\): the clearance (volume of blood or plasma cleared of the drug per time unit)\n\\(t\\): time since administration of the drug\n\n\n\n\n\n\n\n\nDrug concentration due to absorption and elimination in the body over time and three possible approaches to modeling.\n\n\nThe simple linear regression (dashed) model is very simple, it consists of an intercept at time 0 and a slope. It is easy to explain but for data between 0 and 10 units of time the model is inadequate. The drug concentration over the entire observation period cannot be captured by a straight line. However, once the drug concentration is down to 50—60% of the maximum a straight line might be a reasonable model. The LOESS model (blue) is very flexible and follows the data more closely. It picks up some interesting cyclical behavior between time 12 and time 20. Is it a spurious effect of the model/data interaction or does it capture a real biological effect?\nThe compartmental model has a clear biological interpretation, its quantities relate to well-understood processes. That enables us to ask questions such as “at what time has 30% of the maximal absorbed amount cleared the body?”\nSuppose that the LOESS model predicts drug concentration in patients that did not participate in the study (unseen data) more accurately than the nonlinear compartmental model. Would we recommend the LOESS model over the nonlinear model? Does the answer change if testing hypothesis about the absorption/elimination rates is important?\nWhatever your decision you need to be able to communicate it and convince stakeholders of the value of your work.",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Science Project Lifecycle</span>"
    ]
  },
  {
    "objectID": "proj/lifecycle.html#communication",
    "href": "proj/lifecycle.html#communication",
    "title": "3  The Data Science Project Lifecycle",
    "section": "3.6 Communication",
    "text": "3.6 Communication\nThe Communication stage is an essential part of the project life cycle. The solution we have derived so far does not speak for itself. We made choices and tradeoffs and settled on a particular approach. We learned some new things. Now that we have seen the data engineered to solve the problem through data science what are the data telling us? Were we able to solve the problem? What have we learned?\nSuppose that our data science solution checks all boxes and is the greatest things since sliced bread. It will not see the light of day unless we communicate it to the stakeholders of the problem.\nCommunication can take many forms: memos, emails, reports, dashboards, presentations. The best scenario is to get a chance to present the results in person, connecting the data analysis to the original problem, letting data (not math or code) tell the story. During the Discovery stage we translated the business problem into a data science project. Now we need to translate the data science solution back to the business problem and communicate in the language of the business. The stakeholders need to understand the solution, why it matters, and why they should care. If the model output is not presented in the original problem context, why would anyone outside of the data science team trust it?\nLasswell’s model of communication analyzes communication by asking “Who says what to whom in what channel and with what effect?”\nThe communication needs to present a solution and why the team thinks it is viable. This does not need to be done as a live analytics demo, but the audience should be empowered to judge the solution. Decision makers need to be able to decide whether to move forward with implementation, whether to return to an earlier stage in the project cycle, or whether to stop. Model accuracy, test error, or specificity are metrics that matter to the data scientist, the business executive considers KPIs such as bookings, revenue, funnel size and conversion rates.\nDo not overestimate the persuasiveness of a black-box algorithm and do not underestimate how much information a simple bar chart or box plot conveys to a non-data scientist.\nA lot is at stake at this stage.",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Science Project Lifecycle</span>"
    ]
  },
  {
    "objectID": "proj/lifecycle.html#operationalization-deployment",
    "href": "proj/lifecycle.html#operationalization-deployment",
    "title": "3  The Data Science Project Lifecycle",
    "section": "3.7 Operationalization (Deployment)",
    "text": "3.7 Operationalization (Deployment)\nThe Operationalization (Deployment) and Monitoring stages are combined into Model Operations, or ModelOps, the discipline that implements algorithmic logic and monitors it over time. We call out deployment as a stage within ModelOps because so many projects fail here.\n\n\n\n\n\n\n\nModel operations combine deployment, management, and monitoring of analytic assets.\n\n\nDeployment can be simple, sometimes a report or a table in a spreadsheet is all that is needed. It can also be highly complex such as implementing a real-time prediction system across an organization in collaboration with the IT department.\n\n\nExample: Predicting Sepsis in Preemies\n\n\nOne of the major causes of death in premature babies (preemies) is sepsis. The fatality rate of early-onset sepsis in babies with low birth weight is 3—40%, up to 4 times higher than in full-term infants. Preemies have weaker immune system; their skin and gut are also more fragile. The presence of tubes for ventilation or IVs into veins increases the risk of infection.\nDoctors and nurses watch for signs of sepsis such as very high or low temperature, changes in skin color and changes in behavior. Those are non-specific signs, and it would be great if we can develop an early-warning system that alerts doctors when the probability that a preemie might develop system exceeds a threshold.\nA statistical model is developed based on data that integrates sensor information from incubators in neonatal intensive care units (temperature, breathing, vitals, …), computer vision (movement, skin color, …), and clinical records. Data from these three sources was captured over a period of six months and formatted by the data engineering team into a large data frame for the data science team.\nThe model developed by the data scientists based on the provided data is a success in that it can predict the probability of sepsis in preemies based on the integrated data with greater accuracy and more quickly than one would be able based on the clinical records alone.\nWhat difficulties might have to be overcome to deploy the model in the NICU of a hospital?\n\nThe hospital has incubators from different manufacturers. Not all incubators are equipped with sensors and cameras.\nThe model was developed based on 6-month historical data. It will be deployed in a real-time system where data from sensors and cameras must be prepared and merged with clinical records on the fly. The data engineering pipeline needs to be integrated into the operations.\nHow is the sensor and camera data be made available? Is it part of an existing data flow from the NICU? Is it collected over WiFi, Bluetooth, or some other network technology?\nHow does the data get to the model inference engine that calculates the predicted probabilities for sepsis?\nHow do the predicted probabilities of sepsis surface in the workflow of NICU nurses and doctors? A continuous real-time display added to the NICU machinery? Or is it accessed as a separate piece of information on a laptop or mobile device?\nHow do predicted probabilities map to sepsis risk and medical intervention?\n\n\n\nThis section is concerned with the more complex deployment alternatives and the shocking reality that roughly only half of all production-ready models are ever placed in production. Of those that are, many take three months or more to be deployed. If the model has a short shelf life due to seasonal variations it might already be obsolete when it finally sees the light of day.\nWhy is the success rate so low and why does it take so long to finally move into production?\nMany organizations have separate systems for development, testing, and production (dev-test-prod). Differences in architecture, operating systems, network support, language support, security, and access controls can cause delays. When a model is developed in Python or R and needs to run in production on a device that does not support their runtime or containers, then the model must be recoded. Translating a machine learning pipeline from Python or R into a language such as C/C++ or Rust is time-consuming and error prone, requiring testing and verification (confirmation that the model was correctly implemented). If, on the other hand, you are deploying on premises on standard hardware, then you can wrap the Python models in a web-service framework (Flask, Django) and deploy through a Docker container.\nMost algorithms are trained in batch on historical data. Many algorithms are deployed in a real-time system online where they process data points as they arise or periodically in small batches. Technical choices need to be made about how the online system accesses the algorithm:\n\nType of connection and communication\n\none-way stateless webhooks over HTTP\ntwo-way stateful WebSockets over WS\nan asynchronous publish/subscribe (pub/sub) architecture\n\nType of API\n\nRESTful\nSOAP\nRemote procedure calls (RPC, gRPC),\nEvent-driven\nFlask\n\n\nModels can be large, neural networks can have millions or even billions of parameters (GPT-1 has 120 million, GPT-2 has 1.5 billion, GPT-3.5 has 175 billion parameters.) While such large models can be accurate, the process of applying the model to a new data point (a process known as scoring or inferencing the model) can be slow. Loading a large model into memory can require too many resources. Model compression techniques such as pruning, distillation, or quantization are then used to reduce the size of the model without sacrificing prediction or classification accuracy. A model modified in any form after its derivation needs to go through validation: confirming that the model still works as intended.\nAt the operationalization stage the primary responsibility for the data science assets shifts from the data science team to the user-facing teams and the IT department. The model turns into a data product, a technological product that depends on data to function.\nMany organizations have internal processes that examine and approve products (built in-house or by a third party) for\n\nCompliance with regulatory requirements: Payment Card Industry Data Security Standard (PCI DSS), Federal Information Security Management Act (FISMA), General Data Protection Regulation (GDPR) of the EU, Health Insurance Portability and Accountability Act (HIPAA), etc.\nPrivacy\nSecurity\nTransparency\n\nThe output of a data science project is also a product—a data product and has to go through these processes as well; that takes time.\nWhen the data product replaces an existing solution, A/B testing is common to verify that the new solution is indeed superior. Just because a model passes unit tests does not mean it is moving the needle for the business. When data scientists build models, they measure model performance against historical data sets based on statistical metrics such as mean-squared prediction error, mis-classification rate, sensitivity, specificity, area-under-the-curve, etc. The business measures success in terms of OKRs (Objectives and Key Results) and/or KPIs (Key Performance Indicators) such as customer conversion rates, subscriptions, click-thru rates, bookings, revenue. In an A/B test users are randomly divided into two cohorts, experiencing the current and the new solution, respectively. The solution performance is evaluated through business metrics and technical criteria (speed, resource requirements, scalability).",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Science Project Lifecycle</span>"
    ]
  },
  {
    "objectID": "proj/lifecycle.html#management-and-monitoring",
    "href": "proj/lifecycle.html#management-and-monitoring",
    "title": "3  The Data Science Project Lifecycle",
    "section": "3.8 Management and Monitoring",
    "text": "3.8 Management and Monitoring\nOnce a model is deployed in production it enters the second phase of ModelOps, lifecycle model management and model governance. Governance includes the application of DevOps engineering principles such as continuous integration and continuous deployment (CI/CD), versioning of model, data, and code, and automation. The newly deployed model might be challenged by a new solution in a few months and becomes itself the target of A/B testing.\nNew data collected since the initial model deployment can be added to the training data and the model parameters can be re-estimated (continuous training); this leads to a situation where a more recent version of the model is A/B tested against an existing deployment of the same model.\nContinuous monitoring of the model is necessary to detect model drift. This is the condition where model performance deteriorates over time. Performance is tracked using statistical measures (accuracy, sensitivity, mean-squared error, …) and KPIs. There are two primary causes for drift in analytical models:\n\nConcept drift. The task the model was designed to perform changes over time. This happens when the nature of the target variable of the model changes or when fundamental assumptions of the model no longer hold. When the definition of the target changes, e.g., a new definition of spam email or changes in regulations that broaden the meaning of money laundering, a model derived under the previous definition might now be invalid. The distribution of the target can change over time or seasonally from symmetric to skewed, causing model drift. The COVID pandemic has fundamentally voided common assumptions that flow into many types of models: consumer behavior, liquidity of financial instruments, valuations of commercial real estate, gas and power consumption patterns, etc.\n\nIn “Learning under Concept Drift: A Review”, Lu et al. (Arxiv:2004.05785, 2020) distinguish four types of concept drift depending on how the change over time takes place (Figure 3.4).\n\n\n\n\n\n\nFigure 3.4: The four types of concept drift according to Lu et al.\n\n\n\nData drift. The data has drifted when the distributions of the input variables into a model change or when the relationships between the inputs change (Figure 3.5). The data to which the model is applied in production is then not representative of the data on which it was trained. Data drift is a major source of bias in models. The model itself remains correct and is possibly unbiased based on the training data set. Bias is introduced when it is applied to data that is systematically different from the training data.\n\n\n\n\n\n\nFigure 3.5: An example of data drift. The training data for input variable X has a symmetric distribution with a mean of 2. The data seen by the model in production has a right-skewed distribution and a mean of 5. The production model will see values of X with different frequencies as during training and will see values of X it never encountered during training. We do not know whether the model applies for X &gt; 10, for example.",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Science Project Lifecycle</span>"
    ]
  },
  {
    "objectID": "proj/lifecycle.html#case-study-airbnbs-party-pooper",
    "href": "proj/lifecycle.html#case-study-airbnbs-party-pooper",
    "title": "3  The Data Science Project Lifecycle",
    "section": "3.9 Case Study: Airbnb’s Party Pooper",
    "text": "3.9 Case Study: Airbnb’s Party Pooper\nThis case study is based on a Sept 2023 CNBC article.\nAirbnb operates an online marketplace for short- and long-term stays at private homes. The company acts as a middleman to connect homeowners with renters and charges a commission for the bookings. Some years ago, Airbnb became concerned about party bookings that cause “significant disruption to neighbors and the surrounding community.” The COVID-19 pandemic made things worse as folks were looking for creative ways to get together. The pandemic and Airbnb hosts’ fears of property damage were main drivers behind the company taking a stand against party bookings.\n\nDiscovery\nNaba Banerjee joined Airbnb’s trust and safety team in 2020 and was charged with finding a solution. The business problem was clear: Prevent party bookings by blocking reservations but avoid blocking legitimate bookings.\nShe approached it in three phases: What can be done right now, within the next year, and in the future. This hints at an iterative process; one that starts with an immediate simpler intervention, followed by more deeply developed solutions. It also hints at the fact that whatever solution is being put into place will need to be monitored for performance and updated. Some Airbnb guests will try to circumvent the solution and get past the party-blocking methodology.\n\n\nData Engineering & Model Planning\nAll solution phases were driven by data. In the short term, for the right-now solution, they studied the patterns and signals of bookings that were reported as parties. The data for this effort was already available on their internal platform, it just needed to be analyzed. There was no modeling effort involved. It was about understanding the data they had on party bookings, profiling and describing the data, looking for signals that correlate highly with the booking label “party”. The data product of this phase was a simple business rule: ban high-risk reservations by users under age 25, in particular if they have received poor reviews from hosts or do not have much history on Airbnb. This measure, combined with removing the option for hosts to list their homes for gatherings of more than 16 people, was effective.\n\n\nModel Building\nBanerjee and her team had developed a classification model at this point. It was classifying booking attempts as high-risk party bookings or benign bookings based on a black-and-white categorization of bookings. The next step, the mid-term solution, was to build a more sophisticated analytic model that takes into account more factors and that produces a risk score, a probability that the booking is a party booking. Whether to block a booking based on the risk score can vary based on risk tolerance—regionally and temporally. For example, a risk score of x% might not lead to a reservation block during most periods, but around Halloween, New Year’s Eve and Fourth of July will deny the same reservation.\nThe model developed at Airbnb takes into account many factors such as the closeness of the reservation to the user’s birthday, the user’s age, length of stay, proximity to where the user lives, day of the week, location of the listing, etc. The type of model selected was a neural network trained using supervised learning on bookings labeled as parties or property damage. The performance of the model was tested against past incidences as well as hypothetical test cases and normal, “good” behavior. Airbnb developed multiple models to predict the likelihood of a party booking, to predict property damage, etc.\n\n\nOperationalization\nThis is a good example of the data—analytics—decision chain of intelligence. Building the predictive risk model was applying advanced analytics to data. The outcome of the data—analytics sequence was a machine learning model that can produce a risk score for any booking. But the machine learning model itself does not block a booking. It produces a risk score that—to complete the analytics—decision loop—must be translated into a decision: make the reservation or block the reservation. In Airbnb’s case the decision rule is nuanced temporally and geographically. The risk of parties is higher in the U.S. and Canada, followed by Australia, Europe, and Asia. It might also take other factors into account besides the likelihood of a party booking, e.g., the extent of property damage. Furthermore, in situations where the decision is not clear, the booking goes through human review to determine the party risk.\nTo implement the model the prediction logic and the decision logic (analytics—decision) had to be incorporated into the online booking system in real time. As soon as a reservation is attempted to be made, the system needs to calculate the risk score, make a decision and inform the user either by continuing the process or blocking it with the dialog (Figure 3.6).\n\n\n\n\n\n\nFigure 3.6: The Airbnb reservation system blocks a suspected party booking. Source.\n\n\n\nThe implementation was initially rolled out in Australia where Airbnb experienced an uptick in party bookings in early 2021 after travel had stopped because of the pandemic. The system was rolled out there as a pilot in October 2021 using A/B testing methods—some regions of Australia used the system while other regions did not. Party bookings dropped by 35% in the regions where the risk model was deployed.\nIn Fall of 2022 the system went live globally. Over Labor Day weekend alone, the system blocked or redirected 5,000 potential party bookings.\n\n\nModel Management and Monitoring\nAirbnb continuously monitors the performance of the model using statistics such as precision and recall for classification models. Recall, also called the sensitivity of the classification model, is the true positive rate of the model: the ratio of the actual party bookings that the model should have predicted as party bookings. Precision, on the other hand, is the ratio of the actual party bookings relative to any bookings that were predicted to be parties—which includes some false positives.\nThe model can drift as user’s behavior changes. Party-inclined users are trying to find ways to circumvent the booking block. The model has to be re-evaluated and cannot be static. A decline in the sensitivity of the model would suggest that the model is not as capable in detecting party bookings.\nAirbnb also points out that the system has been evaluated by the company’s anti-discrimination team for potential bias. You want to make sure that the system does not have unintended consequences such as unfairly disenfranchising bookings by one group over another.\nWhat is the next phase of solution development at Airbnb for this problem? You guessed it, something that has to do with large language models (LLMs) like ChatGPT.",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Science Project Lifecycle</span>"
    ]
  },
  {
    "objectID": "proj/lifecycle.html#thinking-data-science",
    "href": "proj/lifecycle.html#thinking-data-science",
    "title": "3  The Data Science Project Lifecycle",
    "section": "3.10 Thinking Data Science",
    "text": "3.10 Thinking Data Science\nThe data scientist takes on different roles and personas throughout the project. Whether you are writing analytic code or communicating with team members or working with the IT department on implementation, you always think like a data scientist. What does that mean?\n\nDealing with Uncertainty\nIn “Think Like a Data Scientist”, Brian Godsey lists awareness in the face of uncertainty as one of the biggest strengths of a data scientist. Once source of uncertainty is the inherent complexity of data science projects that involve many parts of the organization. A data science project is full of problems, the real-world problem that needs to be solved and problems that arise during the projects. Deadlines will be missed. Data will not be as available as expected or of lower quality as expected. Budgets can change and goals are reprioritized. Models that work well on paper fall apart when they get in contact with reality. The list goes on and on. The data scientist thinks through problems not from the perspective of the data but from the perspective what data can help us to accomplish. What matters is that we solve the business problem, not that we build a neural network.\nAnother source of uncertainty is the inherent variability of the raw material, data. Variability is contagious, it makes everything produced from data also variable. The point of data processing is to separate the signal from the noise and to find the systematic patterns and relationships in the data, the insights that help make decisions.\nData scientists are sometimes compared to software developers. They do share certain traits; both are using tools, languages, and frameworks to build complex systems with software. But analytic code is different from non-analytic code in that it is processing an uncertain input. A JSON parser also processes variability, each JSON document is different from the next. Does it not also deal with uncertain input? If the parser is free of bugs, the result of parsing is known with certainty. For example, we are convinced that the sentence “this book is certainly concerned with uncertainty” has been correctly extracted from the JSON file. Assessing the sentiment of the sentence, however, is a data science task: a sentiment model is applied to the text and returns a set of probabilities indicating how likely the model believes the sentiment of the text is negative, neutral, or positive. Subsequent steps taken in the software are based on interpreting what is probable.\nThere is uncertainty about which method to use. Whether a software developer uses a quicksort or merge sort algorithm to order an array has impact on the performance of the code but not on the result. Whether you choose a decision tree or a support vector machine to classify the data in the array impacts the performance and the result of the code. A chosen value for a tuning parameter, e.g., the learning rate, can produce stable results with one data set and highly volatile results with another.\nFurther uncertainty is introduced through analytic steps that are themselves random. Splitting data into training and test data sets, creating random folds for cross-validation, drawing bootstrap samples to estimate variability or to stabilize analytics through bagging, random starting values in clustering or neural networks, selecting the predictors in random forests, Monte Carlo estimation, are some examples where data analysis involves drawing random numbers. The data scientist needs to ensure that random number sequences that create different numerical results do not affect the quality of the answers. The results are frequently made repeatable by fixing the seed or starting value of the random number generator. While this makes the program flow repeatable, it is yet another quantity that affects the numerical results. It is also a potential source for misuse: “let me see if another seed value produces a smaller prediction error.”\nData are messy and possibly full of errors. It contains missing values. There is uncertainty about how disparate data sources represent a feature (a customer, a region, a temperature) that affects how you integrate the data sources. These sources of uncertainty can be managed through proper data quality and data integration. As a data scientist you need to be aware and respectful of these issues; they can doom a project if not properly addressed. In an organization without a dedicated data engineering team resolving data quality issues might fall on your shoulders. If you are lucky to work with a data engineering team you still need to be mindful of these challenges and able to confirm that they have been addressed or deal with some of them (missing values).\n\n\nExample: Imputation through Principal Component Analysis (PCA)\n\n\n\nA missing value pattern.\n\n\nID\nType\nX1\nX2\nX3\n\n\n\n\n1\nA\n3.71\n5.93\n55\n\n\n2\nB\n.\n4.29\n32\n\n\n3\nA\n0.98\n5.86\n55\n\n\n4\nB\n.\n4.28\n29\n\n\n5\nA\n6.04\n5.94\n48\n\n\n6\nB\n.\n5.25\n18\n\n\n7\nA\n1.52\n4.01\n61\n\n\n8\nB\n.\n5.55\n30\n\n\n\nThe table above shows eight observations on four features: Type, X1, X2, and X3.\nWhatever types A and B represent, we notice that values for X1 are missing whenever Type equals B. A complete-case analysis based on X1 through X3 would eliminate observations with missing values and leave us only with observations for Type A. This makes a comparison between the two types impossible. To facilitate such a comparison, we could limit any analysis to rely on only X1 and X2, or we could impute the missing values for X3.\nSuch an imputation could be based on a matrix completion algorithm that uses principal component analysis (PCA) to iteratively fill in the missing information for X1 based on the observed data for X1, X2, and X3. That sounds awesome. But what if there are systematic differences between the two types? Does it then make sense to fill in values for X1, Type B with information derived from Type A? Could this possibly bias the analysis and be worse than not using X1 in the analysis at all?\n\n\n\n\nPerspective about Data\nMaintaining proper perspective about data means knowing where the important issues are. This can be a moving target.\n\nThe volume of data can be unproblematic for one analytic method and becomes a limiting factor if you want to derive prediction intervals by way of bootstrapping.\nA large but highly accurate model is being developed on historical training data in an Internet of Things (IoT) application. Due to the complexity of the model the scoring engine that applies the model in real time to data flowing off sensors cannot keep up.\nA stacked ensemble of classification models improves the ability to segment customers but is not interpretable.\n\nThere are many limiting factors in data-centric applications and many moving parts. The data scientist is in a unique position to have perspective and awareness of the factors related to data. It is unlikely that anyone else will have that 360-degree view about data.\nAs Brian Godsey put it:\n\nAs a data scientist, I have as my goal to make sure that no important aspect of a project goes awry unnoticed. When something goes wrong—and something will—I want to notice it so that I can fix it.\n\n\n\nIntuition for Data\nIntuition is the feeling of knowing something instinctively, without conscious reasoning. Developing intuition for data is one of the most valuable skills a data scientist can acquire. It will help to flag things that are surprising, curious, worth another look, or that do not pass the smell test. Developing intuition is helped by strong technical knowledge, it provides the foundation against which our conscious mind can judge surprise. Some intuition for data is innate, and it can be developed with experience and practice. Being curious, asking questions, requesting feedback, working with real data sets, and spending time exploring data go a long way in developing better intuition.\nLet’s test our intuition for data with a few examples.\n\nOutliers in Box and Whisker Plot\nThe following graph shows a box plot constructed from 200 observations on a variable. What does your intuition tell you about the five data points on the right side in of Figure 3.7?\n\n\n\n\n\n\nFigure 3.7: Box plot of 200 observations for a variable.\n\n\n\nAlso called the box-and-whisker plot, this plot places a box that covers the central 50% of the data—called the interquartile range—and extends from the edge of the box whiskers to the observations within \\(\\pm 1.5\\) times the interquartile range. Points that fall outside the whiskers are labeled as outliers. Does knowing the details of box plot construction change your intuition about the data points on the right?\nAfter having seen many box plots you will look at this specimen as an example of a continuous random variable with a right-skewed (long right tail) distribution. Five “outliers” out of 200 is not alarming when the distribution has a long tail.\n\n\nPrediction Error in Test and Training Data\nFigure 3.8 shows graphs of the mean-squared prediction error, a measure of model performance, as a function of model complexity. The complexity is expressed in terms of the model’s ability to capture more wiggly structures (flexibility). The training data is the set of observations used to determine the model parameters. The test data is a separate set of observations used to evaluate how well the fitted (=trained) model generalizes to new data points.\n\n\n\n\n\n\nFigure 3.8: The mean-squared prediction error for a set of training and test data as a function of model complexity.\n\n\n\nSomething is off here. The prediction error on the test data set should not decrease steadily as the model flexibility increases. The performance of the model on the test data will be poor if the model is too flexible or too inflexible. The prediction error on the training data on the other hand will decline with greater model flexibility. Intuition for data suggests that measures of performance are optimized on the training data set and should be lower for the test data set. It is worth checking whether the labels in the legend are reversed.\n\n\nStorks and Babies\nThe next example of data intuition is shown in the following scatterplot based on data in Neyman (1952). Data on the birth rate in 54 counties, calculated as the number of babies born relative to the number of women of child-bearing age, and the density of storks, calculated as the number of storks relative to the same number of women, suggests a trend between the density of storks and the birth rate.\n\n\n\n\n\n\nFigure 3.9: Scatterplot of stork density and birth rate along with a LOESS fit. The variables are calculated by dividing the number of babies and the number of storks in the county by the number of women of child-bearing age in the county (in 10,000).\n\n\n\nOur intuition tells us that something does not seem quite right. The myth of storks bringing babies has been debunked—conclusively. The data seem to tell a different story, however. What does your data intuition tell you?\nThere must be a different reason for the relationship that appears in the plot. Both variables plotted are divided by the same quantity, the number of women of child-bearing age. This number will be larger for larger counties, as will be the number of babies and, if the counties are comparable, the number of storks. In the absence of a relationship between the number of babies and the number of storks, a spurious relationship is introduced by dividing both with the same denominator.\n\n\nCluster Assignment\nClustering is an unsupervised learning technique where the goal is to find groups of observations (or groups of features) that are in some form alike. In statistical terms we try to assign data to groups such that the within-group variability is minimized, and the between-group variability is maximized. In this example of clustering, data on age, income, and a spending score were obtained for 200 shopping mall customers. The spending score is a value assigned by the mall, higher scores indicate a higher propensity of the customer to make purchases at the mall stores.\nA cluster analysis was performed to group similar customers into segments, so that a marketing campaign aimed at increasing mall revenue can target customers efficiently. A few observations are shown in the next table.\n\n\n\nCustomer ID\nAge\nIncome\nSpending Score\n\n\n1\n19\n15\n39\n\n\n2\n20\n16\n6\n\n\n3\n35\n120\n79\n\n\n4\n45\n126\n28\n\n\n\nFigure 3.10 shows a scatter plot of the standardized income and spending score attributes, overlaid with the customer assignment to one of five clusters.\n\n\n\n\n\n\nFigure 3.10: Results of hierarchical clustering with five clusters.\n\n\n\nThere are distinct groups of points with respect to (the standardized) spending and income. One group is near the center of the coordinate system, one group is in the upper-right quadrant, one group is in the lower-right quadrant. If the clustering algorithm worked properly, then these points should not be all assigned to the same cluster. This is an example where we need to take another look at the analysis. It turns out that the customer ID was erroneously included in the analysis. Unless that identifier is meaningful in distinguishing customers, it must be removed from the analysis. The results of the analysis without the customer ID are shown in Figure 3.11, confirming that the clustering algorithm indeed detected groups that have (nearly) distinct value ranges for spending score and income.\n\n\n\n\n\n\nFigure 3.11: Corrected cluster analysis after removing the non-informative customer ID variable from the analysis. The detected clusters map more clearly to groups of points that are distinct with respect to spending and income.\n\n\n\n\n\n\nFigure 3.1: The concept of “T-shaped” competencies, combining deep expertise in one area with breadth in multiple disciplines and systems.\nFigure 3.2: The stages of the data science project lifecycle.\nFigure 3.3: The CRISP-DM model for the data science process.\nThe data science project cycle is iterative. You might have to move back to an earlier stage based on information discovered at a later stage.\nDrug concentration due to absorption and elimination in the body over time and three possible approaches to modeling.\nModel operations combine deployment, management, and monitoring of analytic assets.\nFigure 3.4: The four types of concept drift according to Lu et al.\nFigure 3.5: An example of data drift. The training data for input variable X has a symmetric distribution with a mean of 2. The data seen by the model in production has a right-skewed distribution and a mean of 5. The production model will see values of X with different frequencies as during training and will see values of X it never encountered during training. We do not know whether the model applies for X &gt; 10, for example.\nFigure 3.6: The Airbnb reservation system blocks a suspected party booking. Source.\nFigure 3.7: Box plot of 200 observations for a variable.\nFigure 3.8: The mean-squared prediction error for a set of training and test data as a function of model complexity.\nFigure 3.9: Scatterplot of stork density and birth rate along with a LOESS fit. The variables are calculated by dividing the number of babies and the number of storks in the county by the number of women of child-bearing age in the county (in 10,000).\nFigure 3.10: Results of hierarchical clustering with five clusters.\nFigure 3.11: Corrected cluster analysis after removing the non-informative customer ID variable from the analysis. The detected clusters map more clearly to groups of points that are distinct with respect to spending and income.",
    "crumbs": [
      "Part I. Data Science and Data Science Projects",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Science Project Lifecycle</span>"
    ]
  },
  {
    "objectID": "data/sources_and_files.html",
    "href": "data/sources_and_files.html",
    "title": "4  Data Sources and File Formats",
    "section": "",
    "text": "4.1 Databases\nAccording to Wikipedia, a database is an “organized collection of data stored and accessed electronically through the use of a database management system” (DBMS). The three elements are the data, a form of data storage, and DBMS software that allows a user to interact with the system. The terms database and DBMS are often used interchangeably.\nDatabases play a central role in the IT stack of an organization and choosing the wrong database for an application can have dire consequences with respect to availability, scalability, security, data quality, data validity, etc. The site DB-engines collects information about databases and ranks them by popularity based on a somewhat transparent algorithm. If you think there are a lot of data science tools to choose from, wait until you wade into the world of databases—DB-engines lists over 400.\nUnless you build your own tech stack, write your own application, or start your own company, you will probably not select a database. In most situations the databases have been chosen and implemented as part of the backend tech stack. Often there will be more than one, for example, MongoDB for documents, MySQL for transactions, Redshift or Spark for analytics, Redis as a memory cache—the dreaded database sprawl in organizations. Databases are part of the backend infrastructure of organizations, the part that does not change frequently. The so-called Gartner spaghetti graph in Figure 4.1 shows market share of databases between 2011 and 2021. While there is some stability near the top of the market, several items are noteworthy:\nIt is important for anyone involved with data to understand some basics of database architectures and the strengths and weaknesses of the different database designs. A NoSQL key-value store can be a great way to work with unstructured data but can be a terrible choice for analytics compared to a relational column store. Many modern DBMS are multi-model, supporting multiple data models in a single backend, for example, key-value pairs, relational, and time-series data. There are so many databases because there are so many use cases for working with data; databases try to optimize for specific data access patterns and use cases.\nDatabases can be organized in several ways.",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Sources and File Formats</span>"
    ]
  },
  {
    "objectID": "data/sources_and_files.html#sec-datasource-dbms",
    "href": "data/sources_and_files.html#sec-datasource-dbms",
    "title": "4  Data Sources and File Formats",
    "section": "",
    "text": "A change of guard at the top. Oracle has dominated the market for a long time and is being challenged and/or surpassed by Microsoft, Amazon Web Services, and Google Cloud Platform. The cloud providers are leading the way with compelling cloud database offerings.\nGartner depicts pure-play cloud database vendors in light blue. The spaghetti chart shows the revenue shift in databases to the cloud.\nThe vertical axis displays market share across all software markets. Database expenses make up about 35% of the overall software expenses. That share of the overall software market continues to increase (the chart is fanning out more toward the right).\nMarket share is calculated based on revenue, it does not account for free offerings, such as open-source databases.\nAfter a peak around 2017, Hadoop-based providers of big data resources are on the decline, for example, Cloudera.\nSome cloud database providers had an impressive rise since their inception, AWS since 2013, Alibaba Cloud since 2014, Google Cloud since 2015, Tencent since 2016, Snowflake since 2016.\nA surprising amount of movement in a market segment that is supposed to be relatively stable. Organizations are loath to change database management systems that are key to their operations. Migrating a database that is the backbone of an essential part of the business is the last thing a CIO wants to do. Databases are not as sticky as they used to be.\n\n\n\n\n\n\n\nFigure 4.1: Database market share 2011—2021, according to Gardner. Source.\n\n\n\n\n\n\nRelational and Non-relational databases\nIn a relational DBMS (a RDBMS) the data are organized in 2-dimensional tables of rows and columns. A database consists of a collection of named tables, each with a named collection of attributes (columns). Typically, RDBMS use SQL (Structured Query Language) to manipulate the contents of the DBMS. Each row in a table has a unique key and can be linked to rows in other tables. Relationships are logical connections between tables through keys. The structure of a table (columns, primary keys) is known as the table schema, and it is defined before the table is populated with data—this is also called a schema-on-write design (Figure 4.2).\n\n\n\n\n\n\nFigure 4.2: Schema on write with PostgreSQL. The CREATE TABLE statement creates the table named titanic and defines the schema: names, order, and data types of columns and default values. The \\copy statement populates the database table with data.\n\n\n\nThe relational principle has dominated the world of databases since the 1980s. Seven relational databases are among the top ten databases according to DB-engines: Oracle, MySQL, Microsoft SQL Server, PostgreSQL, IBM DB2, Microsoft Access, and SQLite.\nWith the advent of Big Data more data was collected in ways that did not fit well with the relational paradigm: key-value pairs, documents, time series data. More fluid and dynamic relationships between data points were better captured with graph structures rather than rigid relational structures. Big Data analytics also asked for more flexibility in defining data structures on top of the raw data. The relational model was thought to be too restrictive. If the schema must be declared when the table is created and before rows are added (schema-on-write) it makes it difficult to bring new data in on the fly. Schema-on-write in relational databases requires to define the schema for the table and to structure the data based on the data. Changing the structure of the data, for example from text to numbers, requires changes to the schema and results in table copies.\nSchema-on-read, on the other hand, applies structure to the data when it is read (Figure 4.3). That gives users the ability to structure the same source data differently, depending on the application. Schema-on-read led to a new class of data frameworks and databases that broke with the highly structured relational logical data model. These non-relational databases store data in key-value pairs and documents rather than tables and were also called NoSQL databases because they eschewed the structured nature of SQL-based schema-on-write. However, there is still a structuring step in non-relational databases. It just happens on demand when the data is read instead of at the beginning before the first row of data is inserted.\n\n\n\n\n\n\nFigure 4.3: An example of schema on read with Hadoop and its file system (HDFS). The data are loaded into Hadoop with the hdfs dfs command. No schema is defined at this stage. The data is then processed with the hadoop command. The customer-mapper.py Python script determines how the data is interpreted for this job; the Python script applies the schema on read.\n\n\n\nNon-relational databases are sometimes called schemaless databases; that goes a bit too far as some structure is needed to read the data from a key-value or document store into an application. The best translation of “NoSQL” is “not-only-SQL”, these databases still need a structured way to interact with the database.\nIt is easy to see how the push for more flexibility through schema-on-read goes hand in hand with the greater flexibility achieved by moving from the ETL to the ELT processing paradigm. Non-relational databases also gained popularity because they support horizontal scaling more easily than their relational counterparts. When a system scales horizontally—also called scaling out—additional machines are added to support increasing workload. Vertical scaling—also called scaling up—adds more computing resources (CPU, RAM, GPU, …) to existing machines.\nYou can explain the difference between the scaling models by thinking of a building with a fixed number of rooms. To increase the total number of rooms you can either add more floors (scaling up) or add more buildings (scaling out). Both types of scaling reach limits, a single building cannot be made arbitrarily tall and adding buildings consumes land. The limits for horizontal scaling are much higher than for vertical scaling.\n\n\n\n\n\n\nFigure 4.4: Scaling up (vertically) and scaling out (horizontally). Scaling up adds more rooms to an existing building, scaling out adds more buildings.\n\n\n\nBecause non-relational systems partition data on just a primary key and do not need to maintain relationships between tables, scaling them horizontally is relatively simple. As the database grows (contains more keys) add new machines to hold the additional keys. The data can be re-partitioned by re-hashing keys across the larger cluster.\nWith relational systems it is necessary to maintain the relationships between the tables. When data are distributed over multiple machines, this becomes more difficult. With relational databases, the response to growing data sizes historically was to scale up. NewSQL databases are relational databases designed with horizontal scaling in mind.\nNon-relational (NoSQL) databases are categorized by their underlying data models into pure document stores, key-value stores, and graph databases—more on these designs below. NoSQL is primarily used in transactional databases for OLTP (online transactional processing) and applications where interactions with the database are limited to CRUD operations (Create, Read, Update, Delete). They are not performant for analytical jobs.\n\n\nTransactional (OLTP) and Analytical (OLAP) Databases\nA transactional database is a repository to record the transactions of the business, the ways in which an organization interacts with others and exchanges goods and services. Business interactions that are recorded in transactional systems are, for example, purchases, returns, debits, credits, signups, subscriptions, dividends, interest, trades, payroll, donations.\nDatabases that support business transactions are optimized for a high volume of concurrent write operations: adding new rows, updating rows, inserting rows. These transactions are often executed in real time, hence the name online transaction processing (OLTP).\nThe concept of a database transaction is different from a business transaction. A database transaction is a single logical unit of work that, if successful, changes the state of the database. It can comprise multiple database operations such as reading, writing, indexing, updating logs. A business transaction, a customer orders an item online, can be associated with one or more database transactions.\nDatabase transactions are not just reserved for OLTP systems, although supporting transactional integrity with so-called ACID properties is a must for them. Analytic databases, graph databases, NoSQL databases typically support transactions. In relational SQL systems, look for BEGIN statements to flag the start of a transaction, the COMMIT statement to commit the results of a transaction and the ROLLBACK statement to reset the state of the database prior to the transaction.\nThe ACID properties refer to atomicity, consistency, isolation, and durability to ensure the integrity of the transaction.\n\nAtomicity: all changes are performed as if they were a single operation. Either all of them are performed or none of them.\nConsistency: When the transaction starts and when it finishes, the data are in a consistent state.\nIsolation: intermediate states of the transaction are invisible to the rest of the system. When multiple transactions happen at the same time, it appears as if they execute serially.\nDurability: After a transaction completes, changes to the data are not undone.\n\nTake as an example the transfer of funds from one account to another. Atomicity requires that if a debit is made from one account a credit in the same amount is made to the other account. Consistency requires that the sum of the balances in the two accounts is the same at the start of the transaction and at the end. Isolation implies that a concurrent transaction sees the amount to be transferred either in the debited account or in the credited account. If a concurrent transaction would see the amount in neither account or in both accounts the database fails the isolation test. Durability means that if the database fails for some reason after the transaction completes, the changes made to the accounts will persist.\nExamples of transactional (OLTP) relational databases are MySQL, PostgreSQL, Microsoft SQL Server, Oracle Database. These systems are also called Systems of Record (SoR) because they store the official records of an organization—the ground truth. While NoSQL databases can support transactions, they relax some of the criteria to support greater scalability. The According to the CAP theorem, a distributed database that partitions data across different machines cannot be fully ACID compliant and guarantee availability. When a network partition fails one must choose between availability of the system and consistency. A relaxed condition is eventual consistency, where the data achieves a consistent state sometime in the future. Eventually, all reads will return the most recently written data value. For systems of records in financial services eventual consistency is not acceptable; SoRs must be ACID compliant.\nWhy are these arcane details of database architecture important for a data scientist?\n\nDatabase designers make tradeoffs when optimizing for a use case. You should be aware how these tradeoffs can affect data integrity if that matters for your project—it might not be an issue at all.\nUsing an OLTP system for analytical work is usually a bad idea. Transactional systems need to respond to frequent data updates and tend to return small record sets when queried (look up a customer, look up an account balance). The best way of storing information for this pattern is as a row-store: the data for a record is stored in a chunk of memory which makes retrieval of a record or set of record very efficient.\n\n\n\nRow and Column Storage\nThe following table shows seven of the 150 observations from the famous Iris data set that is used in statistics and machine learning courses to apply regression, classification, and clustering methods. The full data set contains 50 observations for each of three Iris species: Iris setosa, Iris versicolor, and Iris virginica. Four measurements of the flowers, the length and width of the petals (the large leaves on the flowers) and the sepals (the small leaves) were taken on each plant.\n\nFlower measurements for five Iris setosa and two Iris versicolor from the famous iris data set.\n\n\n\n\n\n\n\n\n\nSepalLength\nSepalWidth\nPetalLength\nPetalWidth\nSpecies\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n6.4\n3.2\n4.5\n1.5\nversicolor\n\n\n\nSuppose we store this data in a row-store with a fixed record length. The numerical measurements can be stored in 4-byte floats. The longest string in the Species column is 10 characters long. To align the records on 4-byte boundaries for faster access, we set the record length to 4 x 4 + 12 = 28. Figure 4.5 shows the first row of data set is laid out across 28 bytes.\n\n\n\n\n\n\nFigure 4.5: Layout in memory of row 1 of Iris data set with four-byte storage of floating-point numbers and 12-byte storage for the species string. (It seems that the number 5.1 uses 3 bytes for storage. It uses all 4 x 8 = 32 bits of the 4 bytes to store the number. The most significant bit stores the sign, the next 8 bits store the exponent, the following 23 bits store the mantissa of the float.)\n\n\n\nSuppose that the data is stored in one big chunk of memory, each row starts 28 bytes after the previous row.\n\n\n\n\n\n\nFigure 4.6: The first three rows of the Iris data set in row-store form with contiguous memory.\n\n\n\nLooking up rows is extremely fast. To read the data for the third observation jump to byte 56 and read the next 28 bytes. Adding new records in row-oriented storage is also fast: allocate a new chunk of 28 bytes and fill in the fields. However, if we want to perform an analytic operation such as computing the average petal length, we must jump through the memory and grab values at position 8, 8 + 28, 8 + 56, …., 8 + 150 x 28. That is very inefficient, especially when tables have many columns and records are long.\nTo support fast analytic processing, analytical databases are optimized for an access pattern where data is mostly read and infrequently written (changed). The ease of appending or looking up entire rows is then not as important as the efficiency of accessing memory for a column. Analytical queries are deep in that they process information from many records in the table—rather than looking up records—and they can return large result sets. Also, analytical queries tend to operate on a small subset of the columns of a table rather than all the columns.\nThe best-performing format for storing information for this workload is a column-store: each column in the table is stored in one or more chunks of memory.\n\n\n\n\n\n\nFigure 4.7: The seven observations of the Iris data set in column-store format.\n\n\n\nFigure 4.7 shows the five columns in the Iris data set in column store format. Computing the average of, say, sepal length, can now be done efficiently by scanning the memory for that column. The data for the other columns does not have to be loaded into memory for this operation.\nColumnar storage has other advantages for analytic databases: the data types are homogeneous within a block of memory (all characters, all integers, all floats, etc.) and compress well. Except for sepal length, all columns have repeated values. These can be stored along with an integer that counts the number of times a value appears. This simple technique—called run-length encoding—can greatly reduce the storage requirements when the data are ordered and have repeat values.\nThe following is a non-exhaustive list of analytic databases that use columnar storage:\n\nAmazon Redshift\nGoogle BigQuery\nAzure Cosmos DB\nDuckDB\nVertica\nScyllaDB\nDataStax\nClickHouse\nApache Druid\nApache Cassandra\nApache HBase\n\n\n\nHybrid (HTAP) Databases\nSome databases combine row-oriented storage with column-oriented storage in the same database engine; this paradigm is called hybrid transactional-analytical processing (HTAP). The idea behind HTAP databases is that you do not need two separate databases, one for transactional workloads and one for analytical workloads. The ability to perform both workloads efficiently in the same database reduces data movement and data duplication between databases. This reduces the dreaded database sprawl in organizations. The term HTAP was coined by industry analyst firm Gartner. Forrester, Gartner’s competitor, refers to this processing paradigm as translytical.\nAnother motivation of HTAP databases is that transactional systems, where information about the business is recorded in real time, also need support for real-time analytics to produce insights “in the moment”. Transactional systems are becoming more analytical, the results and side effects of algorithms triggered by a transaction can be the most important piece of information of the transaction.\n\n\nRide Share Booking\n\n\nSuppose you land at the San Francisco airport and use your favorite ride share app to book a ride into the city. This starts a transaction with the ride share company. The data recorded as part of the transaction includes customer information, origin and destination of the trip, time of day, etc. In real-time, as the transaction occurs, the backend of the app calculates the price for the ride based on location, traffic density, ride demand, available drivers, vehicle type, etc.\nThis is an analytical operation that invokes a price-optimization algorithm. Based on the real-time prices offered to you in the app you choose the preferred ride. The optimized price is recorded as part of the transaction.\n\n\nThe following are examples of HTAP databases:\n\nPingCAP\n\n\n\nAerospike\n\n\n\nSingleStoreDB (formerly MemSQL)\nInterSystems\nOracle Exadata\nSplice Machine\nGridGrain\nRedis Lab\nSAP/HANA\nVoltDB\nSnowflake (since the addition of Unistore in 2022)\nGreenplum\n\n\n\nNewSQL Databases\nAs a broad generalization we can state that non-relational NoSQL databases sacrifice consistency for horizontal scalability and that traditional relational SQL databases are ACID compliant but do not scale well horizontally, they are designed for scale-up.\nNewSQL databases try to bridge this gap, providing horizontal scalability and transactional integrity (ACID compliance). There are variations in the types of transactional guarantees they provide, so it is a good idea to read the fine print. For example, a transaction might be defined by a single read or write to the database rather than the operations wrapped in a BEGIN—COMMIT block.\nNewSQL databases tend to have in common:\n\nData partitioning: the data are divided into partitions (also called shards). Partitions can reside on different machines.\nHorizontal scaling: because of sharding, the database can accommodate an increase in size by adding more machines.\nReplication: The data appears in the database more than once. Replicates can be stored on the same or different machines, availability zones or regions. For example, one machine can serve as the primary node for some partitions and as a backup node for replicates of other partitions. This provides failure resilience and the ability to recover the database from disasters.\nConcurrency control: to maintain data integrity under concurrent transactions.\nFailure resilience: the databases recover to a previous consistent state when machines fail or the database crashes.\n\nThe following are examples of NewSQL databases:\n\nCockroachDB\nVoltDB\nSingleStoreDB\nClustrixDB\nPivotal Gemfire\nNuoDB\n\n\n\nNon-relational Database Designs\nBecause non-relational (NoSQL) databases have become popular and play an important role in data analytics, we want to spend a few moments on their principal architecture. The primary designs for NoSQL databases are\n\nKey-value stores\nDocument stores\nGraph databases\n\nNon-relational databases can be multi-model and support more than one design. Many graph databases on the DB-Engines list support document store, key-value, and graph models.\n\nKey-value stores\nIn a key-value store, every item in the database has only two fields, a key and a value. For example, the key could be a product number and the value could be a product name. This seems restrictive, but the value does not have to be a scalar, it can be, for example, a JSON documents with subfields.\n\n\nExamples of keys and values in key-value stores. The schema-less nature of the store is apparent in the last two table rows. Two keys can be associated with values of different types. Database items do not have to conform to a specific structure except that an item consists of a key and a value.\n\n\n\n\n\n\nKey\nValue2\n\n\n\n\n“username”\n“oschabenberger”\n\n\n“account type”\n“personal”\n\n\n345625\n{\n    name: “MacBook Pro 16”,\n    processor: “M2”,\n    memory: 32GB”\n}\n\n\n165437\n{\n    name: “Paper towel”,\n    available: true,\n    discount: 10%”\n}\n\n\n\nTo achieve horizontal scaling (scaling out) of key-value stores, the data are partitioned across multiple machines based on a hash table. For a database with n machines the hash is essentially a mapping from the key to the integers from 0 to n-1 that determines on which machine a particular key is kept. As the database grows (more keys added) more machines can be easily added. Such scaling out is more difficult in relational systems where the relationships between the tables must be maintained as the data is distributed over more machines. That is why not building tables on relations supports horizontal scaling. While documents are grouped into collections (the NoSQL version of a table), there is no association or relationship between one document or any other. The relative ease to scale horizontally is one reason why non-relational databases became so popular for handling large and growing databases.\nDisadvantages of key-value stores are also apparent: items can only be looked up by their (primary) key. Queries that filter on other attributes are less efficient than in relational systems: what is the average order amount of customers who ordered fewer than 5 items in the last 12 month? It is difficult to join data based on keys. They are not as performant for analytical work as relational systems, especially those with columnar storage layers. A good application for key-value stores are CRUD applications where items are merely created, read, updated, and deleted.\n\n\nDocument stores\nA document store extends the simple design of the key-value store. The value is now stored as a document-oriented set of fields in JSON, XML, YAML, BSON, or similar format. While the data in a key-value store is transparent to the application but deliberately opaque to the database, in a document store the values are transparent to the database. This enables more complex queries than just using the primary key, the fields in the document can be queried.\n\n\n\n\n\n\nFigure 4.8: Examples of documents in MongoDB. A simple document on the left and a more complex document on the right. The first field, _id, is a unique identifier of the document.\n\n\n\nThe fields of the documents do not have to be identical across records in the database. For example, documents containing customer names have a field for a middle initial only if the individual has a middle initial. Documents do not contain empty fields. In contrast, a relational system that stores a middle initial will have this field for all records and NULL values are used to signal an empty field when a middle initial is missing.\nLike key-value stores, document databases are great for CRUD applications, but do not perform well for analytical queries and cannot represent relationships and associations. Each document exists as an independent unit unrelated to other documents.\n\n\nGraph databases\nIn graph databases relationships are a first-class citizen. The relationships are not expressed through keys in tables, but through edges that connect nodes (vertices). Nodes are the entities of interest in the database such as people, products, cities. Nodes have attributes (called properties) stored as key-value pairs or documents (Figure 4.9).\n\n\n\n\n\n\nFigure 4.9: A simple graph with 6 nodes and 7 edges.\n\n\n\nBecause of this storage model graph databases are considered NoSQL databases. Labeling them as non-relational databases is only correct with respect to the traditional table-based relational systems. Relationships are a central property of graph databases, and they are more flexible and dynamic compared to RDBMS. Relationships emerge as patterns through the connections of the nodes rather than predefined elements of the database.\nThis list shows the DB-Engines ranking of graph database systems. Note that many of them also support key-value or document stores.",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Sources and File Formats</span>"
    ]
  },
  {
    "objectID": "data/sources_and_files.html#cloud-databases",
    "href": "data/sources_and_files.html#cloud-databases",
    "title": "4  Data Sources and File Formats",
    "section": "4.2 Cloud Databases",
    "text": "4.2 Cloud Databases\nAccording to statista, the share of corporate data stored in the cloud has increased from 30% in 2015 to 60% in 2022. The total addressable market (TAM) in cloud computing is estimated to exceed $200 billion per year and is growing by double digits.\nThe market for storing data in the cloud as files, blocks, or generic objects is a staggering $78 billions of that TAM (2022) and the market for cloud databases is $21 billions in 2023. The cumulative aggregate growth rate (CAGR) for cloud storage and cloud databases is 18% and 22%, respectively. What do we take away from these numbers?\n\nCloud computing is one of the most fundamental revolutions in computing in the last 20 years.\nHalf of the addressable market is in storing data, either in databases or in some other form of cloud storage (files, blocks, objects). Getting your data into their cloud is a key element in the business model of the cloud service providers (CSP). Data has gravity and its “gravitational constant” seems to increase as it comes to rest in a cloud data center.\nThe economics of inexpensive cloud storage will continue to erode Big Data storage systems like Hadoop (HDFS).\nTransactional and operational systems are unlikely to be supported by object storage, while it is least expensive it is also least performant. That is where databases optimized for transactions and/or analytic of cloud data come in.\n\nA database is said to be cloud-ready if it can be run on cloud infrastructure. That is true for most databases. It is said to be cloud-native if the database was designed for the cloud and takes advantage of the full functionality of the cloud, for example, horizontal scaling (scale-out) with increased workload, separation of storage and compute, container deployment and Kubernetes orchestration, multi-tenancy, disaster recovery, bottomless storage.\nAmong cloud databases we can broadly distinguish three service models according to who takes on the responsibility for maintaining the infrastructure, the database instance, and the access to the database: self-managed databases, managed services (DBaaS), and serverless databases.\n\nSelf-managed\nA self-managed cloud database is not much different from a database installed on premises, except that it uses cloud infrastructure. As the name suggests, you are responsible for administering and maintaining all aspects of the deployment. The CSP will provide the infrastructure and make sure that it is operational, but you must make sure that the database running on the platform is operational, updated. You can install any database of your choice, including not cloud-native databases.\nSome cloud-native databases offer a self-managed option, but a managed service or serverless offering is more typical when databases were designed specifically for the cloud.\n\n\nManaged Service\nAlso known as database-as-a-service (DBaaS), this deployment methodology is a special case of software as a service, where the software managed on behalf of the user is a database system. In exchange for a subscription, the service provider handles the management of the database, including provisioning, support, and maintenance. The service provider chooses the hardware instances on which the database runs, frequently using shared resources for multiple databases and customers.\nHere are examples of relational and NoSQL databases offered as a managed service:\n\nRelational\n\nSingleStoreDB\nCockroachDB\nAmazon RDS (Relational Database Service)\nAzure SQL\nMotherDuck (SaaS for DuckDB)\nAzure DB for MySQL, PostgreSQL, …\nGoogle BigQuery\nGoogle Spanner\nOracle Database\n\nNoSQL\n\nMongoDB Atlas\nAzure Cosmos DB\nCouchbase\nGoogle Datastore\nGoogle BigTable\nGoogle FireStore\nRedis\nAmazon DynamoDB\n\n\nCloud service providers have been pushing non-relational systems because horizontal scaling fits well with their business model: adding cloud infrastructure drives revenue for the cloud provider.\n\n\nServerless Systems\nWe need to explain what we mean by “serverless computing” because all code executes on a computer (a server) somewhere. Serverless computing does not do away with servers. It eliminates for software developers the particulars of worrying about which servers their code runs on. This sounds a bit like the SaaS model, but there are important distinctions between a serverless and a serverful (e.g., SaaS) system:\n\nIn serverless computing you execute code without managing resource allocation. You provide a piece of code to the CSP, and the cloud automatically provisions the resources necessary to execute the code.\nn serverless computing you pay for the time the code is executing, not for the resources reserved to (eventually) execute the code. The provider of a serverless service is then encouraged to scale back computing resources as much as possible when not in use (known as scale-to-zero).\n\nServerless computing can be seen as the latest form of virtualization in computing. The user writes a cloud function and ties it to a trigger that runs the function, for example, when a customer opens an online shopping cart. The serverless system then takes care of everything else such as instance selection, logging, scaling, security, etc.\nNot all applications are suitable for serverless computing and for some time databases where thought to be among the backend services that do not fit with the serverless paradigm (see, for example, this view from Berkeley):\n\nServerless computing is essentially stateless computing: the state necessary to execute code is either sent over an API along with the request or is stored somewhere server-side between function calls.\nDatabases have a lot of state, such as connection protocols, metadata, access controls, schemas, etc.\nDatabases often use connection-based protocols which assume a stable connection over a port between a host and a client. That conflicts with the design of serverless systems.\nDatabases have a cold-start problem. A cold start is the time required to instantiate an environment and to get things up and running when a function is called for the first time. In a serverful environment you encounter a cold start only once at the beginning. In a serverless environment cold starts happen the first time you invoke a service and every time the system has scaled back. For example, if the serverless database quiesces after 5 minutes of inactivity and you submit queries every 10 minutes then every query must go through an instantiation of the database environment, including loading the data.\n\nDespite these challenges there has been a lot of progress in serverless database systems in recent years. Here is an incomplete list of relational and NoSQL serverless cloud databases. Notice that the list contains databases listed earlier; some providers make DBaaS and serverless options available:\n\nRelational\n\nMotherDuck (collaborative serverless platform built on DuckDB)\nNeon Serverless Postgres\nPlanetScale DB (Serverless MySQL)\nCockroachDB Serverless\nAmazon Aurora Serverless\n\nNoSQL\n\nFauna DB\nGoogle FireStore\nAmazon DynamoDB\nMongoDB Serverless\n\n\nA special shout-out to DuckDB, a lightweight, embedded, analytical RDBMS that runs inside a host process, for example, inside a Python or R session. DuckDB integrates very well with other systems; it makes working with a relational system from Python or R very easy. MotherDuck is a serverless platform and cloud database built on DuckDB. The integration between the two makes working with databases locally and in the cloud particularly easy. And since DuckDB is optimized for analytic workloads, DuckDB & MotherDuck are great choices for data scientists. We will cover more about DuckDB and MotherDuck in the chapters that follow.",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Sources and File Formats</span>"
    ]
  },
  {
    "objectID": "data/sources_and_files.html#data-warehouse-mart-lake-and-lakehouse",
    "href": "data/sources_and_files.html#data-warehouse-mart-lake-and-lakehouse",
    "title": "4  Data Sources and File Formats",
    "section": "4.3 Data Warehouse, Mart, Lake, and Lakehouse",
    "text": "4.3 Data Warehouse, Mart, Lake, and Lakehouse\nWe used the term data warehouse frequently throughout this document and made several references to data lake. What are they and how do data marts fit into the picture?\nData warehouses and data lakes represent two distinct philosophies to store data and to make it available in enterprises. Many organizations have both a data warehouse and a data lake, some enterprises have several of each. The Data lakehouse is a recent development aimed at combining the pros of warehouses and lakes into a single architecture.\n\nData Warehouse\nThe main differences between a data warehouse, also called an enterprise data warehouse (EDW), and a data lake are the level of curation of the data, the storage technology, and the level of access. The data in data warehouses is highly organized and curated and access to it tends to be more tightly controlled than in a data lake.\nA data warehouse contains structured data for business intelligence, reporting, and visualization. Data warehouses are updated with data from transactional source systems such as CRM, ERP, Salesforce on a regular schedule. Data are extracted to a staging area where they are transformed, normalized, and enriched before being loaded into the data warehouse. This process of extracting—transformation—loading is abbreviated as the ETL approach.\n\n\n\n\n\n\nFigure 4.10: An enterprise data warehouse (EDW) is loaded with data transformed and curated in a staging area.\n\n\n\nData warehouses consist of schema-on-write tables—dimension and fact tables—and indexes that support consistency (ACID) and are optimized for analytical queries. Fact tables hold numerical data and primary keys; the dimension tables hold the descriptive information for all fields included in a fact table. A typical example is to store orders in a fact table and customers and products information in dimension tables. When relations between the fact and dimension tables are represented by a single join, the arrangement is called a star schema due to the central role of the fact table.\n\n\n\n\n\n\nFigure 4.11: One fact table (Orders) and four dimension tables in a star schema. PK denotes primary keys, FK denotes foreign keys. The fact table has foreign keys that support relations with the dimension table. Source.\n\n\n\nThe star schema is one of the simplest ways of organizing data in a data warehouse. The snowflake schema has a more complex structure with normalized dimension tables (without duplicates) and possibly multiple levels of joins.\n\n\n\n\n\n\nFigure 4.12: Snowflake schema with up to two levels of relational joins. Source.\n\n\n\nThe snowflake schema in the preceding figure can require two levels of table joins to query data. For example, to calculate revenue by country requires a join of the fact table with the join of the `Dealer` and Country tables. In a star schema the location and country information would be incorporated into the `Dealer` table; that would increase the size of that dimension table due to multiple locations within countries and multiple dealers at locations, but it would simplify the relationship among the tables.\nData warehouses uses SQL as the primary interface, are highly relational, ACID compliant, and schema-dependent—these are attributes of relational database management systems. But you should not equate EDWs with any specific RDBMS. Data warehouses are built on relational database technology but not every RDMBS can serve as a data warehouse. Examples of data warehouses include:\n\nTeradata (on-premises) and Teradata Vantage (cloud-based)\nGoogle BigQuery\nAmazon Redshift\nMicrosoft SQL Server\nOracle Exadata\nIBM Db2 and IBM Infosphere\nIBM Netezza\nSAP/HANA and SAP Datasphere\nSnowflake\nYellowbrick\n\n\n\nData Mart\nA data mart is a section of a data warehouse where data is curated for a specific line of business, team, or organizational unit. For example, a data warehouse might contain a data mart for the Marketing organization, a data mart for the R&D organization, and a data mart for the Finance team. These teams have very different needs and different levels of access privileges.\n\n\n\n\n\n\nFigure 4.13: A data warehouses supports different data marts. The teams access their data marts rather than the data warehouse directly.\n\n\n\n\n\nData Lake\nData warehouses have dominated enterprise data storage and analysis for decades. They are not without drawbacks and their disadvantages were amplified during the rise of Big Data with new data types, new workloads, and the need for more flexibility. Data warehouses are often expensive, custom-built appliances that do not scale out easily. They use read-on-write schemas with proprietary storage formats. First Hadoop with the Hadoop Distributed File System (HDFS) and then cloud object storage (Amazon S3, Azure Blob Storage, Google Cloud Storage) presented a much cheaper storage option to reimagine data storage, data curation, and data access. Open-source data formats such as Parquet, ORC, and Avro presented an alternative to storing data in a proprietary format and promised multi-use of the data.\nData lakes were born as centralized repositories where data is stored in raw form. The name suggests that data are like water in a lake, free flowing.\nA common data lake architecture is the medallion system named after bronze, silver, and gold medals awarded in competition. The structure and quality improve as one moves from the bronze to the silver to the gold tier. The bronze layer contains the raw data in formats such as CSV, JSON, XML, XLS and is not accessible by the end user. From here the data are cleansed and enriched and formatted into open-source formats such as parquet or Avro. The data in the silver layer is validated and standardized, schemas are defined but can evolve as needed. Users of the silver layer are data scientists and data analysts who perform self-service analysis, data science, and machine learning. Data engineers also use the silver layer to structure and curate data even more for project-specific databases that you find in the gold layer.\n\n\n\n\n\n\nFigure 4.14: Medallion data lake design.\n\n\n\nThe data can be structured, unstructured, or semi-structured. Data lakes support many storage and file formats—CSV, JSON, Parquet, ORC, and Avro are common file formats for data sets. The data is kept in the lake in raw form until it is needed. RDBMS use databases and tables to organize the data. NoSQL databases use document collections and key-value pairs to organize the data. File systems use folders and files to organize files. A data lake uses a flat structure where elements have unique identifiers and are tagged with metadata. Like a NoSQL database, data lakes are schema-on-read systems.\nData lakes are another result of the Big Data era. Increasingly heavy analytical workloads that burned CPU cycles and consumed memory were not welcome in databases and data warehouses that were meant to serve existing business reporting needs. A new data architecture was needed where data scientists and machine learning engineers can go to work, unencumbered by the rigidity of existing data infrastructure and not encumbering the existing data infrastructure with additional number crunching.\nThe data lake is manifestation of the belief that great insights will (magically) result when you throw together all your data. When data are stored in a data lake without a pre-defined reason and in arbitrary formats, and poorly organized, they can quickly turn into data swamps.\nHere are some vendors of data lake solutions:\n\nMicrosoft Azure Data Lake\nGoogle Cloud’s Data Lake\nAWS Lake Formation\nCloudera Data Lake Service\nIBM data lake solution\nDelta Lake (an open-source metadata layer that sits on top of open file formats like Delta and Parquet)\n\nData lakes are flexible, scale horizontally (scale-out), and are quite cost effective. They are often built on cheap but scalable object/block/file storage systems that helps reduce costs at the expense of performance. A data warehouse on the other hand is a highly optimized, highly performant system for business intelligence. EDWs often come in the form of packaged appliances (Teradata, Oracle Exadata, IBM Netezza) that makes scaling more difficult. These systems are easier to scale up (adding more memory, more CPUs, etc.) rather than scale horizontally (scale out by adding more machines). EDWs support ACID transactions and allow updates, inserts, and deletes of data. Altering data in a data lake is very difficult, it supports mostly append operations.\nData science and machine learning is more directed toward the data lake whereas business intelligence is directed at the warehouse. Can the two worlds come together, enabling cost-effective and performant business intelligence and data science on all data in a central place? Maybe. That is the premise and promise of the data lakehouse!\n\n\nData Lakehouse\nThe term data lakehouse was coined by Databricks:\n\na new, open data management architecture that combines the flexibility, cost-efficiency, and scale of data lakes with the data management and ACID transactions of data warehouses, enabling business intelligence (BI) and machine learning (ML) on all data.\n\nThe lakehouse adds a layer on top of the low-cost storage of a data lake that provides data structures and data management akin to a data warehouse.\n\n\n\n\n\n\nFigure 4.15: Data warehouse, data lake, and data lakehouse as seen by Databricks. Source.\n\n\n\nThis is a compelling vision that, if it delivers what it promises, would be a major step into the right direction: to reduce the number of data architectures while enabling more teams to work with data.\nIt is early days for the data lakehouse but there is considerable momentum. Some vendors are quick to point out that their cloud data warehouses also operate as a lakehouse. Other vendors position SQL query engines on top of S3 object storage as lakehouse solutions. Then there are solutions designed as a data lakehouse, e.g.,\n\nDatabricks Lakehouse\nOracle Cloud Infrastructure Data Lakehouse\nBlossom Sky\nDataLakeHouse.io",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Sources and File Formats</span>"
    ]
  },
  {
    "objectID": "data/sources_and_files.html#file-formats",
    "href": "data/sources_and_files.html#file-formats",
    "title": "4  Data Sources and File Formats",
    "section": "4.4 File Formats",
    "text": "4.4 File Formats\nIf you are working with proprietary data analytics tools such as SAS or JMP, you probably do not worry much about the ways in which data are stored, read, and written by the software, except that you want the tool to support import and export to your favorite non-proprietary format.\nMost data science is performed on open-source or open standard file formats. Proprietary formats are not easily exchanged between tools and applications. Data science spans many technologies and storing data in a form accessible by only a subset of the tools is counterproductive. You end up reformatting the data into an exchangeable format at some point anyway.\nThe most important file formats in data science are CSV, JSON, Parquet, ORC, and Avro. CSV and JSON are text-based, human-readable formats, whereas Parquet, ORC, and Avro are binary formats. The last three were designed specifically to handle Big Data use cases, Parquet and ORC in particular are associated with the Hadoop ecosystem. Parquet, ORC, and Avro are Apache open-source projects.\nAlthough CSV and JSON are ubiquitous, they were never meant for large data volumes.\nYou will encounter many other data formats in data science work, but these five file formats cover a lot of ground, you should have a basic understanding of their advantages and disadvantages.\n\nCSV\nIn data science projects you will invariably work with comma-separated values (CSV) files. Not because that is a great file format, but because CSV files are ubiquitous for rectangular data sets made up of rows and columns. Each line of the file stores one record with values in plain text separated by commas.\nFigure 4.16 shows the contents of a CSV file with ten observations. The first line lists the column names. Strings are enclosed in quotes and values are separated by commas.\n\n\n\n\n\n\nFigure 4.16: A CSV file with ten records.\n\n\n\nAmong the advantages of CSV files are\n\nUbiquitous: every data tool can read and write CSV files. It is thus a common format to exchange (export/import) data between tools and applications.\nHuman readable: since the column names and values are stored in plain text, it is easy to look at the contents of a CSV file. When data are stored in binary form, you need to know exactly how the data are laid out in the file to access it.\nCompression: since the data are stored in plain text it is easy to compress CSV files.\nExcel: CSV files are easily exported from and imported to Microsoft Excel.\nSimple: the structure of the files is straightforward to understand and can represent tabular data well if the data types can be converted to text characters.\n\nThere are some considerable disadvantages of CSV files, however:\n\nHuman readable: To prevent exposing the contents of the file you need to use access controls and/or encryption. It is not a recommended file format for sensitive data.\nSimple structure: Complex data types such as documents with multiple fields and sub-fields cannot be stored in CSV files.\nPlain text: Some data types cannot be represented as plain text, for example, images, audio, and video. If you kluge binary data into a text representation the systems writing and reading the data need to know how to kluge and un-kluge the information—it is not a recommended practice.\nEfficiency: much more efficient formats for storing data exist, especially large data sets.\nBroken: CSV files can be easily broken by applications. Examples include inserting line breaks, limiting line width, not handling embedded quotes correctly, blank lines.\nMissing values (NaNs): The writer and reader of CSV files need to agree how to represent missing values and values representing not-a-number. Inconsistency between writing and reading these values can have disastrous results. For example, it is a bad but common practice to code missing values with special numbers such as 99999; how does the application reading the file know this is the code for a missing value?\nEncodings: When CSV files contain more than plain ASCII text, for example, emojis or Unicode characters, the file cannot be read without knowing the correct encoding (UTF-8, UTF-16, EBCDIC, US-ASCII, etc.). Storing encoding information in the header section of the CSV file throws off CSV reader software that does not anticipate the extra information.\nMetadata: The only metadata supported by the CSV format are the column names in the first row of the file. This information is optional and you will find CSV files without column names. Additional metadata common about columns in a table such as data types, format masks, number-to-string maps, cannot be stored in a CSV file.\nData Types: Data types need to be inferred by the CSV reader software when scanning the file. There is no metadata in the CSV header to identify data types, only column names.\nLoss of Precision: Floating point numbers are usually stored in CSV files with fewer decimal places than their internal representation in the computer. A double-precision floating point number occupies 64-bits (8 bytes) and has 15 digits of precision. Although it is not necessary, floating-point numbers are often rounded or truncated when they are converted to plain text.\n\nDespite these drawbacks, CSV is one of the most common file formats. It is the lowest common denominator format to exchange data between disparate systems.\n\n\nJSON\nJSON stands for JavaScript Object Notation, and although it was borne out of interoperability concerns for JavaScript applications and is based on a JavaScript standard, it is a language-agnostic data format. Initially used to pass information in human readable form between applications over APIs (Application Programmer Interfaces), JSON has grown into a general-purpose format for text-based, structured information. It is the standard for communicating on the web. The correct pronunciation of JSON is apparently akin to the name “Jason”, but “JAY-sawn” has become common.\nA binary, non-human readable form of JSON was created at MongoDB and is called BSON (binary JSON).\nIn contrast to CSV, JSON is not based on rows of data but three basic data elements:\n\nValue: a string, number, reserved word, or one of the following:\nObject: a collection of name—value pairs similar to a key-value store.\nArray: An ordered list of values\n\nAll modern programming languages support key—values and arrays, they might be calling it by different names (object, record, dictionary, struct, list, sequence, map, hash table, …). This makes JSON documents highly interchangeable between programming languages—JSON documents are easy to parse (read) and write by computers. Any modern data processing system can read and write JSON data, making it a frequent choice to share data between systems and applications.\nA value in JSON can be a string in double quotes, a number, true, false, or null, an object or an array. Objects are unordered collection of name—value pairs. An array is an ordered collection of values. Since values can contain objects and arrays, JSON allows highly nested data structures that do not fit the row structure of CSV files.\n\n\n\n\n\n\nFigure 4.17: Elements of a JSON document. Because values can contain objects and arrays, JSON documents can be highly structured and deeply nested.\n\n\n\nJSON documents are self-describing, the schema to make the data intelligible is built into the structures. It is also a highly flexible format that does not impose any structure on the data, except that it must comply with the JSON rules and data types—JSON is schema-free.\n\n\n\n\n\n\nFigure 4.18: A simple JSON document. The entire document is a name—value pair with name “menu”. The value is an object with names “id”, “value”, and “popup”. The value of “popup” is an object with name “menuitem” whose value is an array. The elements of the array are objects with names “value” and “onclick”.\n\n\n\nMany databases support JSON as a data type, allowing you to store hierarchical information in a cell of a row-column layout, limited only by the maximum size of a text data type in the database. Non-relational document databases often use JSON as the format for their documents.\nSince so much data is stored in JSON format, you need to get familiar and comfortable with working with JSON files. Data science projects are more likely consumers of JSON files rather than producer of files.\n\n\nApache Parquet\nThe Apache Parquet open-source file format is a binary format—data are not stored in plain text but in binary form. Originally conceived as a column-based file format in the Hadoop ecosystem, it has become popular as a general file format for analytical data inside and outside of Hadoop and its file system HDFS: for example, as an efficient analytic file format for data exported to data lakes or in data processing with Spark.\nWorking with Parquet files for large data is an order of magnitude faster than working with CSV files. The drawbacks of CSV files discussed previously all melt away with Parquet files.\nParquet was designed from the ground up with complex data structures and read-heavy analytics in mind. It uses principally columnar storage but does it cleverly by storing chunks of columns in row groups rather than entire columns.\n\n\n\n\n\n\nFigure 4.19: The Parquet file architecture. Chunks of columns are stored in row groups. The footer contains important metadata. Source: Parquet File Format: Everything You Need to Know, by Nikola Ilic.\n\n\n\nThis hybrid storage model is very efficient when queries select specific columns and filter rows at the same time; a common pattern in data science: compute the correlation between homeValue and NumberOfRooms for homes where ZipCode = 24060.\nParquet stores metadata about the row chunks to speed access to rows, the metadata tells the reader which row chunks to skip. Also, a single write to the Parquet format can generate multiple .parquet files. The total data is divided into multiple files collected within a folder. Like NoSQL and NewSQL databases, data are partitioned, but since Parquet is a file format and not a database engine, the partitioning results in multiple files. This is advantageous for parallel processing frameworks like Spark that can work on multiple partitions (files) concurrently.\nParquet uses several compression techniques to reduce the size of the files such as run-length encoding, dictionary encoding, Snappy, GZip, LZO, LZ4, ZSTD. Because of columnar storage compression methods can be specified on a per-column basis; Parquet files compress much more than text-oriented CSV files.\nBecause of its complex file structure, Parquet files are relatively slow to write. The file format is optimized for the WORM paradigm: write-once, read many times.\n\nComparison of popular file formats in data science.\n\n\n\n\n\n\n\n\n\nCSV\nJSON\nParquet\n\n\n\n\nColumnar\nNo\nNo\nYes\n\n\nCompression\nYes\nYes\nYes\n\n\nHuman Readable\nYes\nYes\nNo\n\n\nNestable\nNo\nYes\nYes\n\n\nComplex Data Structures\nNo\nYes\nYes\n\n\nNamed Columns\nYes, if in header\nBased on scan\nYes, metadata\n\n\nData Types\nBased on scan\nBased on scan\nYes, metadata\n\n\n\n\n\nApache ORC\nThe Apache ORC (Optimized Row Columnar) open-source file format, like the Parquet file format, was originally associated with the Hadoop Big Data ecosystem. ORC files have a purely columnar storage format unlike the row-group/column chunk hybrid storage in Parquet files.\nORC files are split into individual files that contain a collection of records; with columnar storage within the file. ORC files are organized into stripes, a stripe combines an index, the data in columnar form, and a footer with metadata. Stripes are essential to ORC files; they are treated independently of each other to support parallel data processing.\n\n\n\n\n\n\nFigure 4.20: Layout of an ORC file in stripes. Source.\n\n\n\nA stripe is by default 64 MB in size and ORC files can have multiple stripes. This gives you an idea that these file types were designed to store large amounts of data.\nThe ORC format was optimized for Hive, the data warehouse in the Hadoop ecosystem. For example, Hive and ORC together support ACID transactions on top of Hadoop. You will find that outside of Hadoop, support for ORC files is not as generous as for Parquet files. For example, you can read a multi-file Parquet file with Pandas in a single line of code. To read multi-file ORC files you need to append the results of reading the individual files. The Pandas functions read_orc() and to_orc() to read and write ORC files are not supported on Windows.\n\n\nApache Avro\nApache Avro is an open-source, row-based, schema-based, file format. It is often used in conjunction with the streaming platform Apache Kafka, but Avro files are useful for batch processing as well. The storage format in Avro files is mixed in that the schema information is stored in JSON format while the data is stored in binary form. The schema is thus human readable.\nLike the Parquet format, Avro files are self-describing, the information to access and to deserialize the records is stored in the file itself. Avro files can be compressed but they do not compress as well as column stores. Heterogeneous data types within a row do not compress as well as homogeneous data within a column. Furthermore, column-oriented storage can choose optimal compression algorithms based on the data type of a column whereas row-oriented storage uses the same compression algorithm across all data types.\nLike other row stores, Avro files are great for applications that write more than they read; the exact opposite of column stores.\n\n\nThe Apache Arrow Project\nIf you work with large data sets, you perform data analytics, you process batch and streaming data, you want to perform in-memory analytics and query processing, then you should pay attention to Apache Arrow. This is an open-source development platform for in-memory analytics that supports many programming environments, including R and Python, the primary languages for data science.\nMain contributors to the Python pandas project are also involved with Arrow, so there is a lot of portability between Arrow tables and pandas DataFrames. The pyarrow package is the backend for some of the read and write functions in pandas that handle Big Data formats such as Parquet and ORC.\nNewer releases of Pandas are leaning more on pyarrow, e.g., Pandas 2.0.\n\n\nSpreadsheets\nMicrosoft Excel is one of the most common formats to store data. Why are we discussing it here, after all the other file formats. Like CSV files, Excel files are ubiquitous and a horrific format to store data for analytics. What goes for Excel goes for all other spreadsheet formats. Do not use spreadsheets for analytic data! There be dragons!\nWe like rectangular (tabular) layouts in rows and columns: every row is an observation, every column is a variable. While spreadsheets appear to be organized this way, there are many ways in which the basic tabular layout can be messed with:\n\nCells can refer to other cells on the same or on another sheet.\nCells can contain data, figures, calculations, text, etc.\n\nCells can be merged\n\nOne of the most problematic features of spreadsheets is how easily the data can be modified—accidentally. How many times have you been in a spreadsheet, typed something, and wondered where the input went and whether contents of a cell were changed?\n\n\n\n\n\n\nTip\n\n\n\nThis is another great question to ask a potential employer when you interview: “What data formats and/or databases are you using for analytic data?” If the answer is “We email xlsx files around once a week” run; run like it is the plague. If the answer is “The primary data is stored in Parquet files in S3 buckets”, reply “Tell me more!”\n\n\n\n\n\nFigure 4.1: Database market share 2011—2021, according to Gardner. Source.\nFigure 4.2: Schema on write with PostgreSQL. The CREATE TABLE statement creates the table named titanic and defines the schema: names, order, and data types of columns and default values. The \\copy statement populates the database table with data.\nFigure 4.3: An example of schema on read with Hadoop and its file system (HDFS). The data are loaded into Hadoop with the hdfs dfs command. No schema is defined at this stage. The data is then processed with the hadoop command. The customer-mapper.py Python script determines how the data is interpreted for this job; the Python script applies the schema on read.\nFigure 4.4: Scaling up (vertically) and scaling out (horizontally). Scaling up adds more rooms to an existing building, scaling out adds more buildings.\nFigure 4.5: Layout in memory of row 1 of Iris data set with four-byte storage of floating-point numbers and 12-byte storage for the species string. (It seems that the number 5.1 uses 3 bytes for storage. It uses all 4 x 8 = 32 bits of the 4 bytes to store the number. The most significant bit stores the sign, the next 8 bits store the exponent, the following 23 bits store the mantissa of the float.)\nFigure 4.6: The first three rows of the Iris data set in row-store form with contiguous memory.\nFigure 4.7: The seven observations of the Iris data set in column-store format.\nFigure 4.8: Examples of documents in MongoDB. A simple document on the left and a more complex document on the right. The first field, _id, is a unique identifier of the document.\nFigure 4.9: A simple graph with 6 nodes and 7 edges.\nFigure 4.10: An enterprise data warehouse (EDW) is loaded with data transformed and curated in a staging area.\nFigure 4.11: One fact table (Orders) and four dimension tables in a star schema. PK denotes primary keys, FK denotes foreign keys. The fact table has foreign keys that support relations with the dimension table. Source.\nFigure 4.12: Snowflake schema with up to two levels of relational joins. Source.\nFigure 4.13: A data warehouses supports different data marts. The teams access their data marts rather than the data warehouse directly.\nFigure 4.14: Medallion data lake design.\nFigure 4.15: Data warehouse, data lake, and data lakehouse as seen by Databricks. Source.\nFigure 4.16: A CSV file with ten records.\nFigure 4.17: Elements of a JSON document. Because values can contain objects and arrays, JSON documents can be highly structured and deeply nested.\nFigure 4.18: A simple JSON document. The entire document is a name—value pair with name “menu”. The value is an object with names “id”, “value”, and “popup”. The value of “popup” is an object with name “menuitem” whose value is an array. The elements of the array are objects with names “value” and “onclick”.\nFigure 4.19: The Parquet file architecture. Chunks of columns are stored in row groups. The footer contains important metadata. Source: Parquet File Format: Everything You Need to Know, by Nikola Ilic.\nFigure 4.20: Layout of an ORC file in stripes. Source.",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Sources and File Formats</span>"
    ]
  },
  {
    "objectID": "data/data_access.html",
    "href": "data/data_access.html",
    "title": "5  Data Access",
    "section": "",
    "text": "5.1 Accessing Local Files",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Access</span>"
    ]
  },
  {
    "objectID": "data/data_access.html#accessing-local-files",
    "href": "data/data_access.html#accessing-local-files",
    "title": "5  Data Access",
    "section": "",
    "text": "CSV files\nTo convert CSV files to a pandas DataFrame use the pandas read_csv() method. The default behavior is to look for a header line with column names, to interpret commas as field delimiters, to skip blank lines in the file, and to apply utf-8 encoding.\nConsider the following file, fitness.csv:\ndata&gt; head fitness.csv\nAge,Weight,Oxygen,RunTime,RestPulse,RunPulse,MaxPulse\n44,89.47,44.609,11.37,62,178,182\n40,75.07,45.313,10.07,62,185,185\n44,85.84,54.297, 8.65,45,156,168\n42,68.15,59.571, 8.17,40,166,172\n38,89.02,49.874, 9.22,55,178,180\nThe first line contains comma-separated names for the columns. The following lines contain the data for the attributes, one record per line. To convert the CSV file to a pandas DataFrame use the read_csv() method:\n\nimport numpy as np\n\ndf = pd.read_csv(\"../datasets/fitness.csv\")\nprint(\"Default result from read_csv\")\nprint(df.head())\ndf.describe(include=[np.float64])\n\nDefault result from read_csv\n   Age  Weight  Oxygen  RunTime  RestPulse  RunPulse  MaxPulse\n0   44   89.47  44.609    11.37         62       178       182\n1   40   75.07  45.313    10.07         62       185       185\n2   44   85.84  54.297     8.65         45       156       168\n3   42   68.15  59.571     8.17         40       166       172\n4   38   89.02  49.874     9.22         55       178       180\n\n\n\n\n\n\n\n\n\n\nWeight\nOxygen\nRunTime\n\n\n\n\ncount\n31.000000\n31.000000\n31.000000\n\n\nmean\n77.444516\n47.375806\n10.586129\n\n\nstd\n8.328568\n5.327231\n1.387414\n\n\nmin\n59.080000\n37.388000\n8.170000\n\n\n25%\n73.200000\n44.964500\n9.780000\n\n\n50%\n77.450000\n46.774000\n10.470000\n\n\n75%\n82.325000\n50.131000\n11.270000\n\n\nmax\n91.630000\n60.055000\n14.030000\n\n\n\n\n\n\n\n\nYou can assign your own variable names with the names= option:\n\ndf2 = pd.read_csv(\"../datasets/fitness.csv\", \n                  header=0,\n                  names=[\"Age\", \"Wgt\",\"Oxy\",\"RT\",\"ReP\",\"RuP\",\"MP\" ])\ndf2.head()\n\n\n\n\n\n\n\n\n\nAge\nWgt\nOxy\nRT\nReP\nRuP\nMP\n\n\n\n\n0\n44\n89.47\n44.609\n11.37\n62\n178\n182\n\n\n1\n40\n75.07\n45.313\n10.07\n62\n185\n185\n\n\n2\n44\n85.84\n54.297\n8.65\n45\n156\n168\n\n\n3\n42\n68.15\n59.571\n8.17\n40\n166\n172\n\n\n4\n38\n89.02\n49.874\n9.22\n55\n178\n180\n\n\n\n\n\n\n\n\nString variables are indicated in the CSV file with quotes:\ndata&gt;  head herding-cats.csv\n\"address_full\",\"street\",\"coat\",\"sex\",\"age\",\"weight\",\"fixed\",\"wander_dist\",\"roamer\",\"cat_id\"\n\"15 Fillmer Ave Los Gatos 95030\",\"15 Fillmer Ave\",\"brown\",\"male\",5.594,5.016,1,0.115,\"yes\",1\n\"244 Harding Ave Los Gatos 95030\",\"244 Harding Ave\",\"tabby\",\"male\",6.852,6.314,1,0.129,\"yes\",2\n\"16570 Marchmont Dr Los Gatos 95032\",\"16570 Marchmont Dr\",\"maltese\",\"female\",4.081, 2.652,0, 0.129,\"yes\",3\n\"100 Stonybrook Rd Los Gatos 95032\",\"100 Stonybrook Rd\",\"tabby\",\"female\",3.806,3.413,1, 0.107,\"yes\",4\n\ncats = pd.read_csv(\"../datasets/herding-cats.csv\",\n                   index_col=\"cat_id\")\nprint(cats.head())\n\n                              address_full              street     coat  \\\ncat_id                                                                    \n1           15 Fillmer Ave Los Gatos 95030      15 Fillmer Ave    brown   \n2          244 Harding Ave Los Gatos 95030     244 Harding Ave    tabby   \n3       16570 Marchmont Dr Los Gatos 95032  16570 Marchmont Dr  maltese   \n4        100 Stonybrook Rd Los Gatos 95032   100 Stonybrook Rd    tabby   \n5           266 Kennedy Rd Los Gatos 95032      266 Kennedy Rd   calico   \n\n           sex    age  weight  fixed  wander_dist roamer  \ncat_id                                                    \n1         male  5.594   5.016      1        0.115    yes  \n2         male  6.852   6.314      1        0.129    yes  \n3       female  4.081   2.652      0        0.129    yes  \n4       female  3.806   3.413      1        0.107    yes  \n5         male  5.164   6.820      1        0.039     no  \n\n\nPandas is smart about assigning data types to columns, note that string columns are stored as data type object.\n\ncats.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 400 entries, 1 to 400\nData columns (total 9 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   address_full  400 non-null    object \n 1   street        394 non-null    object \n 2   coat          400 non-null    object \n 3   sex           400 non-null    object \n 4   age           400 non-null    float64\n 5   weight        400 non-null    float64\n 6   fixed         400 non-null    int64  \n 7   wander_dist   400 non-null    float64\n 8   roamer        400 non-null    object \ndtypes: float64(3), int64(1), object(5)\nmemory usage: 31.2+ KB\n\n\nYou can overwrite the data types by supplying either a default type or a dictionary of types. To change the data type for the fixed column, for example:\n\ncats = pd.read_csv(\"../datasets/herding-cats.csv\",\n                   dtype={'fixed':np.float16})\n\nMissing values (see below) can be indicated in CSV files in several ways. The common technique is to leave the entry for the unobserved value empty (no spaces). In the following file the value for appraisal is missing in the third row. The fifth row contains values for land and appraisal only.\ndata&gt; head landsales2.csv\nland, improve, total, sale, appraisal\n30000,64831,94831,118500,1.25\n30000,50765,80765,93900,1.16\n46651,18573,65224,,\n45990,91402,137392,184000,1.34\n42394,,,168000,\n,133351,,169000,\n63596,2182,65778,,\n56658,153806,210464,255000,1.21\nread_csv() converts this file correctly to missing values using the NaN (not-a-number) sentinel for the unobserved data:\n\nland = pd.read_csv(\"../datasets/landsales2.csv\")\nland\n\n\n\n\n\n\n\n\n\nland\nimprove\ntotal\nsale\nappraisal\n\n\n\n\n0\n30000.0\n64831.0\n94831.0\n118500.0\n1.25\n\n\n1\n30000.0\n50765.0\n80765.0\n93900.0\n1.16\n\n\n2\n46651.0\n18573.0\n65224.0\nNaN\nNaN\n\n\n3\n45990.0\n91402.0\n137392.0\n184000.0\n1.34\n\n\n4\n42394.0\nNaN\nNaN\n168000.0\nNaN\n\n\n5\nNaN\n133351.0\nNaN\n169000.0\nNaN\n\n\n6\n63596.0\n2182.0\n65778.0\nNaN\nNaN\n\n\n7\n56658.0\n153806.0\n210464.0\n255000.0\n1.21\n\n\n8\n51428.0\n72451.0\n123879.0\nNaN\nNaN\n\n\n9\n93200.0\nNaN\nNaN\n422000.0\nNaN\n\n\n10\n76125.0\n78172.0\n275297.0\n290000.0\n1.14\n\n\n11\n154360.0\n61934.0\n216294.0\n237000.0\n1.10\n\n\n12\n65376.0\nNaN\nNaN\n286500.0\nNaN\n\n\n13\n42400.0\nNaN\nNaN\nNaN\nNaN\n\n\n14\n40800.0\n92606.0\n133406.0\n168000.0\n1.26\n\n\n\n\n\n\n\n\nIn addition to empty entries, read.csv() interprets a number of entries as missing (unobserved) values: “#N/A”, “#N/A N/A”, “#NA”, “-1.#IND”, “-1.#QNAN”, “-NaN”, “-nan”, “1.#IND”, “1.#QNAN”, “&lt;NA&gt;”, “N/A”, “NA”, “NULL”, “NaN”, “None”, “n/a”, “nan”, “null “. Despite those, CSV files might have special values to indicate missing values, for example, when they are exported from other software packages. Suppose that the missing values are indicated by a dot (“.”). Also, the columns in the CSV file are formatted with white space:\ndata&gt; head landsales.csv\nland, improve, total, sale, appraisal\n30000,     64831,     94831,    118500,   1.25\n30000,     50765,     80765,     93900,   1.16\n46651,     18573,     65224,         .,    .\n45990,     91402,    137392,    184000,   1.34\n42394,         .,         .,    168000,    .\n    .,    133351,         .,    169000,    .\n63596,      2182,     65778,         .,    .\n56658,    153806,    210464,    255000,   1.21\n51428,     72451,    123879,         .,    .\nThe file is much more human readable, but requires additional options to convert to the intended DataFrame:\n\nland2 = pd.read_csv(\"../datasets/landsales.csv\", \n                   na_values=\".\",\n                   skipinitialspace=True)\nland2.head()\n\n\n\n\n\n\n\n\n\nland\nimprove\ntotal\nsale\nappraisal\n\n\n\n\n0\n30000.0\n64831.0\n94831.0\n118500.0\n1.25\n\n\n1\n30000.0\n50765.0\n80765.0\n93900.0\n1.16\n\n\n2\n46651.0\n18573.0\n65224.0\nNaN\nNaN\n\n\n3\n45990.0\n91402.0\n137392.0\n184000.0\n1.34\n\n\n4\n42394.0\nNaN\nNaN\n168000.0\nNaN\n\n\n\n\n\n\n\n\nThe na.values= option specifies additional values that should be recognized as missing values. The skipinitalspace= option tellsread_csv() to ignore spaces following the delimiters. The result is as intended.\n\n\nJSON files\nPandas has great support for reading and writing JSON files. A general issue is that JSON is a hierarchical data format that allows nested data structures whereas pandas DataFrames have a row—column layout. Fortunately, the pandasread_json() method has many options to shape nested JSON structures into DataFrames.\nYou can convert any JSON document into a Python dictionary with the load() function in the json library. Suppose we want to read this document, stored in ../datasets/JSON/simple1.json into Python:\n&gt; cat ../datasets/JSON/simple1.json\n{\n    \"firstName\": \"Jane\",\n    \"lastName\": \"Doe\",\n    \"hobbies\": [\"running\", \"sky diving\", \"singing\"],\n    \"age\": 35,\n    \"children\": [\n        {\n            \"firstName\": \"Alice\",\n            \"age\": 6\n        },\n        {\n            \"firstName\": \"Bob\",\n            \"age\": 8\n        }\n    ]\n}\n\nimport numpy as np\nimport pandas as pd\nimport json\nwith open(\"../datasets/JSON/simple1.json\",\"r\") as f:\n    data = json.load(f)\ndata\n\n{'firstName': 'Jane',\n 'lastName': 'Doe',\n 'hobbies': ['running', 'sky diving', 'singing'],\n 'age': 35,\n 'children': [{'firstName': 'Alice', 'age': 6},\n  {'firstName': 'Bob', 'age': 8}]}\n\n\nThe next example reads a record-oriented JSON file (../datasets/JSON/simple.json) directly into a DataFrame. Each record consists of fields id, name, math, statistics, and weight.\ncat simple.json \n[\n  {\n    \"id\": \"A00123\",\n    \"name\": \"David\",\n    \"math\": 70,\n    \"statistics\": 86,\n    \"weight\": 156.3\n  },\n  {\n    \"id\": \"B00422\",\n    \"name\": \"Andrew\",\n    \"math\": 89,\n    \"statistics\": 80,\n    \"weight\": 210.6\n  },\n  {\n    \"id\": \"C004543\",\n    \"name\": \"Tobias\",\n    \"math\": 79,\n    \"statistics\": 90,\n    \"weight\": 167.0\n  }\n]\n\ndf = pd.read_json(\"../datasets/JSON/simple.json\")\ndf.info()\ndf\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   id          3 non-null      object \n 1   name        3 non-null      object \n 2   math        3 non-null      int64  \n 3   statistics  3 non-null      int64  \n 4   weight      3 non-null      float64\ndtypes: float64(1), int64(2), object(2)\nmemory usage: 252.0+ bytes\n\n\n\n\n\n\n\n\n\n\nid\nname\nmath\nstatistics\nweight\n\n\n\n\n0\nA00123\nDavid\n70\n86\n156.3\n\n\n1\nB00422\nAndrew\n89\n80\n210.6\n\n\n2\nC004543\nTobias\n79\n90\n167.0\n\n\n\n\n\n\n\n\nIf you’d rather assign data type string to the id and name fields than data type object, you can set the data type with\n\ndf2 = pd.read_json(\"../datasets/JSON/simple.json\", \n                   dtype= {\"id\": \"string\", \"name\" : \"string\"})\ndf2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   id          3 non-null      string \n 1   name        3 non-null      string \n 2   math        3 non-null      int64  \n 3   statistics  3 non-null      int64  \n 4   weight      3 non-null      float64\ndtypes: float64(1), int64(2), string(2)\nmemory usage: 252.0 bytes\n\n\nHow can we handle a nested JSON structure? The data in ../datasets/JSON/nested_array.json is\ndata&gt; cat nested_array.json\n{\n    \"school_name\": \"Virginia Tech\",\n    \"class\": \"Class of 2026\",\n    \"students\": [\n      {\n        \"id\": \"A00123\",\n        \"name\": \"David\",\n        \"math\": 70,\n        \"statistics\": 86,\n        \"weight\": 156.3\n      },\n      {\n        \"id\": \"B00422\",\n        \"name\": \"Andrew\",\n        \"math\": 89,\n        \"statistics\": 80,\n        \"weight\": 210.6\n      },\n      {\n        \"id\": \"C004543\",\n        \"name\": \"Tobias\",\n        \"math\": 79,\n        \"statistics\": 90,\n        \"weight\": 167.0\n      }]\n}\nThe students fields have a record structure but there are additional fields school_name and class that do not fit that structure. Converting the JSON file directly into a DataFrame does not produce the desired result:\n\ndf3 = pd.read_json('../datasets/JSON/nested_array.json')\ndf3\n\n\n\n\n\n\n\n\n\nschool_name\nclass\nstudents\n\n\n\n\n0\nVirginia Tech\nClass of 2026\n{'id': 'A00123', 'name': 'David', 'math': 70, ...\n\n\n1\nVirginia Tech\nClass of 2026\n{'id': 'B00422', 'name': 'Andrew', 'math': 89,...\n\n\n2\nVirginia Tech\nClass of 2026\n{'id': 'C004543', 'name': 'Tobias', 'math': 79...\n\n\n\n\n\n\n\n\nTo create the appropriate DataFrame we take two steps: convert the JSON file into a dictionary, then shape the dictionary into a DataFrame with json_normalize():\n\nwith open('../datasets/JSON/nested_array.json','r') as f:\n    data = json.load(f)\n\ndf4 = pd.json_normalize(data, \n                        record_path =[\"students\"],\n                        meta=[\"school_name\", \"class\"])\ndf4\n\n\n\n\n\n\n\n\n\nid\nname\nmath\nstatistics\nweight\nschool_name\nclass\n\n\n\n\n0\nA00123\nDavid\n70\n86\n156.3\nVirginia Tech\nClass of 2026\n\n\n1\nB00422\nAndrew\n89\n80\n210.6\nVirginia Tech\nClass of 2026\n\n\n2\nC004543\nTobias\n79\n90\n167.0\nVirginia Tech\nClass of 2026\n\n\n\n\n\n\n\n\nThe meta= option lists the JSON fields that are used as data that applies to each record in the result table. The record_path= option points at the field that contains the list of records.\nNow suppose that in addition to the list of records the JSON metadata contains a more complex structure:\ncat nested_obj_array.json\n{\n   \"school_name\": \"Virginia Tech\",\n   \"class\": \"Class of 2026\",\n    \"info\": {\n      \"president\": \"Timothy D. Sands\",\n      \"address\": \"Office of the President\",\n      \"social\": {\n        \"LinkedIn\": \"tim-sands-49b8b95\",\n        \"Twitter\": \"@VTSandsman\"\n      }\n    },\n    \"students\": [\n      {\n        \"id\": \"A00123\",\n        \"name\": \"David\",\n        \"math\": 70,\n        \"statistics\": 86,\n        \"weight\": 156.3\n      },\n. . .\n      }]\n}\nYou can specify which fields to extract from the metadata in the meta= option of json_normalize():\n\nwith open('../datasets/JSON/nested_obj_array.json','r') as f:\n    data = json.load(f)\n\ndf5 = pd.json_normalize(data,\n                       record_path =[\"students\"], \n                       meta=[\"school_name\",\"class\",\n                             [\"info\", \"president\"], \n                             [\"info\", \"social\", \"Twitter\"]])\ndf5\n\n\n\n\n\n\n\n\n\nid\nname\nmath\nstatistics\nweight\nschool_name\nclass\ninfo.president\ninfo.social.Twitter\n\n\n\n\n0\nA00123\nDavid\n70\n86\n156.3\nVirginia Tech\nClass of 2026\nTimothy D. Sands\n@VTSandsman\n\n\n1\nB00422\nAndrew\n89\n80\n210.6\nVirginia Tech\nClass of 2026\nTimothy D. Sands\n@VTSandsman\n\n\n2\nC004543\nTobias\n79\n90\n167.0\nVirginia Tech\nClass of 2026\nTimothy D. Sands\n@VTSandsman\n\n\n\n\n\n\n\n\nHow are missing values handled? In the next example, the first has a complete record. The second student misses a statistics score, the third student has no weight data. pd.json_normalize() fills in NaN for the unobserved data.\n\ndata = [\n    {\n        \"students\": [\n        {\n            \"id\": \"A00123\",\n            \"name\": \"David\",\n            \"math\": 70,\n            \"statistics\": 86,\n            \"weight\": 156.3\n        },\n        {\n            \"id\": \"B00422\",\n            \"name\": \"Andrew\",\n            \"math\": 89,\n            \"weight\": 210.6\n          },\n        {\n            \"id\": \"C004543\",\n            \"name\": \"Tobias\",\n            \"math\": 79,\n            \"statistics\": 90,\n        }\n        ]\n}\n]\n\ndf6 = pd.json_normalize(data, record_path =[\"students\"])\ndf6\n\n\n\n\n\n\n\n\n\nid\nname\nmath\nstatistics\nweight\n\n\n\n\n0\nA00123\nDavid\n70\n86.0\n156.3\n\n\n1\nB00422\nAndrew\n89\nNaN\n210.6\n\n\n2\nC004543\nTobias\n79\n90.0\nNaN\n\n\n\n\n\n\n\n\njson_normalize() is less forgiving if fields in the metadata are unobserved; it will throw an error. To prevent the error and assign NaN values, use the errors=’ignore’ option. In the following example the teachers meta data field does not always have a l_name entry.\n\ndata = [\n    { \n        'class': 'STAT 5525', \n        'student count': 60, \n        'room': 'Yellow',\n        'teachers': { \n                'f_name': 'Elon', \n                'l_name': 'Musk',\n            },\n        'students': [\n            { 'name': 'Tom', 'sex': 'M' },\n            { 'name': 'James', 'sex': 'M' },\n        ]\n    },\n    { \n        'class': 'STAT 5526', \n        'student count': 45, \n        'room': 'Blue',\n         'teachers': { \n                'f_name': 'Albert'\n            },\n        'students': [\n            { 'name': 'Tony', 'sex': 'M' },\n            { 'name': 'Jacqueline', 'sex': 'F' },\n        ]\n    },\n]\n\ndf7 = pd.json_normalize(data, \n                        record_path =[\"students\"], \n                        meta=[\"class\", \"room\", [\"teachers\", \"l_name\"]],\n                        errors='ignore')\n\ndf7\n\n\n\n\n\n\n\n\n\nname\nsex\nclass\nroom\nteachers.l_name\n\n\n\n\n0\nTom\nM\nSTAT 5525\nYellow\nMusk\n\n\n1\nJames\nM\nSTAT 5525\nYellow\nMusk\n\n\n2\nTony\nM\nSTAT 5526\nBlue\nNaN\n\n\n3\nJacqueline\nF\nSTAT 5526\nBlue\nNaN\n\n\n\n\n\n\n\n\n\n\nParquet files\nTo read parquet files into pandas DataFrames we have two choices: the read_parquet() pandas method or the read_table() method from pyarrow.parquet.\nThe data we work with in this section is stored in a parquet multi-file structure; that means the data are split into multiple files stored in a local directory:\nParquet ll userdata\ntotal 1120\n-rw-r--r--@ 1 olivers  staff   111K Aug 31 14:36 userdata1.parquet\n-rw-r--r--@ 1 olivers  staff   110K Aug 31 14:36 userdata2.parquet\n-rw-r--r--@ 1 olivers  staff   111K Aug 31 14:37 userdata3.parquet\n-rw-r--r--@ 1 olivers  staff   110K Aug 31 14:37 userdata4.parquet\n-rw-r--r--@ 1 olivers  staff   111K Aug 31 14:37 userdata5.parquet\nEach of the *.parquet files in the directory contains 1,000 observations.\nPandas’ read_parquet() reads the entire directory and combines the records from the multiple files into a single DataFrame.\n\ndf = pd.read_parquet(\"../datasets/Parquet/userdata\", engine=\"pyarrow\")\ndf.info()\ndf.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5000 entries, 0 to 4999\nData columns (total 13 columns):\n #   Column             Non-Null Count  Dtype         \n---  ------             --------------  -----         \n 0   registration_dttm  5000 non-null   datetime64[ns]\n 1   id                 4999 non-null   float64       \n 2   first_name         5000 non-null   object        \n 3   last_name          5000 non-null   object        \n 4   email              5000 non-null   object        \n 5   gender             5000 non-null   object        \n 6   ip_address         5000 non-null   object        \n 7   cc                 5000 non-null   object        \n 8   country            5000 non-null   object        \n 9   birthdate          5000 non-null   object        \n 10  salary             4689 non-null   float64       \n 11  title              5000 non-null   object        \n 12  comments           4966 non-null   object        \ndtypes: datetime64[ns](1), float64(2), object(10)\nmemory usage: 507.9+ KB\n\n\n\n\n\n\n\n\n\n\nregistration_dttm\nid\nsalary\n\n\n\n\ncount\n5000\n4999.000000\n4689.000000\n\n\nmean\n2016-02-03 21:04:13.699399936\n500.598720\n150772.222890\n\n\nmin\n2016-02-03 00:00:07\n1.000000\n12068.960000\n\n\n25%\n2016-02-03 08:51:55.750000128\n251.000000\n83480.380000\n\n\n50%\n2016-02-03 20:01:17\n501.000000\n152877.190000\n\n\n75%\n2016-02-04 07:42:54.500000\n750.500000\n215405.220000\n\n\nmax\n2016-02-04 23:59:55\n1000.000000\n286735.820000\n\n\nstd\nNaN\n288.648331\n78171.513062\n\n\n\n\n\n\n\n\nThe size of df confirms that all 5,000 observations were retrieved from the five component files.\nWith pyarrow we have more control over the files, for example, we can inquire about the schema and metadata of the component files.\n\nimport pyarrow.parquet as pq\n\npq_file = pq.ParquetFile(\"../datasets/Parquet/userdata/userdata1.parquet\")\npq_file.metadata\npq_file.schema\n\n&lt;pyarrow._parquet.ParquetSchema object at 0x15c31a3c0&gt;\nrequired group field_id=-1 hive_schema {\n  optional int96 field_id=-1 registration_dttm;\n  optional int32 field_id=-1 id;\n  optional binary field_id=-1 first_name (String);\n  optional binary field_id=-1 last_name (String);\n  optional binary field_id=-1 email (String);\n  optional binary field_id=-1 gender (String);\n  optional binary field_id=-1 ip_address (String);\n  optional binary field_id=-1 cc (String);\n  optional binary field_id=-1 country (String);\n  optional binary field_id=-1 birthdate (String);\n  optional double field_id=-1 salary;\n  optional binary field_id=-1 title (String);\n  optional binary field_id=-1 comments (String);\n}\n\n\nTo convert all component files into a pandas DataFrame with pyarrow, use the read_table() method and send the result to_pandas():\n\ndf2 = pq.read_table(source=\"../datasets/Parquet/userdata/\").to_pandas()\ndf2.describe()\n\n\n\n\n\n\n\n\n\nregistration_dttm\nid\nsalary\n\n\n\n\ncount\n5000\n4999.000000\n4689.000000\n\n\nmean\n2016-02-03 21:04:13.699399936\n500.598720\n150772.222890\n\n\nmin\n2016-02-03 00:00:07\n1.000000\n12068.960000\n\n\n25%\n2016-02-03 08:51:55.750000128\n251.000000\n83480.380000\n\n\n50%\n2016-02-03 20:01:17\n501.000000\n152877.190000\n\n\n75%\n2016-02-04 07:42:54.500000\n750.500000\n215405.220000\n\n\nmax\n2016-02-04 23:59:55\n1000.000000\n286735.820000\n\n\nstd\nNaN\n288.648331\n78171.513062\n\n\n\n\n\n\n\n\n\n\nORC files\nPandas (and pyarrow) supports ORC files but working with ORC files is not quite as convenient as with parquet files. The read_orc() method reads one ORC file rather than an entire directory and it is not supported on Windows.\n\ndf = pd.read_orc(\"../datasets/ORC/userdata/userdata1.orc\")\n\nWhen data is stored in multi-file format, as in the parquet example, you need to concatenate the contents into a pandas DataFrame. The glob module is helpful to retrieve the list of file names that match a pattern.\n\nimport glob\nfolder_path = \"../datasets/ORC/userdata\"\nfile_list = glob.glob(folder_path + \"/*.orc\")\ndf_from_orc = pd.DataFrame(pd.read_orc(file_list[0]))\n\nfor i in range(1,len(file_list)):\n    data = pd.read_orc(file_list[i])\n    df = pd.DataFrame(data)\n    df_from_orc = pd.concat([df_from_orc,df],axis=0)\n\nYou can also use the read_table() method for ORC files in pyarrow to read ORC an file into a pyarrow table and to_pandas() to convert the table to a pandas DataFrame:\n\nfrom pyarrow import orc\n\ndf = orc.read_table(source=\"../datasets/ORC/userdata/userdata1.orc\").to_pandas()",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Access</span>"
    ]
  },
  {
    "objectID": "data/data_access.html#working-with-a-database",
    "href": "data/data_access.html#working-with-a-database",
    "title": "5  Data Access",
    "section": "5.2 Working with a Database",
    "text": "5.2 Working with a Database\nAs a data scientist you need to be comfortable working with databases. In analytical work, you will encounter schema-centric relational OLAP databases that use SQL (structured query language) to interact with the database and the data. SQL interfaces are standard for data warehouses and common for many data platforms and tools.\nData scientists generally prefer writing Python or R code over SQL. SQL is extremely powerful to express relational concepts and can make some data processing tasks much easier than rolling your own code. Also, databases are optimized for joins, merges, deep queries, aggregations, indexes, handling JSON, etc. It would be a shame not to take advantage of that power or to lose it behind the syntactic sugar of a tool’s API. Many employers have voiced concerns that SQL ranks high among the technical skills data scientists often lack. As the founder and chairman of a data science research and consultancy company put it:\n\nThere are a lot of things folks don’t agree on. But everyone agrees that as a data scientist you will need to retrieve data from a database, and you will need version control. Learn SQL and learn git.\n\nWe are covering SQL basics in Chapter 9 after sprinkling some SQL statements along the way.\nIf you want to work with SQL directly from Python, there are several alternatives.\n\nSQLAlchemy\nSQLAlchemy is a SQL toolkit for Python that gives Python developers the power of SQL. While some tools try to hide SQL and relational details behind syntactic sugar, SQLAlchemy’s philosophy is to fully expose the relational details of SQL within transparent tools. Specific databases, beyond those for which support is built into SQLAlchemy, are supported as dialects.\n\n\nIbis\nIbis is a portable Python dataframe library that supports multiple database backends. After making a connection to a backend database, you interact with the data using Python syntax rather than SQL statements. The Ibis function calls are translated into SQL for the specific backend under the covers. For example, the first statement in the next code block creates a Python object named penguins through which you can interact with the database table of the same name. The second statement filters the data for a specific species.\npenguins = con.table(\"penguins\")\npenguins.filter(penguins.species == \"Adelie\")\nThe SQL equivalent would be a SELECT statement with a WHERE clause:\nSELECT * FROM penguins WHERE species = “Adelie”\nHaving stable Python code for different database backends makes it easy to move from one database to another. During development of analytic code you might be working with SQLite and the production database is Oracle. The same Python code will work in both environments.\nThe default backend of Ibis is the embedded database DuckDB. If you are working with DuckDB you can issue SQL statements directly from Python or you can rely on its dedicated Python API.\n\n\nDuckDB\nDuckDB has all the ingredients we are looking for in a database for data science work:\n\nDesigned to support analytical queries (OLAP workloads) characterized by complex queries that process large amounts of data.\nColumnar, vectorized storage engine. Columnar storage is more efficient for analytical queries, vectorized queries execute much faster than systems that process rows sequentially.\nProvides transactional guarantees (ACID properties)\nDeeply integrated with Python or R for efficient data analysis.\nEasy to install and operate, without any external dependencies.\nOpen-source database that is free to use.\nCan query CSV, JSON, Parquet files and DataFrames directly.\nSupports a streaming model for analyzing large data sets that exceed memory capacity, e.g., streaming from Parquet files.\nIn-memory database with graceful on-disk operation.\nEasy transition to the cloud through partnership with MotherDuck.\nSQL code can be submitted from the CLI or through the Python API, so you can take advantage of the power of SQL.\nRuns as an embedded database within a host process.\n\nBecause DuckDB is an embedded database, there is no database server to install and to maintain. Because the database runs within the host process, data can be transferred very quickly. And if you want to move the data to the cloud, you can do that easily with the MotherDuck serverless platform.\nThe Python package for DuckDB can run queries directly from Pandas DataFrame without copying any data.\nWhy is it called DuckDB? We let the folks who created it explain it:\n\nDucks are amazing animals. They can fly, walk and swim. They can also live off pretty much everything. They are quite resilient to environmental challenges. A duck’s song will bring people back from the dead and inspires database research. They are thus the perfect mascot for a versatile and resilient data management system. Also the logo designs itself.\n\nFor the Python user, installation is as simple as with any other Python package:\npip install duckdb\nIf you install with conda, the command is\nconda install python-duckdb -c conda-forge\nDuckDB is an in-memory database. If you want to persist the database across Python sessions, then you need to create a connection that stores the data in a single file on disk. Otherwise, the database will go away when the Python session ends. You can work without a connection to a database by using methods on the duckdb module. For example, the following statements directly query a CSV file using DuckDB:\n\nimport duckdb\nduckdb.sql('select * from \"../datasets/fitness.csv\" where Age &gt; 50')\n\n┌───────┬────────┬────────┬─────────┬───────────┬──────────┬──────────┐\n│  Age  │ Weight │ Oxygen │ RunTime │ RestPulse │ RunPulse │ MaxPulse │\n│ int64 │ double │ double │ double  │   int64   │  int64   │  int64   │\n├───────┼────────┼────────┼─────────┼───────────┼──────────┼──────────┤\n│    54 │  83.12 │ 51.855 │   10.33 │        50 │      166 │      170 │\n│    51 │  69.63 │ 40.836 │   10.95 │        57 │      168 │      172 │\n│    51 │  77.91 │ 46.672 │    10.0 │        48 │      162 │      168 │\n│    57 │  73.37 │ 39.407 │   12.63 │        58 │      174 │      176 │\n│    54 │  79.38 │  46.08 │   11.17 │        62 │      156 │      165 │\n│    52 │  76.32 │ 45.441 │    9.63 │        48 │      164 │      166 │\n│    51 │  67.25 │ 45.118 │   11.08 │        48 │      172 │      172 │\n│    54 │  91.63 │ 39.203 │   12.88 │        44 │      168 │      172 │\n│    51 │  73.71 │  45.79 │   10.47 │        59 │      186 │      188 │\n│    57 │  59.08 │ 50.545 │    9.93 │        49 │      148 │      155 │\n│    52 │  82.78 │ 47.467 │    10.5 │        53 │      170 │      172 │\n├───────┴────────┴────────┴─────────┴───────────┴──────────┴──────────┤\n│ 11 rows                                                   7 columns │\n└─────────────────────────────────────────────────────────────────────┘\n\n\nThe object returned by duckdb is a relation, a symbolic representation of the query that can be referenced in other queries. The next statements save the result of the previous query and reference the relation in the second query:\n\nrel = duckdb.sql('select * from \"../datasets/fitness.csv\" where Age &gt; 50')\nduckdb.sql(\"select Oxygen, RunTime, RestPulse from rel where RestPulse &lt; 49\")\n\n┌────────┬─────────┬───────────┐\n│ Oxygen │ RunTime │ RestPulse │\n│ double │ double  │   int64   │\n├────────┼─────────┼───────────┤\n│ 46.672 │    10.0 │        48 │\n│ 45.441 │    9.63 │        48 │\n│ 45.118 │   11.08 │        48 │\n│ 39.203 │   12.88 │        44 │\n└────────┴─────────┴───────────┘\n\n\nThe results of a DuckDB query can be easily converted into other formats. fetchnumpy() fetches data as a dictionary of NumPy arrays, df() fetches a Pandas DataFrame, pl() fetches a Polars DataFrame, and arrow() fetches an Arrow table.\n\nduckdb.sql(\"select Oxygen, RunTime, RestPulse from rel where RestPulse &lt; 49\").df()\n\n\n\n\n\n\n\n\n\nOxygen\nRunTime\nRestPulse\n\n\n\n\n0\n46.672\n10.00\n48\n\n\n1\n45.441\n9.63\n48\n\n\n2\n45.118\n11.08\n48\n\n\n3\n39.203\n12.88\n44\n\n\n\n\n\n\n\n\n\nReading files\nTo read the major data science file formats into DuckDB, use the read_csv(), read_parquet(), read_json(), and read_arrow() methods. For example, to read the simple JSON file from the previous section:\n\nduckdb.read_json('../datasets/JSON/simple.json')\n\n┌─────────┬─────────┬───────┬────────────┬────────┐\n│   id    │  name   │ math  │ statistics │ weight │\n│ varchar │ varchar │ int64 │   int64    │ double │\n├─────────┼─────────┼───────┼────────────┼────────┤\n│ A00123  │ David   │    70 │         86 │  156.3 │\n│ B00422  │ Andrew  │    89 │         80 │  210.6 │\n│ C004543 │ Tobias  │    79 │         90 │  167.0 │\n└─────────┴─────────┴───────┴────────────┴────────┘\n\n\nTo read a directory of Parquet files, you can use wildcard syntax in the path of the read_parquet() function:\n\nduckdb.read_parquet('../datasets/Parquet/userdata/user*.parquet')\n\n┌─────────────────────┬───────┬────────────┬───┬────────────┬───────────┬──────────────────────┬──────────────────────┐\n│  registration_dttm  │  id   │ first_name │ … │ birthdate  │  salary   │        title         │       comments       │\n│      timestamp      │ int32 │  varchar   │   │  varchar   │  double   │       varchar        │       varchar        │\n├─────────────────────┼───────┼────────────┼───┼────────────┼───────────┼──────────────────────┼──────────────────────┤\n│ 2016-02-03 16:07:46 │     1 │ Ernest     │ … │            │ 140639.36 │                      │                      │\n│ 2016-02-03 21:52:07 │     2 │ Anthony    │ … │ 1/16/1998  │ 172843.61 │ Developer II         │ 👾 🙇 💁 🙅 🙆 🙋 …  │\n│ 2016-02-03 02:22:19 │     3 │ Ryan       │ … │ 11/21/1978 │ 204620.66 │ Developer I          │ ␢                    │\n│ 2016-02-03 04:20:04 │     4 │ Brenda     │ … │ 10/29/1998 │ 260474.12 │ GIS Technical Arch…  │                      │\n│ 2016-02-03 00:15:16 │     5 │ Jacqueline │ … │ 7/12/1959  │ 286038.78 │ Marketing Assistant  │                      │\n│ 2016-02-03 19:48:14 │     6 │ Paul       │ … │            │ 241518.24 │                      │                      │\n│ 2016-02-03 08:59:05 │     7 │ Linda      │ … │ 3/30/1988  │ 192756.38 │ Professor            │                      │\n│ 2016-02-03 08:04:51 │     8 │ Frances    │ … │            │ 188511.28 │                      │ &lt;svg&gt;&lt;script&gt;0&lt;1&gt;a…  │\n│ 2016-02-03 08:12:33 │     9 │ Jason      │ … │ 7/29/1982  │ 238068.56 │ Web Designer III     │                      │\n│ 2016-02-03 17:08:02 │    10 │ Carolyn    │ … │ 4/28/1977  │ 132718.26 │ Research Nurse       │                      │\n│          ·          │     · │  ·         │ · │     ·      │     ·     │       ·              │          ·           │\n│          ·          │     · │  ·         │ · │     ·      │     ·     │       ·              │          ·           │\n│          ·          │     · │  ·         │ · │     ·      │     ·     │       ·              │          ·           │\n│ 2016-02-04 04:48:13 │   991 │ Fred       │ … │ 1/25/1975  │ 280835.07 │ Cost Accountant      │                      │\n│ 2016-02-04 00:03:33 │   992 │ Anna       │ … │ 5/29/1962  │ 286181.88 │ Automation Special…  │                      │\n│ 2016-02-04 07:05:48 │   993 │ Donna      │ … │ 6/3/1962   │  245704.0 │ Senior Financial A…  │                      │\n│ 2016-02-04 21:15:41 │   994 │ Annie      │ … │            │ 194931.47 │                      │                      │\n│ 2016-02-04 05:05:12 │   995 │ Carlos     │ … │ 8/11/1986  │  39424.88 │ Teacher              │                      │\n│ 2016-02-04 21:45:05 │   996 │ Stephanie  │ … │ 5/9/1962   │  273504.1 │ Chief Design Engin…  │                      │\n│ 2016-02-04 09:20:21 │   997 │ Kathy      │ … │ 12/4/1986  │ 109525.48 │ Director of Sales    │                      │\n│ 2016-02-04 00:26:05 │   998 │ Louis      │ … │ 11/20/1982 │  13134.47 │ Office Assistant IV  │                      │\n│ 2016-02-04 04:50:33 │   999 │ Elizabeth  │ … │ 3/14/1976  │ 207194.65 │ Research Assistant I │                      │\n│ 2016-02-04 16:14:42 │  1000 │ Susan      │ … │ 12/19/1999 │ 229961.89 │ Human Resources Ma…  │                      │\n├─────────────────────┴───────┴────────────┴───┴────────────┴───────────┴──────────────────────┴──────────────────────┤\n│ 5000 rows (20 shown)                                                                           13 columns (7 shown) │\n└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n\n\n\n\nConnecting to the database\nIf you make an explicit connection to the database with the connect() method, the arguments determine whether the database resides completely in memory—and is destroyed when the Python process exits—or whether the database persists on disk. The following statements are equivalent and create a non-persistent database:\nconn = duckdb.connect(database=\":default:\")\nconn = duckdb.connect(database=\":memory:\")\nconn = duckdb.connect(\":memory:\")\nconn = duckdb.connect()\nOnce the connection is established you use the same methods on the connection object as on the duckdb object:\n\nconn = duckdb.connect()\nconn.sql(\"select Oxygen, RunTime, RestPulse from rel where RestPulse &lt; 49\").df()\n\n\n\n\n\n\n\n\n\nOxygen\nRunTime\nRestPulse\n\n\n\n\n0\n46.672\n10.00\n48\n\n\n1\n45.441\n9.63\n48\n\n\n2\n45.118\n11.08\n48\n\n\n3\n39.203\n12.88\n44\n\n\n\n\n\n\n\n\nTo create or access a persisted database, specify the name of the database using a .db, .ddb, or .duckdb extension. The following statements create or open the database ads5064.db, read a CSV file into a Pandas DataFrame and create the table fitness from the DataFrame in the database.\nconn = duckdb.connect(database=\"ads5064.db\")\ndf = conn.read_csv('../datasets/fitness.csv').df()\nconn.execute('CREATE TABLE fitness AS SELECT * FROM df')\n\nYou can see all the tables in a database by querying the information schema:\n\nconn.sql('select * from information_schema.tables')\n\n┌───────────────┬──────────────┬────────────┬────────────┬───┬────────────────────┬──────────┬───────────────┐\n│ table_catalog │ table_schema │ table_name │ table_type │ … │ is_insertable_into │ is_typed │ commit_action │\n│    varchar    │   varchar    │  varchar   │  varchar   │   │      varchar       │ varchar  │    varchar    │\n├────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n│                                                   0 rows                                                   │\n└────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n\n\nOr with\n\nconn.sql(\"show tables\")\n\n┌─────────┐\n│  name   │\n│ varchar │\n├─────────┤\n│ 0 rows  │\n└─────────┘\n\n\nAnd you can see the available databases with\n\nconn.sql(\"show databases\")\n\n┌───────────────┐\n│ database_name │\n│    varchar    │\n├───────────────┤\n│ memory        │\n└───────────────┘\n\n\nYou could have created the table in the database from the CSV file in a single step:\nconn = duckdb.connect(database=\"ads5064.db\")\nconn.execute('CREATE TABLE fitness AS SELECT * FROM “../data/fitness.csv” ')\n\nNotice the use of single and double quotes to distinguish the string for the file name inside the SQL statement.",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Access</span>"
    ]
  },
  {
    "objectID": "data/data_access.html#cloud-data-resources",
    "href": "data/data_access.html#cloud-data-resources",
    "title": "5  Data Access",
    "section": "5.3 Cloud Data Resources",
    "text": "5.3 Cloud Data Resources\nCloud resources are increasingly important for data storage and analytics. The growth trends in the cloud are staggering. Storing data in cloud file, block, or object storage is a $78.6 billion market with a compound annual growth rate (CAGR) of 18.5% (2022 numbers). Cloud databases in 2023 are a $21.3 billion market that grows at 22%.\nYou will encounter cloud-based data in your data science work, and you will take advantage of cloud resources, especially for handling and analyzing large data sets.\nThe most important cloud storage solutions for data are\n\nObject storage\nCloud databases\nCloud data warehouses (Snowflake, BigQuery)\nCloud lakehouse (Databricks)\n\n\nObject storage\nObject, file, and block storage work differently. In object storage, data is managed as generic objects. These can be files or parts of files and combine the data to be stored with metadata about the object being stored. Metadata could be, for example, a unique identifier, size, creation date, time, and location for an image. Objects are stored in a flat structure; the object’s identifier is used to locate it in the storage system. Amazon Web Services (AWS) calls the container objects are stored in a bucket (rather than a directory).\nFile storage is what you are used to from a directory-based file system on your computer’s operating system. Data is stored in files, organized in hierarchical folders. You look up data by traversing the directory until you have located the file with the correct name.\nBlock storage breaks data into fixed-sized blocks and stores the blocks as units. The Hadoop Distributed File System (HDFS) is a block-oriented storage system, where data is split into 16 MB blocks and the blocks are replicated on multiple machines in a distributed system for durability and high availability.\nCloud object storage has nearly replaced other Big Data storage solutions such as Hadoop and HDFS. You can still find Hadoop environments in organizations—lots of money was invested in building those and organizations are slow to migrate off Hadoop. But the writing is on the wall: HDFS is replaced with cloud storage because it is cheaper and more durable, in particular cloud object storage. Since object storage supports storing files as objects, is the most economical storing alternative, and can scale out without limits, it is the primary storage form for fie-based data in the cloud. Even cloud data warehouses have been built with cloud object storage as the primary storage layer.\nThe object storage systems of the three major cloud providers are\nAmazon S3 (Simple Storage Service) from AWS, Azure Blob Storage, and Google Cloud Storage. Common file formats for you find in these storage systems are CSV, JSON, and Parquet.\n\n\nManaged cloud database\nTODO\n\n\nMotherDuck serverless cloud database\nThe convenience of a serverless database is not having to worry about the cloud instances the database is running on. With a local, on-premises database, you manage the storage and compute infrastructure—the machines—the database is running on. With a managed service (DBaaS), this aspect is taken care of by the managed service provider. However, you still must manage your database instances. For example, during periods of low usage, you might want to scale down the database or even shut it down to reduce cloud expenses.\n\n\nExample: Please turn off your instances\n\n\nA common mistake by cloud newbies is to assume that just because you are not actively using the cloud, you are not incurring expenses. When you start an instance in the cloud you need to stop it when it is not in use. Otherwise, it keeps running. Even if there is no workload on the instance, it will cost you money.\nA class at a major university—which shall not be named—used cloud resources and the instructors forgot to tell students to shut down their cloud instances. Instead, the students started new instances every time they needed to do cloud computing. The academic unit conducting the class had allocated a $40,000 cloud computing budget over the next three years. The class spent $140,000 in one month.\n\n\nA serverless system automatically scales to zero when not in use for a while. With serverless systems you tend to pay only for what you use, when you use it. An exception are charges for storing data, you cannot turn the storage off. The MotherDuck serverless database is a great complement to the DuckDB analytic database. In fact, MotherDuck is built on DuckDB and you can think of it as the cloud version of DuckDB. That makes MotherDuck a good choice for analytical workloads since DuckDB is a relational column-store, an analytic database.\nAnother nice feature of MotherDuck is the ability to connect to local databases and to cloud databases at the same time. This hybrid mode is useful if you have large data sets in cloud storage and small-to-large data sets stored locally. Since we already know how to work with DuckDB from Python, working with MotherDuck is a snap. The following statements create a connection to a MotherDuck serverless database:\nimport duckdb\ncon = duckdb.connect('md:')\n\nNotice that we use the same duckdb library as previously. The special string ‘md:’ or ‘motherduck:’ inside the connect() function triggers a connection attempt to MotherDuck. Whether the connection attempt succeeds depends on whether you can authenticate to MotherDuck. By default, the connect() function will open a browser through which you can authenticate to the MotherDuck service using a previously established account.\nAlternatively, you can click on Settings in the UI of the MotherDuck console at https://app.motherduck.com/ and download your authentication token. Store this token in the environment variable motherduck_token on your system and you can authenticate and connect directly to MotherDuck with the previous duckdb.connect() statement. It is also possible to list the authentication token directly in the duckdb.connect() function,\ncon = duckdb.connect('md:?motherduck_token=&lt;token&gt;')\nbut this is discouraged. Access credentials should not be shown in clear text in a source file that can easily be read and that might be shared with others. That is a security nightmare waiting to happen.\nDuckDB and MotherDuck are relational database management systems, they manage more than one database. You can see the list of databases with the SHOW DATABASES command:\ncon.sql(\"SHOW DATABASES\")\nBy default, MotherDuck creates a database called my_db and a shared database with sample data. One of the cool features of MotherDuck is the ability to attach local databases to the instance. The following statement attaches a local version of the database for this course to MotherDuck.\ncon.sql(\"ATTACH 'ads5064.ddb'\")\ncon.sql(\"SHOW DATABASES\")\n\nSince MotherDuck manages multiple databases you have to tell it which database to work with. This is called the current database, by default this is my_db when you first connect. You can see which database is current with\ncon.sql(\"SELECT current_database()\").show()\nYou can set any database as the current one with the USE statement:\ncon.sql(\"USE ads5064\")\ncon.sql(\"SHOW TABLES\")\n\nYou can access a table in a database that is not the current database by using a two-level name (database.table). This even works if one table is local and the other is in a cloud database. Suppose that ads5064 is a local version of the database for the course and that ads5064_md is stored in the cloud.\ncon.sql(\"SELECT count(*) FROM ads5064.apples\")\ncon.sql(\"SELECT count(*) FROM ads5064_md.glaucoma\")\n\nThis comes in very handy if you want to process local and cloud data in the same query, for example when joining tables.\nMotherDuck also supports shared read-only databases, to which you connect through a URL. The URL is created by an administrator of the database with the CREATE SHARE command. You can then attach the database by using the URL:\ncon.sql(\"ATTACH 'md:_share/ads5064_md/e00063c9-568c-45ec-a372-dca85f6915fd' as course_db\")\nThe URL is a link to a point-in-time snapshot of the database when it was shared. The administrator can update the share of the database, the link will remain valid.",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Access</span>"
    ]
  },
  {
    "objectID": "data/quality.html",
    "href": "data/quality.html",
    "title": "6  Data Quality",
    "section": "",
    "text": "6.1 Data Profiling",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Quality</span>"
    ]
  },
  {
    "objectID": "data/quality.html#sec-data-profiling",
    "href": "data/quality.html#sec-data-profiling",
    "title": "6  Data Quality",
    "section": "",
    "text": "The First Date with your Data\nData profiling is one of the first steps to take when you encounter a data set for the first time. It is how you kick-start the exploratory data analysis (EDA). (Borne 2021) refers to it as “having that first date with your data.” We are not looking to derive new insights from the data or to build amazing machine learning models at this stage; we want to create a high-level report of the data’s content and condition. We want to know what we are dealing with. Common questions and issues addressed during profiling are\n\nWhich variables (attributes) are in the data?\nHow many rows and columns are there?\nWhich variables are quantitative (represent numbers), and which variables are qualitative (represent class memberships)\nAre qualitative variables coded as strings, objects, numbers?\nAre there complex data types such as JSON documents, images, audio, video encoded in the data?\nWhat are the ranges (min, max) of the variables. Are these reasonable or do they suggest outliers or measurement errors?\nWhat is the distribution of quantitative variables?\nWhat is the mean, median, and standard deviation of quantitative variables?\nWhat are the unique values of qualitative variables?\nDo coded fields such as ZIP codes, account numbers, email addresses, state codes have the correct format?\nAre there attributes that have only a single value?\nAre there duplicate entries?\nAre there missing values in one or more variables?\nWhat are the strengths and direction of pairwise associations between the variables?\nAre some attributes perfectly correlated, for example, birthdate and age or temperatures in degree Celsius and degree Fahrenheit.\n\n\n\nCalifornia Housing Prices\nIn this subsection we consider a data set on housing prices in California, based on the 1990 census and available on Kaggle. The data contain information about geographic location, housing, and population within blocks. California has over 8,000 census tracts and a tract can have multiple block groups. There are over 20,000 census block groups and over 700,000 census blocks in California.\nThe variables in the data are:\n\nVariables in the California Housing Prices data set.\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nlongitude\nA measure of how far west a house is; a higher value is farther west\n\n\nlatitude\nA measure of how far north a house is; a higher value is farther north\n\n\nhousing_median_age\nMedian age of a house within a block; a lower number is a newer building\n\n\ntotal_rooms\nTotal number of rooms within a block\n\n\ntotal_bedrooms\nTotal number of bedrooms within a block\n\n\npopulation\nTotal number of people residing within a block\n\n\nhouseholds\nTotal number of households, a group of people residing within a home unit, for a block\n\n\nmedian_income\nMedian income for households within a block of houses (measured in tens of thousands of US Dollars)\n\n\nmedian_house_value\nMedian house value for households within a block (measured in US Dollars)\n\n\nocean_proximity\nLocation of house with respect to ocean/sea\n\n\n\nThe variable description is important metadata to understand the data. A variable such as totalRooms could be understood as the number of rooms in a building and the medianHouseValue could then mean the median of the houses that have that number of rooms. However, since the data are not for individual houses but blocks, totalRooms represents the sum of all the rooms in all the houses in that block.\nWe start data profiling a pandas DataFrame of the data with getting basic info and a listing of the first few observations.\n\nimport numpy as np\nimport pandas as pd\nCA_houses = pd.read_csv(\"../datasets/CaliforniaHousing_1990.csv\")\n\nCA_houses.info()\nCA_houses.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\n\nWe can immediately answer several questions about the data:\n\nThere are 20,640 rows and 10 columns\nExcept for ocean_proximity, which is of object type (string), all other variables are stored as 64-bit floats. Variables total_rooms, total_bedrooms, population, and households are naturally integers; since they appeared in the CSV file with a decimal point, they were assigned a floating point data type.\nOnly the total_bedrooms variable has missing values, 20,640 – 20,433 = 207 values for this variable are unobserved. This shows a high level of completeness of the data set. (More on missing values in the next section).\nThe listing of the first five observations confirms that variables are counts or sums at the block-level, rather than data for individual houses.\nThe latitude and longitude values differ in the second decimal place, suggesting that blocks (=rows of the data set) have unique geographic location, but we cannot be sure.\nThe ocean_proximity entries are in all caps. We want to see the other values in that column to make sure the entries are consistent. Knowing the format of strings is important for filtering (selecting) or grouping observations.\n\nThe next step is to use pandas describe() function to compute basic summaries of the variables.\n\nCA_houses.describe()\n\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20433.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n-119.569704\n35.631861\n28.639486\n2635.763081\n537.870553\n1425.476744\n499.539680\n3.870671\n206855.816909\n\n\nstd\n2.003532\n2.135952\n12.585558\n2181.615252\n421.385070\n1132.462122\n382.329753\n1.899822\n115395.615874\n\n\nmin\n-124.350000\n32.540000\n1.000000\n2.000000\n1.000000\n3.000000\n1.000000\n0.499900\n14999.000000\n\n\n25%\n-121.800000\n33.930000\n18.000000\n1447.750000\n296.000000\n787.000000\n280.000000\n2.563400\n119600.000000\n\n\n50%\n-118.490000\n34.260000\n29.000000\n2127.000000\n435.000000\n1166.000000\n409.000000\n3.534800\n179700.000000\n\n\n75%\n-118.010000\n37.710000\n37.000000\n3148.000000\n647.000000\n1725.000000\n605.000000\n4.743250\n264725.000000\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6445.000000\n35682.000000\n6082.000000\n15.000100\n500001.000000\n\n\n\n\n\n\n\n\nFor each of the numeric variables, describe() computes the number of non-missing values (count), the sample mean (mean), the sample standard deviation (std), the minimum (min), maximum (max) and three percentiles (25%, 50%, 75%).\nThe results confirm that all variables have complete data (no missing values) except for total_bedrooms. The min and max values are useful to see the range (range = max – min) for the variables and to spot outliers and unusual values. It is suspicious that there is one or more blocks with a single household. This is not necessarily the same record that has 2 total_rooms and a population of 3.\n\nCA_houses[CA_houses[\"households\"] == 1]\n\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n16171\n-122.5\n37.79\n52.0\n8.0\n1.0\n13.0\n1.0\n15.0001\n500001.0\nNEAR BAY\n\n\n\n\n\n\n\n\nThis is indeed a suspicious record. There is a single household in the block, but thirteen people are living in the block in a house with eight rooms and one bedroom. An unusual configuration that should be examined for possible data entry errors.\n\n\nProfiling Tools\nYou can accelerate the data profiling task by using packages such as ydata_profiling (fka pandas_profiling), lux, or sweetviz. Sweetviz, for example, generates detailed interactive visualizations in a web browser or a notebook that help to address some of the profiling questions we raised at the beginning of the section.\nTo create a profile report for the housing prices data with sweetviz, use the following:\n\nimport sweetviz as sv\nmy_report = sv.analyze(CA_houses)\nmy_report.show_html(filepath='Profiling_CA_1.html', open_browser=False)\n\n\n\n\nReport Profiling_CA_1.html was generated.\n\n\nFigure 6.1 displays the main screen of the visualization. You can access the entire interactive html report here. For each numeric variable sweetviz reports the number of observed and missing values, the number of distinct values, and a series of summary statistics similar to the output from describe().  A histogram is also produced that gives an idea of the distribution of the variable in the data set. For example, housing_median_age has a fairly symmetric distribution, whereas total_rooms and total_bedrooms are highly concentrated despite a wide range.\n\n\n\n\n\n\nFigure 6.1: Sweetviz visualization for California Housing Prices. Main screen, some numeric variables.\n\n\n\nFigure 6.2 shows the bottom of the main screen that displays information on the ocean_proximity variable. We now see that there are five unique values for the variable with the majority in the category &lt;1H OCEAN.\n\n\n\n\n\n\nFigure 6.2: Profiling information for qualitative variable ocean_proximity.\n\n\n\nClicking on any variable brings up more details. For housing_median_age that detail is shown in Figure 6.3. It includes a detailed histogram of the distribution, largest, smallest, and most frequent values.\n\n\n\n\n\n\nFigure 6.3: Sweetviz detail for the variable housing_median_age.\n\n\n\nThe graphics are interactive, the number of histogram columns can be changed to the desired resolution.\nSweetviz displays pairwise associations between variables. You can see those for housing_median_age in Figure 6.3 or for all pairs of variables by clicking on Associations (Figure 6.4).\n\n\n\n\n\n\nFigure 6.4: Sweetviz visualization of pairwise associations in California Housing Prices data.\n\n\n\nAssociations are calculated and displayed differently depending on whether the variables in a pair are quantitative or not. For pairs of quantitative variables, sweetviz computes the Pearson correlation coefficient. It ranges from –1 to +1; a coefficient of 0 indicates no (linear) relationship between the two variables, they are uncorrelated. A coefficient of +1 indicates a perfect positive correlation, knowing one variable allows you to perfectly predict the other variable. Similarly, a Pearson coefficient of –1 means that the variables are perfectly correlated and one variable decreases as the other increases.\nStrong positive correlations are present between households and the variables total_rooms, total_bedrooms, and population. That is expected as these variables are accumulated across all households in a block. There is a moderate positive association between median income and median house value. More expensive houses are associated with higher incomes—not surprising. A strong negative correlation exists between longitude and latitude, a consequence of the geography of California: as you move further west (east) the state reaches further south (north). \nAssociations between quantitative and qualitative variables are calculated as the correlation ratio that ranges from 0 to 1 and displayed as squares in the Associations matrix. The correlation ratio is based on means within the categories of the qualitative variables. A ratio of 0 means that the means of the quantitative variable are identical for all categories. Since the data contains only one qualitative variable, ocean_proximity, squares appear only in the last row and column of the Associations matrix.\nIf the data contains an obvious target variable for modeling, you can indicate that when creating the profiling report. Sweetviz then adds information on that variable to the visualizations. Suppose that we are interested in modeling the median house value as a function of other attributes. The following statement requests a report with median_house_value as the target.\n\nhousevalue_report = sv.analyze(CA_houses, target_feat=\"median_house_value\")\nhousevalue_report.show_html(filepath='Profiling_CA_2.html', open_browser=False)\n\n\n\n\nReport Profiling_CA_2.html was generated.\n\n\nFigure 6.5 shows the detail on ocean_proximity from this analysis; the complete report is here. The average of the block’s median housing values in the five groups of ocean proximity are shown on top of the histogram. The highest average median house value is found on the island, the lowest average in the inland category.\n\n\n\n\n\n\nFigure 6.5: Profiling details for ocean_proximity with target median_house_value.",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Quality</span>"
    ]
  },
  {
    "objectID": "data/quality.html#sec-missing-values",
    "href": "data/quality.html#sec-missing-values",
    "title": "6  Data Quality",
    "section": "6.2 Missing Values",
    "text": "6.2 Missing Values\nWhen observations do not have values assigned to them, we say that the value is missing. This is a fact of life in data analytics; whenever you work with a set of data you should expect values to be missing.\n\n\nDefinition: Missing Value\n\n\nA missing value is an observation that has no value assigned to it.\n\n\nMissingness is obvious when you see incomplete columns in the data. The problem can be inconspicuous when entire records are missing from the data. A survey that fails to include a key demographic misses the records of those who should have been sampled in the survey.\nYou should check the software packages used for data analysis on how they handle missing values—by default and how the behavior can be affected through options. In many cases the default behavior is casewise deletion, also known as complete-case analysis: any record that have a missing value in one or more of the analysis variables is excluded from the analysis. Pairwise deletion removes only those records that have missing values for a specific analysis. To see the difference, consider the data in Table 6.1 and suppose you want to compute the matrix of correlations among the variables.\n\n\n\nTable 6.1: Three variables with different missing value patterns.\n\n\n\n\n\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\n\n\n\n1.0\n3.0\n.\n\n\n2.9\n.\n3.4\n\n\n3.8\n.\n8.2\n\n\n0.5\n3.7\n.\n\n\n\n\n\n\nA complete-case analysis of \\(X_1\\), \\(X_2\\) , and \\(X_3\\) would result in a data frame without observations since each row of the table has a missing value in one column. Pairwise deletion computes the correlation between \\(X_1\\) and \\(X_2\\) based on the first and last observation, the correlation between \\(X_1\\) and \\(X_3\\) based on the second and third observation and fail to compute a correlation between \\(X_2\\) and \\(X_3\\).\nWhat are some possible causes for missing values:\n\nMembers of the target population not included (missing records)\nData entry errors\nVariable transformations that lead to invalid values: division by zero, logarithm of zeros or negative values\nMeasurement equipment malfunction\nMeasurement equipment limits exceeded\nAttrition (drop-outs) of subject in longitudinal studies (death, moving, refusal, changes in medical condition, …)\nNonresponse of subjects in surveys\nVariables not measured\nNot all combinations of factors are observable. For example, the data set of your Netflix movie ratings is extremely sparse, unless you “finished Netflix” and rated all movies.\nRegulation requires removal of sensitive information\n\nData transformations can introduce missing values into data sets when mathematical operations are not valid. To accommodate nonlinear relationships between target and input variables, transformations of inputs such as ratios, square roots, and logarithms are common. These transformations are sometimes applied to change the distribution of data, for example, to create more symmetry by taking logarithms of right-skewed data (Figure 6.6).\n\n\n\n\n\n\nFigure 6.6: Distribution of home values and logarithm of home values in Albemarle County, VA. The log-transformed data is more symmetric distributed. Since all home values are positive, the transformation does not lead to missing values.\n\n\n\nThe log-transformation is meaningful in the home values example, it is not unreasonable to proceed with an analysis that assumes log(value) is normally distributed. Suppose you are log-transforming another highly skewed variable, the amount of individual’s annual medical out-of-pocket expenses. Most people have a moderate amount of out-of-pocket expenses, a smaller percentage have very high annual expenses. However, many will have no out-of-pocket expenses at all. Taking the logarithm will invalidate the records of those individuals. To get around the numerical issue of taking logarithms of zeros, transformations are sometimes changed to log(expenses + 1). This avoids missing values but fudges the data by pretending that everyone has at least some medical expenses.\nRemoving missing values from the analysis is appropriate only when the reason for the missingness is unrelated to any other information in the study. The relationship between absence of information and the study is known as the missing value process.\n\nMissing Value Process\n\n\n\n\n\n\nImportant\n\n\n\nMaking the wrong assumption about the missing value process can bias the results. A complete case analysis is not necessarily unbiased only if the data are missing completely at random. But the bias can at least be corrected in that case.\n\n\nYou need to be aware of three types of missing data based on that process:\n\nMCAR: Data is said to be missing completely at random when the missingness is unrelated to any study variable, including the target variable. There are no systematic differences between the records with missing data and the records with complete data. MCAR is a very strong assumption, and it is the best you can hope for. If the data are MCAR, you can safely delete records with missing values because the complete cases are a representative sample of the whole. Case deletion reduces the size of the available data but does not introduce bias into the analysis.\nMAR: Data is said to be missing at random when the pattern of missingness is related to the observed data but not to the unobserved data. Suppose you are conducting a survey regarding depression and mental health. If one group is less likely to provide information in the survey for reasons unrelated to their level of depression, then the group’s data is missing at random. Complete-case analysis of a data set that contains MAR data can result in bias.\nMNAR: Data is said to be missing not at random if the absence of information is systematically related to the unobserved data. For example, employees do not report salaries in a workspace survey or a group that is less likely to report in a depression survey because of their level of depression.\n\nA complete-case analysis if the data are MAR or MNAR does not necessarily bias the results. If the missingness is related to the primary target variable, then the results are biased. In the MAR case that bias can be corrected. As noted by the NIH in the context of patient studies,\n\nThe import of the MAR vs. MNAR distinction is therefore not to indicate that there definitively will or will not be bias in a complete case analysis, but instead to indicate – if the complete case analysis is biased – whether that bias can be fully removed in analysis.\n\nMissing values are represented in data sets in different ways. The two basic methods are to use masks or extra bits to indicate whether a value is available and to use sentinel value, special entries that indicate that a value is not available (missing).\n\n\nDefinition: Sentinel Value\n\n\nIn programming, a sentinel value is a special placeholder that indicates a special condition in the data or the program. Applications of sentinel values are to indicate when to break out of loops or to indicate unobserved values.\n\n\nSentinel values such as –9999 to indicate a missing value are dangerous, they can be mistaken too easily for a valid numerical entry. The only sentinel value one should use is NaN (not-a-number), a specially defined IEEE floating-point value. Software implements special logic for handling NaNs. Unfortunately, NaN is available only for floating point data types, so software uses different techniques to implement missing values across all data types. For example, in databases you find masking based on the concept of NULL values to indicate absence (=nullity) of a value.\n\n\n\n\n\n\nCaution\n\n\n\nDo not use sentinel values that could be confused with real data values to indicate that a value is missing.\n\n\n\n\nMissing Values in Pandas\nPython has the singleton object None which can be used to indicate missingness and it supports the IEEE NaN (not a number) to indicate missing values for floating-point types. Pandas uses the sentinel value approach based on NaN for floating-point and None for all other data types. This choice has some side effects, None and NaN do not behave the same way.\n\nimport numpy as np\nimport pandas as pd\n\nx1 = np.array([1, None, 3, 4])\nx2 = np.array([1, 2, 3, 4])\ndisplay(x1)\ndisplay(x2)\n\narray([1, None, 3, 4], dtype=object)\n\n\narray([1, 2, 3, 4])\n\n\nThe array with None value is represented internally as an array of Python objects. Operating on objects is slower than on basic data types such as integers. Having missing values in non-floating-point columns thus incurs some drag on performance.\nFor floating point data use np.nan to indicate missingness.\n\nf1 = np.array([1, np.nan, 3, 4])\nf1\n\narray([ 1., nan,  3.,  4.])\n\n\nThe behavior of None and NaN in operations is different. For example, arithmetic operations on NaNs result in NaNs, whereas arithmetic on None values results in errors.\n\nf1.sum()\n\nnan\n\n\n\nx1.sum()\n\nTypeError: unsupported operand type(s) for +: 'int' and 'NoneType'\n\n\nWhile None values result in errors and stop program execution, NaNs are contagious; they turn everything they come in touch with into NaNs—but the program keeps executing. Pandas mixes None and NaN values and follows casting rules when np.nan is stored.\n\nx2 = pd.Series([1,2,3,4], dtype=int)\ndisplay(x2)\nx2[2] = np.nan\ndisplay(x2)\n\n0    1\n1    2\n2    3\n3    4\ndtype: int64\n\n\n0    1.0\n1    2.0\n2    NaN\n3    4.0\ndtype: float64\n\n\nThe integer series is converted to a float series when a NaN was inserted. The same happens when you use None instead of NaN:\n\nx3 = pd.Series([1,2,3,4], dtype=int)\nx3[1] = None\nx3\n\n0    1.0\n1    NaN\n2    3.0\n3    4.0\ndtype: float64\n\n\n\n\nWorking with Missing Values in Data Sets\nThe following statements create a Pandas DataFrame from a CSV file that contains information about 7,303 traffic collisions in New York City. You can use the info() attribute of the DataFrame for information about the columns, including missing value counts.\n\ncollisions = pd.read_csv(\"../datasets/nyc_collision_factors.csv\")\ncollisions.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7303 entries, 0 to 7302\nData columns (total 26 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   DATE                           7303 non-null   object \n 1   TIME                           7303 non-null   object \n 2   BOROUGH                        6920 non-null   object \n 3   ZIP CODE                       6919 non-null   float64\n 4   LATITUDE                       7303 non-null   float64\n 5   LONGITUDE                      7303 non-null   float64\n 6   LOCATION                       7303 non-null   object \n 7   ON STREET NAME                 6238 non-null   object \n 8   CROSS STREET NAME              6166 non-null   object \n 9   OFF STREET NAME                761 non-null    object \n 10  NUMBER OF PERSONS INJURED      7303 non-null   int64  \n 11  NUMBER OF PERSONS KILLED       7303 non-null   int64  \n 12  NUMBER OF PEDESTRIANS INJURED  7303 non-null   int64  \n 13  NUMBER OF PEDESTRIANS KILLED   7303 non-null   int64  \n 14  NUMBER OF CYCLISTS INJURED     0 non-null      float64\n 15  NUMBER OF CYCLISTS KILLED      0 non-null      float64\n 16  CONTRIBUTING FACTOR VEHICLE 1  7303 non-null   object \n 17  CONTRIBUTING FACTOR VEHICLE 2  6218 non-null   object \n 18  CONTRIBUTING FACTOR VEHICLE 3  303 non-null    object \n 19  CONTRIBUTING FACTOR VEHICLE 4  59 non-null     object \n 20  CONTRIBUTING FACTOR VEHICLE 5  14 non-null     object \n 21  VEHICLE TYPE CODE 1            7245 non-null   object \n 22  VEHICLE TYPE CODE 2            5783 non-null   object \n 23  VEHICLE TYPE CODE 3            284 non-null    object \n 24  VEHICLE TYPE CODE 4            54 non-null     object \n 25  VEHICLE TYPE CODE 5            12 non-null     object \ndtypes: float64(5), int64(4), object(17)\nmemory usage: 1.4+ MB\n\n\nColumns DATE and TIME contain no null (missing) values, their count equals the number of observation (7,303). The number of cyclists injured or killed in the accidents in columns 14 and 15 contain only missing values. Is this a situation where the data was not entered, or should the entries be zeros? We should get back with the domain experts who put the data together to find out how to handle these columns.\nTable 6.2 displays Pandas DataFrame methods to operate on missing values.\n\n\n\nTable 6.2: Methods to operate on missing values in pandas DataFrames.\n\n\n\n\n\n\n\n\n\n\nMethod\nDescription\nNotes\n\n\n\n\nisnull()\nReturns a boolean same-sized object indicating if the missing values are missing\nisna() is an alias\n\n\nnotnull()\nOpposite of isnull(), indicating if values are not missing\nnotna() is an alias\n\n\ndropna()\nRemove missing values, dropping either rows (axis=0) or columns (axis=1)\nhow={'any','all'} to determine when to drop a row or column\n\n\nfillna()\nReplace missing values with designated values\nmethod= to propagate last valid value or backfill with next valid value\n\n\ninterpolate()\nFill in missing values using an interpolation method\n\n\n\n\n\n\n\nThe isnull() method can be used to return a data frame of Boolean (True/False) values that indicate missingness. You can sum across rows or columns of the data frame to count the missing values:\n\ncollisions.isnull().sum()\n\nDATE                                0\nTIME                                0\nBOROUGH                           383\nZIP CODE                          384\nLATITUDE                            0\nLONGITUDE                           0\nLOCATION                            0\nON STREET NAME                   1065\nCROSS STREET NAME                1137\nOFF STREET NAME                  6542\nNUMBER OF PERSONS INJURED           0\nNUMBER OF PERSONS KILLED            0\nNUMBER OF PEDESTRIANS INJURED       0\nNUMBER OF PEDESTRIANS KILLED        0\nNUMBER OF CYCLISTS INJURED       7303\nNUMBER OF CYCLISTS KILLED        7303\nCONTRIBUTING FACTOR VEHICLE 1       0\nCONTRIBUTING FACTOR VEHICLE 2    1085\nCONTRIBUTING FACTOR VEHICLE 3    7000\nCONTRIBUTING FACTOR VEHICLE 4    7244\nCONTRIBUTING FACTOR VEHICLE 5    7289\nVEHICLE TYPE CODE 1                58\nVEHICLE TYPE CODE 2              1520\nVEHICLE TYPE CODE 3              7019\nVEHICLE TYPE CODE 4              7249\nVEHICLE TYPE CODE 5              7291\ndtype: int64\n\n\nIf you choose to remove records with missing values, you can use the dropna() method. The how=’any’|’all’ option specifies whether to remove records if any variable is missing (complete-case analysis) or if all variables is missing. Because the columns referring to cyclists contain only missing values, a complete-case analysis will result in an empty DataFrame.\n\ncoll_no_miss = collisions.dropna(how='any')\ndisplay(len(collisions))\ndisplay(len(coll_no_miss))\n\n7303\n\n\n0\n\n\nSuppose we verified that the missing values in columns 14 & 15 were meant to indicate that no cyclists were injured or killed. Then we can replace the missing values with zeros using the fillna() method.\n\ncollisions[\"NUMBER OF CYCLISTS INJURED\"].fillna(0,inplace=True)\ncollisions[\"NUMBER OF CYCLISTS KILLED\"].fillna(0,inplace=True)\n\n/var/folders/ff/6m670wks1776ck_gbfxynxrc0000gq/T/ipykernel_16417/612697346.py:1: FutureWarning:\n\nA value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n\n/var/folders/ff/6m670wks1776ck_gbfxynxrc0000gq/T/ipykernel_16417/612697346.py:2: FutureWarning:\n\nA value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n\n\n\nTechniques for imputing missing values are discussed in more detail below.\nThe notnull() method is useful to select records without missing values. Since it returns a boolean same-sized object, you can use it to filter:\n\nbool_series = pd.notnull(collisions[\"CONTRIBUTING FACTOR VEHICLE 5\"]) \ncollisions.loc[bool_series, [\"DATE\", \"TIME\", \"BOROUGH\", \"ON STREET NAME\"]] \n\n\n\n\n\n\n\n\n\nDATE\nTIME\nBOROUGH\nON STREET NAME\n\n\n\n\n217\n04/11/2016\n13:22:00\nBROOKLYN\nSTUYVESANT AVENUE\n\n\n896\n10/08/2016\n14:10:00\nQUEENS\nNaN\n\n\n1112\n10/18/2016\n07:10:00\nQUEENS\n124 STREET\n\n\n1348\n11/23/2016\n14:11:00\nBROOKLYN\nSUTTER AVENUE\n\n\n1551\n11/16/2016\n11:11:00\nQUEENS\n112 AVENUE\n\n\n1554\n11/29/2016\n04:11:00\nBROOKLYN\nSNYDER AVENUE\n\n\n2794\n11/15/2016\n03:11:00\nMANHATTAN\nNaN\n\n\n2899\n03/07/2016\n08:45:00\nQUEENS\n105 AVENUE\n\n\n3292\n10/25/2016\n18:10:00\nBROOKLYN\nHIGHLAND BOULEVARD\n\n\n3813\n10/03/2016\n17:10:00\nSTATEN ISLAND\nMANOR ROAD\n\n\n4346\n02/08/2016\n12:08:00\nBROOKLYN\nEAST 35 STREET\n\n\n5286\n01/14/2016\n20:00:00\nNaN\nNaN\n\n\n5556\n01/07/2016\n18:50:00\nQUEENS\n31 AVENUE\n\n\n6737\n01/11/2016\n09:03:00\nBROOKLYN\nBROADWAY\n\n\n\n\n\n\n\n\n\n\nVisualizing Missing Value Patterns\nThe Missingno Python package has some nice methods to inspect the missing value patterns in data. This is helpful to see the missing value distribution across multiple columns. The matrix() method displays the missing value pattern for the DataFrame.\n\nimport missingno as msno\nmsno.matrix(collisions)\n\n\n\n\n\n\n\n\nColumns without missing values (DATE, TIME) are shown as a solid gray bar. Missing values are displayed in white. The following graph shows the result of matrix() prior to filling in zeros in the cyclist columns. The sparkline at right summarizes the general shape of the data completeness and points out the rows with the maximum and minimum number of missing values in the dataset. At best 11 of the columns have missing values, at worst 23 of the 26 values are missing.\nTable 6.3 contains data on property sales. The total value of the property is the sum of the first two columns, the last column is the ratio between sales price and total value. A missing value in one of the first two columns triggers a missing value in the Total column. If either Total or Sales are not present, the appraisal ratio in the last column must be missing.\n\n\n\nTable 6.3: Data with column dependencies that propagate missing values.\n\n\n\n\n\nLand\nImprovements\nTotal\nSale\nAppraisal Ratio\n\n\n\n\n30000\n64831\n94831\n118500\n1.25\n\n\n30000\n50765\n80765\n93900\n1.16\n\n\n46651\n18573\n65224\n.\n.\n\n\n45990\n91402\n137392\n184000\n1.34\n\n\n42394\n.\n.\n168000\n.\n\n\n.\n133351\n.\n169000\n.\n\n\n63596\n2182\n65778\n.\n.\n\n\n56658\n153806\n210464\n255000\n1.21\n\n\n51428\n72451\n123879\n.\n.\n\n\n93200\n.\n.\n422000\n.\n\n\n76125\n78172\n275297\n290000\n1.14\n\n\n154360\n61934\n216294\n237000\n1.10\n\n\n65376\n.\n.\n286500\n.\n\n\n42400\n.\n.\n.\n.\n\n\n40800\n92606\n133406\n168000\n1.26\n\n\n\n\n\n\nThe heatmap() method shows a matrix of nullity correlations between the columns of the data. Note that the CSV file contains dots (“.”) for the missing values. To make sure the data are correctly converted to numerical types and the dots are interpreted as missing values, the na_values= and skipinitalspace= options are added to pd.read_csv().\n\nland = pd.read_csv(\"../datasets/landsales.csv\", \n                   na_values=\".\",\n                   skipinitialspace=True)\n\nmsno.heatmap(land)\n\n\n\n\n\n\n\n\nThe nullity correlations are Pearson correlation coefficients computed from the isnull() boolean object for the data, excluding columns that are completely observed or completely unobserved. A correlation of –1 means that presence/absence of two variables is perfectly correlated: if one variable appears the other variable does not appear. A correlation of +1 similarly means that the presence of one variable goes together with the presence of another variable. The total is not perfectly correlated with land or improve columns because a null value in either or both of these can cause a null value for the total. Similarly, the large correlations between appraisal & total and appraisal & sale are indicative that their missing values are likely to occur together.\n\n\nData Imputation\nIn a previous example we used the fillna() method to replace missing values with actual values: the unobserved values for the number of cyclists in the collisions data set were interpreted as no cyclists were injured, replacing NaNs with zeros. This is an example of data imputation.\n\n\nDefinition: Data Imputation\n\n\nData imputation is the process of replacing unobserved (missing) values with usable values.\n\n\n\nImputation must be carried out with care. It is tempting to replace absent values with numbers and to complete the data: records are not removed from the analysis, the sample size is maintained, and calculations no longer fail. Imputing values that are not representative introduces bias into the data.\nCompleting missing values based on information in other columns often seems simple on the surface, but it is fraught with difficulties—there be dragons! Suppose using address information to fill in missing zip codes. It is not sufficient to know that the city is Blacksburg. If we are talking about Blacksburg, SC, then we know the ZIP code is 29702. If it is Blacksburg, VA, however, then there are four possible ZIP codes; we need the street address to resolve to a unique value. Inferring a missing attribute such as gender should never be done. You cannot safely do it using names. Individuals might have chosen to not report gender information. You cannot afford to get it wrong.\nIf string-type data is missing, and you want to include them into the analysis, you can replace the missing values with an identifying string such as “Unobserved” or “Unknown”. That allows you to break out results for these observations in group-by observations, for example.\nIf you decide to proceed with imputation of missing values based on algorithms, here are some options:\n\nRandom replacement. Also called hot-deck imputation, the missing value is replaced with a randomly selected similar record in the same data set that has complete information.\nLOCF. The missing value is replaced with the last complete observation preceding it: the last observation is carried forward. It is also called a forward fill. This method requires that the order of the observations in the data is somehow meaningful. If observations are grouped by city, it is probable that a missing value for the city column represents the same city as the previous record, unless the missing value falls on the record boundary between two cities.\nBackfill. This is the opposite of LOCF; the next complete value following one or more missing values is propagated backwards.\nMean/Median imputation. This technique applies to numeric data; the missing value is replaced based on the sample mean, the sample median, or other statistical measures of location calculated from the non-missing values in a column. If the data consists of groups or classes, then group-specific means can be used. For example, if the data comprises age groups or genders, then missing values for a numeric variable can be replaced with averages for the variabler by age groups or genders.\nInterpolation methods. For numerical data, missing values can be interpolated from nearby values. Interpolation calculations are based on linear, polynomial, or spline methods. Using the interpolate() method in pandas, you can choose between interpolating across rows or columns of the DataFrame.\nRegression imputation. The column with missing values is treated as the target variable of a regression model, using one or more other columns as input variables of the regression. The missing values are then treated as unobserved observations for which the target is predicted.\nMatrix completion. Based on principal component analysis, missing values in a \\(r \\times c\\) numerical array are replaced with a low-rank approximation of the missing values based on the observed values.\nGenerative imputation. If the data consists of images or text and portions are unobserved, for example, parts of the image are obscured, then generative methods can be used to fill in missing pixels in the image or missing words in text.\n\nTo demonstrate some of these techniques, consider this simple 6 x 3 DataFrame.\n\nnp.random.seed(42)\ndf = pd.DataFrame(np.random.standard_normal((6, 3)))\ndf.iloc[:4,1] = np.nan\ndf.iloc[1:3,2] = np.nan\ndf\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.496714\nNaN\n0.647689\n\n\n1\n1.523030\nNaN\nNaN\n\n\n2\n1.579213\nNaN\nNaN\n\n\n3\n0.542560\nNaN\n-0.465730\n\n\n4\n0.241962\n-1.913280\n-1.724918\n\n\n5\n-0.562288\n-1.012831\n0.314247\n\n\n\n\n\n\n\n\nYou can choose a common fill value for all columns or vary the value by column.\n\ndf.fillna({1:0.3, 2:0})\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.496714\n0.300000\n0.647689\n\n\n1\n1.523030\n0.300000\n0.000000\n\n\n2\n1.579213\n0.300000\n0.000000\n\n\n3\n0.542560\n0.300000\n-0.465730\n\n\n4\n0.241962\n-1.913280\n-1.724918\n\n\n5\n-0.562288\n-1.012831\n0.314247\n\n\n\n\n\n\n\n\nMissing values in the second column are replaced with 0.3, those in the last column with zero. Forward (LOCF) and backward imputation are available by setting the method= parameter of fillna():\n\ndf.fillna(method=\"ffill\")\n\n/var/folders/ff/6m670wks1776ck_gbfxynxrc0000gq/T/ipykernel_16417/3944122520.py:1: FutureWarning:\n\nDataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.496714\nNaN\n0.647689\n\n\n1\n1.523030\nNaN\n0.647689\n\n\n2\n1.579213\nNaN\n0.647689\n\n\n3\n0.542560\nNaN\n-0.465730\n\n\n4\n0.241962\n-1.913280\n-1.724918\n\n\n5\n-0.562288\n-1.012831\n0.314247\n\n\n\n\n\n\n\n\nNotice that the forward fill does not replace the NaNs in the second column as there is no non-missing last value to be carried forward. The second and third row in the third column are imputed by carrying forward 0.647689 from the first row. With a backfill imputation the DataFrame is fully completed, since both columns have an observed value after the last missing value.\n\ndf.fillna(method=\"bfill\")\n\n/var/folders/ff/6m670wks1776ck_gbfxynxrc0000gq/T/ipykernel_16417/3896554658.py:1: FutureWarning:\n\nDataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.496714\n-1.913280\n0.647689\n\n\n1\n1.523030\n-1.913280\n-0.465730\n\n\n2\n1.579213\n-1.913280\n-0.465730\n\n\n3\n0.542560\n-1.913280\n-0.465730\n\n\n4\n0.241962\n-1.913280\n-1.724918\n\n\n5\n-0.562288\n-1.012831\n0.314247\n\n\n\n\n\n\n\n\nTo replace the missing values with the column-specific sample means, simply use\n\ndf.fillna(df.mean())\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.496714\n-1.463056\n0.647689\n\n\n1\n1.523030\n-1.463056\n-0.307178\n\n\n2\n1.579213\n-1.463056\n-0.307178\n\n\n3\n0.542560\n-1.463056\n-0.465730\n\n\n4\n0.241962\n-1.913280\n-1.724918\n\n\n5\n-0.562288\n-1.012831\n0.314247\n\n\n\n\n\n\n\n\nThe sample means of the non-missing observations in the second and third columns are -1.463056 and -0.307178, respectively. Imputing with the sample mean has the nice property to preserve the sample mean of the completed data:]\n\ndf.fillna(df.mean()).mean()\n\n0    0.636865\n1   -1.463056\n2   -0.307178\ndtype: float64\n\n\nYou can use other location statistics than the mean. The following statement imputes with the column-specific sample median:\n\ndf.fillna(df.median())\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.496714\n-1.463056\n0.647689\n\n\n1\n1.523030\n-1.463056\n-0.075741\n\n\n2\n1.579213\n-1.463056\n-0.075741\n\n\n3\n0.542560\n-1.463056\n-0.465730\n\n\n4\n0.241962\n-1.913280\n-1.724918\n\n\n5\n-0.562288\n-1.012831\n0.314247\n\n\n\n\n\n\n\n\nTo demonstrate imputation by interpolation, consider this simple series:\n\ns = pd.Series([0, 2, np.nan, 8])\ns\n\n0    0.0\n1    2.0\n2    NaN\n3    8.0\ndtype: float64\n\n\nThe default interpolation method is linear interpolation. You can choose other method, for example, spline or polynomial interpolation, with the method= option.\n\ns.interpolate()\ns.interpolate(method='polynomial', order=2)\n\n0    0.000000\n1    2.000000\n2    4.666667\n3    8.000000\ndtype: float64\n\n\nTo see interpolation operate on a DataFrame, consider this example:\n\ndf = pd.DataFrame([(0.0, np.nan, -1.0, 1.0),\n                    (np.nan, 2.0, np.nan, np.nan),\n                    (2.0, 3.0, np.nan, 9.0),\n                    (np.nan, 4.0, -4.0, 16.0)],\n                   columns=list('abcd'))\ndf\n\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n0.0\nNaN\n-1.0\n1.0\n\n\n1\nNaN\n2.0\nNaN\nNaN\n\n\n2\n2.0\n3.0\nNaN\n9.0\n\n\n3\nNaN\n4.0\n-4.0\n16.0\n\n\n\n\n\n\n\n\nThe default interpolation method is linear with a forward direction across rows.\n\ndf.interpolate()\n\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n0.0\nNaN\n-1.0\n1.0\n\n\n1\n1.0\n2.0\n-2.0\n5.0\n\n\n2\n2.0\n3.0\n-3.0\n9.0\n\n\n3\n2.0\n4.0\n-4.0\n16.0\n\n\n\n\n\n\n\n\nBecause there was no observed value preceding the missing value in column b, the NaN cannot be interpolated. Because there was no value past the last missing value in column a, the NaN is replaced with the last value carried forward.\nTo interpolate in the forward and backward direction, use limit_direction=’both’:\n\ndf.interpolate(method='linear', limit_direction='both')\n\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n0.0\n2.0\n-1.0\n1.0\n\n\n1\n1.0\n2.0\n-2.0\n5.0\n\n\n2\n2.0\n3.0\n-3.0\n9.0\n\n\n3\n2.0\n4.0\n-4.0\n16.0\n\n\n\n\n\n\n\n\nTo interpolate across the columns, set the axis= option to axis=1:\n\ndf.interpolate(method='linear', limit_direction='forward', axis=1)\n\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n0.0\n-0.5\n-1.0\n1.0\n\n\n1\nNaN\n2.0\n2.0\n2.0\n\n\n2\n2.0\n3.0\n6.0\n9.0\n\n\n3\nNaN\n4.0\n-4.0\n16.0\n\n\n\n\n\n\n\n\nImputation seems convenient and relatively simple, and there are many methods to choose from. As you can see from this discussion, there are also many issues and pitfalls. Besides introducing potential bias by imputing with bad values, an important issue is the level of uncertainty associated with imputed values. Unless the data were measured with error or entered incorrectly, you have confidence in the observed values. You cannot have the same level of confidence in the imputed values. The imputed values are estimates based on processes that involve randomness. One source of randomness is that our data represents a random sample—a different set of data gives a different estimate for the missing value. Another source of randomness is the imputation method itself. For example, hot-deck imputation chooses the imputed value based on randomly selected observations with complete data.\nAccounting for these multiple levels of uncertainty is non-trivial. If you pass imputed data to any statistical algorithm, it will assume that all values are known with the same level of confidence. There are ways to take imputation uncertainty into account. You can assign weights to the observations where the weight is proportional to your level of confidence. Assigning smaller weights to imputed values reduces their impact on the analysis. A better approach is to use bootstrap techniques to capture the true uncertainty and bias in the quantities derived from imputed data. This is more computer intensive than a weighted analysis but very doable with today’s computing power.\n\n\nDefinition: Bootstrap\n\n\nTo bootstrap a data set is to repeatedly sample from the data with replacement. Suppose you have a data set of size \\(n\\) rows. \\(B\\) = 1,000 bootstrap samples are 1,000 data sets of size \\(n\\), each drawn independently from each other, and the observations in each bootstrap sample are drawn with replacement.\nBootstrapping is a statistical technique to derive the sample distribution of a random quantity by simulation. You apply the technique to each bootstrap sample and average the results.\n\n\nI hope you take away from this discussion that imputation is possible, imputation is not always necessary, and imputation must be done with care.\n\n\n\nFigure 6.1: Sweetviz visualization for California Housing Prices. Main screen, some numeric variables.\nFigure 6.2: Profiling information for qualitative variable ocean_proximity.\nFigure 6.3: Sweetviz detail for the variable housing_median_age.\nFigure 6.4: Sweetviz visualization of pairwise associations in California Housing Prices data.\nFigure 6.5: Profiling details for ocean_proximity with target median_house_value.\nFigure 6.6: Distribution of home values and logarithm of home values in Albemarle County, VA. The log-transformed data is more symmetric distributed. Since all home values are positive, the transformation does not lead to missing values.\n\n\n\nBorne, Kirk. 2021. “Data Profiling–Having That First Data with Your Data.” Medium. https://medium.com/codex/data-profiling-having-that-first-date-with-your-data-2e05de50fca7.",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Quality</span>"
    ]
  },
  {
    "objectID": "data/summarization.html",
    "href": "data/summarization.html",
    "title": "7  Data Summarization",
    "section": "",
    "text": "7.1 Location and Dispersion Statistics\nCommon location and dispersion measures for quantitative variables are shown in the next tables.\nIn the parlance of databases, the descriptive statistics in the previous tables are called aggregates.",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Summarization</span>"
    ]
  },
  {
    "objectID": "data/summarization.html#location-and-dispersion-statistics",
    "href": "data/summarization.html#location-and-dispersion-statistics",
    "title": "7  Data Summarization",
    "section": "",
    "text": "Important statistics measuring location attributes of a variable. Sample mean, sample median, and sample mode are measures of the central tendency of a variable. \\(Y^*\\) denotes the ordered sequence of observations and \\(Y^*[k]\\) the value at the \\(k\\)th position in the ordered sequence. The min is defined as the smallest non-missing values because NaNs often sort as smallest values in software packages.\n\n\n\n\n\n\n\n\nSample Statistic\nSymbol\nComputation\nNotes\n\n\n\n\nMin\n\n\\(Y^*[1]\\)\nThe smallest non-missing value\n\n\nMax\n\n\\(Y^*[n]\\)\nThe largest value\n\n\nMean\n\\(\\overline{Y}\\)\n\\(\\frac{1}{n}\\sum_{i=1}^n Y_i\\)\nMost important location measure, but can be affected by outliers\n\n\nMedian\nMed\n\\[= \\left \\{    \\begin{array}{cc}       Y^* \\left[ \\frac{n+1}{2} \\right ] & n \\text{ is even} \\\\        \\frac{1}{2} \\left( Y^* \\left[\\frac{n}{2} \\right] + Y^* \\left[\\frac{n}{2}+1\\right] \\right) & n\\text{ is odd} \\end{array}\\right .\\]\nHalf of the observations are smaller than the median; robust against outliers\n\n\nMode\nMode\n\nThe most frequent value; not useful when real numbers are unique\n\n\n1st Quartile\n\n\\(Y^*\\left[\\frac{1}{4}(n+1) \\right]\\)\n25% of the observations are smaller than \\(Q_1\\)\n\n\n2nd Quartile\n\nSee Median\n50% of the observations are smaller than \\(Q_2\\). This is the median\n\n\n3rd Quartile\n\n\\(Y^*\\left[\\frac{3}{4}(n+1) \\right]\\)\n75% of the observations are smaller than \\(Q_3\\)\n\n\nX% Percentile\n\n\\(Y^*\\left[\\frac{X}{100}(n+1) \\right]\\)\nFor example, 5% of the observations are larger than \\(P_{95}\\), the 95% percentile\n\n\n\n\nImportant statistics measuring dispersion (variability) of a variable.\n\n\n\n\n\n\n\n\nSample Statistic\nSymbol\nComputation\nNotes\n\n\n\n\nRange\n\\(R\\)\n\\(Y^*[n] - Y^*[1]\\)\nSimply largest minus smallest value\n\n\nInter-quartile Range\nIQR\n\\(Q_3 - Q_1\\)\nUsed in constructing box plots; covers the central 50% of the data\n\n\nStandard Deviation\n\\(S\\)\n\\(\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n\\left( Y_i - \\overline{Y}\\right)^2}\\)\nMost important dispersion measure; in the same units as the sample mean (the units of \\(Y\\))\n\n\nVariance\n\\(S^2\\)\n\\(\\frac{1}{n-1}\\sum_{i=1}^n \\left( Y_i - \\overline{Y} \\right)^2\\)\nImportant statistical measure of dispersion; in squared units of \\(Y\\)\n\n\n\n\n\n\nDefinition: Aggregation\n\n\nAggregation in a database management system (DBMS) is the process of combining records into meaningful quantities called aggregates.",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Summarization</span>"
    ]
  },
  {
    "objectID": "data/summarization.html#iris-data",
    "href": "data/summarization.html#iris-data",
    "title": "7  Data Summarization",
    "section": "7.2 Iris Data",
    "text": "7.2 Iris Data\nWe are now calculating these summary statistics with Pandas, Polars, and with SQL for the famous Iris data set; a staple of data science and machine learning instruction. The data was used by R.A. Fisher, a pioneer and founder of modern statistics, in a 1936 paper “The use of multiple measurements in taxonomic problems.” The data set contains fifty measurements of four flower characteristics, the length and width of sepals and petals for each of three iris species: Iris setosa, Iris versicolor, and Iris virginica. The petals are the large leaves on the flowers, sepals are the smaller leaves at the bottom of the flower. You will likely see the iris data again, it is used to teach visualization, regression, classification, clustering, etc.\n\n\n\n\n\n\nFigure 7.1: Iris versicolor flowers\n\n\n\nWe can load the data from our DuckDB database into Pandas and Polars DataFrames with the following statements:\n\nimport pandas as pd\nimport polars as pl\nimport duckdb\ncon = duckdb.connect(database=\"../ads5064.ddb\")\n\npd_df = con.sql('select * from iris').df()\npd_dl = con.sql('select * from iris').pl()\n\nA basic set of statistical measures is computed with the describe() methods of these libraries:\n\npd_df.describe()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.054000\n3.758667\n1.198667\n\n\nstd\n0.828066\n0.433594\n1.764420\n0.763161\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\n\npd_dl.describe()\n\n\n\nshape: (9, 6)\n\n\n\nstatistic\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\nstr\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n\"count\"\n150.0\n150.0\n150.0\n150.0\n\"150\"\n\n\n\"null_count\"\n0.0\n0.0\n0.0\n0.0\n\"0\"\n\n\n\"mean\"\n5.843333\n3.054\n3.758667\n1.198667\nnull\n\n\n\"std\"\n0.828066\n0.433594\n1.76442\n0.763161\nnull\n\n\n\"min\"\n4.3\n2.0\n1.0\n0.1\n\"setosa\"\n\n\n\"25%\"\n5.1\n2.8\n1.6\n0.3\nnull\n\n\n\"50%\"\n5.8\n3.0\n4.4\n1.3\nnull\n\n\n\"75%\"\n6.4\n3.3\n5.1\n1.8\nnull\n\n\n\"max\"\n7.9\n4.4\n6.9\n2.5\n\"virginica\"\n\n\n\n\n\n\n\nPandas and Polars produce very similar output statistics for the numeric variables. Polars adds a row for the number of missing observations (null_count) and some basic summaries for the qualitative species variable. Apart from formatting, the results agree. The output column std represents the sample standard deviation, 25%, 50%, and 75% are the first, second, and third quartile (the 25th, 50th, and 75th percentiles), respectively.\nTo produce this table of summary statistics with SQL requires a bit more work. To compute the statistics for a particular column, say, sepal_length,\n\ncon.sql(\"select count(sepal_length) as count, \\\n        mean(sepal_length) as mean,  \\\n        stddev(sepal_length) as std,\\\n        min(sepal_length) as min, \\\n        quantile(sepal_length,.25) as q1, \\\n        quantile(sepal_length,.50) as q2, \\\n        quantile(sepal_length,.75) as q3, \\\n        max(sepal_length) as max from iris\")\n\n┌───────┬───────────────────┬────────────────────┬────────┬────────┬────────┬────────┬────────┐\n│ count │       mean        │        std         │  min   │   q1   │   q2   │   q3   │  max   │\n│ int64 │      double       │       double       │ double │ double │ double │ double │ double │\n├───────┼───────────────────┼────────────────────┼────────┼────────┼────────┼────────┼────────┤\n│   150 │ 5.843333333333335 │ 0.8280661279778637 │    4.3 │    5.1 │    5.8 │    6.4 │    7.9 │\n└───────┴───────────────────┴────────────────────┴────────┴────────┴────────┴────────┴────────┘\n\n\nAnd now we have to repeat this for other columns. In this case, SQL is much more verbose and clunky compared to the simple describe() call. However, the power of SQL becomes clear when you further refine the analysis. Suppose you want to compute the previous result separately for each species in the data set. This is easily done by adding a GROUP BY clause to the SQL statement (group by species) and listing species in the SELECT:\n\ncon.sql(\"select species, count(sepal_length) as count, \\\n        mean(sepal_length) as mean,  \\\n        stddev(sepal_length) as std,\\\n        min(sepal_length) as min, \\\n        quantile(sepal_length,.25) as q1, \\\n        quantile(sepal_length,.50) as q2, \\\n        quantile(sepal_length,.75) as q3, \\\n        max(sepal_length) as max from iris group by species\")\n\n┌────────────┬───────┬───────────────────┬────────────────────┬────────┬────────┬────────┬────────┬────────┐\n│  species   │ count │       mean        │        std         │  min   │   q1   │   q2   │   q3   │  max   │\n│  varchar   │ int64 │      double       │       double       │ double │ double │ double │ double │ double │\n├────────────┼───────┼───────────────────┼────────────────────┼────────┼────────┼────────┼────────┼────────┤\n│ virginica  │    50 │ 6.587999999999998 │  0.635879593274432 │    4.9 │    6.2 │    6.5 │    6.9 │    7.9 │\n│ setosa     │    50 │ 5.005999999999999 │ 0.3524896872134513 │    4.3 │    4.8 │    5.0 │    5.2 │    5.8 │\n│ versicolor │    50 │             5.936 │ 0.5161711470638635 │    4.9 │    5.6 │    5.9 │    6.3 │    7.0 │\n└────────────┴───────┴───────────────────┴────────────────────┴────────┴────────┴────────┴────────┴────────┘\n\n\nInterpretation: The average sepal length increases from I. setosa (5.006) to I. versicolor (5.936) to I. virginica (6.588). The variability of the sepal length measurements also increased in that order. You can find I. versicolor plants with large sepal length for that species that exceed the sepal length of the I. virginica specimens at the lower end of the spectrum. That can be gleaned from the proximity of the sample means and the size of the standard deviation. It is confirmed by comparing max and min of the species.",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Summarization</span>"
    ]
  },
  {
    "objectID": "data/summarization.html#group-by-aggregation",
    "href": "data/summarization.html#group-by-aggregation",
    "title": "7  Data Summarization",
    "section": "7.3 Group-by Aggregation",
    "text": "7.3 Group-by Aggregation\nA group-by analysis computes analytic results separately for each group of observations. It is a powerful tool to gain insight on how data varies within a group and between groups. Groups are often defined by the unique values of qualitative variables, but they can also be constructed by, for example, binning real-valued variables. In the Iris example, the obvious grouping variable is species.\nGoing from an ungrouped to a grouped analysis is easy with SQL—just add a GROUP BY clause. With Pandas and Polars we need to do a bit more work. Suppose we want to compute the min, max, mean, and standard deviation for the petal_width of I. setosa and I. virginica. This requires filtering records (excluding I. versicolor) and calculating summary statistics separately for the two remaining species.\nThe syntax for aggregations with group-by is different in Pandas and Polars. With Pandas, you can use a statement such as this:\n\npd_df[pd_df[\"species\"] != \"versicolor\"][[\"species\",\"sepal_width\"]] \\\n    .groupby(\"species\").agg(['min','max','mean','std'])\n\n\n\n\n\n\n\n\n\nsepal_width\n\n\n\nmin\nmax\nmean\nstd\n\n\nspecies\n\n\n\n\n\n\n\n\nsetosa\n2.3\n4.4\n3.418\n0.381024\n\n\nvirginica\n2.2\n3.8\n2.974\n0.322497\n\n\n\n\n\n\n\n\nThe first set of brackets applies a filter (selects rows), the second set of brackets selects the species and sepal_width columns. The resulting DataFrame is then grouped by species and the groups are aggregated (summarized); four statistics are requested in the aggregation.\n\nEager and lazy evaluation\nThe execution model in Pandas is called an eager evaluation. Polars can support eager and lazy evaluation.\n\n\nDefinition: Eager and Lazy Evaluation\n\n\nIn an eager evaluation model, operations are executed as soon as they are encountered. In a lazy evaluation model, the execution of operations is delayed until their results are needed. For example, lazy evaluation of a file read delays the retrieval of data until records need to be processed.\n\n\nLazy evaluation has many advantages:\n\nThe overall query can be optimized.\nFilters (called predicates in data processing parlance) can be pushed down as soon as possible, eliminating loading data into memory for subsequent steps.\nColumn selections (called projections) can also be done early to reduce the amount of data passed to the following step.\nYou can write a lazily evaluated query and analyze it prior to execution; restructuring the query can lead to further optimization.\n\nYou can see the full list of lazy query optimization in Polars here.\nThe Polars syntax to execute the group-by aggregation eagerly is:\n\npd_dl.filter(pl.col(\"species\") != \"versicolor\").group_by(\"species\").agg(\n    pl.col(\"sepal_width\").min().alias('min'),\n    pl.col(\"sepal_width\").max().alias('max'),\n    pl.col(\"sepal_width\").mean().alias('mean'),\n    pl.col(\"sepal_width\").std().alias('std')\n    )\n\n\n\nshape: (2, 5)\n\n\n\nspecies\nmin\nmax\nmean\nstd\n\n\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n\"virginica\"\n2.2\n3.8\n2.974\n0.322497\n\n\n\"setosa\"\n2.3\n4.4\n3.418\n0.381024\n\n\n\n\n\n\n\nThe Polars syntax should be familiar to users of the dplyr package in R. Operations on the DataFrame are piped from step to step. .filter() selects rows, .group_by() groups the filtered data, .agg() aggregates the resulting rows. Since variable sepal_width is used in multiple aggregates, we are adding an .alias() to give the calculated statistic a unique name in the output.\nThe following statements perform lazy evaluation for the same query. The result of the operation is what Polars calls a LazyFrame, rather than a DataFrame. A LazyFrame is a promise on computation. The promise is fulfilled—the query is executed—with q.collect(). Notice that we call the .lazy() method on the DataFrame pd_dl. Because of this, the object q is a LazyFrame. If we had not specified .lazy(), q would be a DataFrame.\n\nq = (\n    pd_dl.lazy()\n    .filter(pl.col(\"species\") != \"versicolor\")\n    .group_by(\"species\")\n    .agg(\n        pl.col(\"sepal_width\").min().alias('min'),\n        pl.col(\"sepal_width\").min().alias('max'),\n        pl.col(\"sepal_width\").mean().alias('mean'),\n        pl.col(\"sepal_width\").min().alias('std'),\n \n        )\n)\n\nq.collect()\n\n\n\nshape: (2, 5)\n\n\n\nspecies\nmin\nmax\nmean\nstd\n\n\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n\"setosa\"\n2.3\n2.3\n3.418\n2.3\n\n\n\"virginica\"\n2.2\n2.2\n2.974\n2.2\n\n\n\n\n\n\n\nThe results of eager and lazy evaluation are identical. The performance and memory requirements can be different, especially for large data sets. Fortunately, the eager API in Polars calls the lazy API under the covers in many cases and collects the results immediately.\nYou can see the optimized query with\n\nq.explain(optimized=True)\n\n'AGGREGATE\\n\\t[col(\"sepal_width\").min().alias(\"min\"), col(\"sepal_width\").min().alias(\"max\"), col(\"sepal_width\").mean().alias(\"mean\"), col(\"sepal_width\").min().alias(\"std\")] BY [col(\"species\")] FROM\\n  DF [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]; PROJECT 2/5 COLUMNS; SELECTION: \"[(col(\\\\\"species\\\\\")) != (String(versicolor))]\"'\n\n\n\n\nStreaming execution\nAnother advantage of lazy evaluation in Polars is the support for streaming execution—where possible. Rather than loading the selected rows and columns into memory, Polars processes the data in batches allowing you to process large data volumes that exceed the memory capacity. To invoke streaming on a LazyFrame, simply add streaming=True to the collection:\n\nq.collect(streaming=True)\n\n\n\nshape: (2, 5)\n\n\n\nspecies\nmin\nmax\nmean\nstd\n\n\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n\"virginica\"\n2.2\n2.2\n2.974\n2.2\n\n\n\"setosa\"\n2.3\n2.3\n3.418\n2.3\n\n\n\n\n\n\n\nStreaming capabilities in Polars are still under development and not supported for all operations. However, the operations for which streaming is supported include many of the important data manipulations, including group-by aggregation:\n\nfilter, select,\nslice, head, tail\nwith_columns\ngroup_by\njoin\nsort\nscan_csv, scan_parquet\n\nIn the following example we are calculating the sample mean and standard deviation for three variables (num_7, num_8, num_9) grouped by variable cat_1 for a data set with 30 million rows and 18 columns, stored in a Parquet file.\n\nnums = ['num_7','num_8', 'num_9']\n\nq3 = (\n    pl.scan_parquet('../datasets/Parquet/train.parquet')\n    .group_by('cat_1')\n    .agg(\n        pl.col(nums).mean().suffix('_mean'),\n        pl.col(nums).std().suffix('_std'),\n    )\n)\nq3.collect(streaming=True)\n\n/var/folders/ff/6m670wks1776ck_gbfxynxrc0000gq/T/ipykernel_16470/2211535918.py:7: DeprecationWarning:\n\n`suffix` is deprecated. It has been moved to `name.suffix`.\n\n/var/folders/ff/6m670wks1776ck_gbfxynxrc0000gq/T/ipykernel_16470/2211535918.py:8: DeprecationWarning:\n\n`suffix` is deprecated. It has been moved to `name.suffix`.\n\n\n\n\n\nshape: (4, 7)\n\n\n\ncat_1\nnum_7_mean\nnum_8_mean\nnum_9_mean\nnum_7_std\nnum_8_std\nnum_9_std\n\n\ni64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n3\n0.499942\n0.499939\n0.500096\n0.288645\n0.288696\n0.288626\n\n\n4\n0.499922\n0.500028\n0.500063\n0.288682\n0.288736\n0.288723\n\n\n1\n0.499961\n0.499809\n0.499931\n0.288738\n0.288651\n0.2887\n\n\n2\n0.500114\n0.500171\n0.499994\n0.288637\n0.288664\n0.288766\n\n\n\n\n\n\n\nThe scan_parquet() function reads lazily from the file train.parquet, .group_by() and .agg() functions request the statistics for a list of columns. The .suffix() method adds a string to the variable names to distinguish the results in the output.\nThe streaming Polars code executes on my machine in 0.3 seconds. The Pandas equivalent produces identical results in about 4 seconds—an order of magnitude slower with much higher memory consumption.\nYou can use lazy execution with data stored in other formats. Suppose we want to read data from a DuckDB database and analyze it lazily with minibatch streaming. The following statements do exactly that for the Iris data stored in DuckDB\n\nqduck = (\n    con.sql('select * from iris').pl().lazy()\n    .select([\"species\", \"sepal_width\"])\n    .filter(pl.col(\"species\") != \"versicolor\")\n    .group_by(\"species\")\n    .agg(\n        pl.col(\"sepal_width\").min().alias('min'),\n        pl.col(\"sepal_width\").min().alias('max'),\n        pl.col(\"sepal_width\").mean().alias('mean'),\n        pl.col(\"sepal_width\").min().alias('std'),\n \n        )\n)\nqduck.collect(streaming=True)\n\n\n\nshape: (2, 5)\n\n\n\nspecies\nmin\nmax\nmean\nstd\n\n\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n\"setosa\"\n2.3\n2.3\n3.418\n2.3\n\n\n\"virginica\"\n2.2\n2.2\n2.974\n2.2\n\n\n\n\n\n\n\n\n\n\nFigure 7.1: Iris versicolor flowers",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Summarization</span>"
    ]
  },
  {
    "objectID": "data/visualization.html",
    "href": "data/visualization.html",
    "title": "8  Data Visualization",
    "section": "",
    "text": "8.1 Visual Learning\nIt is often said that “a picture is worth more than a thousand words”. That does not mean that you should always choose a data visualization over a tabular data summary or over a descriptive statement. To choose the best medium for communicating information, you need to understand what medium works best for the audience. Tabular and graphical displays complement each other, or in the words of Gelman and Unwin (2013):\nAccording to the VARK theory, there are four predominant learning styles, Visual, Auditory (aural), Read/Write, and Kinesthetic (tactile). Kinesthetic learners learn best through experience and interactions—they are hands-on learners. Read/Write learners prefer text as input and output; they interact with material through reading and writing. Aural learners prefer to retain information via listening—lectures, podcasts, discussions, talking out loud. Visual learners, finally, retain information best when it is presented in the forms of graphs, figures, images, charts, photos, videos.\nMost humans are visual learners, one estimate claims 65% of us learn best through visual means. Visuals add speed to communication. If the mode of communication matches the preferred learning style, information is retained more easily, the learning process is more engaging and fun, and the memory created is stronger. As a visual learner you are more likely to recall a graphic than you are a paragraph of text.\nWhy is that?\nThe answer lies in the way in which we process information in our sub-conscious and conscious minds. The sub-conscious is uncontrolled, always on, and effortless—it is your brain on autopilot. The conscious mind is where the hard work happens, it requires effort to engage.\nWe can take in much more information through sight than through any other sense. The amount of information processed in the conscious mind is much lower compared to the sub-conscious mind for any of the senses, as shown in the following figure. That means the sub-conscious acts as a filter for information, passing to the conscious mind that which needs more in-depth processing and engagement. The combination of processing power and bandwidth is why sight is most suited for understanding data.",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data/visualization.html#visual-learning",
    "href": "data/visualization.html#visual-learning",
    "title": "8  Data Visualization",
    "section": "",
    "text": "a picture may be worth a thousand words, but a picture plus 1000 words is more valuable than two pictures or 2000 words.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.1: Sensory bandwidth of the sub-conscious and the conscious mind. Source.",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data/visualization.html#twenty-apple-trees",
    "href": "data/visualization.html#twenty-apple-trees",
    "title": "8  Data Visualization",
    "section": "8.2 Twenty Apple Trees",
    "text": "8.2 Twenty Apple Trees\nTo describe data, visualization is key in data analytics. Tabular data is how computers process information. It is not how we can best process data. However, do not discount showing numbers in tables. Whether raw data or summarized data, tabular displays have their place. Let’s look at an example.\nTable 8.1 displays the diameters in inches of 20 apples from 2 trees over six measurements periods. The measurements are spaced 2 weeks apart and were collected at the Winchester Agricultural Experiment Station of Virginia Tech. The total data set contains 80 apples from 10 trees.\n\n\n\nTable 8.1: Diameters in inches of twenty apples from two trees over six two-week measurement periods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTree\nApple\nPeriod 1\nPeriod 2\nPeriod 3\nPeriod 4\nPeriod 5\nPeriod 6\n\n\n\n\n1\n1\n2.90\n2.90\n2.90\n2.93\n2.94\n2.94\n\n\n\n2\n2.86\n2.90\n2.93\n2.96\n2.99\n3.01\n\n\n\n3\n2.75\n2.78\n2.80\n2.82\n2.82\n2.84\n\n\n\n4\n2.81\n2.84\n2.88\n2.92\n2.92\n2.95\n\n\n\n5\n2.75\n2.78\n2.80\n2.82\n2.83\n2.90\n\n\n\n6\n2.92\n2.96\n2.96\n3.02\n3.02\n3.04\n\n\n\n7\n3.08\n\n\n\n\n\n\n\n\n8\n3.04\n3.10\n3.11\n3.15\n3.18\n3.21\n\n\n\n9\n2.78\n2.82\n2.83\n2.86\n2.87\n\n\n\n\n10\n2.76\n2.78\n2.82\n2.85\n2.86\n2.87\n\n\n\n11\n2.79\n2.86\n2.88\n2.93\n2.95\n3.98\n\n\n\n12\n2.76\n2.81\n2.82\n2.86\n2.90\n2.90\n\n\n\n\n\n\n\n\n\n\n\n\n2\n1\n2.84\n2.89\n2.92\n2.93\n2.95\n\n\n\n\n2\n2.75\n2.80\n2.82\n2.84\n2.86\n2.86\n\n\n\n3\n2.78\n2.81\n2.84\n2.85\n2.87\n2.90\n\n\n\n4\n2.84\n2.86\n2.86\n\n\n\n\n\n\n5\n2.83\n2.88\n2.89\n2.92\n2.93\n2.93\n\n\n\n6\n2.80\n2.86\n2.89\n2.92\n2.93\n2.95\n\n\n\n7\n2.86\n2.89\n2.92\n2.96\n2.96\n2.99\n\n\n\n8\n2.75\n2.80\n2.83\n2.85\n2.86\n2.88\n\n\n\n\n\n\nIf we were to display the entire data in a table, it would use four times as much space and it would be difficult to comprehend the data—to see what is going on. However, the tabular display is useful in some respects:\n\nThe exact values are shown.\nWe see that there are missing values for some apples. For example, apple #14 on tree 1 has only one diameter measurement at the first occasion. Apple #15 on tree 2 has thee measurements.\nOnce measurements are missing, they remain missing, at least for the apples displayed in the table. That suggests that apples dropped out of the study, maybe they fell to the ground, were eaten or harvested.\nApple identifiers are not unique across the trees. Apples with id 11, 15, and 17 appear on tree #1 and on tree #2. That is important information; if we want to calculate summary statistics for apples, we must also take tree numbers into account. The technical term for this arrangement is that apple ids are nested within the tree ids.\n\nOther aspects of the data that are difficult to ascertain from the table:\n\nApple diameters should not decrease over time. You need to scan every row to check for possible violations; they would suggest measurement errors.\nWhat do the trends over time look like? Are there significant changes in apple diameter over the 12-week period? If so, what is the shape of the trend?\nDo apples grow at similar rates on the different trees?\nWhat does the data from the other trees look like?\nHow many apples were measured on each tree?\n\nWe can see trends much better than we can read trends.\nFigure 8.2 shows a trellis plot of the diameters of all eighty apples over the twelve-week study period. This type of graph is also called a lattice plot or a conditional plot. The display is arranged by one or more variables, and a separate plot is generated for the data associated with values of the conditioning variables. Here, the trellis plot is conditioned on the tree number, producing ten scatter plots of apple diameters for a given tree. The plots are not unrelated, however. They share the same \\(x\\)-axis and \\(y\\)-axis to facilitate comparisons across the plots.\n\n\n\n\n\n\nFigure 8.2: Trellis (lattice) graphic showing the diameters over time for apples on ten trees.\n\n\n\nFrom the trellis plot we can easily see information that is difficult to read from the tabular display:\n\nThe varying number of apples per tree\nThere is a definite trend over time in apple growth and it appears to be linear over time. Every two weeks each apple seems to grow by a steady amount; the amount differs between apples and trees.\nThere is variability between the trees and variability among the apples from the same tree. The apple-to-apple variability is small on tree #2, and it is larger on trees #9 and #10, for example.\nAll eighty apples are shown in a compact display.\nThe smallest diameter seems to be around 2.8 inches. In fact, these apples are a subset of a larger sample of apples, limited to fruit with at least 2.75 inches of diameter at the initial measurement.\nThe measurements are evenly spaced.\n\nThe graph is helpful to see patterns: trends, groupings, variability.\nIt does make consuming some information more difficult. For example, we are losing track of the actual diameter measurements. We see which measurements are larger and smaller (the pattern), but not their actual value. Showing the actual values to two decimal places is not the purpose of the graph. If we want to know the diameter of apple id 4 on tree #7 at time 5, we can query the data to retrieve the exact value.\n\nimport duckdb\nimport polars as pl\ncon = duckdb.connect(database=\"../ads5064.ddb\")\napples = con.sql(\"SELECT * FROM apples\").pl()\napples.filter((pl.col(\"Tree\")==7) & (pl.col(\"appleid\")==4) & (pl.col(\"measurement\")==5))\n\n\n\nshape: (1, 4)\n\n\n\nTree\nappleid\nmeasurement\ndiameter\n\n\ni64\ni64\ni64\nf64\n\n\n\n\n7\n4\n5\n2.92\n\n\n\n\n\n\n\nWe could add labels to the data symbols in the graph, or even replace the circles with labels that show the actual value, but this would make the graph really busy and messy to read.\nWe also have lost information about which data point belongs to which apple. Since diameters grow over time, our brain naturally interpolates the sequence of dots. For the largest measurements on say, tree #7 and tree #10, we are easily convinced that the dots belong to the same apple. We cannot be as confident when data points group together more. And we cannot be absolutely certain that the largest observations on tree #7 and tree #10 belong to the same apple. And without further identifying information, we do not know which apple that is.\nThere are other ways in which the graphic can be enhanced or improved:\n\nAdd trends over time for individual apples and/or trees. That can help identify a model and show the variability between and within trees.\nAdding the word “Tree” to the trellis labels if it is not clear what the numbers in the grey bars refer to. A downside would be that repeating the word “Tree” ten times does not add new information beyond the first cell of the plot.\nVarying the plotting symbols or colors within a cell of a lattice to show which data points belong to the same apple. (Is it better to vary colors or symbols or both?)\nAdd an apple id, maybe in the right margin of each cell, aligned with the last measurement. (Would that work if an apple is not observed at the last measurement?)\n\nThe tableau and the trellis graph display the raw data. We can also choose to work with summaries. Suppose we are interested in understanding the distribution of apple diameters by measurement time. The following statements compute several descriptive statistics from diameters at each measurement occasion.\n\nq = (\n    apples.lazy()\n    .filter(pl.col(\"diameter\") != None)\n    .group_by(\"measurement\")\n    .agg(\n        pl.col(\"diameter\").count().alias('count'),\n        pl.col(\"diameter\").mean().alias('mean'),\n        pl.col(\"diameter\").min().alias('min'),\n        pl.col(\"diameter\").quantile(0.25).alias('q1'),\n        pl.col(\"diameter\").quantile(0.5).alias('median'),\n        pl.col(\"diameter\").quantile(0.75).alias('q3'),\n        pl.col(\"diameter\").max().alias('max')\n        )\n    .sort(\"measurement\")\n)\n\nq.collect()\n\n\n\nshape: (0, 8)\n\n\n\nmeasurement\ncount\nmean\nmin\nq1\nmedian\nq3\nmax\n\n\ni64\nu32\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\n\n\n\n\nThe output shows that the number of observations contributing at each measurement time decreases; this is expected as apples are lost during the study. The location statistics (mean, min, max, median, Q1, and Q3) all increase over time, showing that the average apple grows.\nA visual display that produces the same information and conveys the distributions as well as the trend over time more clearly arranges box plots for the measurement times.\n\n\n\n\n\n\nFigure 8.3: Box plots for apple diameters by measurement time.\n\n\n\nThe grey box in the center of the box plot extends from the first to the third quartile; its width is called the inter-quartile range (IQR = \\(Q_3 - Q_1\\)). The box thus covers the central 50% of the distribution. The median is shown as the mid line of the box. The upper and lower extensions of the box are called the whiskers of the box plot. They extend to the largest and smallest observations, respectively, that are within 1.5 times the IQR from the edge of the box. If there are no dots plotted beyond the whiskers, they fall on the max and min, respectively. “Outliers” are shown as dots above and below the whiskers. In the apple data, they appear only at the upper end of the measurements. These outliers are not incorrect observations, they are simply unusual given the probability distribution. If you collect enough samples from any distribution, you will expect to draw a certain number of values from the tails of the distribution.\nThe box plot does not reveal the exact values of the points plotted, but it allows us to see the pattern in the data more easily than the tabular display of descriptive statistics. Except for outlier information, the tabular display contains the same information as the box plots.\nMany decisions go into creating a good data visualization. What should we pay attention to? How do we choose a good plot type? How much embellishment of the graph is too little or too much.",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data/visualization.html#process-and-best-practices",
    "href": "data/visualization.html#process-and-best-practices",
    "title": "8  Data Visualization",
    "section": "8.3 Process and Best Practices",
    "text": "8.3 Process and Best Practices\nThe data visualization process starts with defining the intention of the visualization:\n\nWhat is the context for the graphic? Are you in exploratory mode to learn about the data or are you in presentation mode to tell the story of the data.\nWhat is the statistical purpose of the visualization? Are you in a classification, regression, or clustering application? Are you describing the distributions and relationships in the data or the result of training a model?\nWhat is the data to be visualized? Is it raw data or summarized data or data derived from a model?\n\nThe next step in the process is to examine candidate visualizations and to select a concept. Libraries of graphics examples come in handy at this step. Here are some sites that show worked graphics examples:\n\nR Graph Gallery: an extensive collection of graphics in R using base R graphing functions and ggplot2.\nPython Graph Gallery: an extensive collection of graphics in Python organized like the R gallery. Also offers tutorials for Matplotlib, Seaborn, Plotly, and Pandas.\nMakeoverMonday: this is a social data visualization project in the UK that posts a new data set every Monday and invites users to visualize the data. Several visualizations are chosen and displayed in the gallery and the blog each week. The site is also a great resource for data sets.\nData Viz Project: a collection of data visualizations to get inspired and find the right visualization for your data. The project is run by Ferdio, an infographic and data viz company in Copenhagen.\nDataviz Inspiration: Hundreds of impactful data visualization projects from the person behind the R Graph Gallery. With code where available.\nInformation is Beautiful: great visualizations of good news, positive trends, and uplifting statistics.\nDatylon’s Inspiration: Datylon is a data visualization platform, this page shows visualizations by category created with Datylon.\n\nThe next step is to implement the chosen design and to perform the Trifecta checkup discussed in more detail below. Finally, it is a good idea to test the visualization with others and to get constructive feedback on what works and what does not. Does the visualization achieve the goals set out in the first step of the process? What attracts the audience, what distracts the audience?\n\nPre-attentive attributes\nPre-attentive attributes are attributes such as length, width, size, shape, etc., that our brain processes almost instantaneously and with little effort.\n\n\n\n\n\n\n\nVisual processing from light entering to comprehension. Source.\n\n\nWhen information (light) enters the retina, it is processed in iconic memory, a short-term buffer and processor that maintains an image of the world and enriches the information. This is where pre-attentive attributes are processed, rapid and massively parallel in the sub-conscious mind. The visual information is then passed on to visual working memory, short term storage that is combined with information form long-term memory for conscious processing.\nEffective data visualizations encode as much possible information through pre-attentive attributes.\n\n\n\n\n\n\nFigure 8.4: Pre-attentive attributes of data visualization. Source.\n\n\n\nTo encode quantitative values, length and position (in 2-dimensional space) are best as they are naturally interpreted as quantitative. Lines of different lengths are more easily interpreted as smaller and larger values than lines of different widths. Shapes are not useful as a pre-attentive attribute to encode quantitative values. Is a circle larger than a square? That takes more conscious processing to answer and slows down comprehension of the visualization.\nOn the other hand, if we want to identify groups of data points that belong together, then shape or color are very effective pre-attentive attributes, as well as proximity and connecting points.\nYou cannot infer actual values from pre-attentive attributes such as length or size, we only get a greater—lesser impression. Most of the times that is sufficient. Information about actual values has to be added through text, labels, grid lines. These are not pre-attentive attributes but learned symbols that require conscious processing. The effort to comprehend a visualization increases with the addition of non-pre-attentive attributes. You should weigh whether the additional processing effort is justified relative to the information gained. A labeled axis requires fewer annotations and mental processing than labeling every data point with its actual value.\nIf your visualization is used in a context where comparisons are required, the choice of attributes and features determines whether comparisons are more accurate or more generic. The most accurate comparisons are possible using 2-dimensional position and length attributes. Color intensity, color hue, area, and volume allow more generic comparisons but are not useful when accuracy is required.\nA good example of the importance of pre-attentive attributes is displaying quantitative information in pie charts versus bar charts.\nThe pie chart in Figure 8.5 displays five values and uses area to compare and color to differentiate. Can you tell from the chart whether foo is larger than bar? How does ipsum compare to bar and lorem?\n\n\n\n\n\n\nFigure 8.5: A pie chart displaying five values.\n\n\n\nSince it is difficult to make accurate comparisons based on area, why not add labels to the chart. While we are at it, we can also dress up the display by using more color and 3-dimensional plotting.\n\n\n\nA 3-dimensional pie chart. Yikes.\n\n\nAdding percentages to the labels allows us to compare the values and conclude which slices of the pie are larger and which slices are smaller. But if we show the percentages, then why use a graphic in the first place? By using labels to show values the chart requires as much cognitive engagement as a table of values:\n\nThe information from the pie chart as a tabular display.\n\n\nCategory\nfoo\nbar\nbaz\nlorem\nipsum\n\n\n\n\nPercentage\n20\n24\n8\n32\n16\n\n\n\nThe more vibrant colors do not add to the comprehension of the data and the 3-dimensional display makes things worse: comparing values based on volume is more difficult than comparing values based on area which in turn is more difficult than comparing values based on length.\nQuantitative values can be encoded as pre-attentive attributes and comparisons are most accurate for lines and 2-dimensional positions. The following horizontal bar chart visualizes the data using pre-attentive attributes.\n\n\n\n\n\n\nFigure 8.6: Bar chart. The values are easily distinguished based on the length (height) of the bars. The categories have been ordered by magnitude. No color is necessary to distinguish the categories, the labels are sufficient.\n\n\n\n\n\nChartjunk\nThe term chartjunk was coined by Edward Tufte in his influential (cult) 1983 book “The Display of Quantitative Information” (Tufte 1983, 2001). Chartjunk are the elements of a data visualization that are not necessary to comprehend the information. Chartjunk distracts from the information the graph wants to convey. Tufte, who taught at Princeton together with John W. Tukey, subscribed to minimalist design: if it does not add anything to the interpretation of the data, do not add it to the chart.\nThe excessive use of colors, heavy grid lines, ornamental shadings, unnecessary color gradients, excessive text, background images, excessive annotations and decorations are examples of chartjunk. Not all graphical elements are chartjunk—you should ask yourself before adding an element to a graphic: is it helpful? Text annotations can be extremely helpful, overdoing it can lead to busy charts that are not intelligible. By adding too many text labels, the label avoidance algorithm of graphing software might place labels in areas of the graph where they are misleading. If grid lines are not necessary for the interpretation of the graphic, leave them off. If grid lines are helpful, add them in a color or with transparency that does not distract from the data in the graph.\nUnfortunately, it is all too easy to add colors, styles, annotations, grid lines, inserts, legends, etc. to graphics. Software makes it easy to overdo it.\nTufte’s war path on chartjunk needs to be moderated for our purposes. The minimalist view that anything that is unnecessary to comprehend the information is junk goes too far. We must keep the purpose of the data visualization in mind. In exploratory mode, you generate lots of graphics and different views of the data to learn about the data, find patterns, and stimulate thought—the audience is you. Naturally, we eschew adding too many elements to graphics, everything is about speed and flexibility—polish takes time. In presentation mode, the data visualization needs to grab attention and open the door for the audience to engage with the data. Annotations, colors, labels, headlines, titles, which would be chartjunk in exploratory mode, have a different role and place in presentation mode. They might not be necessary to comprehend the information but can reduce the cognitive burden the audience members have to expend.\nTufte also introduced the concept of the data-ink ratio: a visualization should maximize the amount of ink it uses on displaying data and minimize the amount of ink used on embellishments and annotations.\nFigure 8.7 is a version of the previous bar chart—this time full of chartjunk and too much ink devoted to things other than data:\n\nThe legend is not necessary, the categories are identified already through the labels.\nThe axis label “Category” is not necessary, it is clear what is being displayed. The vertical orientation of the axis label and the horizontal orientation of the categories is visually distracting.\nVarying the colors of the bars is distracting and does not add new information.\nThe grid lines are intrusive and add too much ink to the plot.\n\n\n\n\n\n\n\nFigure 8.7: A junkified version of the horizontal bar chart from the previous subsection.\n\n\n\n\n\nThe Trifecta checkup\nKaiser Fung, a data visualization expert created a framework to criticize data visualizations. It is recommended that you run your visualizations through this checkup. It rests on three simple investigations:\n\nWhat question are we trying to answer (Q)?\nWhat do the data say (D)?\nWhat does the visualization say (V)?\n\nHopefully, the answers to the tree lines of inquiry are the same. The framework is arranged in a Question—Data—Visualization triangle, in what Fung calls the junk charts trifecta:\n\n\n\n\n\n\nFigure 8.8: The Junk Charts Trifecta Checkup according to Kaiser Fung. Source.\n\n\n\nA good visualization scores on all three dimensions, Q—D—V. It poses an interesting question, uses quality data that are relevant to the question, and visualization techniques that follow best practices and bring out the relevant information (without adding chartjunk). Tufte’s concerns about chartjunk and maximizing the data-ink ratio fall mostly in the V corner of the trifecta. But even the best visualization techniques are useless when applied to bad or irrelevant data or attempting to answer an irrelevant or ill-formed question.\nAn example of a graphic that fails on all three dimensions is discussed by Fung here and shown below. This graphic appeared in the Wall Street Journal.\n\n\n\n\n\n\nFigure 8.9: Citi Bike riders.\n\n\n\nQ-corner: What question does the graph address: how many riders use Citi Bike during a weekday? That is not a very interesting question, unless you are a city planner. Even then, you are more interested in when and where the bikes are used, rather than some overall number. The chart breaks the daily usage down over time. We see that most riders are in the morning and early evening—going to work and leaving work. That too is not very interesting and not at all surprising.\nD-corner: Are the data relevant to answer the question being posed? The data were taken on two days last autumn. What does this represent? Certainly not the average usage over the year. How were those two days selected? Where was the data collected? Randomly throughout the city, only downtown, in certain districts? What was sampled? The days of the months? The riders? The city districts?\nV-corner: Does the graphic answer the question using best practices for data visualization? There is much going wrong here.\n\nThe city background is chartjunk, an unnecessary embellishment that does not add any information.\nSimilarly, the bicycle icon is unnecessary. It might be moderately helpful in clarifying that “Bike” refers to bicycle and not motorbike, but that could be made clear without adding a graphics element.\nThe blue dots and the connecting lines are a real problem. How are the connections between the dots drawn? Are the segments (some curved, some jagged) based on observations taken at those times? If so, the data should be displayed. If not, what justifies connecting the dots in an irregular way?\nThe scale of the data is misleading. If there was a vertical axis with labels, one could clearly see that the dots are not plotted along an even scale. The vertical distance between the points labeled 65 and 166.5 is about 100 units and is greater than the distance between the points labeled 166.5 and 366.5, about 200 units apart. Once we discover this, we can no longer trust the vertical placement of the dots. Instead, we must make mental arithmetic to interpret the data by value. Displaying the data in a table would have had the same effect.\n\n\nFigure 8.10 is a screenshot from a local TV newscast in Virginia; it shows a histogram for the five most active years in terms of number of tornadoes. Does this graphic pass the trifecta checkup?\n\n\n\n\n\n\nFigure 8.10: Tornado frequency reported by local news in Virginia, June 2024.\n\n\n\n\nQ—What question are we trying to answer? Is 2024 an unusual year in terms of tornado activity?\nD—What do the data say? 2024 is among the top-5 years of tornado activity. But we do not know whether this is for the entire U.S., the entire world, or for the state of Virginia. It is probably not the latter, 1,000 reported tornadoes per year in Virginia is a bit much. How far along are we in 2024 when this was reported? 2024 might not be an unusual year compared to the top-5 if the report was issued at the end of tornado season; 2024 might be an outlier if the report was issued early on in the tornado season.\nV—What does the visualization say? there is a lot going on in the visualization. The bars are ordered from high to low, which disrupts the ordering by time. The eye is naturally drawn to the year labels at the bottom of the bars and has to work overtime to make sense of the chronology. When data are presented in a temporal context our brain wants the data arranged by time (Figure 8.11). The asterisk near the number 1,109 for 2024 leads nowhere. We associate it with the text in the bar for that year anyway, so the asterisk is not necessary. The visualization does not tell us what geographic region the numbers belong to. Is this global data or U.S. data?\n\n\ntornadoes &lt;- data.frame(Year=c(2008,2011,2017,2019,2024),\n                        Number=c(1044,1304,930,971,1109))\n\nbp &lt;- barplot(Number ~ Year, data=tornadoes)\ntext(bp,y=1000,cex=0.7,labels=c(\"\",\"\",\"\",\"\",\"Through June 4\"))\n\n\n\n\n\n\n\nFigure 8.11: Tornado frequencies arranged chronologically.\n\n\n\n\n\nThe problem with using a bar chart for select years only is not to give an accurate impression of how far the data points are separated in time. The first two bars are 3 years apart, the next two bars are 6 years apart.\nInfographics are often guilty of adding extraneous information or displaying data in sub-optimal (nonsensical) ways. Below is another example from Junk Charts. The graphic visualizes the 2022 oil production measured in 1,000 barrels per day by country. The data are projected onto a barrel. Countries are grouped by geographic region and by an industry-specific classification into OPEC, non-OPEC, and OPEC+ countries. The geographic regions are delineated on the barrel surface with thick white lines. Thin white lines mark polygons associated with each country. Aggregations by geographic region are shown below the barrel. The industry-specific categorization is displayed as colored rings around the country flag.\nShapes are not a good pre-attentive attribute to display values, and polygons are particularly difficult to comprehend. Presumably, the size of the country polygons is proportional to their oil production—but who knows, there is no way of validating this. The use of polygons increases the cognitive burden to comprehend the visualization.\nThe visualization contains duplicated information:\n\nEach polygon is labeled with the countries’ oil production; this information duplication is required to make sense of the data because the polygon area is difficult to interpret.\nGreater/lesser oil production by country is also displayed through the size of the map inserts and the font (boldness and font size) of the country names.\nThe country information is duplicated unnecessarily. Countries are shown by name and with their flags. Some country names are abbreviated, and this adds extra mental processing to identify the country. If you are not familiar with working with country codes, identifying the oil production for Angola is tricky (AGO).\nThe geographic summaries display totals as labels and graphically by stacking barrel symbols; each barrel corresponds to 1,000 barrels of oil produced per day.\n\n\n\n\n\n\n\nFigure 8.12: A display of 2022 oil production by country; full of chartjunk. Source.",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data/visualization.html#choosing-the-data-visualization",
    "href": "data/visualization.html#choosing-the-data-visualization",
    "title": "8  Data Visualization",
    "section": "8.4 Choosing the Data Visualization",
    "text": "8.4 Choosing the Data Visualization\nAndrew Abela published the Chart Chooser, a great visualization to help select the appropriate data visualization based on the data type and the goal of the visualization (Figure 8.13). An online version with templates for PowerPoint and Excel is available here.\n\n\n\n\n\n\nFigure 8.13: Choosing a good chart type for data visualization. Source.\n\n\n\nTo select a chart type, start in the center of the chooser with the purpose of the visualization. Do you want to compare items or variables? Do you want to see the distribution of one or more variables? Do you want to show the relationship between variables or how totals disaggregate?\nThe next two figures show adaptations of the Chart Chooser for continuous and discrete target variables by Prof. Van Mullekom (Virginia Tech).\n\n\n\n\n\n\nFigure 8.14: Chart chooser for continuous target variable.\n\n\n\n\n\n\n\n\n\nFigure 8.15: Chart chooser for discrete target variable.\n\n\n\nSuppose you wish to display the monthly temperatures in four cities, Chicago, Houston, San Diego, and Death Valley. The goal is to compare the temperature throughout the year between the cities. According to the chart chooser for continuous target variables, a paneled scatterplot or overlaid time series plot could be useful. Since the data are cyclical, we could also consider a cyclical chart type.\nA sub-optimal chart type, possibly inspired by plotting temperature, would be a “heat” map. Heat maps are used to display the values of a target variable across the values of two other variables, using color-type attributes (color gradient, transparency, …) to distinguish values. Here, temperature is the target variable, displayed across city and month.\n\n\n\n\n\n\nFigure 8.16: Heat map of temperatures in four cities.\n\n\n\nYou can think of a heat map as the visualization of a matrix; the row-column grid defines the cells, and the color depends on the values in the cells. When properly executed, the heat map reveals patterns between the variables, such as hot spots. Correlation matrices are good examples for the use of heat maps. Also, when you are plotting large data sets, binning the data and using a heat map can reveal patterns while limiting the amount of memory needed to generate the graph. An example is a residual plot for a model with millions of data points.\nThe problems with using the heat map in this example are:\n\nThere is no specific ordering between Chicago, San Diego, Houston, and Death Valley. The cities on the vertical axis are not arranged from North to South either. Death Valley Junction is further north than San Diego and Houston; Houston is the southern-most city of the four. Heat maps are best used when the vertical and horizontal axis can be interpreted in a greater/lesser sense or when both axis refer to the same categories.\nThe cyclical nature of the year is somewhat lost in the display. January connects to December in the same way that it connects to February.\nColors are not a good pre-attention attribute for value comparisons. It is clear that the summer months are hotter in Death Valley than in the other cities, but how much hotter?\nA lot of ink is spent on coloring the squares.\n\nA simpler—and more informative—display of the same data is shown below. A line chart of temperature by month, separate for each location. The differences between the cities are easier to see. The cyclical nature of the data is hinted at through the \\(x\\)-axis label—it begins and ends with January. The grid lines help to identify the actual temperature values.\n\n\n\n\n\n\nFigure 8.17: Line chart for temperatures in four cities.\n\n\n\nData visualization is only one method of information visualization. The interactive periodic table of visualization methods, presents visualization of data, methods, information, strategy, metaphors, and concepts, in the form of a periodic table. Hover over an element to see an example of the visualization method.",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data/visualization.html#data-visualization-with-python",
    "href": "data/visualization.html#data-visualization-with-python",
    "title": "8  Data Visualization",
    "section": "8.5 Data Visualization with Python",
    "text": "8.5 Data Visualization with Python\nA large number of Python tools are available for data visualization. You can find the open-source software tools at PyViz.org. From this page of all OSS Python visualization tools on PyViz you see that many of them are built on the same backends, primarily matplotlib, bokeh, and plotly.\n\nMatplotlib\nThe matplotlib library was one of the first Python visualization libraries and is built on NumPy arrays and the SciPy stack. It pre-dates Pandas and was originally conceived as a Python alternative for MATLAB users; that explains the name and why it has a MATLAB-style API. However, it also has an object-oriented API that is used for complex visualizations.\nWhile you can do anything in matplotlib, it does require a lot of boilerplate code for complex graphic; other libraries are providing higher-level APIs to speed up the creation of good data visualizations. Packages such as seaborn are built on matplotlib, so the general vernacular and layout of a seaborn chart is the same as for matplotlib. You can find the extensive matplotlib documentation here.\n\n\nSeaborn\nThe seaborn library has a higher-level API built on top of matplotlib and is deeply integrated with Pandas, remedying two of the frequent complaints about matplotlib. Seaborn alone will get you far, but code often calls matplotlib functions. A typical preamble in Python modules using seaborn is thus something like this:\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nimport seaborn.objects as so\n\nSeaborn categorizes plotting functions into figure-level and axes-level functions. In matplotlib vernacular, axes-level functions plot data onto a matplotlib.pyplot.Axes object. Figure-level functions such as relplot(), displot(), and catplot(), interact with matplotlib through the seaborn FacetGrid object.\n\n\n\n\n\n\nFigure 8.18: Seaborn plotting function overview. Source.\n\n\n\nThe figure-level functions, e.g, displot(), provide an interface to its axes-level functions, e.g., histplot(), and each figure-level module has a default axes-level function (histplot() in the displot() module).\n\nimport seaborn as sns\nimport seaborn.objects as so\n\npenguins = sns.load_dataset(\"penguins\")\nsns.histplot(data=penguins, x=\"flipper_length_mm\", hue=\"species\", multiple=\"stack\")\n\n\n\n\n\n\n\n\nThe figure-level histogram is created by calling the displot() function. You can explicitly ask for histograms with kind=”hist”, or let the function default to producing histograms.\n\nsns.displot(data=penguins, x=\"flipper_length_mm\", hue=\"species\", multiple=\"stack\",kind=\"hist\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA side-effect of using a figure-level function is that the figure owns the canvas. The legend is placed outside of the chart of the figure-level function and inside the chart of the axes-level function.\nOne advantage of figure-level functions is that they can create subplots easily. Removing multiple=\"stack\" produces:\n\nsns.displot(data=penguins, x=\"flipper_length_mm\", hue=\"species\", col=\"species\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemoving multiple=\"stack\" from the axes-level chart produces three overlaid histograms that are difficult to interpret:\n\nsns.histplot(data=penguins, x=\"flipper_length_mm\", hue=\"species\")\n\n\n\n\n\n\n\n\n\n\nPlotly\nPlotly is a Python library for interactive graphics, based on the d3.js JavaScript library. The makers of plotly also have a commercial analytic platform, Dash, but plotly is open source and free to use.\nTo install plotly, assuming you are using pip to manage Python packages, simply run\npip install plotly\nOn my system I also had to\npip install --upgrade nbformat\nand restart VSCode after the upgrade. You will know that this step is necessary when fig.show() throws an error about requiring a more recent version of nbformat than is installed.\nThe plotly library has two APIs, plotly graph objects and plotly express. The express API allows you to generate interactive graphics quickly with minimal code.\nThe southern_oscillation table contains monthly measurements of the southern oscillation index (SOI) from 1951 until today. The SOI is a standardized index based on sea-level pressures between Tahiti and Darwin, Australia. Although the two locations are nearly 5,000 miles apart, that pressure difference corresponds well to changes in ocean temperatures and coincides with El Niño and La Niña weather patterns. Prolonged periods of negative SOI values coincide with abnormally warm ocean waters typical of El Niño. Prolonged positive SOI values correspond to La Niña periods.\nThe following statements load the SOI data from the DuckDB database into a Pandas DataFrame and use the express API of plotly to produce a box plot of SOI values for each year.\n\nimport pandas as pd\nimport plotly.express as px\n\nso = con.sql(\"SELECT * FROM southern_oscillation\").df()\nfig = px.box(x=so[\"year\"],y=so[\"soi\"])\nfig.show() \n\n                        \n                                            \n\n\nThere is a lot happening with just one line of code. The graphic produced by fig.show() is interactive. Hovering over a box reveals the statistics from which the box was constructed. The buttons near the top of the graphic enable you to zoom in and out of the graphic, pan the view, and export it as a .png file.\nThe graph object API of plotly is more detailed, requiring a bit more code, but giving more control. With this API you initiate a visualization with go.Figure(), and update it with update_layout(). The following statements recreate the series of box plots above using plotly graph objects.\n\nimport plotly.graph_objects as go\n\nfig = go.Figure(data=[go.Box(x=so[\"year\"],y=so[\"soi\"])])\nfig.update_layout(title=\"Box Plots of SOI by Year\",\n                  xaxis_title=\"Year\",\n                  yaxis_title=\"SOI\")\n\n                        \n                                            \n\nfig.show()\n\n                        \n                                            \n\n\nNext is a neat visualization that you do not see every day. A parallel coordinate plot represents each row of a data frame as a line that connects the values of the observation across multiple variables. The following statements produce this plot across the sepal and petal measurements of the Iris data. The three species are identified in the plot through colors, which requires a numeric value. Species 1 corresponds to I. setosa, species 2 corresponds to I. versicolor, and species 3 to I. virginica.\n\nfrom functools import reduce\niris = con.sql(\"SELECT * FROM iris\").df()\n\n# recode species as numeric so it can be used as a value for color\nunique_list = reduce(lambda l, x: l + [x] if x not in l else l, iris[\"species\"], [])\nres = [unique_list.index(i) for i in iris[\"species\"]] \ncolors = [x + 1 for x in res]\n\nfig = px.parallel_coordinates(\n    iris, \n    color=colors, \n    labels={\"color\"       : \"Species\",\n            \"sepal_width\" : \"Sepal Width\", \n            \"sepal_length\": \"Sepal Length\", \n            \"petal_width\" : \"Petal Width\", \n            \"petal_length\": \"Petal Length\", },\n    color_continuous_scale=px.colors.diverging.Tealrose,\n    color_continuous_midpoint=2)\n\nfig.update_layout(coloraxis_showscale=False)\n\n                        \n                                            \n\nfig.show()\n\n                        \n                                            \n\n\nThe parallel coordinates plot shows that the petal measurements for I. setosa are smaller than for the other species and that I. setosa has fairly wide sepals compared to the other species. If you wish to classify iris species based on flower measurements, petal length and petal width seem like excellent candidates. Sepal measurements, on the other hand, are not as differentiating between the species.\n\n\nVega-Altair\nVega-Altair is a declarative visualization package for Python and is built on the Vega-Lite grammar. The key concept is to declare links between data columns and visual encoding channels such as the axes and colors. The library attempts to handle a lot of things automatically, for example, deciding chart types based on column types in data frames. The following image is from the Vega-Altair documentation:\n\nimport altair as alt\n\nfrom vega_datasets import data\ncars = data.cars()\n\nalt.Chart(cars).mark_point().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    color='Origin',\n).interactive()\n\n\n\n\n  \n  \n  \n  \n\n\n  \n  \n\n\n\n\nalt.Chart(cars) starts the visualization. mark_point() instructs to display points encoded as follows: Horsepower on the \\(x\\)-axis, Miles_per_Gallon on the \\(y\\)-axis and the color of the points associated with the Origin variable.\n\n\n\nPandas vs Polars\nMatplotlib, Seaborn, Plotly, ggplot, and Altair (since v5+) can work with Polars DataFrames out-of-the-box. If you are running into problems passing a Polars DataFrame to a visualization routine that works fine with a Pandas DataFrame you can always convert using the .to_pandas() function. For example, the parallel coordinates plot in plotly express works with a Pandas DataFrame but generates an AttributeError with a Polars DataFrame. Using the .to_pandas() method took care of the problem.\n\niris = con.sql(\"SELECT * FROM iris\").pl()\n\nfig = px.parallel_coordinates(\n    iris.to_pandas(), \n    color_continuous_scale=px.colors.diverging.Tealrose,\n    color_continuous_midpoint=2)\n\nfig.update_layout(coloraxis_showscale=False)\n\n                        \n                                            \n\nfig.show()\n\n                        \n                                            \n\n\n\n\nGrammar of Graphics (ggplot)\nThe grammar of graphics was described by statistician Leland Wilkinson and conceptualizes data visualization as a series of layers, in an analogy with linguistic grammar. Just like a sentence has subject and predicates, a scientific graph has parts.\nThe grammar of graphics is helpful because we associate data visualization not by the name of this plot or that chart type, but by a series of elements, depicted as layers.\nEach graphic consists of at least the following layers:\n\nThe data itself\nThe mappings from data attributes to perceptible qualities\nThe geometrical objects that represent the data\n\nIn addition, we might apply statistical transformations of the data, must place all objects in a 2-dimensional space and decide on presentation elements such as fonts and colors. And if the data consist of multiple groups, we need a faceting specification to organize the graphic or the page in multiple groups.\nFigure 8.19 shows the layered representation of the grammar of graphics.\n\n\n\n\n\n\nFigure 8.19: The grammar of graphics; a layered approach to building data visualizations.\n\n\n\nThinking about data visualization in these terms is helpful because we get away from thinking about pie charts and box plots and line charts, and instead about how to organize the basic elements of a visualization.\nR programmers are familiar with the grammar of graphics from the ggplot2() package. The grammar of graphics paradigm is implemented in Python in the plotnine library.\nThe following statements generate a data visualization from data frame df (layer 1). This data frame contains data from 196 observations of the optic nerve head from patients with and without glaucoma. The aes() function defines the aesthetics layer, associating variable eag with the \\(x\\)-axis and specifying variable Glaucoma as a grouping variable (eag is the global effective area of the optic nerve head measured on a confocal laser image). The geometries and statistics layers are added to the previous layers with the geom_density() function, requesting a kernel density plot. The scale_x_log10() function modifies the data-to-aesthetics mapping by applying a log10 scale to the \\(x\\)-axis. The result is a grouped density plot on the log10 scale.\n\nfrom plotnine import ggplot, aes, geom_density, scale_x_log10   \n\ndf = con.sql(\"SELECT eag, Glaucoma from glaucoma\").df()\n\n(ggplot(df, aes(x=\"eag\", group=\"factor(Glaucoma)\", fill=\"factor(Glaucoma)\"))\n    + geom_density() \n    + scale_x_log10()\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\nggplot works out of the box with Polars DataFrames:\n\nglauc_pl = con.sql(\"SELECT eag, Glaucoma from glaucoma\").pl()\n\nggplot(glauc_pl) + aes(x=\"eag\", group=\"factor(Glaucoma)\", fill=\"factor(Glaucoma)\") \\\n    + geom_density() + scale_x_log10()\n\n&lt;Figure Size: (640 x 480)&gt;",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data/visualization.html#snafu-or-misinformation",
    "href": "data/visualization.html#snafu-or-misinformation",
    "title": "8  Data Visualization",
    "section": "8.6 Snafu or Misinformation?",
    "text": "8.6 Snafu or Misinformation?\nData visualizations are powerful tools, they allow the author to focus our attention on information. In choosing a visualization type, filling it with data, and annotating it, you exercise control over what is displayed and how it is interpreted. In 1954, Darrell Huff published one of the most widely sold statistics text, “How to Lie with Statistics” (Huff 1954). Huff covered such topics as how to introduce bias through sampling, how to cherry-pick statistics (“the well-chosen average”), how to misinterpret correlation for causation, and how to distort with statistical graphics. He said\n\nMany a statistic is false on its face. It gets by only because the magic of numbers brings about a suspension of common sense.\n\nExamples of poorly designed graphics and misleading graphics abound. The reasons might be malfeasance, chicanery, disingenuity, or incompetence.\n\n\nExample: Covid-19 Statistics\n\n\nThe following graphic aired on a local TV channel in North Carolina on April 5, 2020. It was the beginning of the Covid-19 pandemic and audiences were keen to hear about the local case counts. Is there anything wrong with the visualization?\n\n\n\n\n\n\n\nCovid-19 cases per day as reported on April 5, 2020 by a local TV station.\n\n\nThe number of daily cases has been more or less steadily increasing since March 18 (33 cases) and two weeks later stands at 376 cases per day.\nThe placement of the bubbles seems odd. The first bubble, 33 cases, seem further away from the horizontal grid line than, say, the bubble for 112 cases on March 21 is distant from the grid line at 100 cases. Maybe it is the angle at which the graph is viewed, but 112 (March 21) and 116 (March 22) should be about 1/2 way between 100 and 130, they are drawn closer to the line at 100.\nSpending a bit more time with the \\(y\\)-axis you notice that the grid lines are evenly spaced, but that the reference labels are not equi-distant. The differences between the grid labels are 30, 30, 10, 30, 30, 30, 50, 10, 50, 50, and 50 units. Why would someone do this?\n\n\nIn a blog entitled “How to Spot Visualization Lies”, Nathan Yau gives numerous examples how chart construction can mislead. For example, a common device to exaggerate differences between groups is to draw bar charts with a baseline different than zero. The height of the bar is the information conveyed by the bar chart so bars should always start at zero (Figure 8.20, data from NOAA).\n\n\n\n\n\n\n\n\nFigure 8.20: Three-month (February–April) tornado occurrence in the U.S. from 1950–2023.\n\n\n\n\n\nWhen the baseline of the bar chart is changed, the length of the bars represent the difference from the baseline and the data need to be presented as such. The coloring of the bars in Figure 8.21 draws extra attention to the fact that we are looking at deviations from a baseline.\n\n\n\n\n\n\n\n\nFigure 8.21: Three-month (February–April) tornado occurrence in the U.S. from 1950–2023 compared to 1980.\n\n\n\n\n\nStatistics that are false on their face are comparisons that are based on absolute numbers but should be done on a relative scale. The absolute tornado numbers are comparable in Figure 8.20 because they relate to time intervals of the same length, February–April in each year. Someone could raise an objection here, because leap years have an extra day in February compared to non-leap years, so 1/4 of the bars have a basis of 90 days and 3/4 of the bars have a basis of 89 days. OK, sue me. The comparisons between the bars is still pretty darn fair.\nHowever, if you compare, say, crime statistics, cancer incidences, service subscribers, etc., between regions or states, then large absolute numbers in large regions are probably not a surprise. There being fewer crimes in Blacksburg, VA than in Chicago does not make Blacksburg a safer place to live. It is a safer place because there are fewer crimes per resident or per 10,000 residents in Blacksburg, VA compared to Chicago.\n\n\n\nFigure 8.1: Sensory bandwidth of the sub-conscious and the conscious mind. Source.\nFigure 8.2: Trellis (lattice) graphic showing the diameters over time for apples on ten trees.\nFigure 8.3: Box plots for apple diameters by measurement time.\nVisual processing from light entering to comprehension. Source.\nFigure 8.4: Pre-attentive attributes of data visualization. Source.\nFigure 8.6: Bar chart. The values are easily distinguished based on the length (height) of the bars. The categories have been ordered by magnitude. No color is necessary to distinguish the categories, the labels are sufficient.\nFigure 8.7: A junkified version of the horizontal bar chart from the previous subsection.\nFigure 8.10: Tornado frequency reported by local news in Virginia, June 2024.\nFigure 8.12: A display of 2022 oil production by country; full of chartjunk. Source.\nFigure 8.13: Choosing a good chart type for data visualization. Source.\nFigure 8.14: Chart chooser for continuous target variable.\nFigure 8.15: Chart chooser for discrete target variable.\nFigure 8.18: Seaborn plotting function overview. Source.\nFigure 8.19: The grammar of graphics; a layered approach to building data visualizations.\nCovid-19 cases per day as reported on April 5, 2020 by a local TV station.\nFigure 8.20: Three-month (February–April) tornado occurrence in the U.S. from 1950–2023.\nFigure 8.21: Three-month (February–April) tornado occurrence in the U.S. from 1950–2023 compared to 1980.\n\n\n\nGelman, A., and A. Unwin. 2013. “Infovis and Statistical Graphics: Different Goals, Different Looks.” Journal of Computational and Graphical Statistics 22: 2–28. https://www.tandfonline.com/doi/full/10.1080/10618600.2012.761137.\n\n\nHuff, Darrell. 1954. How to Lie with Statistics. W.W. Norton & Company, New York.\n\n\nTufte, E. 1983. The Visual Display of Quantitative Information. Graphics Press.\n\n\n———. 2001. The Visual Display of Quantitative Information, 2nd Ed. Graphics Press.",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data/sqlbasics.html",
    "href": "data/sqlbasics.html",
    "title": "9  SQL Basics",
    "section": "",
    "text": "9.1 Introduction\nSQL is a declarative programming language, you express through programming statements what you want to happen, not how to control the flow of the program. Other examples of declarative languages are CSS, HTML, XML, and Prolog. To support writing scripts, functions, and procedures, database vendors have added flow-control statements (if-then, loops, switches, etc.), but these are extensions of the database and not of the SQL language.\nIs it pronounced “S-Q-L” or “sequel”? Both are acceptable but if you want to give a nod to its origin, then you would pronounce it ”sequel” like SEQUEL; the name given to the project by its inventors at IBM in the 1970s. The description as structured query language and the acronym SQL came later.\nThe concept of a query in SQL is very general: any interaction with a (relational) database. A query in the narrow sense asks questions about the data in the database or retrieves data. Commands that create tables, alter databases, update or delete records, and so on, are also considered queries.\nSQL has been standardized and revised since 1986. SQL-92, for example, refers to the revision of the standard in 1992. However, SQL implementations are vendor-specific and do not necessarily comply with any standard. The SQL syntax between databases is similar enough that you can move your SQL code from one dialect to another without too much pain. Moving between databases you will find that the devil is not in the details of the SQL language, but in the details of how NULLs are represented, how date-time values are handled, how indexes work, how data are partitioned, whether foreign keys are supported, and so on. Since we use DuckDB in this chapter, we are following the DuckDB SQL syntax.\nSQL can be difficult to master.\nAn interesting aspect (quirk) of domain-specific logic in SQL is three-valued logic. In classical (boolean) logic, there are two logical values, TRUE and FALSE, expressing the relation of a proposition to the truth. In three-valued logic there is a third value, NULL, expressing an unknown state. NULL values are used in databases to represent absent entries and are akin to the concept of the missing value in analytics. NULL values are not zero or empty values. An empty string (“”) in a text column is a known value, a zero-byte string. A NULL value on the other hand states that a string is absent.\nThe examples that follow in this section use DuckDB with the ads5064.ddb database from the command line as follows:\nBecause NULL is a logical state, SQL defines operations on NULL values. Suppose A is TRUE, B is FALSE, and C is NULL, then A or B and A or C both evaluate to TRUE, but A and B is FALSE whereas A and C is also NULL (unknown). To achieve “expected” results when operating on NULL values, special syntax is needed. For example,\nreturns no rows since the comparison x = NULL is never TRUE, its result is always unknown (NULL). If you use this syntax to check for missing values in your data you will conclude that there are none.\nThe correct query uses syntax designed to operate with NULL values:",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data/sqlbasics.html#introduction",
    "href": "data/sqlbasics.html#introduction",
    "title": "9  SQL Basics",
    "section": "",
    "text": "Many programmers are more familiar and comfortable with imperative languages that specify the program flow step by step.\nDeclarative languages are domain specific, the domain for SQL is the manipulation of data in relational systems. This is associated with a lot of special terminology and domain-specific logic. The data scientist thinks in terms of rows and columns, the database engineer thinks in terms of predicates, projections, clauses, constraints, and relations.\nWhile the number of relevant SQL commands is rather small, SQL queries can become astonishingly complex and unwieldy. A single SQL query that accesses multiple tables with correlated (dependent) subqueries, joins, and common table expressions can be hundreds of lines long. If you are not used to reading SQL, a simple subquery such as this one might give you a headache:\n\nSELECT employee_number, name,\n        (SELECT AVG(salary) \n           FROM employees\n          WHERE department = emp.department) AS department_average\n   FROM employees emp\n\nThe point of a declarative language is that the programmer specifies the desired result and leaves it to the particular implementation to achieve it. SQL novices often get frustrated because they know exactly what they want but not how to ask for it using SQL syntax.\nDebugging SQL code is more difficult than debugging code in an imperative language. Error messages can be cryptic. A query that runs successfully is never wrong from the databases point of view, but it is wrong from the coder’s point of view if it does not produce the desired result.\nA query that executes quickly for one database can run slowly in another database. It depends on database architecture, implementation, optimizations, etc. You must learn to ask for the right thing in the right way.\n\n\n\n&gt; duckdb ads5064.ddb\nD\n\nSELECT * FROM landsales WHERE improve = NULL;\n\n┌───────┬─────────┬───────┬───────┬───────────┐\n│ land  │ improve │ total │ sale  │ appraisal │\n│ int64 │  int64  │ int64 │ int64 │  double   │\n├─────────────────────────────────────────────┤\n│                   0 rows                    │\n└─────────────────────────────────────────────┘\n\n\nSELECT * FROM landsales WHERE improve IS NULL;\n┌───────┬─────────┬───────┬────────┬───────────┐\n│ land  │ improve │ total │  sale  │ appraisal │\n│ int64 │  int64  │ int64 │ int64  │  double   │\n├───────┼─────────┼───────┼────────┼───────────┤\n│ 42394 │         │       │ 168000 │           │\n│ 93200 │         │       │ 422000 │           │\n│ 65376 │         │       │ 286500 │           │\n│ 42400 │         │       │        │           │\n└───────┴─────────┴───────┴────────┴───────────┘\n\nSELECT count(*) FROM landsales WHERE improve IS NULL;\n┌──────────────┐\n│ count_star() │\n│    int64     │\n├──────────────┤\n│            4 │\n└──────────────┘",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data/sqlbasics.html#types-of-sql-commands",
    "href": "data/sqlbasics.html#types-of-sql-commands",
    "title": "9  SQL Basics",
    "section": "9.2 Types of SQL Commands",
    "text": "9.2 Types of SQL Commands\nSQL commands are arranged in logical groups. The most important group for the data scientist is the Data Manipulation Language (DML) group, it consists of statements that change the data stored within a database table and traditionally also includes the most important SQL statement, SELECT. The second most important group of SQL commands is the Data Definition Language (DDL), it provides syntax to create and modify database objects.\n\nData Definition Language (DDL): syntax to create and modify database objects. The commands are about the objects themselves, not the data they contain. For example, the CREATE TABLE statement defines the schema of a table and creates it, but it does not populate the table with data. Commands in this group include CREATE, DROP, ALTER.\nData Manipulation Language (DML): syntax to query and modify the contents of tables. The most important SQL command in this group is SELECT. Other commands in this group are INSERT, UPDATE, DELETE. The SELECT statement is sometimes pulled into a separate group, the Data Querying Language (DQL). Since it would be the only statement in that group and because SELECT INTO also modifies rows in a table, the SELECT statement is often included as part of the DML.\nData Control Language (DCL): to control access and manage permissions for users of the database. Commands such as GRANT, DENY, and REVOKE are part of the DCL. As a data scientist you are unlikely to work with DCL commands, they are the domain of the database administrator (DBA). However, you might be charged with setting up and maintaining a database and DCL will help you to change what users (or roles) are allowed to do in the database. The following statements allow user1 to run a SELECT statement on table_foo but disallows updates to the table.\n\nGRANT SELECT ON table_foo TO user1;\nDENY UPDATE ON table_foo TO user1;\n\nNot all databases support DCL. SQLite and DuckDB do not.",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data/sqlbasics.html#most-common-sql-commands",
    "href": "data/sqlbasics.html#most-common-sql-commands",
    "title": "9  SQL Basics",
    "section": "9.3 Most Common SQL Commands",
    "text": "9.3 Most Common SQL Commands\nThe most important SQL commands for a data scientist are the following:\n\nCREATE TABLE\nALTER TABLE\nDROP TABLE\nSELECT\nINSERT INTO\nUPDATE\nDELETE FROM\n\n\nCreating and Modifying a Table\nWe demonstrate these with a small example using DuckDB. You can find the complete documentation for SQL in DuckDB here. A separate section then dives into the SELECT statement, the workhorse for data analysis in relational DBMS.\nFirst, let’s create a small table with information on some cities around the world.\nCREATE TABLE Cities (Country VARCHAR, Name VARCHAR, Year INT, Population INT);\nThe table has four columns named Country, Name, Year, and Population, respectively. The first two are of type VARCHAR (allowing character strings of variable length), the last two are of type INT.\nIf we were to issue the CREATE TABLE statement again, the database would throw an error because the Cities table now exists. To replace an existing table, use CREATE OR REPLACE TABLE:\nCREATE OR REPLACE TABLE Cities (Country    VARCHAR, \n                                Name       VARCHAR, \n                                Year       INT, \n                                Population INT);\n\nA very useful feature is the addition of check constraints. Suppose you want to make sure that only data after the year 2000 are entered for the Year column. You can add a check constraint like this:\nCREATE OR REPLACE TABLE Cities (Country    VARCHAR, \n                                Name       VARCHAR, \n                                Year       INT CHECK (Year &gt;= 2000), \n                                Population INT);\n\nTrying to insert a record that fails the check will result in an error:\nINSERT INTO Cities VALUES ('NL', 'Amsterdam', 1999, 1005);\n\nError: Constraint Error: CHECK constraint failed: Cities\nOnce the table exists in the catalog, we can use the ALTER TABLE statement to change its structure, for example, by renaming, adding, or removing columns, changing data types or defaults.\nALTER TABLE Cities RENAME Name to CityName;\nALTER TABLE Cities ADD Column k INTEGER;\nALTER TABLE Cities RENAME to WorldCities;\n\nThe first statement renames the Name column to CityName, the second statement adds an integer column named k, and the third statement renames the table.\nTo populate the table with data, we insert rows. The following statements add one row at a time and then retrieves the entire contents of the table:\nINSERT INTO WorldCities VALUES ('NL', 'Amsterdam', 2000, 1005, 1);\nINSERT INTO WorldCities VALUES ('NL', 'Amsterdam', 2010, 1065, 3);\nINSERT INTO WorldCities VALUES ('NL', 'Amsterdam', 2020, 1158, 4);\nINSERT INTO WorldCities VALUES ('US', 'Seattle', 2000, 564, 5432);\nINSERT INTO WorldCities VALUES ('US', 'Seattle', 2010, 608, 46);\nINSERT INTO WorldCities VALUES ('US', 'Seattle', 2020, 738, 986);\nINSERT INTO WorldCities VALUES ('US', 'New York City', 2000, 8015, 0);\nINSERT INTO WorldCities VALUES ('US', 'New York City', 2010, 8175, 987);\nINSERT INTO WorldCities VALUES ('US', 'New York City', 2020, 8772, 23);\n\nSELECT * FROM WorldCities;\n\n┌─────────┬───────────────┬───────┬────────────┬───────┐\n│ Country │   CityName    │ Year  │ Population │   k   │\n│ varchar │    varchar    │ int32 │   int32    │ int32 │\n├─────────┼───────────────┼───────┼────────────┼───────┤\n│ NL      │ Amsterdam     │  2000 │       1005 │     1 │\n│ NL      │ Amsterdam     │  2010 │       1065 │     3 │\n│ NL      │ Amsterdam     │  2020 │       1158 │     4 │\n│ US      │ Seattle       │  2000 │        564 │  5432 │\n│ US      │ Seattle       │  2010 │        608 │    46 │\n│ US      │ Seattle       │  2020 │        738 │   986 │\n│ US      │ New York City │  2000 │       8015 │     0 │\n│ US      │ New York City │  2010 │       8175 │   987 │\n│ US      │ New York City │  2020 │       8772 │    23 │\n└─────────┴───────────────┴───────┴────────────┴───────┘ \n\nWe won’t need the column k anymore, so we can drop it with\nALTER TABLE WorldCities DROP COLUMN k;\n\n\nPivoting a Table\nA pivot table is a summary for data that contains categorical variables. If the aggregation is a simple count, the pivot table is also called a contingency table. These tables are useful to summarize data by categories and to see trends. DuckDB implements the SQL PIVOT statement; the basic syntax is\nPIVOT ⟨dataset⟩ \n  ON ⟨columns⟩\n  USING ⟨values⟩ \n  GROUP BY ⟨rows⟩  \n  ORDER BY ⟨columns_with_order_directions⟩\n  LIMIT ⟨number_of_rows⟩;\n\nThe ON clause lists the columns on which to pivot the table. In the WorldCities example we might want to pivot on Year to retrieve the average population for each year:\n PIVOT WorldCities on YEAR USING mean(population);\n\n┌─────────┬───────────────┬────────┬────────┬────────┐\n│ Country │   CityName    │  2000  │  2010  │  2020  │\n│ varchar │    varchar    │ double │ double │ double │\n├─────────┼───────────────┼────────┼────────┼────────┤\n│ US      │ New York City │ 8015.0 │ 8175.0 │ 8772.0 │\n│ NL      │ Amsterdam     │ 1005.0 │ 1065.0 │ 1158.0 │\n│ US      │ Seattle       │  564.0 │  608.0 │  738.0 │\n└─────────┴───────────────┴────────┴────────┴────────┘ \n\nThe USING clause determines how to aggregate the values that are split into columns. If it is not provided, the PIVOT statement defaults to count(*) aggregation.\nThe GROUP BY clause allows you to further aggregate the results and the ORDER BY statement affects how the output is displayed:\nPIVOT WorldCities on YEAR\n      USING mean(population) \n      GROUP BY Country \n      ORDER BY Country DESC;\n\n┌─────────┬────────┬────────┬────────┐\n│ Country │  2000  │  2010  │  2020  │\n│ varchar │ double │ double │ double │\n├─────────┼────────┼────────┼────────┤\n│ US      │ 4289.5 │ 4391.5 │ 4755.0 │\n│ NL      │ 1005.0 │ 1065.0 │ 1158.0 │\n└─────────┴────────┴────────┴────────┘ \n\nTo obtain multiple aggregations, provide a list in the USING clause:\nPIVOT WorldCities on YEAR\n      USING mean(population) as mn,\n            max(population) as mx\n      GROUP BY Country \n      ORDER BY Country DESC;\n\n┌─────────┬─────────┬─────────┬─────────┬─────────┬─────────┬─────────┐\n│ Country │ 2000_mn │ 2000_mx │ 2010_mn │ 2010_mx │ 2020_mn │ 2020_mx │\n│ varchar │ double  │  int32  │ double  │  int32  │ double  │  int32  │\n├─────────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n│ US      │  4289.5 │    8015 │  4391.5 │    8175 │  4755.0 │    8772 │\n│ NL      │  1005.0 │    1005 │  1065.0 │    1065 │  1158.0 │    1158 │\n└─────────┴─────────┴─────────┴─────────┴─────────┴─────────┴─────────┘ \n\nYou might want to combine creation of a table and populating it with data into a single step. For example, you might want to create a database table from the contents of a CSV file. That is accomplished by adding an AS SELECT clause to the CREATE TABLE statement:\nCREATE OR REPLACE TABLE iris AS SELECT * FROM \"../datasets/iris.csv”;\nYou can use just FROM as a shorthand for SELECT * FROM:\nCREATE OR REPLACE TABLE iris AS FROM \"../dataswts/iris.csv”;\nThis shorthand works in general in DuckDB:\nFROM WorldCities WHERE Country='NL';\n\n┌─────────┬───────────┬───────┬────────────┐\n│ Country │ CityName  │ Year  │ Population │\n│ varchar │  varchar  │ int32 │   int32    │\n├─────────┼───────────┼───────┼────────────┤\n│ NL      │ Amsterdam │  2000 │       1005 │\n│ NL      │ Amsterdam │  2010 │       1065 │\n│ NL      │ Amsterdam │  2020 │       1158 │\n└─────────┴───────────┴───────┴────────────┘ \n\nIf you want to create a temporary table that resides in memory and is automatically dropped when the connection to DuckDB is closed, use CREATE TEMP TABLE:\nCREATE TEMP TABLE foo AS FROM \"../datasets/iris.csv\";\n\n\nDropping a Table\nFinally, when the table is no longer needed, we can remove it from the catalog with DROP TABLE:\nDROP TABLE WorldCities;",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data/sqlbasics.html#select-statement",
    "href": "data/sqlbasics.html#select-statement",
    "title": "9  SQL Basics",
    "section": "9.4 SELECT Statement",
    "text": "9.4 SELECT Statement\nThe SELECT statement is the workhorse for querying, analyzing, combining, and ordering data in database tables. The more important basic syntax elements of SELECT are\nSELECT select_list\n   FROM tables\n   WHERE condition\n   GROUP BY groups\n   HAVING group_filter\n   ORDER BY order_expr\n   LIMIT n;\n\nThe logical start of the SELECT query is the FROM clause where you specify the data source for the query. The select_list prior to the FROM clause specifies the columns in the result set of the query. This can be columns from results in the FROM clause, aggregations, and combinations. If you want to specifically name element of the select_list, you can assign an alias with the AS clause, as in this example:\nSELECT sepal_length AS SL, sepal_width AS SW from iris LIMIT 5;\n\n┌────────┬────────┐\n│   SL   │   SW   │\n│ double │ double │\n├────────┼────────┤\n│    5.1 │    3.5 │\n│    4.9 │    3.0 │\n│    4.7 │    3.2 │\n│    4.6 │    3.1 │\n│    5.0 │    3.6 │\n├────────┴────────┤\n│     5 rows      │\n└─────────────────┘ \n\nTo include everything in the FROM clause into the select_list, use the asterisk wildcard (star notation):\nSELECT * FROM iris LIMIT 4;\n\n┌──────────────┬─────────────┬──────────────┬─────────────┬─────────┐\n│ sepal_length │ sepal_width │ petal_length │ petal_width │ species │\n│    double    │   double    │    double    │   double    │ varchar │\n├──────────────┼─────────────┼──────────────┼─────────────┼─────────┤\n│          5.1 │         3.5 │          1.4 │         0.2 │ setosa  │\n│          4.9 │         3.0 │          1.4 │         0.2 │ setosa  │\n│          4.7 │         3.2 │          1.3 │         0.2 │ setosa  │\n│          4.6 │         3.1 │          1.5 │         0.2 │ setosa  │\n└──────────────┴─────────────┴──────────────┴─────────────┴─────────┘ \n\nSELECT queries can quickly get complicated because the FROM clause can contain a single table, multiple tables that are joined together, or another SELECT query (this is called a subquery):\nSELECT land, improve, \n       (SELECT avg(sale) FROM landsales as average) \nFROM (SELECT * from landsales where total &gt; 100000);\n\n┌────────┬─────────┬──────────────────────────────────────────────┐\n│  land  │ improve │ (SELECT avg(sale) FROM landsales AS average) │\n│ int64  │  int64  │                   double                     │\n├────────┼─────────┼──────────────────────────────────────────────┤\n│  45990 │   91402 │                           217445.45454545456 │\n│  56658 │  153806 │                           217445.45454545456 │\n│  51428 │   72451 │                           217445.45454545456 │\n│  76125 │   78172 │                           217445.45454545456 │\n│ 154360 │   61934 │                           217445.45454545456 │\n│  40800 │   92606 │                           217445.45454545456 │\n└────────┴─────────┴──────────────────────────────────────────────┘ \n\nIn the previous SQL code, the select_list contains a SELECT statement that returns the average sale price for all land sales. The FROM clause contains another SELECT statement with a WHERE clause. The result set consists of the land and improvement value for sales where the total value exceeded 100,000 and the average sale for all properties (including the ones with total value below 100,000).\nThe WHERE clause in the SELECT applies filters to the data. Only rows that match the WHERE clause are processed.\nSELECT * FROM iris WHERE species LIKE '%osa' LIMIT 10;\nOutput\n┌──────────────┬─────────────┬──────────────┬─────────────┬─────────┐\n│ sepal_length │ sepal_width │ petal_length │ petal_width │ species │\n│    double    │   double    │    double    │   double    │ varchar │\n├──────────────┼─────────────┼──────────────┼─────────────┼─────────┤\n│          5.1 │         3.5 │          1.4 │         0.2 │ setosa  │\n│          4.9 │         3.0 │          1.4 │         0.2 │ setosa  │\n│          4.7 │         3.2 │          1.3 │         0.2 │ setosa  │\n│          4.6 │         3.1 │          1.5 │         0.2 │ setosa  │\n│          5.0 │         3.6 │          1.4 │         0.2 │ setosa  │\n│          5.4 │         3.9 │          1.7 │         0.4 │ setosa  │\n│          4.6 │         3.4 │          1.4 │         0.3 │ setosa  │\n│          5.0 │         3.4 │          1.5 │         0.2 │ setosa  │\n│          4.4 │         2.9 │          1.4 │         0.2 │ setosa  │\n│          4.9 │         3.1 │          1.5 │         0.1 │ setosa  │\n├──────────────┴─────────────┴──────────────┴─────────────┴─────────┤\n│ 10 rows                                                 5 columns │\n└───────────────────────────────────────────────────────────────────┘ \n\nThe GROUP BY clause specifies the groupings for aggregations.\nSELECT count(*), max(sepal_length) FROM iris GROUP BY species;\n\n┌──────────────┬───────────────────┐\n│ count_star() │ max(sepal_length) │\n│    int64     │      double       │\n├──────────────┼───────────────────┤\n│           50 │               7.9 │\n│           50 │               5.8 │\n│           50 │               7.0 │\n└──────────────┴───────────────────┘ \n\nYou can specify multiple columns in the grouping or use the GROUP BY ALL shorthand. The following are equivalent:\nSELECT League, Division, mean(RBI) FROM Hitters GROUP BY League, Division;\nSELECT League, Division, mean(RBI) FROM Hitters GROUP BY ALL;\n\n┌─────────┬──────────┬───────────────────┐\n│ League  │ Division │     mean(RBI)     │\n│ varchar │ varchar  │      double       │\n├─────────┼──────────┼───────────────────┤\n│ A       │ E        │ 54.37647058823529 │\n│ A       │ W        │ 48.81111111111111 │\n│ N       │ E        │ 44.94444444444444 │\n│ N       │ W        │ 42.85333333333333 │\n└─────────┴──────────┴───────────────────┘\n\nThe HAVING clause in the SELECT statement causes confusion sometimes, but it is fairly easy to understand as a filter that is applied after the GROUP BY clause. The WHERE clause on the other hand is a filter applied before the GROUP BY clause. In other words, the WHERE clause selects which rows participate in the GROUP BY operation. The HAVING clause determines which result of the GROUP BY operation are filtered.\nSELECT League, Division, \n       count(*) as count, \n       avg(RBI) AS average \nFROM Hitters \nGROUP BY ALL\nHAVING count &gt; 80;\n\n┌─────────┬──────────┬───────┬───────────────────┐\n│ League  │ Division │ count │      average      │\n│ varchar │ varchar  │ int64 │      double       │\n├─────────┼──────────┼───────┼───────────────────┤\n│ A       │ E        │    85 │ 54.37647058823529 │\n│ A       │ W        │    90 │ 48.81111111111111 │\n└─────────┴──────────┴───────┴───────────────────┘ \n\nWithout the HAVING clause, the SELECT statement would return results on two additional groupings:\nSELECT League, Division, \n       count(*) as count, \n       avg(RBI) AS average \nFROM Hitters \nGROUP BY ALL;\n\n┌─────────┬──────────┬───────┬───────────────────┐\n│ League  │ Division │ count │      average      │\n│ varchar │ varchar  │ int64 │      double       │\n├─────────┼──────────┼───────┼───────────────────┤\n│ A       │ E        │    85 │ 54.37647058823529 │\n│ A       │ W        │    90 │ 48.81111111111111 │\n│ N       │ E        │    72 │ 44.94444444444444 │\n│ N       │ W        │    75 │ 42.85333333333333 │\n└─────────┴──────────┴───────┴───────────────────┘  \n\nThe ORDER BY and LIMIT clause also operate on the result set of the query. LIMIT is applied at the end of the query and returns only a specified number of rows in the result set. The ORDER BY clause affects how the rows of the result set are arranged. LIMIT without an ORDER BY can be non-deterministic, multiple runs can produce different results depending on the parallel processing of the data. However, combining ORDER BY and LIMIT generates reproducible results and is a common technique to fetch the top or bottom rows. The following SELECT statement retrieves statistics for the baseball players with the five highest RBI values:\nSELECT AtBat, Hits, HmRun, Runs, RBI, Walks, Errors \n   FROM Hitters ORDER BY RBI DESC LIMIT 5;\n\n┌───────┬───────┬───────┬───────┬───────┬───────┬────────┐\n│ AtBat │ Hits  │ HmRun │ Runs  │  RBI  │ Walks │ Errors │\n│ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64  │\n├───────┼───────┼───────┼───────┼───────┼───────┼────────┤\n│   663 │   200 │    29 │   108 │   121 │    32 │      6 │\n│   600 │   144 │    33 │    85 │   117 │    65 │     14 │\n│   637 │   174 │    31 │    89 │   116 │    56 │      9 │\n│   677 │   238 │    31 │   117 │   113 │    53 │      6 │\n│   618 │   200 │    20 │    98 │   110 │    62 │      8 │\n└───────┴───────┴───────┴───────┴───────┴───────┴────────┘",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data/sqlbasics.html#subqueries",
    "href": "data/sqlbasics.html#subqueries",
    "title": "9  SQL Basics",
    "section": "9.5 Subqueries",
    "text": "9.5 Subqueries\nA subquery is a query inside another query. The subquery is also called the inner query and the query containing it is referred to as the outer query. Subqueries are said to be uncorrelated if they are self-contained and can be run without the outer query. For example, the following SQL statement contains an uncorrelated subquery, select min(weight) from auto, which you can run as a stand-alone query\nselect name from auto where weight = (select min(weight) from auto);\n\n┌─────────────┐\n│    name     │\n│   varchar   │\n├─────────────┤\n│ datsun 1200 │\n└─────────────┘ \n\nThe outer query in this example is SELECT name FROM auto WHERE weight =.\nYou can use subqueries together with SELECT EXISTS to test for the existence of rows in the subquery:\nselect exists(select * from auto where horsepower &gt; 300);\n\n┌─────────────────────────────────────────────────────┐\n│ EXISTS(SELECT * FROM auto WHERE (horsepower &gt; 300)) │\n│                       boolean                       │\n├─────────────────────────────────────────────────────┤\n│ false                                               │\n└─────────────────────────────────────────────────────┘\n\nA subquery is said to be correlated if the subquery uses values from the outer query. You can think of the subquery as a function that is run for every row in the data. Suppose we want to find the minimum reaction time for each subject in the sleep table. The data represent reaction times for 18 subjects measured on ten days of a sleep deprivation study.\nSELECT *\n  FROM sleep AS s\n  WHERE Reaction =\n      (SELECT MIN(Reaction)\n       FROM sleep\n       WHERE sleep.Subject=s.Subject);\n\n┌──────────┬───────┬─────────┐\n│ Reaction │ Days  │ Subject │\n│  double  │ int64 │  int64  │\n├──────────┼───────┼─────────┤\n│   249.56 │     0 │     308 │\n│ 202.9778 │     2 │     309 │\n│ 194.3322 │     1 │     310 │\n│ 280.2396 │     6 │     330 │\n│    285.0 │     1 │     331 │\n│ 234.8606 │     0 │     332 │\n│ 276.7693 │     2 │     333 │\n│ 243.3647 │     2 │     334 │\n│  235.311 │     7 │     335 │\n│ 291.6112 │     2 │     337 │\n│ 230.3167 │     1 │     349 │\n│ 243.4543 │     1 │     350 │\n│ 250.5265 │     0 │     351 │\n│ 221.6771 │     0 │     352 │\n│ 257.2424 │     2 │     369 │\n│  225.264 │     0 │     370 │\n│ 259.2658 │     6 │     371 │\n│ 269.4117 │     0 │     372 │\n├──────────┴───────┴─────────┤\n│ 18 rows          3 columns │\n└────────────────────────────┘ \n\nSubject 308 has the smallest reaction time on day 0 of the study, subject 309 has the smallest reaction time at day 2, and so forth. The subquery is correlated, because it uses the column s.Subject from the outer query.\nWe could have achieved the same aggregation of smallest reaction time by subject with a simple query with GROUP BY clause:\nselect min(Reaction), Subject from sleep group by subject limit 5;\n\n┌───────────────┬─────────┐\n│ min(Reaction) │ Subject │\n│    double     │  int64  │\n├───────────────┼─────────┤\n│        249.56 │     308 │\n│      202.9778 │     309 │\n│      194.3322 │     310 │\n│      280.2396 │     330 │\n│         285.0 │     331 │\n└─────────────────────────┘ \n\nThe difference is that the correlated subquery returns actual records from the table whereas the SELECT with GROUP BY clause returns the result of aggregation. Put it another way: do you want to see the min reaction time for each subject or do you want to see the records that match the min reaction time for each subject?",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data/integration.html",
    "href": "data/integration.html",
    "title": "10  Data Integration",
    "section": "",
    "text": "10.1 Combining Tables with SQL\nThe process of combining tables is based on set operations or joins. A join uses the values in specific columns of the tables to match records. A set operation is a merging of columns without considering the values in the columns. Appending the rows of one table to another table is a set operation. What happens to columns that exist in one table but not in the other during the append depends on the implementation. Similarly, what happens to columns that share the same name when tables are merged horizontally depends on the implementation.",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Integration</span>"
    ]
  },
  {
    "objectID": "data/integration.html#combining-tables-with-sql",
    "href": "data/integration.html#combining-tables-with-sql",
    "title": "10  Data Integration",
    "section": "",
    "text": "Set Operations\nWe draw on an example in the DuckDB documentation to explain how tables can be merged by rows or columns. First, let’s create two small tables with information on cities:\nCREATE TABLE capitals(city VARCHAR, country VARCHAR);\nINSERT INTO capitals VALUES ('Amsterdam', 'NL'), ('Berlin', 'Germany');\n\nCREATE TABLE weather(city VARCHAR, degrees INTEGER, date DATE);\nINSERT INTO weather VALUES ('Amsterdam', 10, '2022-10-14'), \n                           ('Seattle', 8, '2022-10-12');\n\nFROM capitals;\n\n┌───────────┬─────────┐\n│   city    │ country │\n│  varchar  │ varchar │\n├───────────┼─────────┤\n│ Amsterdam │ NL      │\n│ Berlin    │ Germany │\n└───────────┴─────────┘\n\nFROM weather;\n\n┌───────────┬─────────┬────────────┐\n│   city    │ degrees │    date    │\n│  varchar  │  int32  │    date    │\n├───────────┼─────────┼────────────┤\n│ Amsterdam │      10 │ 2022-10-14 │\n│ Seattle   │       8 │ 2022-10-12 │\n└───────────┴─────────┴────────────┘  \n\nSet operations involve two SELECT queries. The queries are connected with clauses that control how the rows are combined:\n\nUNION: combines rows from queries that have the same columns (number and types) and eliminates duplicates.\nUNION ALL: combines rows from queries that have the same columns (number and types) and preserves duplicates.\nINTERSECT: selects rows that occur in both queries and removes duplicates.\nEXCEPT: selects rows that occur in the left query and removes duplicates.\nUNION BY NAME: works like UNION but does not require the queries to have the same number and types of columns. Eliminates duplicates, like UNION.\nUNION ALL BY NAME: works like UNION BY NAME but does not eliminate duplicate rows.\n\nHere are the result of UNION and UNION ALL clauses:\nSELECT city FROM capitals UNION SELECT city FROM weather;\n\n┌───────────┐\n│   city    │\n│  varchar  │\n├───────────┤\n│ Amsterdam │\n│ Seattle   │\n│ Berlin    │\n└───────────┘\n\nSELECT city FROM capitals UNION ALL SELECT city FROM weather;\n\n┌───────────┐\n│   city    │\n│  varchar  │\n├───────────┤\n│ Amsterdam │\n│ Berlin    │\n│ Amsterdam │\n│ Seattle   │\n└───────────┘ \n\nHere are the results of INTERSECT and EXCEPT clauses:\nSELECT city FROM capitals INTERSECT SELECT city FROM weather;\n\n┌───────────┐\n│   city    │\n│  varchar  │\n├───────────┤\n│ Amsterdam │\n└───────────┘\n\nSELECT city FROM capitals EXCEPT SELECT city FROM weather;\n\n┌─────────┐\n│  city   │\n│ varchar │\n├─────────┤\n│ Berlin  │\n└─────────┘ \n\nNotice that UNION, UNION ALL, INTERSECT, and EXCEPT require the two queries to return the same columns. The horizontal column matching is done by position. To merge rows across tables with different columns, use UNION (ALL) BY NAME:\nSELECT * FROM capitals UNION BY NAME SELECT * FROM weather;\n\n┌───────────┬─────────┬─────────┬────────────┐\n│   city    │ country │ degrees │    date    │\n│  varchar  │ varchar │  int32  │    date    │\n├───────────┼─────────┼─────────┼────────────┤\n│ Amsterdam │ NL      │         │            │\n│ Seattle   │         │       8 │ 2022-10-12 │\n│ Berlin    │ Germany │         │            │\n│ Amsterdam │         │      10 │ 2022-10-14 │\n└───────────┴─────────┴─────────┴────────────┘ \n\n\n\nJoins\nThe previous set operations combine rows of data (vertically). To combine tables horizontally we use join operations. Joins typically are based on the values in columns of the tables to find matches. There are two exceptions, positional and cross joins.\nFor data scientists working with rectangular data frames in which observations have a natural order, merging data horizontally is a standard operation. In relational databases this is a somewhat unnatural operation because relational tables do not work from a natural ordering of the data, they are based on keys and indices. The positional join matches row-by-row such that rows from both tables appear at least once:\nselect capitals.*, weather.* from capitals positional join weather;\n\n┌───────────┬─────────┬───────────┬─────────┬────────────┐\n│   city    │ country │   city    │ degrees │    date    │\n│  varchar  │ varchar │  varchar  │  int32  │    date    │\n├───────────┼─────────┼───────────┼─────────┼────────────┤\n│ Amsterdam │ NL      │ Amsterdam │      10 │ 2022-10-14 │\n│ Berlin    │ Germany │ Seattle   │       8 │ 2022-10-12 │\n└───────────┴─────────┴───────────┴─────────┴────────────┘ \n\nThe cross join is actually the simplest join, it returns all possible pairs of rows:\nselect capitals.*, weather.* from capitals cross join weather;\n\n┌───────────┬─────────┬───────────┬─────────┬────────────┐\n│   city    │ country │   city    │ degrees │    date    │\n│  varchar  │ varchar │  varchar  │  int32  │    date    │\n├───────────┼─────────┼───────────┼─────────┼────────────┤\n│ Amsterdam │ NL      │ Amsterdam │      10 │ 2022-10-14 │\n│ Amsterdam │ NL      │ Seattle   │       8 │ 2022-10-12 │\n│ Berlin    │ Germany │ Amsterdam │      10 │ 2022-10-14 │\n│ Berlin    │ Germany │ Seattle   │       8 │ 2022-10-12 │\n└───────────┴─────────┴───────────┴─────────┴────────────┘ \n\nJoins are categorized as outer or inner joins depending on whether rows with matches are returned. An outer join returns rows that do not have any matches whereas the inner join returns only rows that get paired. The two tables in a join are called the left and right sides of the relation and outer joins are further classified as\n\nLeft outer join: all rows from the left side of the relation appear at least once.\nRight outer join: all rows from the right side of the relation appear at least once.\nFull outer join: all rows from both sides of the relation appear at least once.\n\nTo demonstrate the joins in DuckDB, let’s set up some simple tables:\nCREATE TABLE weather (\n      city           VARCHAR,\n      temp_lo        INTEGER, -- minimum temperature on a day\n      temp_hi        INTEGER, -- maximum temperature on a day\n      prcp           REAL,\n      date           DATE\n  );\nCREATE TABLE cities (\n      name            VARCHAR,\n      lat             DECIMAL,\n      lon             DECIMAL\n  );\nINSERT INTO weather VALUES ('San Francisco', 46, 50, 0.25, '1994-11-27');\nINSERT INTO weather (city, temp_lo, temp_hi, prcp, date)\n      VALUES ('San Francisco', 43, 57, 0.0, '1994-11-29');\nINSERT INTO weather (date, city, temp_hi, temp_lo)\n      VALUES ('1994-11-29', 'Hayward', 54, 37);\n\nFROM weather;\n\n┌───────────────┬─────────┬─────────┬───────┬────────────┐\n│     city      │ temp_lo │ temp_hi │ prcp  │    date    │\n│    varchar    │  int32  │  int32  │ float │    date    │\n├───────────────┼─────────┼─────────┼───────┼────────────┤\n│ San Francisco │      46 │      50 │  0.25 │ 1994-11-27 │\n│ San Francisco │      43 │      57 │   0.0 │ 1994-11-29 │\n│ Hayward       │      37 │      54 │       │ 1994-11-29 │\n└───────────────┴─────────┴─────────┴───────┴────────────┘ \n\nINSERT INTO cities VALUES ('San Francisco', -194.0, 53.0);\nFROM cities;\n\n┌───────────────┬───────────────┬───────────────┐\n│     name      │      lat      │      lon      │\n│    varchar    │ decimal(18,3) │ decimal(18,3) │\n├───────────────┼───────────────┼───────────────┤\n│ San Francisco │      -194.000 │        53.000 │\n└───────────────┴───────────────┴───────────────┘ \n\nAn inner join between the tables on the columns that contain the city names will match the records for San Francisco:\nSELECT * FROM weather INNER JOIN cities ON (weather.city = cities.name);\n\n┌───────────────┬─────────┬─────────┬───────┬────────────┬───────────────┬───────────────┬───────────────┐\n│     city      │ temp_lo │ temp_hi │ prcp  │    date    │     name      │      lat      │      lon      │\n│    varchar    │  int32  │  int32  │ float │    date    │    varchar    │ decimal(18,3) │ decimal(18,3) │\n├───────────────┼─────────┼─────────┼───────┼────────────┼───────────────┼───────────────┼───────────────┤\n│ San Francisco │      46 │      50 │  0.25 │ 1994-11-27 │ San Francisco │      -194.000 │        53.000 │\n│ San Francisco │      43 │      57 │   0.0 │ 1994-11-29 │ San Francisco │      -194.000 │        53.000 │\n└───────────────┴─────────┴─────────┴───────┴────────────┴───────────────┴───────────────┴───────────────┘ \n\nNote that the values for lat and lon are repeated for every row in the weather table that matches the join in the relation. Because this is an inner join (the DuckDB default), and the weather table had no matching row for city Hayward, this city does not appear in the join result. We can change that by modifying the type of join to a left outer join:\nSELECT * FROM weather LEFT OUTER JOIN cities ON (weather.city = cities.name);\n\n┌───────────────┬─────────┬─────────┬───────┬────────────┬───────────────┬───────────────┬───────────────┐\n│     city      │ temp_lo │ temp_hi │ prcp  │    date    │     name      │      lat      │      lon      │\n│    varchar    │  int32  │  int32  │ float │    date    │    varchar    │ decimal(18,3) │ decimal(18,3) │\n├───────────────┼─────────┼─────────┼───────┼────────────┼───────────────┼───────────────┼───────────────┤\n│ San Francisco │      46 │      50 │  0.25 │ 1994-11-27 │ San Francisco │      -194.000 │        53.000 │\n│ San Francisco │      43 │      57 │   0.0 │ 1994-11-29 │ San Francisco │      -194.000 │        53.000 │\n│ Hayward       │      37 │      54 │       │ 1994-11-29 │               │               │               │\n└───────────────┴─────────┴─────────┴───────┴────────────┴───────────────┴───────────────┴───────────────┘\n\nBecause the join is an outer join, rows that do not have matches in the relation are returned. Because the outer join is a left join, every row on the left side of the relation is returned (at least once). If you change the left- and right-hand side of the relation you can achieve the same result by using a right outer join:\nSELECT * FROM cities  RIGHT OUTER JOIN weather ON (weather.city = cities.name);\n\n┌───────────────┬───────────────┬───────────────┬───────────────┬─────────┬─────────┬───────┬────────────┐\n│     name      │      lat      │      lon      │     city      │ temp_lo │ temp_hi │ prcp  │    date    │\n│    varchar    │ decimal(18,3) │ decimal(18,3) │    varchar    │  int32  │  int32  │ float │    date    │\n├───────────────┼───────────────┼───────────────┼───────────────┼─────────┼─────────┼───────┼────────────┤\n│ San Francisco │      -194.000 │        53.000 │ San Francisco │      46 │      50 │  0.25 │ 1994-11-27 │\n│ San Francisco │      -194.000 │        53.000 │ San Francisco │      43 │      57 │   0.0 │ 1994-11-29 │\n│               │               │               │ Hayward       │      37 │      54 │       │ 1994-11-29 │\n└───────────────┴───────────────┴───────────────┴───────────────┴─────────┴─────────┴───────┴────────────┘ \n\nNow let’s add another record to the cities table without a matching record in the weather table:\nINSERT INTO cities VALUES ('New York',40.7, -73.9);\nFROM cities;\n\n┌───────────────┬───────────────┬───────────────┐\n│     name      │      lat      │      lon      │\n│    varchar    │ decimal(18,3) │ decimal(18,3) │\n├───────────────┼───────────────┼───────────────┤\n│ San Francisco │      -194.000 │        53.000 │\n│ New York      │        40.700 │       -73.900 │\n└───────────────┴───────────────┴───────────────┘ \n\nA full outer join between the two tables ensures that rows from both sides of the relation appear at least once:\nSELECT * FROM cities FULL OUTER JOIN weather ON (weather.city = cities.name);\n\n┌───────────────┬───────────────┬───────────────┬───────────────┬─────────┬─────────┬───────┬────────────┐\n│     name      │      lat      │      lon      │     city      │ temp_lo │ temp_hi │ prcp  │    date    │\n│    varchar    │ decimal(18,3) │ decimal(18,3) │    varchar    │  int32  │  int32  │ float │    date    │\n├───────────────┼───────────────┼───────────────┼───────────────┼─────────┼─────────┼───────┼────────────┤\n│ San Francisco │      -194.000 │        53.000 │ San Francisco │      46 │      50 │  0.25 │ 1994-11-27 │\n│ San Francisco │      -194.000 │        53.000 │ San Francisco │      43 │      57 │   0.0 │ 1994-11-29 │\n│               │               │               │ Hayward       │      37 │      54 │       │ 1994-11-29 │\n│ New York      │        40.700 │       -73.900 │               │         │         │       │            │\n└───────────────┴───────────────┴───────────────┴───────────────┴─────────┴─────────┴───────┴────────────┘",
    "crumbs": [
      "Part II. Data Preparation and Understanding",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Integration</span>"
    ]
  },
  {
    "objectID": "statlearning/general.html",
    "href": "statlearning/general.html",
    "title": "11  General Topics",
    "section": "",
    "text": "11.1 Introduction\nThis chapter dives into selected topics to set the stage for a more rigorous mathematical treatment later. The goal is to understand the concepts and to become familiar with the notation and terminology of data analytics. The material depends more on formulas than previous chapter but is not very “mathy”.\nAt the end of the chapter you will know about important classes of statistical models and how to express them mathematically. The difference between regression and classification models will be clear and how regression predictions are often the precursor to classifications. The bias-variance tradeoff is a balance every data science project needs to strike—you need to understand the origin of the tradeoff and how to measure model performance to find the balance between overfitting and underfitting. Training, test, and validation data sets are universal in data science to help build good models. We cover the basics in this chapter and the details of data splitting and cross-validation later.",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>General Topics</span>"
    ]
  },
  {
    "objectID": "statlearning/general.html#statistical-models",
    "href": "statlearning/general.html#statistical-models",
    "title": "11  General Topics",
    "section": "11.2 Statistical Models",
    "text": "11.2 Statistical Models\n\nStatistical Learning and Machine Learning\nMuch is being made of the difference between statistical models and machine learning models, or to be more precise, between statistical learning (SL) and machine learning (ML).\n\n\nDefinition: Statistical Learning\n\n\nStatistical Learning is the process of understanding data through the application of tools that describe structure and relationships in data. Models are formulated based on the structure of data to predict outcomes from inputs, to test hypothesis about relationships, to group data, or to reduce the dimensionality of a problem.\n\n\nStatistical learning emphasizes prediction more than the testing of hypothesis, as compared to statistical modeling. Many model classes used in statistical learning are the same models one uses to test hypothesis about patterns and relationships in data. Emphasis of prediction over hypothesis testing—or vice versa—flows from the nature of the problem we are trying to solve. The same model can be developed with focus on predictive capability or with focus on interpretability. We do not want to overdo the distinction between statistical learning and statistical modeling: statistical learning uses statistical models.\nLearning is the process of converting experience into knowledge and machine learning is an automated way of learning by using computers. Rather than directly programming computers to perform a task, machine learning is used when the tasks are not easily described and communicated (e.g., driving, reading, image recognition) or when the tasks exceed human capabilities (e.g., analyzing large and complex data sets). Modern machine learning discovered data as a resource for learning and that is where statistical learning and machine learning meet.\nSL and ML have more in common, than what separates them:\n\nThe input to a learning algorithm is data; the raw material is the same.\nThe data are thought of as randomly generated, there is some sense of variability in the data that is attributed to random sources.\nBoth disciplines distinguish supervised and unsupervised forms of learning\nThey use many of the same models and algorithms for regression, classification, clustering, dimension reduction, etc.\n\nMachine learning uses observed data to describe relationships and “causes”; the emphasis is on predicting new and/or future outcomes. There is comparatively little emphasis on experimentation and hypothesis testing.\nA key difference between SL and ML is what Breiman describes as the difference between data modeling and algorithmic modeling. The difference aligns closely with statistical and machine learning thinking. In data modeling, theory focuses on the probabilistic properties of the model and of quantities derived from it. In algorithmic modeling, the focus is on the properties of the algorithm itself: starting values, optimization, convergence behavior, parallelization, hyperparameter tuning, and so on. Consequently, statisticians are concerned with the asymptotic distributional behavior of estimators and methods as \\(n \\rightarrow \\infty\\). Machine learning focuses on finite sample properties and ask what accuracy can be expected based on the available data.\nThe strong assumptions statisticians make about the stochastic data-generating mechanism that produced the data set in hand as a realization are not found in machine learning. That does not mean that machine learning models are free of stochastic elements and assumptions—quite the contrary. It means that statisticians use the data-generating mechanism as the foundation for conclusions rather than the data alone.\nWhen you look at a p-value in a table of parameter estimates, you rely on all assumptions about distributional properties of the data, correctness of the model, and (asymptotic) distributional behavior of the estimator. They flow explicitly from the data-generating mechanism or implicitly from somewhere else. Otherwise, the p-value does not make much sense. (Many argue that p-values are not very helpful and possibly even damaging to decision making but this is not the origin of this discussion.)\nIf you express the relationship between a target variable \\(Y\\) and inputs \\(x_1, \\cdots, x_p\\) as\n\\[\nY = f(x_1,\\cdots,x_p) + \\epsilon\n\\]\nwhere \\(\\epsilon\\) is a random variable, it does not matter whether you perform data modeling or algorithmic modeling. We need to think about \\(\\epsilon\\) and its properties. How does \\(\\epsilon\\) affect the algorithm, the prediction accuracy, the uncertainty of statements about \\(Y\\) or \\(f(x_1, \\cdots, x_p)\\)? That is why all data professionals need to understand about stochastic models and statistical models.\n\n\nStochastic and Statistical Models\nA stochastic model describes the probability distribution of outcomes by allowing one or more of the model elements to be random variables.\nSuppose we are charged with developing a model to predict recurrence of cancer. There are many possible aspects that influence the outcome:\n\nAge, gender\nMedical history\nLifestyle factors (nutrition, exercise, smoking, …)\nType of cancer\nSize of the largest tumor\nSite of cancer\nTime since diagnostic, time from treatment\nType of treatment\nand so on\n\nIf we were to try and build a deterministic model that predicts cancer recurrence perfectly, all influences would have to be taken into account and their impact on the outcome would have to be incorporated correctly. That would be an incredibly complex model, and impractical.\nBy taking a stochastic approach we acknowledge that there are processes that affect the variability in cancer recurrence we observe from patient to patient. The modeling can now focus on the most important factors and how they drive cancer recurrence. The other factors are included through random effects. If the model captures the salient factors and their impact correctly, and the variability contributed by other factors is not too large, and not systematic, the model is very useful. It possibly is much more useful than an inscrutably complex model that tries to accommodate all influences perfectly.\nThe simplest stochastic model for cancer recurrence is to assume that the outcome is a Bernoulli (binary) random variable taking on two states (cancer recurs, cancer does not recur) with probabilities \\(\\pi\\) and \\(1-\\pi\\). If we code the two states numerically, cancer recurs as 1, cancer does not recur as 0, the probability mass function of cancer recurrence is that of the random variable \\(Y\\),\n\\[\n\\Pr(Y=y) = \\left \\{ \\begin{array}{cl} \\pi & y=1 \\\\ 1-\\pi & y = 0\\end{array} \\right .\n\\]\n\n\nDefinition: Statistical Model\n\n\nA statistical model is a stochastic model that contains unknown constants, called parameters. Parameters are estimated based on data. Parameters are constants, not random variables. The estimator of a parameter that depends on data is a random variable since the data are random.\n\n\nThe parameter in our cancer model is \\(\\pi\\), the probability that \\(Y\\) takes on the value 1. In statistics, this probability is often called the “success” probability and its complement is called the “failure” probability. We prefer to call them the “event” and “non-event” probabilities instead. The event is the binary outcome coded as a 1.\nBecause we cannot visit with all cancer patients, a sample of patients is used to estimate \\(\\pi\\). This process introduces uncertainty into the estimator of \\(\\pi\\), a larger sample will lead to a more precise (a less uncertain) estimator.\nThe model is overly simplistic in that it captures all possible effects on cancer recurrence in the single quantity \\(\\pi\\). Regardless of age, gender, type of cancer, etc., we would predict a randomly chosen cancer patient’s likelihood to experience a recurrence as \\(\\pi\\). To incorporate input variables that affect the rate of recurrence we need to add structure to \\(\\pi\\). A common approach in statistical learning and in machine learning is that inputs have a linear effect on a transformation of the probability \\(\\pi\\):\n\\[\ng(\\pi) = \\beta_0 + \\beta_1 x_1+\\cdots + \\beta_p x_p\n\\]\nWhen \\(g(\\pi)\\) is the logit function\n\\[\\log\\left \\{ \\frac{\\pi}{1-\\pi} \\right\\}\\]\nthis is called a logistic regression model. \\(x_1,\\cdots,x_p\\) are the inputs of the model, \\(\\beta_0, \\cdots, \\beta_p\\) are the parameters of the model. If we accept that the basic structure of the logistic model applies to the problem of predicting cancer occurrence, we use our sample of patient data to\n\nestimate the parameters \\(\\beta_0, \\cdots, \\beta_p\\);\ndetermine which inputs and how many inputs are adequate: we need to determine \\(p\\) and the specific input variables;\ndetermine whether the logit function is the appropriate transformation to linearity.\n\nThe effect of the inputs is called linear on \\(g(\\pi)\\) if \\(g(\\pi)\\) is a linear function of the parameters. To test whether this is the case take derivatives of the function with respect to all parameters. If the derivatives do not depend on parameters, the effect is linear.\n\\[\\frac{\\partial g(\\pi)}{\\partial\\beta_{0}} = 1\\]\n\\[\\frac{\\partial g(\\pi)}{\\partial\\beta_{1}} = x_{1}\\]\n\\[\\frac{\\partial g(\\pi)}{\\partial\\beta_{p}} = x_{p}\\] None of the derivatives depends on any of the \\((\\beta_{0},\\ldots,\\beta_{p})\\); \\(g(\\pi)\\) is linear in the parameters. A non-linear function is non-linear in at least one parameter.\n\n\nExample: Plateau (hockey stick) Model\n\n\nA plateau model reaches a certain amount of output and remains flat afterwards. When the model prior to the plateau is a simple linear model, the plateau model is also called a hockey-stick model.\n\n\n\n\n\nThe point at which the plateau is reached is called a change point. Suppose the change point is denoted \\(\\alpha\\). The hockey-stick model can be written as\n\\[\n\\text{E}\\lbrack Y\\rbrack = \\left\\{\n\\begin{matrix}\n\\beta_{0} + \\beta_{1}x      & x \\leq \\alpha \\\\\n\\beta_{0} + \\beta_{1}\\alpha & x &gt; \\alpha\n\\end{matrix} \\right.\n\\]\nIf \\(\\alpha\\) is an unknown parameters that is estimated from the data, this is a non-linear model.\n\n\n\n\nModel Components\nThe expression for the logistic regression model\n\\[g(\\pi) = \\beta_{0} + \\beta_{1}x_{1} + \\ldots + \\beta_{p}x_{p}\\]\nlooks quite different from the model introduced earlier,\n\\[Y = f\\left( x_{1},\\ldots,x_{p} \\right) + \\epsilon\\]\nWhere is the connection?\nThe error term \\(\\epsilon\\) is a random variable and we need to specify some of its distributional properties to make progress. At a minimum we provide the mean and variance of \\(\\epsilon\\). If the model is correct—correct on average—then the error terms should have a mean of zero and not depend on any input variables (whether those in the model or other inputs). A common assumption is that the variance of the errors is a constant and not a function of other effects (fixed or random). The two assumptions are summarized as \\(\\epsilon \\sim \\left( 0,\\sigma^{2} \\right)\\); read as \\(\\epsilon\\) follows a distribution with mean 0 and variance \\(\\sigma^{2}\\).\n\nMean function\nNow we can take the expected value of the model and find that\n\\[\n\\text{E}\\lbrack Y\\rbrack = \\text{E}\\left\\lbrack f\\left( x_{1},\\ldots,x_{p} \\right) + \\epsilon \\right\\rbrack = f\\left( x_{1},\\ldots,x_{p} \\right) + \\text{E}\\lbrack\\epsilon\\rbrack = f\\left( x_{1},\\ldots,x_{p} \\right)\n\\]\nBecause the errors have zero mean and because the function \\(f\\left( x_{1},\\ldots,x_{p} \\right)\\) does not contain random variables, \\(f\\left( x_{1},\\ldots,x_{p} \\right)\\) is the expected value (mean) of \\(Y\\). \\(f\\left( x_{1},\\ldots,x_{p} \\right)\\) is thus called mean function of the model.\n\n\nExample: Curvilinear models\n\n\nPolynomial models such as a quadratic model \\[Y = \\beta_{0} + \\beta_{1}x + \\beta_{2}x^{2} + \\epsilon\\] or cubic model \\[Y = \\beta_{0} + \\beta_{1}x + \\beta_{2}x^{2} + \\beta_{3}x^{3} + \\epsilon\\] have a curved appearance when \\(Y\\) is plotted against \\(x\\). They are linear models, however.\nTo test this, take derivatives of the mean function with respect to the parameters. For the quadratic model the partial derivatives with respect to \\(\\beta_{0}\\), \\(\\beta_{1}\\), and \\(\\beta_{2}\\) are 1, \\(x\\), and \\(x^{2}\\), respectively. The model is linear in the parameters.\nTo emphasize that the models are not just straight lines in \\(x\\), a linear model with curved appearance is called curvilinear.\n\n\nWhat does the mean function look like in the logistic regression model? The underlying random variable \\(Y\\) has a Bernoulli distribution. Its mean is\n\\[\\text{E}\\lbrack Y\\rbrack = \\sum y\\, \\Pr(Y = y) = 1 \\times \\pi + 0 \\times (1 - \\pi) = \\pi\\]\nThe logit function \\[g(\\pi) = \\log \\left\\{ \\frac{\\pi}{(1 - \\pi)} \\right\\}\\] is invertible and the model \\[g(\\pi) = \\beta_0 + \\beta_1 x_{1} + \\ldots + \\beta_p x_p\\]\ncan be written as\n\\[\\text{E}\\lbrack Y\\rbrack = \\pi = g^{- 1}\\left( \\beta_{0} + \\beta_{1}x_{1} + \\ldots + \\beta_{p}x_{p} \\right)\\]\nThe mean function of the logistic model is also a function of the inputs. It is a neat exercise to show that if \\(g(\\pi)\\) is the logit function the mean function is\n\\[\\pi = \\frac{1}{1 + \\exp\\left\\{ - \\beta_0 - \\beta_1 x_1 - \\cdots - \\beta_p x_p \\right\\}}\\]\nYou can now also show that although \\(g(\\pi)\\) is linear in the parameters, \\(\\pi\\) is a non-linear function of the parameters.\n\n\nSystematic component\nThe mean functions \\[f\\left( x_{1},\\ldots,x_{p} \\right)\\] and \\[\\frac{1}{1 + \\exp\\left\\{ - \\beta_{0} - \\beta_{1}x_{1} - \\ldots - \\beta_p x_p \\right\\} }\\] look rather different, except for the input variables \\(x_{1},\\ldots,x_{p}\\).\nFor the model \\(Y = f\\left( x_{1},\\ldots,x_{p} \\right) + \\epsilon\\) we left it open how the mean function depends on parameters. There are three general approaches.\nThe systematic component has the form of a linear predictor, that is, a linear combination of the inputs. The linear predictor is frequently denoted as \\(\\eta\\):\n\\[\\eta = \\beta_{0} + \\beta_1 x_1 + \\cdots + \\beta_p x_p\\]\nThe parameter \\(\\beta_0\\) is called the intercept of the linear predictor. Although optional, it is included in most models to capture the effect on the mean if no input variables are present. Models with a linear predictor and an intercept have \\(p + 1\\) parameters in the mean function.\nThe logistic regression model also contains a linear predictor. Depending on whether you write the model in terms of \\(\\pi\\) or \\(g(\\pi)\\), the expressions are\n\\[g(\\pi) = \\eta\\]\n\\[\\pi = \\frac{1}{1 + \\exp\\{ - \\eta \\}}\\]\nThe mean function can be a general non-linear function of the parameters. The number of input variables and the number of parameters can be quite different.\nThe Mitscherlich model is popular in agricultural studies of plant growth as a function of an input such as a fertilizer. The plant species is a commercial crop. If \\(Y\\) denotes plant yield and \\(x\\) the amount of input, the Mitscherlich model is\n\\[Y = f(x,\\xi,\\lambda,\\kappa) + \\epsilon = \\lambda + (\\xi - \\lambda)\\exp\\left\\{ - \\kappa x \\right\\} + \\epsilon\\]\nThe mean function \\(f\\)() depends on one input variable \\(x\\) and three parameters \\((\\xi,\\lambda,\\kappa)\\). Taking derivatives, it is easily established that the mean function is non-linear:\n\\[\\frac{\\partial f(x,\\xi,\\lambda,\\kappa)}{\\partial\\xi} = \\exp\\{ - \\kappa x \\}\\]\nThe derivative with respect to \\(\\xi\\) depends on the \\(\\kappa\\) parameter.\nNon-linear models like the Mitscherlich equation are appealing because they are easily interpretable. The parameters have meaning in terms of the subject domain:\n\n\\(\\xi\\) is the crop yield if no fertilizer is applied, the mean of \\(Y\\) at \\(x = 0\\). This is the baseline yield\n\\(\\lambda\\) is the upper yield asymptote as \\(x\\) increases\n\\(\\kappa\\) relates to a rate of change, how quickly the yield increases from \\(\\xi\\) and reaches \\(\\kappa\\).\n\nFigure 11.1 shows the Mitscherlich model fitted to a set of plant yield data, the input variable is the nitrogen rate applied (in kg/ha). Visual estimates for the baseline yield and the asymptotic yield are \\(\\widehat{\\xi} = 40\\) and \\(\\widehat{\\lambda} = 80\\).\n\n\n\n\n\n\nFigure 11.1: Mitscherlich yield equation for plant yield as a function of nitrogen rate fitted to a set of data.\n\n\n\nInterpretability of the parameters enables mapping of research questions to the model:\n\nIs the asymptotic yield greater than 75? This can be answered with a confidence interval for the estimate of \\(\\lambda\\).\nAt what level of \\(x\\) does yield achieve 75% of the maximum? This is an inverse prediction problem. Set yield to 75% of \\(\\lambda\\) and solve the model for \\(x\\).\nThe rate of change in yield is less than ½ unit once \\(x = 100\\) are applied. This can be answered with a hypothesis test for \\(\\kappa\\).\n\nThe third method of specifying the systematic component is to not write it as a function of inputs and parameters. This is common for non-parametric methods such as smoothing splines, local regression, generalized additive models, and kernel methods. These models still have parameters, but the relationship between inputs and parameters is implied through the method of training the models.\nFor example, LOESS is a local polynomial regression method. A LOESS model of degree 2 fits a quadratic polynomial model to a portion of the data (a window). Within window \\(k\\), the model takes the form\n\\[Y = \\beta_{0k} + \\beta_{1k}x + \\beta_{2k}x^{2} + \\epsilon\\]\nAs the window moves across the range of \\(x\\), different observations are captured in the window. The underlying model in each window is a quadratic polynomial but the values of the parameter estimates change from window to window.\n\n\nRandom component\nThe random components of a statistical model are the stochastic elements that describe the distribution of the target variable \\(Y\\). By now we are convinced that most data we work with are to some degree the result of random processes and that incorporating randomness into models makes sense. The model does not need to be correct for every observation, but it needs to be correct on average—an additive zero-mean random error is OK. Even if all influences on the output \\(Y\\) were known, it might be impossible to measure them, or to include them correctly into the model. Randomness is often introduced deliberately by sampling observations from a population or by randomly assigning treatments to experimental units. Finally, stochastic models are often simpler and easier to explain than other models. Among competing explanations, the simpler one wins (Occam’s Razor).\nWe have seen two basic ways to reflect randomness in a statistical model:\n\nBy adding an additive error term to a mean function\nBy describing the distribution of the target variable\n\nThe Mitscherlich model is an example of the first type of specification:\n\\[Y = f(x,\\xi,\\lambda,\\kappa) + \\epsilon = \\lambda + (\\xi - \\lambda)\\exp\\left\\{ - \\kappa x \\right\\} + \\epsilon\\]\nUnder the assumption that \\(\\epsilon \\sim \\left( 0,\\sigma^{2} \\right)\\), it follows that \\(Y\\) is randomly distributed with mean \\(f(x,\\xi,\\lambda,\\kappa)\\) and variance \\(\\sigma^{2}\\); \\(Y \\sim \\left( f(x,\\xi,\\lambda,\\kappa),\\sigma^{2} \\right)\\). If the model errors were normally distributed, \\(\\epsilon \\sim N\\left( 0,\\sigma^{2} \\right)\\), then \\(Y\\) would also be normally distributed. Randomness is contagious.\nThe logistic regression model is an example of the second type of specification:\n\\[g\\left( \\text{E}\\lbrack Y\\rbrack \\right) = \\beta_{0} + \\beta_{1}x_{1} + \\ldots + \\beta_{p}x_{p}\\]\nand \\(Y\\) follows a Bernoulli distribution. It does not make sense to write the model with an additive error term unless the target variable is continuous.\nModels can have more than one random element. In the cancer recurrence example, suppose we want to explicitly associate a random effect with each patient, \\(b_{i} \\sim \\left( 0,\\sigma_{b}^{2} \\right)\\), say. The modified model is now\n\\[g\\left( \\pi\\ |\\ b_{i} \\right) = \\beta_{0} + b_{i} + \\ \\beta_{1}x_{1} + \\ldots + \\beta_{p}x_{p}\\]\nConditional on the patient-specific value of \\(b_{i}\\) the model is still a logistic model with intercept \\(\\beta_{0} + b_{i}\\). Because the parameters \\(\\beta_{0},\\ \\cdots,\\beta_{p}\\) are constants (not random variables), they are also referred to as fixed effects. Models that contain both random and fixed effects are called mixed models.\nMixed models occur naturally when the sampling process is hierarchical.\nFor example, you select apples on trees in an orchard to study the growth of apples over time. You select at random 10 trees in the orchard and chose 25 apples at random on each tree. The apple diameters are then measured in two-week intervals. To represent this data structure, we need a few subscripts.\nLet \\(Y_{ijk}\\) denote the apple diameter at the \\(k\\)th measurement of the \\(j\\)th apple from the \\(i\\)th tree. A possible decomposition of the variability of the \\(Y_{ijk}\\) could be\n\\(Y_{ijk} = \\beta_{0} + a_{i} + \\eta_{ijk} + \\epsilon_{ijk}\\)\nwhere \\(\\beta_{0}\\) is an overall (fixed) intercept, \\(a_{i} \\sim \\left( 0,\\sigma_{a}^{2} \\right)\\) is a random tree effect, \\(\\eta_{ijk}\\) is an effect specific to apple and measurement time, and \\(\\epsilon_{ijk} \\sim \\left( 0,\\sigma_{\\epsilon}^{2} \\right)\\) are the model errors. This is a mixed model because we have multiple random effects (\\(a_{i}\\) and \\(\\epsilon_{ijk}\\)). In addition, we need to decide how to parameterize \\(\\eta_{ijk}\\). Suppose that a simple linear regression trend is reasonable for each apple over time. Estimating a separate slope and intercept for each of the 10 x 25 apples would result in a model with over 500 parameters. A more parsimonious parameterization is to assume that the apples share a tree-specific (fixed) intercept and slope and to model the apple-specific deviations from the tree-specific trends with random variables:\n\\[\\eta_{ijk} = \\left( \\beta_{0i} + b_{0ij} \\right) + {(\\beta}_{1i} + b_{1ij})t_{ijk}\\]\n\\(t_{ijk}\\) is the time that a given apple on a tree is measured. The apple-specific intercept offsets from the tree-specific intercepts \\(\\beta_{0i}\\) are model as random variables \\(b_{0ij} \\sim \\left( 0,\\sigma_{b_{0}}^{2} \\right)\\). Similarly, \\(b_{1ij} \\sim \\left( 0,\\sigma_{b_{1}}^{2} \\right)\\) models the apple-specific offset for the slopes as random variables. Putting everything together we obtain\n\\(Y_{ijk} = \\beta_{0} + \\left( \\beta_{0i} + b_{0ij} \\right) + {(\\beta}_{1i} + b_{1ij})t_{ijk} + \\epsilon_{ijk}\\)\nNote that \\(a_{i}\\) was no longer necessary in this model, that role is now played by \\(\\beta_{0i}\\).\nThe total number of parameters in this model is 24 (1 overall intercept, 10 tree-specific intercepts, 10 tree-specific slopes, and 3 variances (\\(\\sigma_{\\epsilon}^{2}, \\sigma_{b_{0}}^{2}\\), \\(\\sigma_{b_{1}}^{2}\\)).\nThis is a relatively complex model and included here only to show how the sampling design can be incorporated into the model formulation to achieve interpretable and parsimonious models and how this naturally leads to multiple random effects.\nA further refinement of this model is to recognize that the measurements over time for each apple are likely not independent. Furthermore, diameter measurements on the same apple close in time are more strongly correlated than measurements further apart. Incorporating this correlation structure into the models leads to a mixed model with correlated errors.\n\n\nResponse (Target) variable\nA model has inputs that are processed by an algorithm to produce an output. When the output is a variable to be predicted, classified, or grouped, we refer to it with different—but interchangeable—names as the response variable, or the target variable, or the dependent variable. We are not particular about what you call the variable, as long as we agree on what we are talking about—the left-hand side of the model.\nThe target variable is a random variable and can be of different types. This matters greatly because we have to match distributional assumptions to the natural type of the target. Applying an analytic method designed for continuous variables that can take on infinitely many values to a binary variable that takes on two values is ill advised. However, it happens. A lot.\nThe first distinction is whether the target variable is continuous or discrete.\n\nContinuous: the number of possible values of the variable is not countable. Typical examples are physical measurements such as weight, height, length, pressure, temperature. If the values of a variable are countable but the cardinality is high, applying methods for continuous data can make sense—for example, number of days since birth.\nDiscrete: the number of possible values is countable. Even if the number of possible values is infinite, the variable is still discrete. The number of fish caught per day does not have a theoretical upper limit, although it is highly unlikely that a weekend warrior will catch 1,000 fish. A commercial fishing vessel might.\n\nDiscrete variables are further divided into the following groups:\n\nCount Variables: the values are true counts, obtained by enumeration. There are two types of counts:\n\nCounts per unit: the count relates to a unit of measurement, e.g., the number of fish caught per day, the number of customer complaints per quarter, the number of chocolate chips per cookie, the number of cancer incidences per 100,000.\nProportions (Counts out of a total): the count can be converted to a proportion by dividing it with a maximum value. Examples are the number of heads out of 10 coin tosses, the number of larvae out of 20 succumbing to an insecticide,\n\nCategorical Variables: the values consist of labels, even if numbers are used for labeling.\n\nNominal variables: The labels are unordered, for example the variable “fruit” takes on the values “apple”, “peach”, “tomato” (yes, tomatoes are fruit but do not belong in fruit salad).\nOrdinal variables: the category labels can be arranged in a natural order in a lesser-greater sense. Examples are 1—5 star reviews or ratings of severity (“mild”, “modest”, “severe”).\nBinary variables: take on exactly two values (dead/alive, Yes/No, 1/0, fraud/not fraud, diseased/not diseased)",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>General Topics</span>"
    ]
  },
  {
    "objectID": "statlearning/general.html#supervised-and-unsupervised-methods",
    "href": "statlearning/general.html#supervised-and-unsupervised-methods",
    "title": "11  General Topics",
    "section": "11.3 Supervised and Unsupervised Methods",
    "text": "11.3 Supervised and Unsupervised Methods\nStatistical learning and machine learning distinguish supervised and unsupervised methods of learning. Machine learning covers a third technique of learning not found in statistics, reinforcement learning.\n\nSupervised Learning\n\n\nDefinition: Supervised Learning\n\n\nSupervised learning trains statistical learning models through a target variable.\n\n\nSupervised learning is characterized by the presence of a target variable, also called a dependent variable, response variable, or output variable. This is the attribute we wish to model. The training and test data sets contain values for the target variable, in machine learning these values are often called the labels and are described as the “ground truth”. All other variables in the data set are potentially input variables. In short, we know the values of the target variable, now we need to use it in analytical methods to learn how outputs and inputs connect.\nThe goals of supervised learning are to\n\nPredict the target variable from input variables.\nDevelop a function that approximates the underlying relationship between inputs and outputs.\nUnderstand the relationship between inputs and outputs.\nClassify observations into categories of the target variable based on the input variables.\nGroup the observations into sets of similar data based on the values of the target variable and based on values of the inputs.\nReduce the dimensionality of the problem by transforming target and inputs from a high-dimensional to a lower-dimensional space.\nTest hypotheses about the target variable.\n\nStudies can pursue one or more of these goals. For example, you might be interested in understanding the relationship between target and input variables and use that relationship for predictions.\nThe name supervised learning comes from thinking of learning in an environment that is supervised by a teacher. The teacher asks questions for which they know the correct answer (the ground truth) and judge a student’s response to the questions. The goal is to increase students’ knowledge as measured by the quality of their answers. But we do not want students to just memorize answers, we want to teach them to be problem solvers, to apply the knowledge to new problems, to generalize.\nThe parallel between the description of supervised learning in a classroom and training an algorithm on data is obvious: the problems asked by the teacher, the learning algorithm, are the data points, \\(Y\\) is the correct answer, the inputs \\(x_{1},\\cdots,x_{p}\\) are the information used by the students to answer the question. The discrepancy between question and answer is measured by \\((y - \\widehat{y})^2 = (y - \\widehat{f}\\left( x_{1},\\cdots x_{p} \\right))^2\\) or some other error metric. The training of the model stops when we found a model that generalizes well to previously unseen problems. We are not interested in models that follow the observed data too closely.\nTable 11.1 contains a non-exhaustive list of algorithms and models you find in supervised learning.\n\n\n\nTable 11.1: A sampling of supervised learning methods.\n\n\n\n\n\n\n\n\n\n\nLinear regression\nNonlinear regression\nRegularized regression\n(Lasso, Ridge, Elastic nets)\n\n\nLocal polynomial regression (LOESS)\nSmoothing splines\nKernel methods\n\n\nLogistic regression (binary & binomial)\nMultinomial regression (nominal and ordinal)\nPoisson regression (counts and rates)\n\n\nDecision trees\nRandom forests\nBagged trees\n\n\nAdaptive boosting\nGradient boosting machine\nExtreme gradient boosting\n\n\nNaïve Bayes classifier\nNearest-neighbor methods\nDiscriminant analysis (linear and quadratic)\n\n\nPrincipal component regression\nPartial least squares\nGeneralized linear models\n\n\nGeneralized additive models\nMixed models (linear and nonlinear)\nModels for correlated data (spatial, time series)\n\n\nSupport-vector machines\nNeural networks\nExtreme gradient boosting\n\n\n\n\n\n\nThere is a lot to choose from, and for good reason. The predominant application of data analytics is supervised learning with batch (or mini-batch) data. In batch data analysis the data already exist as a historical data source in one place. We can read all records at once or in segments (called mini-batches). If we have to read the data multiple times, for example, because an iterative algorithm passes through the data at each iteration, we can do so.\nBatch-oriented learning contrasts with online learning where the data on which the model is trained is generated and consumer in real time.\n\n\nUnsupervised Learning\n\n\nDefinition: Unsupervised Learning\n\n\nIn unsupervised learning methods a target variable is not present.\n\n\nUnsupervised learning does not utilize a target variable; hence it cannot predict or classify observations. However, we are still interested in discovering structure, patterns, and relationships in the data.\nThe term unsupervised refers to the fact that we no longer know the ground truth because there is no target variable. Hence the concept of a teacher who knows the correct answers and supervises the learning progress of the student does not apply. In unsupervised learning there are no clear error metrics by which to judge the quality of an analysis, which explains the proliferation of unsupervised methods and the reliance on heuristics. For example, a 5-means cluster analysis will find five groups of observations in the data, whether this is the correct number or not, and it is up to us to interpret what differentiates the groups and to assign group labels.\nOften, unsupervised learning is used in an exploratory fashion, improving our understanding of the joint distributional properties of the data and the relationships in the data. The findings then help lead us toward supervised approaches.\nA coarse categorization of unsupervised learning techniques also hints at their application:\n\nAssociation analysis: which values of the variables \\(x_{1},\\cdots,x_{p}\\) tend to occur together in the data? An application is market basket analysis, where the \\(X\\)s are items are in a shopping cart (or a basket in the market), and \\(x_{i} = 1\\) if the \\(i\\)th item is present in the basket and \\(x_{i} = 0\\) if the item is absent. If items frequently appear together, bread and butter, or beer and chips, for example, then maybe they should be located close together in the store. Association analysis is also useful to build recommender systems: shoppers who bought this item also bought the following items \nCluster analysis: can data be grouped based on \\(x_{1},\\cdots,x_{p}\\) into sets such that the observations within a set are more similar to each other than they are to observations in other sets? Applications of clustering include grouping customers into segments. Segmentation analysis is behind loyalty programs, lower APRs for customers with good credit rating, and churn models.\nDimension reduction: can we transform the inputs \\(x_{1},\\cdots,x_{p}\\) into a set \\(c_{1},\\cdots,c_{k}\\), where \\(k \\ll p\\) without losing relevant information? Applications of dimension reduction are in high-dimensional problems where the number of inputs is large relative to the number of observations. In problems with wide data, the number of inputs \\(p\\) can be much larger than \\(n\\), which eliminates many traditional methods of analysis from consideration.\n\nMethods of unsupervised learning often precede supervised learning; the output of an unsupervised learning method can serve as the input to a supervised method. An example is dimension reduction through principal component analysis (PCA) prior to supervised regression. Suppose you have \\(n\\) observations on a target variable \\(Y\\) and a large number of potential inputs \\(x_{1},\\cdots,x_{p}\\) where \\(p\\) is large relative to \\(n\\). PCA computes linear combinations of the \\(p\\) inputs that account for decreasing amounts of variability among the \\(X\\)s. These linear combinations are called the principal components. For example, the first principal component explains 70% of the variability in the inputs, the second principal component explains 20% and the third principal component 5%. Rather than building a regression model with \\(p\\) predictors, we use only the first three principal components as inputs in the regression model. The resulting regression model is called a principal component regression (PCR) because its inputs are the result of a PCA. The PCA is an unsupervised model because it does not use information about \\(Y\\) in forming the principal components. If \\(p = 250\\), using the first three principal components replaces\n\\[Y = \\beta_{0} + \\beta_{1}{\\ x}_{1} + \\beta_{2}x_{2} + \\beta_{3}x_{3} + \\beta_{4}x_{4} + \\cdots + \\beta_{250}x_{250} + \\epsilon\\]\nwith\n\\[Y = \\alpha_{0} + \\alpha_{1}c_{1} + \\alpha_{2}c_{2} + \\alpha_{3}c_{3} + \\epsilon\\]\nwhere \\(c_{1}\\) denotes the first principal component, itself a linear combination of the 250 inputs\n\\[c_{1} = \\gamma_{1}x_{1} + \\gamma_{2}x_{2} + \\gamma_{3}x_{3} + \\cdots + \\gamma_{250}x_{250}\\]\n\n\nReinforcement Learning\nReinforcement learning (RL) is unique to machine learning and does not fall neatly in the supervised/unsupervised learning buckets. It is a very powerful method that received a lot of attention when algorithms could be trained on data to play games extremely well.\nThe approach, based on reinforcement learning, was fundamentally different from the expert system-based approach used so far to teach computers how to play games. An expert system translates the rules of the game into machine code and adds strategy logic. For example, the Stockfish open-source chess program, released first in 2008, has developed with community support into (one of) the best chess engines in the world. In 2017, Google’s DeepMind released AlphaZero, a chess system trained using reinforcement learning. After only 24 hours of training, the data-driven AlphaZero algorithm crushed Stockfish, the best chess engine humans have been able to build over 10 years.\nPreviously, Google’s DeepMind had developed AlphaGo, a reinforcement-trained system that beat the best Go player in the world, Lee Sedol, four to one. This was a remarkable achievement as Go had been thought to be so complex and requiring intuition that would escape computerization at the level of expert players.\nIn reinforcement learning, an agent (a player) is taking actions (makes moves) in an environment (the game). The agent learns by interacting with the environment by receiving feedback on the moves. Actions are judged by a reward function (a score) and the system is trained to maximize the sum of future rewards. In other words, given your current position in the game, choose the next move to maximize the score from here on out.\nAn interesting difference between AlphaGo and AlphaZero is the nature of the training data. Both systems are trained using reinforcement learning. AlphaGo was trained on records of many expert-level games. It was trained to play against historic experts. Success was getting better compared to how human experts played. AlphaZero was trained by playing against itself. Success was beating its former self.\nUnlike supervised learning, inputs and outputs do not need to be present in reinforcement learning. The technique is commonly used in robotics, gaming, and recommendation systems.\nUntil recently, a limitation of RL was the need for a good reward function. It is important that actions in the environment are properly judged. In situation where the result of a move is difficult to judge, reinforcement learning was difficult to apply. For example, in natural language processing, where an action produces some prose, how do we rate the quality of the answer?\nThis was the problem faced by systems like Chat-GPT. How do you score the answer produced during training to make sure the algorithm continuously improves? The solution was a form of reinforcement learning modified by human intervention. RLHF, reinforcement learning with human feedback, uses human interpreters to assign scores to the actions (the Chat-GPT answers).\nIn 2017, when AlphaGo beat Lee Sedol, it was thought that reinforcement learning would change the world. Despite its remarkable achievement in gameplay and robotics, the impact of RL fell short of expectations.\n\n\n\n\n\n\nFigure 11.2: Yann LeCun weighs in on the impact of reinforcement learning (RL).\n\n\n\nWhy did RL fall short? Developing and training reinforcement learning models is an expensive undertaking. The barrier to entry is very high, limiting RL research and development to large tech-savvy organizations. The main reason is the Sim2Real problem mentioned in the tweet above. Reinforcement learning trains an agent in a simulated, artificial environment. The real world is much more complex and transferring training based on simulation to reality is difficult. The RL agents end up performing poorly in real applications.",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>General Topics</span>"
    ]
  },
  {
    "objectID": "statlearning/general.html#regression-and-classification",
    "href": "statlearning/general.html#regression-and-classification",
    "title": "11  General Topics",
    "section": "11.4 Regression and Classification",
    "text": "11.4 Regression and Classification\n\nRegression and the Regression to the Mean Fallacy\nThe term regression was coined by Sir Francis Galton in 1877 in his study of genetics. Galton observed a relationship between physical attributes of offspring and their parents. He found that offspring deviated less from the mean value of the population than their parents did. For example, taller parents tend to have taller children, but children of taller parents tend to be shorter than their parents and children of shorter parents tend to be taller than their parents.\nThe cause Galton attributed to this phenomenon—which he called regression toward mediocrity—is controversial, his arguments had implications about natural selection and eugenics. As a statistical phenomenon, it is well understood. Attributes are distributed randomly; if you draw an extreme observation from a symmetric distribution, then a subsequent draw is likely to be less extreme, there is a regression to the mean.\nFigure 11.3 displays this phenomenon based on the distribution of HDL cholesterol values in a single person. Our cholesterol levels vary from day to day and if the distribution is normally distributed, it might look like the density in the figure, centered at a mean of 50 mg/dl. Suppose you measure a person’s HDL cholesterol, and it results in a first measurement of 30 mg/dl. The value is on the low side of the distribution, but it is not implausible. A follow-up measurement is more likely to be observed near the center of the distribution where most of the probability density is located. A second measurement might thus return a value of 55 mg/dl. A regression to the mean occurred between the first and second measurements.\n\n\n\n\n\n\nFigure 11.3: An example of regression to the mean. An extreme observation is more likely to be followed by a less extreme observation, one that falls near the center of the distribution.\n\n\n\nAnother fun example of regression to the mean is answering multiple-choice questions on an exam at random. If you administer the exam, all students answer questions completely at random, and you choose the students with the top 10% of scores to take the exam again—choosing answers at random again—then their scores will regress to the mean. The top 10% in the first test will get about ½ the questions correct. Their initial high scores were due to luck.\nThe term regression has evolved to describe statistical methods that model the mean behavior of an attribute and separate it from non-systematic, random behavior. The regression to the mean phenomenon does not mean regression methods are bad. It is a fact of random variation and a real fallacy when interpreting data. In the cholesterol example, is the change from 30 to 50 mg/dl of HDL due to natural variation or due to a systematic effect, for example, a medical intervention? Regression is intended to separate signal from noise. The regression to the mean fallacy is to misinterpret random variation as a signal.\n\n\nExample: Testing a Treatment Effect\n\n\nYou want to determine whether a change in diet reduces the risk of heart disease. From a group of individuals, you select those at greatest risk of heart disease and put them on the diet. After some time, you reassess their risk of heart disease.\nBecause of regression to the mean, the follow-ups will likely show an improvement even if the diet has no effect at all. The correct way of studying whether the diet has an effect is to randomly divide the individuals into two groups and assign the diet to one group (the treated group) while the other group stays on their normal diet. If the individuals in the treated group improve more than the untreated group, to a degree that cannot be attributed just to chance, then we can make a statement that the diet is effective in reducing the risk of heart disease.\n\n\n\n\nRegression Models\n\n\nDefinition: Regression Model\n\n\nA regression model is a statistical model that describes how the mean of a random variable depends on other factors. The factors are often called inputs, predictor variables, predictors, or independent variables.\nThe variable whose mean is modeled is called the target variable, response variable, or dependent variable.\n\n\nRegression models are not just for continuous response data, they apply to all response types. The defining characteristic is to model the mean as a function of inputs:\n\\[\\text{E}\\lbrack Y\\rbrack = f\\left( x_{1},\\cdots,x_{p},\\theta_{1},\\cdots,\\theta_{k} \\right)\\]\nThis expression explicitly lists parameters \\(\\theta_{1},\\cdots,\\theta_{k}\\) in the mean function. All regression models involve the estimation of unknown, fixed quantities (=parameters), even if they do so obliquely. As we have seen in the non-linear regression example earlier, the number of parameters and the number of inputs do not have to be directly related.\nEven if the target variable is categorical, we might be interested in modeling the mean of the variable. The simplest case of categorical variables are binary variables with two levels (two categories). This is the domain of logistic regression. As seen earlier, if the categories are coded numerically as \\(Y = 1\\) for the category of interest (the “event”) and \\(Y = 0\\) for the “non-event” category, the mean of \\(Y\\) is a probability. A regression model for a binary target variable is thus a model to predict probabilities. It is sufficient to predict one of the probabilities in the binary context, we call this the event probability \\(\\pi\\). The complement can be obtained by subtraction, \\(1 - \\pi\\).\nExtending this principle to more than two categories leads to regression models for multinomial data. If the category variable has \\(k\\) levels with labels \\(C_{1},\\cdots,C_{k}\\), we are dealing with \\(k\\) probabilities; \\(\\pi_{j}\\) is the probability to observe the label \\(C_{j}\\). Suppose that we are collecting data on ice cream preferences on college campuses. A random sample of students are given three ice cream brands in a random order and report the taste as \\(C_{1} =\\)’yuck’, \\(C_{2} =\\)’meh’, and \\(C_{3} =\\)’great’. Modeling these data with regression techniques, we develop a model for the probability to observe category \\(j\\) as a function of inputs. A multinomial version of logistic regression looks like the following:\n\\[\\text{Pr}\\left( Y = j \\right) =\n\\frac{\\exp\\left\\{ \\beta_{0j} + \\beta_{1j}x_1 + \\cdots + \\beta_{pj}x_p \\right\\}}\n     {\\sum_{l=1}^k \\exp\\{\\beta_{0l} + \\beta_{1l}x_1 + \\cdots + \\beta_{pl}x_p\\}}\n\\]\nThis is a rather complicated model, but we will see later that it is a straightforward generalization of the two-level case. Instead of one linear predictor we now have separates predictors for the categories. The point of introducing the model here is to show that even in the categorical case we can apply regression methods—they predict category probabilities rather than the mean of a continuous variable.\n\n\nClassification Problems\nClassification applies to categorical variables, binary and multinomial variables that take on a discrete number of categories, \\(k\\). In the binary case \\(k = 2\\), and in the multinomial case \\(k &gt; 2\\).\nThe classification problem is to predict not the mean of the variable but to assign a category to an observation. The algorithm that maps from input variables to a category is called a classifier and the assignment decision is called the classification rule. Applications of classifications occur in many domains, for example,\n\nMedical diagnosis: Given a patient’s symptoms, assign a medical condition.\nFinancial services: Determine whether a payment transaction is fraudulent.\nCustomer intelligence: Assign a new customer to a customer profile (segmentation).\nComputer vision: Detect defective items on an assembly line.\nComputer vision: Identify objects in an image.\nText classification: Categorize incoming emails as spam.\nDigital marketing: predict which advertisement a user is most likely to click.\nSearch engine: Given a user’s query and search history predict what link they will follow.\n\nClassification problems are also interested in predicting. Rather than the mean of a random variable, they predict the membership in a category.\nWhile the mean-squared prediction error is the standard measure of model performance in regression models, in classification models the quality of a classifier is measured by the misclassification rate (MCR) and related statistics based on contrasting the number of correct and incorrect classifications.\n\nMisclassification rate\n\n\nDefinition: Misclassification Rate\n\n\nThe misclassification rate (MCR) of a classifier is the proportion of observations that are predicted to fall into the wrong category. If \\(y_{i}\\) is the observed category of the \\(i\\)th data point, and \\({\\widehat{y}}_{i}\\) is the predicted category, the MCR for a sample of \\(n\\) observations is\n\\[\\text{MCR} = \\frac{1}{n}\\sum_{i = 1}^{n}{I\\left( y_{i} \\neq {\\widehat{y}}_{i} \\right)}\\]\n\\(I(x)\\) is the indicator function,\n\\[I(x) = \\left\\{ \\begin{matrix} 1 & \\text{if }x\\text{ is true} \\\\ 0 & \\text{otherwise} \\end{matrix} \\right. \\]\n\n\nThe misclassification rate is simply the proportion of observations we predicted incorrectly. the term \\(\\sum_{i = 1}^{n}{I\\left( y_{i} \\neq {\\widehat{y}}_{i} \\right)}\\) counts the number of incorrect predictions. The complement of MCR, the proportion predicted correctly, is called the accuracy of the classification model.\n\n\nFrom probabilities to classification\nRegression methods play an important role in classification problems because classification rules are tied to the likelihood to observe categories. Suppose we have a three-category problem with \\(\\pi_{1} = 0.7,\\ \\pi_{2} = 0.2,\\pi_{3} = 0.1\\), and you are asked to predict the category of the next randomly drawn observation. The most likely category to appear is \\(C_{1}.\\)\nThis classification rule is known as the Bayes classifier.\n\n\nDefinition: Bayes Classifier\n\n\nThe Bayes classifier assigns an observation with inputs \\(x_{1},\\cdots,x_{p}\\) to the class \\(C_{j}\\) for which\n\\[\\Pr(Y = j \\, | \\, x_{1},\\cdots,x_{p})\\]\nis largest.\n\n\nThe Bayes classifier is written as a conditional probability, the probability to observe category \\(C_{j}\\), given the values of the input variables. The reason for this will become clearer later when we cover different methods for obtaining category probabilities. Some methods for deriving category probabilities assume that the \\(X\\)s are random. In regression problems it is assumed that they are fixed, so there is no difference between the unconditional probability \\(\\Pr\\left( Y = j \\right)\\) and the conditional probability \\(\\Pr( Y = j \\, | \\, x_1,\\cdots,x_p)\\).\nWe can now see the connection between regression and classification. Develop first a regression model that predicts the category probabilities \\(\\Pr( Y = j \\, | \\, x_1,\\cdots,x_p)\\). Then apply a classification rule to assign a category based on the predicted probabilities. If you go with the Bayes classifier, you choose the category that has the highest predicted probability. For a 2-category problem where events are coded as \\(Y=1\\) and non-events are coded as \\(Y=0\\), this means classifying an observation as an event if\n\\[\\Pr\\left( Y = 1\\, | \\,x_1,\\cdots,x_p \\right) \\geq 0.5\\]",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>General Topics</span>"
    ]
  },
  {
    "objectID": "statlearning/general.html#prediction-and-explanation",
    "href": "statlearning/general.html#prediction-and-explanation",
    "title": "11  General Topics",
    "section": "11.5 Prediction and Explanation",
    "text": "11.5 Prediction and Explanation\nThe goal in developing models is to perform inference, to reach conclusions and make decisions based on data. Broadly, the goals fall into two categories:\n\nPredictive inference: concerned with developing an algorithm that predicts the target variable well and generalizes to observations not used in training the model.\nExplanatory inference: also called confirmatory inference, it is concerned with understanding the relationship between target and input variables, understanding the relevance of the inputs, and testing hypotheses about the target variable.\n\nIn machine learning, the term inference is used to describe the process of predicting new observations after training a model. Statisticians call this part of data analytics scoring the model. The predicted value is the “score” associated with the new observation. Our view of inference is broader than just predicting (scoring) observations. It includes any application of the trained model to derive information of interest: hypothesis testing, confidence and prediction intervals, predicted values, forecasts, etc.\nData projects are not necessarily either predictive or confirmatory. Many projects have elements of both, as in the following example.\n\n\nExample: Dose-response study of insect mortality\n\n\nFigure 11.4 shows logits of sample proportions in a dose-response study of insect larvae mortality as a function of the concentration of an insecticide. Suppose \\(y_{i}\\) denotes the number of larvae out of \\(n_{i}\\) that succumb to the insecticide at concentration \\(x_{i}\\). The right panel of the figure shows the logit of the sample proportion \\(p_{i} = \\frac{y_{i}}{n_{i}}\\),\n\\[\\log\\left\\{ \\frac{p_{i}}{1 - p_{i}} \\right\\}\\]\nas a function of the log insecticide concentration. A simple linear model seems appropriate,\n\\[\\log\\left\\{ \\frac{\\text{E}\\lbrack p_{i}\\rbrack}{1 - \\text{E}\\lbrack p_{i}\\rbrack} \\right\\} = \\beta_0 + \\beta_1\\log_{10}x_i\\]\nNote that the expected value \\(\\text{E}\\left\\lbrack p_{i} \\right\\rbrack\\) is a probability. This model is a generalization of logistic regression for binary data (a 0/1 response) to binomial sample proportions. \\(\\text{E}\\left\\lbrack p_{i} \\right\\rbrack = \\pi_{i}\\) is the probability that an insect dies when \\(\\log_{10}x_{i}\\) amount of insecticide is applied.\n\n\n\n\n\n\nFigure 11.4: Logits of larvae mortality sample proportion against insecticide concentration and log concentration.\n\n\n\nThe investigators want to understand the relationship between larvae mortality and insecticide concentration. The parameter estimate for \\(\\beta_{1}\\) is of interest, it describes the change in logits that corresponds to a unit-level change in the log concentration. A hypothesis test for \\(\\beta_{1}\\) might compare the dose-response in this study with the known dose-response slope \\(c\\) of a standard insecticide. The null hypothesis of this test specifies that the insecticide is as effective as the standard:\n\\[H_{0}:\\beta_{1} = c\\]\nAnother value of interest in dose-response studies is the concentration that achieves a specified effect. For example, the lethal dosage \\(LD_{50}\\) is the concentration that kills 50% of the subjects. Determining the \\(LD_{50}\\) value is known as an inverse prediction problem: rather than predicting \\(\\text{E}\\lbrack Y\\rbrack\\) for a given value of \\(X\\), we are interested in finding the value \\(X\\) that corresponds to a given a value of \\(\\text{E}\\lbrack Y\\rbrack\\).\n\n\n\n\n\n\nFigure 11.5: Fitted dose-response curve and inverse prediction of $LD_{50}$. The $LD_{50}$ is calculated from the value where the vertical line intersects the horizontal axis.\n\n\n\nThe \\(LD_{50}\\) value can be calculated from the model equation. More generally, we can find any value on the x-axis that corresponds to a particular mortality rate \\(\\alpha\\) by solving the following equation for \\(\\alpha\\):\n\\[\\text{logit}(\\alpha) = \\log\\left\\{ \\frac{\\alpha}{1 - \\alpha} \\right\\} = \\beta_0 + \\beta_1\\log_{10}x_\\alpha\\]\nThe solution is\n\\[x_{\\alpha} = 10^{\\frac{\\left( \\text{logit}(\\alpha) - \\beta_{0} \\right)}{\\beta_{1}}}\\]\nFor the special value \\(\\alpha = 0.5\\), the \\(LD_{50}\\) results,\n\\[LD_{50} = 10^{\\frac{- \\beta_{0}}{\\beta_{1}}}\\]\nIn addition to hypothesis testing about \\(\\beta_{1}\\) and calculating the \\(LD_{50}\\), the investigators are also interested in predicting the mortality rate at concentrations not used in the study.\nThe inference in the study has explanatory (confirmatory) and predictive elements.\n\n\nIt is important to point out that many studies are not completely confirmatory or predictive, because models that are good at confirmatory inference are not necessarily good at predicting. Similarly, models that predict well are not necessarily good at testing hypotheses. Interpretability of the model parameters is important for confirmatory inference because hypotheses about the real world are cast as statements about the model parameters. Many disciplines place a premium on interpretability, e.g., biology, life sciences, economics, physical sciences, geosciences, natural resources, financial services. Experiments designed to answer specific questions rely on analytic methods designed for confirmatory inference.\nInterpretability of the model parameters might not be important for a predictive model. A biased estimator that reduces variability and leads to a lower mean-squared prediction error (see the next section) can be appealing in a predictive model but can be unacceptable in a project where confirmatory inference is the primary focus.",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>General Topics</span>"
    ]
  },
  {
    "objectID": "statlearning/general.html#correlation-and-causation",
    "href": "statlearning/general.html#correlation-and-causation",
    "title": "11  General Topics",
    "section": "11.6 Correlation and Causation",
    "text": "11.6 Correlation and Causation\n\nCorrelation\nYou learn in any basic statistics course that ``correlation is not causation’’. Two random variables are correlated if they vary together, values of one variable tend to be associated with certain values of the other variable.\n\n\nDefinition: Correlation\n\n\nThe correlation between random variables \\(X\\) and \\(Y\\), denoted \\(\\rho_{xy}\\) or \\(\\text{Corr}(X,Y)\\), is the ratio of their covariance, \\(\\text{Cov}(X,Y)\\), and the product of their standard deviations:\n\\[\\text{Cov}(X,Y) = \\text{E}\\lbrack\\left( X - \\text{E}\\lbrack X\\rbrack \\right)\\left( Y - \\text{E}\\lbrack Y\\rbrack \\right) = \\text{E}\\lbrack XY\\rbrack - \\text{E}\\lbrack X\\rbrack\\text{E}\\lbrack Y\\rbrack\\]\n\\[\\rho_{xy} = \\text{Corr}(X,Y) = \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}\\lbrack X\\rbrack\\text{Var}\\lbrack Y\\rbrack}}\\]\nWhen the correlation between \\(X\\) and \\(Y\\) is non-zero, we say that \\(X\\) and \\(Y\\) are related to each other or are associated with each other. The covariance measures how \\(X\\) and \\(Y\\) vary jointly: as \\(X\\) deviates from its mean, how does \\(Y\\) change relative to its mean? When large values of \\(X\\) are associated with large values of \\(Y\\), the correlation is positive. Dividing by the product of the standard deviations scales the correlation so that \\(- 1 \\leq \\rho_{xy} \\leq 1\\).\n\n\nThe correlation—like the covariance—is an expected value, it describes long-run behavior of the joint distribution of \\(X\\) and \\(Y\\). The correlation is not directly knowable and is estimated from pairs of observations \\((x_1, y_1),\\cdots,\\ (x_n, y_n)\\). The most common estimator when \\(X\\) and \\(Y\\) are continuous random variables is the Pearson product-moment correlation coefficient.\n\n\nDefinition: Pearson product-moment correlation coefficient\n\n\nThe Pearson product-moment estimate of the correlation \\(\\text{Corr}(X,Y),\\) based on a sample \\((x_1, y_1),\\cdots,(x_n,y_n),\\) is given by\n\\[{\\widehat{\\rho}}_{xy} = \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}\\]\n\\[S_{xy} = \\sum_{i = 1}^{n}{\\left( x_{i} - \\overline{x} \\right)\\left( y_{i} - \\overline{y} \\right) = \\sum_{i = 1}^{n}{x_{i}y_{i}} - n\\overline{x}\\overline{y}}\\]\n\\[S_{xx} = \\sum_{i = 1}^{n}{\\left( x_{i} - \\overline{x} \\right)^{2} = \\sum_{i = 1}^{n}x_{i}^{2} - n{\\overline{x}}^{2}}\\]\n\\[S_{yy} = \\sum_{i = 1}^{n}{\\left( y_{i} - \\overline{y} \\right)^{2} = \\sum_{i = 1}^{n}y_{i}^{2} - n{\\overline{y}}^{2}}\\]\n\\(S_{xx}\\) and \\(S_{yy}\\) are called the observed sum of squares of \\(X\\) and \\(Y\\), respectively. \\(S_{xy}\\) is the observed sum of cross-products between the variables.\n\n\nIf \\(X\\) and \\(Y\\) are correlated, we are very careful not to say that \\(X\\) causes \\(Y\\) or that \\(Y\\) causes \\(X\\). The crow of a rooster and the rising of the sun are correlated, but the rooster’s crow does not cause the sun to rise.\nWhen one event is the result of another event, the events have a cause—effect relationship. Flipping a light switch causes the light to turn on or off. Taking ibuprofen causes the inflammation to subside. Nothing in the definition or formulas for \\(\\rho_{xy}\\) or \\({\\widehat{\\rho}}_{xy}\\) implies causation.\n\n\nSpurious Correlation\nCorrelation itself is not a reliable concept either. Correlation can be the result of a direct relationship between the variables, or it can be induced by mediating or latent (confounding) variables. Correlations that are not the result of direct relationships are called spurious.\n\n\nExample: Donuts and High School Graduation\n\n\nThe number of high school graduates and donut consumption are positively correlated—with increasing donut consumption the number of high school graduates increases. This is a spurious correlation induced by the latent variable population size. Both variables increase over time with an increasing population. Even if the proportion of high school graduates and the donut consumption per person are unrelated, the total numbers will be higher in a larger population.\n.\n\n\n\n\nExample: Confounded Customer Churn\n\n\nA common request to the data science team is to use data analytics to improve customer retention. That starts with understanding why customers leave the company (churn).\nAn analysis of historical customer data reveals a positive correlation between churn rate and discounts offered. This association is induced by a confounding factor: customer satisfaction. Customers who are dissatisfied are more likely to complain to customer service, which results in discount offers to entice the complaining customer to stay with the vendor. The discount offer cannot overcome the customer’s dissatisfaction and they churn the company. Without understanding the confounding factor—customer satisfaction—an analysis of the raw data could suggest that higher discount rates lead to higher customer attrition.\n\n\n\n\nExample: Storks and Babies\n\n\nIn central Europe a persistent myth is that storks bring babies. The origin of the association probably goes back to medieval days when conception was more common in mid-summer during the celebration of the summer solstice which is also a pagan holiday of marriage and fertility. The white stork is a migratory bird that flies to Africa in the fall and returns to Europe nine months later. The return of the storks coincided with the arrival of newborns; the connection was made that storks brought the babies.\nAlthough the myth has been debunked, there have been several studies of the connection between fertility and the stork abundance. Neyman (1952) describes a study of 54 counties that comprises the following attributes:\n\\(W\\): Number of women of child-bearing age in the county (in 10,000)\n\\(S\\): Number of storks in the county\n\\(B\\): Number of babies born in the county\nSince it is likely that these numbers increase with the size of the county, the variables analyzed were \\(Y = B/W\\) and \\(X = S/W\\), the birth rate per 10,000 women and the density of storks per 10,000 women. Figure 3.9 reproduces the scatterplot of \\(Y\\) vs \\(X\\).\nThere seems to be a clear trend, the birth rate increases with the stork density. Is that evidence the myth is correct after all?\nThe reason for the apparent trend is the use of \\(W\\) in the denominator of both \\(X\\) and \\(Y\\). Even if \\(S\\) and \\(B\\) are unrelated, the ratio with a common variable induces a correlation between \\(X\\) and \\(Y\\).\n\n\nThere are many examples of spurious correlations—from hilarious to frightening—and these are often trotted out to try and explain the difference between correlation and causation. The debate between correlation and causation is not because correlations can be spurious. The rooster crows when the sun rises, this is not a spurious relationship. But the rooster does not cause the sun to rise. Correlation of any kind, true dependence or spurious, does not imply causation—but then what does?\nSmoking causes lung cancer but smoking does not cause alcoholism. However, there is an association (correlation) between smoking and alcoholics. How did we establish causation in one instance and correlation in the other? The battle for the statement “smoking causes lung cancer” raged for many years and it was not straightforward to settle the question based on the established methodology for establishing causation.\n\n\nExperimentation\nStatisticians are taught that causation can only be established by following rigorous principles of designed experiments, systematically varying factors of interest (treatments) and controlling other factors in a scheme that allocates experimental units at random to treatments. In a designed experiment we control the data-generating mechanism and create the data to measure the effect of an intervention. Because we are manipulating the environment, and because influences other than the ones we are interested in are neutralized by randomization, can we answer the question why something happened with confidence.\nOn the other hand, when we only observe the outcome of a data-generating process, rather than making the data-generating process, we cannot say why we observe what we observe. Data in an observational study can tell us that those who took a drug fared better than those who did not take the drug; but the data cannot tell us why they did better. The two groups could be different in other ways that explain a better health outcome.\nThis thinking is deeply embedded in scientific methodology; the line of inquiry whether smoking causes cancer would have been considered unscientific a few decades ago. A designed experiment in which some randomly chosen individuals are forced to smoke is not possible. In the absence of data from a designed experiment it was argued successfully for a long time that cause-and-effect between smoking and lung cancer has not been established—despite overwhelming evidence to the contrary.\nThe difference between the experimental and the observational study is doing versus seeing. Designed experiments are the statistician’s way of “doing”, but it is not always possible to manipulate and intervene with systems in this way. Ethical concerns might rule out giving harmful treatments. Some effects can only be assessed over long periods of time and maintaining control of other effects over time can be difficult. Some systems are altered by interventions in ways that make inferences about the original state meaningless. Some systems defy randomization. If you cannot randomize the stock market, how can we establish that higher returns were caused by a change in trading algorithm? How can we establish that human activity causes climate change? Traditional designed experimentation cannot be used. A prohibition to think about causation unless we are in a randomized controlled trial is not helpful.\nIn our daily lives we make causal inferences, not statistical ones. Human intuition is grounded in causal logic. When I gradually push a book over the edge of a table it will eventually fall off the table, caused by gravity. I do not need to repeat this 20 times to convince me that what was observed—the book fell—was just incredibly unlikely. Human intuition is sufficient to conclude that the physical therapy reduced the pain from tennis elbow. Something was done (to us, physical therapy) and we see the effect (on us, pain reduction). We do not require a statistical experiment to figure out that we have been helped. On the other hand, if we decide among treatment alternatives for tennis elbow, the existence of such experiments can be helpful in deciding on a treatment plan.\n\n\nThe Ladder of Causation\nIn their influential (cult) text “The Book of Why”, Judea Pearl and Dana MacKenzie introduce the Ladder of Causation, three distinct levels of cognitive ability: seeing, doing, and imagining.\n\n\n\n\n\n\nFigure 11.6: The Ladder of Causation according to Pearl and MacKenzie.\n\n\n\n\nFirst rung—seeing\nSeeing (observing) means detection of regularities and irregularities in the environment and acting on it. Most animals have this cognitive ability. An owl observes the movement of its prey and reacts to it. It recognizes regular and irregular patterns—healthy versus unhealthy prey—and changes its reaction to the environment. The first rung of the ladder of causation is where questions are answered by observing. The typical question is “What if I observe …?” A this rung of the ladder we find relationships and associations but cannot establish causality. We cannot determine why something happened.\nModifying an example given by Pearl and MacKenzie, suppose the manager of a grocery store wonders how likely someone who buys diapers also buys a 6-pack of beer. From the database of store sales, we can estimate the probability Pr(customer buys a 6-pack of beer) and the conditional probability Pr(customer buys a 6-pack of beer | customer also bought diapers).\nThe conditional probability is a measure of the association of the two events: buying diapers and buying beer. We cannot learn from this information whether increasing the price of diapers would affect beer sales. Even if the database of store sales contains sales where diapers were more expensive, we cannot conclude an effect on beer sales because the prices could have been higher in the past for other reasons, maybe a diaper supply shortage. The differences we see in the data are not due to the interventions we should have taken to answer the question of interest.\n\n\nSecond rung—doing\nThe second rung of the ladder of causation is reached when we deliberately change the world. As Pearl and MacKenzie put it\n\nSeeing smoke tells us a totally different story about the likelihood of fire than making smoke.\n\nQuestions we answer at this level are “What if we do…?” and “What happens if …?”. The store manager now raises the price of diapers under the current market condition and observes how the sale of beer reacts to the intervention. In particular, they are interested if the conditional probability Pr(beer | diaper) changes. If the store is part of a larger chain, they can run an A/B experiment, raising the price of diapers at some randomly selected stores, and comparing the sales numbers across stores with and without price increase.\n\n\nThird rung—imagining\nThe third rung of the ladder of causation answers a different set of questions, one that requires not just interventions, but theory of interventions that allows us to imagine worlds that have not happened. These “What if…?” questions are called counterfactuals. So we raised the price of diapers and beer sales dropped. Why? What caused that? Was it the change in price of diapers? Was it the cooler weather? Was it the change in the NFL schedule? To answer these questions, we need to imagine and reason about a world where we did not change the price of diapers.\nExperiments cannot answer questions such as “What if I had done…?”; the first two rungs deal with observable phenomenon, either through observing what is or observing an intervention. The final rung of the ladder of causation requires models for the underlying processes, understanding that manifests itself in theories and what we call laws of nature.\n\n\nThe role of data\nInterestingly, Pearl and MacKenzie place artificial intelligence and machine learning, as depicted by the robot, on the first rung of the ladder of causation. By simply observing what is we cannot answer causal “What if” questions at the upper rungs of the ladder: “What if I do …?”, “What if I had done …?”.\nDoes that mean we can never use observational data to make causal statements? Yes, unless the data are supplemented with models and theories that fall outside of the data. The store manager could answer the question ``What happens if I change the price of diapers?’’ without experimentation, based on a model of consumer behavior and market conditions. Combining this model with the observed data on beer—diaper sales can produce a better prediction of the effect on beer sales than the observational data alone, subject to the correctness of the assumed market model.\nWhy are we discussing all this in the context of data science?\nMost of the data you work with is likely observational data, not experimental data. The analysis of observational data with statistical techniques is on the first rung of the ladder of causation. You cannot answer rung-2 ``What if I do..?’’ questions from this data alone; no matter the sophistication of the analytic method. In order to climb the ladder of causation and ask more interesting questions you need to collect data under manipulation or apply an external model that implies manipulation (an economic theory, for example).\nThe belief that with more data we can answer more sophisticated questions and climb the ladder of causality is fundamentally flawed. Answering more sophisticated questions from a higher rung of the ladder requires different cognitive abilities. You cannot answer questions about interventions by analyzing patterns and associations. Understanding and imagination of an autonomous driving system does not come from training on data. It comes from explicit programming—augmenting the information in the data with abilities from a higher rung of the ladder. AI systems trained on data can learn impressive tasks but are limited to tasks that can be learned by watching. They cannot learn tasks based on learning by doing. And they cannot answer counterfactual questions (“What if I had instead done …?”) without understanding and reasoning. AI derived from observational data cannot achieve intelligence.",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>General Topics</span>"
    ]
  },
  {
    "objectID": "statlearning/general.html#bias-variance-tradeoff",
    "href": "statlearning/general.html#bias-variance-tradeoff",
    "title": "11  General Topics",
    "section": "11.7 Bias-Variance Tradeoff",
    "text": "11.7 Bias-Variance Tradeoff\nThe Bias-Variance tradeoff describes a fundamental tension in data science projects. The models we build are approximations because the true relationship between inputs and outputs is not known. If we work with statistical models, then the data-generating mechanism on which the model is based is also an approximation for the true—and unknown—process. The data we work with is typically the result of some selection mechanism. If we were to repeat the selection process different observations result. Apply the same method to a different set of data you will get different answers—there is variability in the results due to the inherent variability in the data.\n\nA Simulation\nTo illustrate the concept, let’s start with a simulated example where we know the true function and collect multiple samples.\nThe following figure shows the relationship between an input variable \\(X\\) and some output function \\(f(x)\\). The function depicts the true relationship, the dots mark design points at which we collect observations. Because the data is inherently variable our sample observations will not fall on the black line. If the sample is unbiased, they should spread evenly about the true trend.\n\n\n\n\n\n\nFigure 11.7: A response function of one input variable.\n\n\n\nSuppose that we repeat the sampling process four times, drawing eleven observations each time.\n\n\n\n\n\n\nFigure 11.8: Four random samples of size eleven.\n\n\n\nThis is an unrealistic situation. In real life, we do not know the solid function \\(f(x)\\) and we draw only one set of data, for example, we would work with only the black triangles or the blue dots in the previous figure.\nNext, we train a model on the data and are considering two types of methods: a linear regression model and a smoothing spline.\n\n\n\n\n\n\nFigure 11.9: Linear regression models fit to the four data sets.\n\n\n\nThe linear regression model is not flexible. It has only two parameters, the intercept of the vertical line at \\(x = 0\\) and the slope of the line. The lines do not follow the curved trend in the function \\(f(x)\\). Because of this rigidity, the four lines are somewhat similar to each other, they do not show a high degree of variability from sample to sample.\n\n\n\n\n\n\nFigure 11.10: Smoothing splines with 6 degrees of freedom fit to the four data sets.\n\n\n\nThe splines show more flexibility than the linear regression lines and follow the observed data more closely. The curviness of the true function \\(f(x)\\) is echoed in the curviness of the splines, but some splines seem to try to connect the dots more than they are picking up the true trend. Because the splines follow the observed data more closely, the four functions show more variability from sample to sample than the linear regression lines.\nSuppose the task is to develop a model that predicts a new observation well, one that did not participate in fitting the model. The model needs to generalize to previously unseen data. Should we choose linear regression or smoothing splines as our method? A method that is highly variable because it follows the data too closely will not generalize well—its predictions will be off because they are highly variable. A method that is not flexible enough also does not generalize well—its predictions will be off because the model is not correct.\nMathematically, we can express the problem of predicting a new observation as follows. Since the true function is unknown, it is also unknown at the new data location \\(x_{0}\\). However, we observed a value \\(y\\) at \\(x_{0}\\). Based on the model we choose the function can be predicted at \\(x_{0}\\). But since we do not know the true function \\(f(x)\\), we can only measure the discrepancy between the value we observe and the value we predicted; this quantity is known as the error of prediction.\n\nComponents that contribute to bias and variance of an estimator. The last column designates whether the quantity can be measured in data science applications.\n\n\n\n\n\n\n\nQuantity\nMeaning\nMeasurable\n\n\n\n\n\\(f(x)\\)\nThe true but unknown function\nNo\n\n\n\\(f\\left( x_{0} \\right)\\)\nThe value of thefunction at a data point \\(x_{0}\\) that was not part of fitting the model\nNo\n\n\n\\(\\widehat{f}\\left( x_{0} \\right)\\)\nThe estimated value of the function at the new data point \\(x_{0}\\)\nYes\n\n\n\\(f\\left( x_{0} \\right) - \\widehat{f}\\left( x_{0} \\right)\\)\nThe function discrepancy\nNo\n\n\n\\(y -\\widehat{f}\\left( x_{0} \\right)\\)\nThe error of prediction\nYes\n\n\n\nMultiple components contribute to the prediction error: the variability of the data \\(y\\), the discrepancy between \\(f\\left( x_{0} \\right)\\) and \\(\\widehat{f}\\left( x_{0} \\right)\\), and the variability of the function \\(\\widehat{f}\\left( x_{0} \\right)\\). The variability of \\(y\\) is also called the irreducible variability or the irreducible error because the observations will vary according to their natural variability. Once we have decided which attribute to observe, how to sample it, and how to measure it, this variability is a given. The other two sources relate to the accuracy and precision of the prediction; or, to use statistical terms, the bias and the variance.\n\n\nAccuracy and Precision\nIn the context of measuring devices, accuracy and precision are defined as\n\nAccuracy: How close are measurements to the true value\nPrecision: How close are measurements to each other\n\nTo demonstrate the difference between accuracy and precision, the dart board bullseye metaphor is helpful. The following figure shows four scenarios of shooting four darts each at a dart board. The goal is to hit the bullseye in the center of the board; the bullseye represents the true value we are trying to measure. A is the result of a thrower who is neither accurate nor precise. The throws vary greatly from each other (lack of precision), and the average location is far from the bullseye. B is the result of a thrower who is inaccurate but precise. The throws group tightly together (high precision) but the average location misses the bullseye (the average distance from the bullseye is not zero). The thrower with pattern C is not precise, but accurate. The throws vary widely (lack of precision) but the average distance of the darts from the bullseye is close to zero—on average the thrower hits the bullseye. Finally, the thrower in D is accurate and precise; the darts group tightly together and are centered around the bullseye.\n\n\n\n\n\n\nFigure 11.11: Accuracy and precision—the dart board bullseye metaphor.\n\n\n\nWe see that both accuracy and precision describe not a single throw, but a pattern over many replications. In statistical terms, this long-run behavior is called an expected value.\n\n\nDefinition: Expected value of a random variable\n\n\nThe expected value of a random variable \\(Y\\) is the mean of the random variable. It describes the long-run central tendency of \\(Y\\) and is calculated as a weighted average of the variable’s value with their likelihood of occurrence. When the expected value is written as an operator, we use the notation \\(\\text{E}\\lbrack Y\\rbrack\\). When using Greek notation, the letter \\(\\mu\\) is commonly used.\nFor a discrete random variable, one that takes on a finite number of possible values with distribution \\(p(y)\\), the expected value is\n\\[\\text{E}\\lbrack Y\\rbrack = \\sum_{y} y \\, p(y)\\]\nFor a random variable with continuous density function \\(h(y)\\), the expected value is\n\\[\\text{E}\\lbrack Y\\rbrack = \\int y\\, g(y)dy\\]\n\n\nWe used the notation \\(g(y)\\) for the density function of the continuous random variable in this definition—rather than the notation \\(f(y)\\) that is common in texts on statistics and probability—to avoid confusion with the function we are trying to estimate.\nRandomness is contagious, if \\(Y\\) is a random variable, then any function of \\(Y\\) is a random variable as well. A special type of expected value of a function of a random variable is the variance.\n\n\nDefinition: Variance of a random variable\n\n\nThe variance of a random variable is the expected value of the squared deviation of the variable from its mean. The variance is often written as an operator, \\(\\text{Var}\\lbrack Y\\rbrack\\). In Greek notation, the expression \\(\\sigma^{2}\\) is common. The variance is defined as\n\\[\\text{Var}\\lbrack Y\\rbrack = \\text{E}\\left\\lbrack \\left(Y - {\\text{E}\\lbrack\n  Y\\rbrack} \\right) ^2 \\right\\rbrack =\n\\text{E}\\left\\lbrack (Y - \\mu^2) \\right\\rbrack\\]\nAn alternative way to write the variance is\n\\[\\text{Var}\\lbrack Y\\rbrack = \\text{E}\\left\\lbrack Y^{2} \\right\\rbrack - {\\text{E}\\lbrack Y\\rbrack}^{2}\\]\n\n\nWhile the expected value (the mean) describes the central tendency of a random variable, its typical value, if you will, the variance measures how a variable is dispersed around its mean.\n\n\nDefinition: Standard deviation of a random variable\n\n\nThe standard deviation of a random variable, often abbreviated \\(\\sigma\\), is the square root of its variance,\n\\[\\sigma = \\sqrt{\\text{Var}\\lbrack Y\\rbrack}\\]\n\n\nThe standard deviation and the variance contain the same information. The standard deviation is measured in the same units as the variable \\(Y\\), the variance is in squared units. If \\(Y\\) is the length of an abalone shell in cm, the variance is in areal units of cm2.\nHow do expectation and variance relate to our discussion of accuracy and precision? The accuracy of a statistical estimator is the proximity of its expected value from the target value. An estimator that is not accurate is said to be biased.\n\n\nDefinition: Bias\n\n\nAn estimator \\(h\\left( \\text{Y} \\right)\\) of the parameter \\(\\theta\\) is said to be biased if its expected value does not equal \\(\\theta\\).\n\\[\\text{Bias}\\left\\lbrack h\\left( \\textbf{Y}\\right);\\theta \\right\\rbrack = \\text{E}\\left\\lbrack h\\left( \\textbf{Y}\\right) - \\theta \\right\\rbrack = \\text{E}\\left\\lbrack h\\left( \\textbf{Y}\\right) \\right\\rbrack - \\theta\\]\n\n\nThe last equality in the definition follows because the expected value of a constant is identical to the constant. In the dartboard example, \\(\\theta\\) is the bullseye and \\(h\\left( \\text{Y} \\right)\\) is the distance of the dart from the bullseye. The bias is the expected value of that distance, the average across many repetitions (dart throws).\n\n\nMean Squared Error\nWith these definitions in place, let’s return to the question whether to favor the linear regression or the smoothing spline to predict a new observation at \\(x_{0}\\)? The model can be written as\n\\[Y = f(x) + \\epsilon\\]\nwhere \\(\\epsilon\\) is a random variable with mean 0 and variance \\(\\sigma^{2}\\), the irreducible variability. The observational model for \\(n\\) observed data points is\n\\[Y_{i} = f\\left( x_{i} \\right) + \\epsilon_{i}\\ \\ \\ \\ \\ \\ \\ \\ \\ i = 1,\\ldots,n\\]\nThe \\(Y_{i}\\) are observed unless there are missing values. However, for a new observation this might not be the case. The model for the new observation is no different than the previous model\n\\[Y_{0} = f\\left( x_{0} \\right) + \\epsilon\\]\nbut only \\(x_{0}\\) is known.\nThere are two possible targets for prediction: \\(f\\left( x_{0} \\right)\\) and \\(f\\left( x_{0} \\right) + \\epsilon\\). The former is the expected value of \\(Y_{0}\\): \\(\\text{E}\\left\\lbrack Y_{0} \\right\\rbrack = f\\left( x_{0} \\right) + \\text{E}\\lbrack\\epsilon\\rbrack = f\\left( x_{0} \\right)\\). This is a fixed quantity (a constant), not a random variable. The latter is a random variable. Interestingly, the estimator of both quantities is the same, \\(\\widehat{f}\\left( x_{0} \\right)\\). The difference comes into play when we consider the uncertainty associated with estimating \\(f\\left( x_{0} \\right)\\) or predicting \\(f\\left( x_{0} \\right) + \\epsilon\\)—more on this later.\nWe need a way to express the discrepancy between the estimator and the target that incorporates the estimator’s accuracy and precision—this is the mean-squared error.\n\n\nDefinition: Mean-squared error (MSE)\n\n\nThe mean-squared error of estimator \\(h\\left( \\textbf{Y}\\right)\\) for target \\(\\theta\\) is\n\\[\\text{MSE}\\left\\lbrack h\\left( \\textbf{Y}\\right);\\ \\theta \\right\\rbrack = \\text{E}\\left\\lbrack \\left( h\\left( \\textbf{Y}\\right) - \\theta \\right)^{2} \\right\\rbrack\\]\n= \\(\\text{E}\\left\\lbrack \\left( h\\left( \\text{Y} \\right) - \\text{E}\\left\\lbrack h\\left( \\textbf{Y}\\right) \\right\\rbrack \\right)^{2} \\right\\rbrack + \\left( \\text{E}\\left\\lbrack h\\left( \\textbf{Y}\\right) \\right\\rbrack - \\theta \\right)^{2}\\)\n\\[= \\text{Var}\\left\\lbrack h\\left( \\textbf{Y}\\right) \\right\\rbrack + \\text{Bias}\\left\\lbrack h\\left( \\textbf{Y}\\right);\\theta \\right\\rbrack^{2}\\]\n\n\nThe mean-squared error is the expected square deviation between the estimator and its target. That is akin to the definition of the variance, but the MSE is only equal to the variance if the estimator is unbiased for the target. As the last line of the definition shows, the MSE has two components, the variability of the estimator and the squared bias. The bias enters in squared terms because the variance is measured in squared units and because negative and positive bias discrepancies should not balance out.\nIf we apply the MSE definition to the problem of using estimator \\(\\widehat{f}\\left( x_{0} \\right)\\) to predict \\(f\\left( x_0 \\right)\\),\n\\[\\text{MSE}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);f\\left( x_{0} \\right)\\  \\right\\rbrack = \\text{Var}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right) \\right\\rbrack + \\text{Bias}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);f\\left( x_{0} \\right) \\right\\rbrack^{2}\\]\nwe see how the variability of the estimator and its squared bias contribute to the overall MSE. Similarly, if the target is to predict the new observation, rather than its mean, the expression becomes\n\\[\\text{MSE}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);Y_{0} \\right\\rbrack\\text{ = MSE}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);f\\left( x_{0} \\right) + \\epsilon\\  \\right\\rbrack = \\text{Var}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right) \\right\\rbrack + \\text{Bias}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);f\\left( x_{0} \\right) \\right\\rbrack^{2} + \\sigma^{2}\\]\nYou now see why \\(\\sigma^{2}\\) is called the irreducible error. Even if the estimator \\(\\widehat{f}\\left( x_{0} \\right)\\) would have no variability and be unbiased, the mean-squared error in predicting \\(Y_{0}\\) can never be smaller than \\(\\sigma^{2}\\).\n\n\nExample: \\(k\\)-Nearest Neighbor Regression\n\n\nThe \\(k\\)-nearest neighbor (\\(k\\)-NN for short) regression estimator is a simple estimator of the local structure between a target variable \\(y\\) and an input variable \\(x\\). The value \\(k\\) represents the number of values in the neighborhood of some target input \\(x_{0}\\) that are used to predict \\(y\\). The extreme case is \\(k = 1\\), the value of \\(f\\left( x_{0} \\right)\\) is predicted as the \\(y\\)-value of the observation closest to \\(x_{0}\\).\nSuppose our data come from a distribution with mean \\(f(x)\\) and variance \\(\\sigma^{2}\\). The mean-square error decomposition for the \\(k\\)-NN estimator is then\n\\[\\text{MSE}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);Y_{0} \\right\\rbrack\\text{ = }\\frac{\\sigma^{2}}{k}{+ \\left\\lbrack f\\left( x_{0} \\right) - \\frac{1}{k}\\sum_{}^{}Y_{(i)} \\right\\rbrack}^{2} + \\sigma^{2}\\]\nwhere \\(y_{(i)}\\) denotes the \\(k\\) observations in the neighborhood of \\(x_{0}\\).\nThe three components of the MSE decomposition are easily identified:\n\n\\(\\sigma^{2}/k\\) is the variance of the estimator, \\(\\text{Var}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right) \\right\\rbrack\\). Not surprisingly, it is the variance of the sample mean of \\(k\\) observations drawn at random from a population with variance \\(\\sigma^{2}\\).\n\\(\\left\\lbrack f\\left( x_{0} \\right) - \\frac{1}{k}\\sum Y_{(i)} \\right\\rbrack^{2}\\) is the squared bias component of the MSE.\n\\(\\sigma^2\\) is the irreducible error, the variance in the population from which the data are drawn.\n\nWhile we cannot affect the irreducible error \\(\\sigma^{2}\\), we can control the magnitude of the other components through the choice of \\(k\\). The variance contribution will be largest for \\(k = 1\\), when prediction relies on only the observation closest to \\(x_{0}\\). The bias contribution for this 1-NN estimator is \\(\\left\\lbrack f\\left( x_{0} \\right) - Y_{(1)} \\right\\rbrack^{2}\\).\nAs \\(k\\) increases, the variance of the estimator decreases. For a large enough value of \\(k\\), all observations are included in the “neighborhood” and the estimator is equal to \\(\\overline{Y}\\). If \\(f(x)\\) changes with \\(x\\), the nearest neighbor method will then have smallest variance but large bias.\n\n\nIf we want to minimize the mean-squared error, we can strive for estimators with low bias and low variance. If we cannot have both, how do we balance between the bias and variance component of an estimator? That is the bias-variance tradeoff.\nStatisticians resolve the tension with the UMVUE principle. Uniformly minimum-variance unbiased estimation requires to first identify unbiased estimators, those which \\(\\text{Bias}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);f\\left( x_{0} \\right) \\right\\rbrack = 0\\), and then to select the estimator with the smallest variance among the unbiased estimators. According to UMVUE you will never consider a biased estimator. It is comforting to know that on average the estimator will be on target. This principle would select estimator C in the dartboard example over estimator B because the latter is biased. If you have only one dart left and you need to get as close to the bullseye as possible, would you ask player B or player C to take the shot for your team?\nUMVU estimators are not necessarily minimum mean-squared error estimators. It is possible that a biased estimator has a sharply reduced variance so that the sum of variance and squared bias is smaller than the variance of the best unbiased estimator. If we want to achieve a small mean-square error, then we should consider estimators with some bias and small variance. Resolving the bias-variance tradeoff by eliminating all biased estimators does not lead to the “best” predictive models. Of course, this depends on our definition of “best”.\nIn practice, \\(f\\left( x_{0} \\right)\\) is not known and the bias component \\(\\text{Bias}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);f\\left( x_{0} \\right) \\right\\rbrack\\) cannot be evaluated by computing the difference of expected values. For many modeling techniques we can calculate—or at least estimate— \\(\\text{Var}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right) \\right\\rbrack\\), the variance component of the MSE. Those derivations depend on strong assumptions about distributional properties and the correctness of the model. So, we essentially need to treat the MSE as an unknown quantity. Fortunately, we can estimate it from data.\n\n\nDefinition: Mean-squared prediction error (MSPE)\n\n\nThe mean-squared prediction error (MSPE) is the average squared prediction error in a sample of \\(n\\) observations,\n\\[MSPE = \\frac{1}{n}\\sum_{i=1}^n\\left( y_i - \\widehat{f}\\left( x_i \\right) \\right)^{2}\\]\n\n\nTaking the sample average replaces taking formal expectations over the distribution of \\(( Y - \\widehat{f}(x) )^2\\).\nBack to choosing between the regression and spline models. If we denote the two approaches \\(\\widehat{f}_{r}(x)\\) and \\(\\widehat{f}_{s}(x)\\), respectively, selecting the winning model based on the mean-squared prediction error reduces to picking the model with the smaller MSPE:\n\\[\\frac{1}{n}\\sum_{i = 1}^{n}\\left( y_{i} - {\\widehat{f}}_{r}\\left( x_{i} \\right) \\right)^{2}\\]\nor\n\\[\\frac{1}{n}\\sum_{i = 1}^{n}\\left( y_{i} - {\\widehat{f}}_{s}\\left( x_{i} \\right) \\right)^{2}\\]\nAs we will see, this is not without problems. These expressions are calculating the MSPE by averaging over the data points used in training the model; we call this the MSPE of the training set or MSETr for short. To identify models that generalize well to new observations, it is recommended to calculate the MSPE across a test set of observations that was not used to fit the model; this is called the MSPE of the test set or the MSETe for short.\nWe will discuss training, test, and validation data sets in more detail below.\nWhether you are working with MSPE in a regression context or MCR in a classification problem, the goal is to develop a model that is neither too complex nor too simple. We want to avoid over- and underfitting the model.\n\n\nOverfitting and Underfitting\nThe preceding discussion might suggest that flexible models such as the smoothing spline have high variability and that rigid models such as the simple linear regression model have large bias. This generalization does not necessarily hold although in practice it often works out this way. The reason for this is not that simple linear regression models are biased—they can be unbiased. The reason why flexible models tend to have high variance and low bias and rigid models tend to have low variance and high bias has to do with overfitting and underfitting.\nAn overfit model follows the observed data \\(Y_{i}\\) too closely and does not capture the mean trend \\(f(x)\\). The overfit model memorizes the training data too much. When you predict a new observation with an overfit model that memory causes high variability. Remember that the variability we are focusing on here is the variability across repetitions of the sample process. Imagine drawing 1,000 sets of \\(n\\) observations, repeating the model training and predicting from each model at the new location \\(x_{0}\\). We now have 1,000 predictions at \\(x_{0}\\). Because the overfit model follows the training data too closely, its predictions will be variable at \\(x_{0}\\).\nAn underfit model, on the other hand, lacks the flexibility to capture the mean trend \\(f(x)\\). Underfit models result, for example, when important predictor variables are not included in the model.\nThe most extreme case of overfitting a model is the saturated model. It perfectly predicts the observed data. Suppose you collect only two pairs of \\((x,y)\\) data: (1,0) and (2,1). A two-parameter straight line model will fit these data perfectly. The straight line has an intercept of –1 and a slope of +1. It passes through the observed points and the mean-squared prediction error is zero.\n\n\n\n\n\n\nFigure 11.12: A straight line model saturates a data set with two $(x,y)$ pairs. The difference between observed values (the dots) and the predicted values (values on the line) is zero at each point. The saturated model has a MSPE of zero.\n\n\n\nSaturated models are not very interesting, they are just a re-parameterization of the data, capturing both signal \\(f(x)\\) and noise \\(\\epsilon\\). A useful model separates the signal from the noise. Saturated models are used behind the scenes of some statistical estimation methods, for example to measure how much of the variability in the data is captured by a model—this type of model metric is known as the deviance. Saturated models are never the end goal of data analytics.\nOn the other extreme lies the constant model; it does not use any input variables. It assumes that the mean of the target variable is the same everywhere:\n\\[Y_{i} = \\mu + \\epsilon_{i}\\]\nThis model, also known as the intercept-only model, is slightly more useful than the saturated model. It is rarely the appropriate model in data science applications; it expresses the signal as a flat line, the least flexible model of all.\nIn our discussion of the model building process during the data science project life cycle we encountered an example of pharmacokinetic data, 500 observations on how a drug is absorbed and eliminated by the body over time (\\(t\\)). The data are replayed in the next figure along with the fit of the constant model. The constant model underpredicts the drug concentration between times \\(t = 3\\) and \\(t = 12\\) and overpredicts everywhere else.\n\n\n\n\n\n\nFigure 11.13: Concentration of a drug in patient’s bodies over time.\n\n\n\nSuppose we draw 1,000 sets of \\(n = 500\\) observations, fit the constant model to each, and predict at the new time \\(t_{0}\\). Because the constant model does not depend on time, we get the same predicted value regardless of the value of \\(t_{0}\\). In each sample of size \\(n\\), the predicted value will be the sample mean, \\(\\overline{y} = \\frac{1}{500}\\sum_{}^{}y_{i}\\). The variability of the 1,000 predictions will be small; it is the variance of the sample mean:\n\\[\\text{Var}\\left\\lbrack \\widehat{f}\\left( x_0 \\right) \\right\\rbrack = \\frac{\\sigma^2}{500}\\]\nIf the true model does depend on \\(t\\)—and the plot of the data suggests this is the case—the bias of the predictions will be large. The mean-squared prediction error is dominated by the squared bias component in this case.\nSomewhere between the two extremes of a hopelessly overfit saturated model and a hopelessly underfit constant model are models that capture the signal \\(f(x)\\) well enough without chasing the noisy signal \\(f(x) + \\epsilon\\) too much. Those models permit a small amount of bias if that results in a reduction of the variance of the predictions.\nTo summarize,\n\nOverfit models do not generalize well because they follow the training data too closely. They tend to have low bias and a large variance.\nUnderfit models do not generalize well because they do not capture the salient trend (signal) in the data. They tend to have high bias and low variance.\nA large mean-squared prediction error can result in either case but is due to a different cause.\nFor a small mean-squared prediction error you need to have small bias and small variance.\nIn practice, zero-bias methods with high variance are rarely the winning approaches. The best MSPE is often achieved by allowing some bias to substantially decrease the variance.\n\nThe danger of overfitting is large when models contain many parameters, and when the number of parameters \\(p\\) is large relative to the sample size \\(n\\). When many attributes (inputs) are available and you throw them all into the model, the result will likely be an overfit model that does not generalize well. It will have a large prediction error. In other words, there is a cost to adding unimportant information to a model. Methods for dealing with such high-dimensional problems play an important role in statistics and machine learning and are discussed in detail in a more advanced section. We mention here briefly:\n\nFeature Selection: Structured approaches that use algorithms to determine which subset of the inputs should be in the model. The decision is binary in that an input is either included or excluded. Also known as variable selection.\nRegularization: Deliberately introducing some bias in the estimation through penalty terms that control the variability of the model parameters which in turn controls the variability of the predictions. The parameters are shrunk toward zero in absolute value compared to an unbiased estimator—regularization is thus also known as shrinkage estimation. The Lasso methods can shrink parameters to zero and thus combines regularization with feature selection. The Ridge regression methods also applies a shrinkage penalty but allows all inputs to contribute.\nEnsemble Methods: Ensemble methods combine multiple methods into an overall, averaged prediction or classification. Ensembles can be homogeneous, where the methods are the same, or heterogeneous. An example of a homogeneous ensemble is a bagged decision tree, where several hundred individual trees are trained independently and the predictions from the trees are averaged to obtain an overall predicted value. Due to averaging, the variance of the ensemble estimator is smaller than any individual estimator. Bagging and boosting are common ensemble methods to reduce variance.",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>General Topics</span>"
    ]
  },
  {
    "objectID": "statlearning/general.html#training-testing-and-validation",
    "href": "statlearning/general.html#training-testing-and-validation",
    "title": "11  General Topics",
    "section": "11.8 Training, Testing, and Validation",
    "text": "11.8 Training, Testing, and Validation\nTraining, testing, and validation refers to different stages of the model building process and also to different types of data used in the model building process.\n\nTraining Data\nTraining data is the set of \\(n\\) observations used to train the model. The training data is useful to diagnose whether model assumptions are met, for example,\n\ndoes the model adequately describe the mean trend in the (training) data,\nare distributional assumptions such as normality of the model errors met,\nis it reasonable to assume that the data points are uncorrelated (or even independent)\n\nWe can also use the training data after the model fit to detect data points that have a high influence of the analysis—that is, the presence of those points substantially affects an important aspect of the model. And based on the training data we can study the interdependence of the model inputs and whether those relationships affect the model performance negatively.\nThe diagnostic techniques just mentioned rely on\n\nResidual diagnostics\nCase-deletion and influence diagnostics\nCollinearity diagnostics\n\nThese diagnostics are all very helpful, but they do not answer an important question: how well does the model generalize to observations not used in training the model; how well does the model predict new observations? We also need to figure out, given a single training data set, how to select the values for the hyperparameters of the various techniques.\n\n\nDefinition: Hyperparameter\n\n\nA hyperparameter is a variable that controls the overall configuration of a statistical model or machine learning technique. Hyperparameters are sometimes referred to as external parameters, whereas the parameters of the model function (slopes, intercepts, etc.) are called the internal parameters.\n\n\nHyperparameters need to be set before a model can be trained and their values impact the performance of the model. The process of determining the values for hyperparameters given a particular data set is called hyperparameter tuning.\nHyperparameters include, for example,\n\nThe number of terms in a polynomial model\nThe smoothing parameters in non-parametric regression models\nThe bandwidth in kernel-based estimation methods such as LOESS, kernel regression, local polynomial regression\nThe shrinkage penalty in Lasso, Ridge regression, smoothing splines\nThe depth of decision trees\nThe number \\(k\\) in \\(k\\)-nearest neighbor methods\nThe convergence rate and other tolerances in numerical optimization\nThe learning rate, number of nodes, and number of layers in neural networks\n\nWe can calculate the MSPE or MCR of the trained model, depending on whether we are dealing with a regression or a classification problem. Doing so for the training data has some serious drawbacks. We have seen earlier that saturated models have no prediction error since they perfectly connect the dots in the data. Trying to minimize the MSPE based on the training data (MSETr) invariably leads to overfit models since you can always drive MSETr toward zero.\n\n\nTest Data\nTo measure the true predictive performance of a model we need to apply the model to a different set of observations; a set that was not used in training the model. This set of observations is called the test data set. With a test data set we can measure how well the model generalizes and we can also use it to select the appropriate amount of flexibility of the model. The following graph shows the general behavior of test and train mean-squared prediction error as a function of model flexibility and complexity.\nThe MSPE of the test data set is on average higher than the MSPE of the training data set. Since these are random variables, it can happen in a particular application that the test error is lower than the training error, but this is rare. The model complexity/flexibility is measured here by the number of inputs in the model. As this number increases, the MSETr decreases toward zero. The MSETe, on the other hand, first decreases, reaches a minimum, and increases again. The MSETe is high for models with few parameters because of bias, it increases with model flexibility past the minimum because of variability. The two contributors to the MSE work at different ends of the spectrum—you find models that balance bias and variance somewhere in-between.\n\n\n\n\n\n\nFigure 11.14: MSETr and MSETe as a function of model flexibility (complexity).\n\n\n\nThe big question is: where do we get the test data?\n\n\nValidation Data\nBefore discussing ways to obtain test data sets, a few words about another type of data set, the validation data. The terms test data and validation data are often used interchangeably, but there is a difference. Test data represents new data that should otherwise be representative of the training data. A test data set drawn at random from the training data set typically satisfies that.\nValidation data can be a separate data set with known properties, for example, a benchmark data set. Such a data set can be used to compare approaches from different model families, for example, a random forest and a neural network. It can be used to measure model performance against known conditions (typical and atypical) to ensure a model works properly.\n\n\nExample: Computer vision\n\n\nImageNet is a data set of images organized according to the WordNet hierarchy. ImageNet provides an average of 1,000 images for each meaningful concept in WordNet. The data set is used as a benchmark for object categorization algorithms and currently contains over 14 million images that are labeled and annotated by humans.\nThe most used subset of ImageNet data is the Large Scale Visual Recognition Challenge (ILSVRC) data set. It is used to evaluate object classification algorithms since 2010. The data sets for the challenges are themselves broken down into training, test, and validation sets.\nThe IARPA Janus Benchmark (IJB) datasets contain images and videos used in face detection and face recognition challenges. There are several data sets, for example IJB-B consists of 1,845 subjects with human-labeled face bounding boxes, eye & nose location, and metadata such as skin tone and facial hair for 21,798 still images and 55,026 video frames. The collection methodology for the IJB-B data set is documented .\n\n\nTest data tells us how well a model performs, validation data tells us which model is best.\n\n\nExample: Programming competition\n\n\nSuppose we want to send one student from a group of students to a programming competition. The goal is to win the competition. In training the students encounter problems from past programming competitions.\nStudents that do well during training are not necessarily the best candidates for the competition. We need to find out whether a student does well because they memorized the solution or whether they truly understand how to solve the programming problem. To answer this a validation step is used and a set of new programming problems is presented, specifically designed to test student’s ability to apply general concepts in problem solving. At the end of the validation step we have identified the best student to represent the group at the competition.\nWe are not done, however. Does the best student in the group have a chance in the competition? We now enter the testing phase to answer the question: how well will the best student perform? After administering a real test with new problems, we find out that the student scores above 90%: they are ready for the competition. If, however, we find out that the student scores below 25%, we will not send them to the competition. Instead, we return to the drawing board with a new training procedure and/or a set of new training problems.\n\n\nValidation and test data are often used interchangeably because the test data is often used as the validation data. The questions “which model is best?” and “how well does the model perform?” are answered simultaneously: the best model is the one that achieves the best metric on the test data set. Often that results in choosing the model with the lowest MSETe or MCRTe.\n\n\nHold-out Sample\nLet’s return to the important question: where do we find the test data set?\nMaybe you just happen to have a separate set of data lying around that is just like the training data, but you did not use it. Well, that is highly unlikely.\nTypically, we use the data collected, generated, or available for the study to carve out observations for training and testing. This is called a hold-out sample, a subset of the observations is held back for testing and validation. If we start with \\(n\\) observations, we use \\(n - m\\) observation to train the model (the training data set), and \\(m\\) observations to test/validate the model.\nIn Python you can create this train:test split with the train_test_split() function in sklearn. The following statements load the fitness data from DuckDB into a Pandas DataFrame and split it into two frames of 15 and 16 observations.\n\nimport pandas as pd\nimport duckdb\n\ncon = duckdb.connect(database=\"../ads5064.ddb\")\nfit = con.sql(\"SELECT * FROM fitness\").df()\n\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(fit,random_state=235,train_size=0.5)\n\nThe random_state= parameter sets the seed for the random number generator. By setting this to a non-zero integer, the random number generator starts to produce numbers with that seed value. This makes the selection reproducible, subsequent runs of the program will produce identical—yet random—results. The train_size= parameter specifies the proportion of observations in the training set—if the value is between 0 and 1—or the number of observations in the training set—if the value is an integer &gt; 1.\n\ndisplay(train.shape)\ndisplay(train.describe())\n\n(15, 7)\n\n\n\n\n\n\n\n\n\n\nAge\nWeight\nOxygen\nRunTime\nRestPulse\nRunPulse\nMaxPulse\n\n\n\n\ncount\n15.000000\n15.000000\n15.000000\n15.000000\n15.000000\n15.00000\n15.000000\n\n\nmean\n49.666667\n75.539333\n47.693067\n10.385333\n52.733333\n171.00000\n174.133333\n\n\nstd\n4.654747\n8.076112\n4.516180\n1.131408\n7.731814\n10.96097\n9.210760\n\n\nmin\n40.000000\n59.080000\n39.203000\n8.170000\n40.000000\n148.00000\n155.000000\n\n\n25%\n48.000000\n70.760000\n45.215500\n9.965000\n48.000000\n166.00000\n169.000000\n\n\n50%\n51.000000\n76.320000\n46.672000\n10.330000\n51.000000\n170.00000\n172.000000\n\n\n75%\n53.000000\n80.400000\n49.772000\n11.100000\n58.500000\n178.00000\n180.500000\n\n\nmax\n57.000000\n91.630000\n59.571000\n12.880000\n67.000000\n186.00000\n188.000000\n\n\n\n\n\n\n\n\n\ndisplay(test.shape)\ndisplay(test.describe())\n\n(16, 7)\n\n\n\n\n\n\n\n\n\n\nAge\nWeight\nOxygen\nRunTime\nRestPulse\nRunPulse\nMaxPulse\n\n\n\n\ncount\n16.000000\n16.000000\n16.000000\n16.000000\n16.000000\n16.000000\n16.000000\n\n\nmean\n45.812500\n79.230625\n47.078375\n10.774375\n54.125000\n168.375000\n173.437500\n\n\nstd\n5.140931\n8.415590\n6.125977\n1.605295\n7.701731\n9.721968\n9.408994\n\n\nmin\n38.000000\n61.240000\n37.388000\n8.630000\n45.000000\n146.000000\n155.000000\n\n\n25%\n43.750000\n73.285000\n43.665750\n9.527500\n48.000000\n162.000000\n167.500000\n\n\n50%\n44.500000\n80.170000\n47.023500\n10.725000\n53.500000\n169.000000\n174.000000\n\n\n75%\n48.500000\n86.295000\n50.040750\n11.612500\n59.000000\n174.500000\n180.000000\n\n\nmax\n57.000000\n91.630000\n60.055000\n14.030000\n70.000000\n186.000000\n192.000000\n\n\n\n\n\n\n\n\nThe two data sets have very similar properties as judged by the descriptive statistics. If the goal is to develop a model that can predict the difficult to measure oxygen intake from easy to measure attributes such as age, weight, and pulse, then we would use the 15 observations in the train frame to fit the model and the 16 observations in the test frame to evaluate the model.\nIf we cull the test data from the overall data, how should we determine an appropriate size for the test data? The previous example used a 50:50 split, would it have mattered if we had taken a 20:80 or a 90:10 split? For the two data sets to serve their respective functions, you need enough observations in the training data set to fit the model well enough so it can be tested, and you need enough observations in the test data set to produce a stable estimate of MSETe. In practice splits that allocate between 50 and 90% of the observations to the training data set are common.\nWith small training proportions you run the risk that the model cannot be fit and/or that the data does not support the intended model. For example, with a 10:90 train:test split in the fitness example, the training data contains only 3 observations and evaluating the effect of all input variables on oxygen intake is not possible—the model is saturated after three inputs are in the model. With categorical inputs, you need to make sure that the training and test data sets contain all the categories. For example, if you categorize age into four age groups and only three groups are present in the training data after the split, the resulting model no longer applies to a population with four age groups.\nFrom this discussion we can glean the general advantages and disadvantages of hold-out test samples.\n\nAdvantages and disadvantages of hold-out samples generated by random train:test splits.\n\n\n\n\n\n\nAdvantages\nDisadvantages\n\n\n\n\nEasy to do\nInvolves a random selection; results change depending on which observations selected\n\n\nNo separate test data set needed\nPotentially large variability from run to run, especially for noisy data\n\n\nA general method that can be applied regardless of how model performance is measured\nMust decide how large to make the training (test) set\n\n\nReproducible if fixing random number seed\nAn observation is used either for testing or for training\n\n\n\nTends to overestimate the test error compared to cross-validation methods\n\n\n\nThe last two disadvantages in the table weigh heavily. Since we cannot rely on the training error for model selection, we are sacrificing observations by excluding them from training. At least we expect then a good estimate of the test error. The reason for overestimating the true test error with a train:test hold-out sample is that models tend to perform worse when trained on fewer observations. Reducing the size of the training data set results in less precise parameter estimates which in turn increases the variability of predictions.\nTo compare the variability of the hold-out sample method with other techniques, we draw on the Auto data set from ISLR2 (An Introduction to Statistical Learning by James et al.). The data comprise information on fuel mileage and other vehicle attributes of 392 automobiles. Suppose we want to model mileage as a function of horsepower. The next figure shows the raw data and fits of a linear and quadratic model\n\\[\\text{mpg}_{i} = \\beta_{0} + \\beta_{1}\\text{horsepower}_{i} + \\epsilon_{i}\\]\n\\[\\text{mpg}_{i} = \\beta_{0} + \\beta_{1}\\text{horsepower}_{i} + {\\beta_{2}\\text{horsepower}_{i}^{2} + \\epsilon}_{i}\\]\n\n\n\n\n\n\nFigure 11.15: Simple linear and quadratic polynomial fit for miles per gallon versus horsepower in Auto data set.\n\n\n\nA simple linear regression—the red line in the figure—does not seem appropriate. The model does not pick up the curvature in the underlying trend. A quadratic model seems more appropriate. Can this be quantified? What about a cubic model\n\\[\\text{mpg}_{i} = \\beta_{0} + \\beta_1\\text{horsepower}_i + \\beta_2\\text{horsepower}_i^2 + \\beta_3\\text{horsepower}_i^3 + \\epsilon_{i}\\]\nFigure 11.16 shows the hold-out test errors for all polynomial models up to degree 10. The simple linear regression (SLR) model has degree 1 and is shown on the left. The test error is large for the SLR model and for the 10-degree polynomial. The former is biased as can be seen from the previous graph. The latter is too wiggly and leads to a poor test error because of high variability. The test error is minimized for the quadratic model but we note that the test error is also low for degrees 7—9.\n\n\n\n\n\n\nFigure 11.16: Hold-out test errors for polynomial models from first to tenth degree. The horizontal line marks the minimum, achieved at degree 2.\n\n\n\nBased on this result we would probably choose the second-degree polynomial. To what extent is this decision the result of having selected the specific 196 observations in the 50:50 split? We can evaluate this by repeating the sampling process a few more times. The next graph shows the results of 9 additional 50:50 random splits.\nThe variability in the results is considerable. Most replications would select a second-degree polynomial as the model with the lowest MSETe, but several replications achieve a smallest MSETe for much higher degree polynomials (5th degree, 7th degree, etc.).\n\n\n\n\n\n\nFigure 11.17: Test errors in ten hold-out samples, 50:50 splits. The errors from the previous graph are shown in red.\n\n\n\nHaving spent time, energy, resources, money to build a great data set, it seems wasteful to use some observations only for training and the others only for testing. Is there a way in which we can use all observation for training and testing and still get a good estimate (maybe even a better estimate) of the test error?\nHow about the following proposal:\n\nSplit the data 50:50 into sets \\(t_1\\) and \\(t_2\\)\nUse \\(t_1\\) as the training data set and determine the mean-squared prediction error from \\(t_{2}\\), call this MSETe(\\(t_{2}\\))\nReverse the roles of \\(t_1\\) and \\(t_2\\), using \\(t_2\\) to train the model and \\(t_1\\) to compute the test error MSETe(\\(t_1\\))\nCompute the overall test error as the average MSETe = 0.5 x (MSETe(\\(t_1\\)) + MSETe(\\(t_2\\)))\n\nEach observation is used once for training and once for testing. Because of averaging, the combined estimate of test error is more reliable than the individual test errors.\nThis proposal describes a special case of cross-validation, namely 2-fold cross-validation.",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>General Topics</span>"
    ]
  },
  {
    "objectID": "statlearning/general.html#cross-validation",
    "href": "statlearning/general.html#cross-validation",
    "title": "11  General Topics",
    "section": "11.9 Cross-validation",
    "text": "11.9 Cross-validation\nCross-validation is a general method to measure the performance of a model. It is commonly used for predictive models to evaluate how well a model generalizes to new observations, but it can also be used to, for example, select hyperparameters. Cross-validation extends the concept of the hold-out sample to address the drawbacks of train:test splits. It is also general in that you are not limited to MSE or MCR as performance measurements. So, first, a few words about loss functions.\n\nLoss Functions\n\n\nDefinition: Loss function\n\n\nA loss function or cost function maps an event to a real number that reflects some loss or cost incurred from the event.\nIn data analytics, loss functions measure the discrepancy between observed and predicted values and the losses are typically referred to as errors.\n\n\nTable 11.2 displays common loss functions in data science.\n\n\n\nTable 11.2: Loss functions common in data science applications. \\(y\\) and \\(\\widehat{y}\\) denote observed and predicted value, respectively. \\(\\widehat{p}_j\\) denotes the sample proportion in category \\(j\\) of a classification problem with \\(k\\) categories.\n\n\n\n\n\n\n\n\n\n\nLoss Function\nExpression\nApplication Example\n\n\n\n\nSquared Error\n\\(\\left( y - \\widehat{y} \\right)^{2}\\)\nRegression with continuous response\n\n\nZero-one (0—1)\n\\(I\\left( y \\neq \\widehat{y} \\right)\\)\nClassification\n\n\nAbsolute Value\n\\(\\left| y - \\widehat{y} \\right|\\)\nRobust regression\n\n\nMisclassification\n\\(1 - \\max_{j}{\\widehat{p}}_{j}\\)\nPruning of decision trees\n\n\nGini Index\n\\(\\sum_{j = 1}^{k}{{\\widehat{p}}_{j}\\left( 1 - {\\widehat{p}}_{j} \\right)}\\)\nGrowing of decision trees, neural networks\n\n\nCross-entropy (deviance)\n\\(- 2\\sum_{j = 1}^{k}{{n_{j}\\log}{\\widehat{p}}_{j}}\\)\nGrowing of decision trees, neural networks\n\n\nEntropy\n\\(- \\sum_{j = 1}^{k}{{\\widehat{p}}_{j}\\log{\\widehat{p}}_{j}}\\)\nGrowing of decision trees\n\n\n\n\n\n\nSquared error and zero-one loss dominate data science work in regression and classification problems. For specific methods you will find additional loss functions used to optimize a particular aspect of the model, for example, growing and pruning of decision trees.\nSuppose the loss associated with an observation is denoted \\(\\mathcal{l}_{i}\\). Cross-validation estimates the average loss for each of \\(k\\) sets of observations and averages the \\(k\\) estimates into an overall cross-validation estimate of the loss.\nSuppose we create two random sets of (near) equal size for the 31 observations in the fitness data set; \\(k = 2\\). The sets will have \\(n_{1} = 15\\) and \\(n_{2} = 16\\) observations. This leads to one cross-validation estimate of the loss function for each set:\n\\[{CV}_{1}\\left( \\mathcal{l} \\right) = \\frac{1}{n_{1}}\\sum_{i = 1}^{n_{1}}\\mathcal{l}_{i}\\]\n\\[{CV}_{2}\\left( \\mathcal{l} \\right) = \\frac{1}{n_{2}}\\sum_{i = 1}^{n_{1}}\\mathcal{l}_{i}\\]\nThe overall cross-validation loss is the average of the two:\n\\[CV\\left( \\mathcal{l} \\right) = \\frac{1}{2}\\left( {CV}_{1}\\left( \\mathcal{l} \\right) + {CV}_{2}\\left( \\mathcal{l} \\right) \\right)\\]\nThis is a special case of \\(k\\)-fold cross-validation; the sets are referred to as folds. The other special case is leave-one-out cross-validation.\n\n\n\\(K\\)-fold Cross-validation\nThe set of \\(n\\) observations is divided randomly into \\(k\\) groups of (approximately) equal size. The groups are called the \\(k\\) folds. The model is fit \\(k\\) times, holding out a different fold each time. After computing the loss in each fold\n\\[{CV}_{j}\\left( \\mathcal{l} \\right) = \\frac{1}{n_{j}}\\sum_{i = 1}^{n_{j}}\\mathcal{l}_{i}\\]\nthe overall loss is calculated as the average\n\\[CV\\left( \\mathcal{l} \\right) = \\frac{1}{k}\\sum_{j = 1}^{k}{{CV}_{j}\\left( \\mathcal{l} \\right)}\\]\nThe following figure shows 5-fold cross-validation for \\(n = 100\\) observations. The observations are randomly divided into 5 groups of 20 observations each. The model is trained five times. The first time around, observations in fold 1 serve as the test data set, folds 2—5 serve as the training data set. The second time around, fold 2 serves as the test data set and folds 1, 3, 4, and 5 are the training data set; and so forth. Each time, the average loss is calculated for the 20 observations not included in training. At the end, five average cross-validation losses are averaged to calculate the overall loss.\n\n\n\n\n\n\nFigure 11.18: Example of 5-fold cross-validation for 100 observations. Numbers in the cells represent observation numbers. The records were randomly arranged prior to assigning the folds.\n\n\n\n\nAdvantages and disadvantages of \\(k\\)-fold cross-validation.\n\n\n\n\n\n\nAdvantages\nDisadvantgages\n\n\n\n\nNot as variable as the train:test hold-out sample\nStill has a random element due to randomly splitting the data into \\(k\\) sets\n\n\nLess bias in test error than train:test hold-out sample\nCan becomputationally intensive if the model must be fit \\(k\\) times\n\n\nNot as computationally intensive as leave-one-out cross-validation (see below)\nMust decide on the number of folds\n\n\nEvery observation is used for training (\\(k - 1\\) times) and testing (once)\n\n\n\nReproducible if fixing random number seed\n\n\n\nA general method that can be applied regardless of how model performance is measured\n\n\n\n\nThe most common values for \\(k\\) found in practice are 5, 10, and \\(n\\). \\(k = n\\) is a special case, called leave-one-out cross-validation; see below. Values of 5 and 10 have shown to lead to good estimates of loss while limiting the variability of the results. The averaging of the losses from the folds has a powerful effect of stabilizing the results.\nFor the Auto data set, the following figures show the results of repeating 5-fold and 10-fold cross-validation ten times. The results vary considerably less than the ten repetitions of the 50:50 hold-out sample in the previous section.\n\n\n\n\n\n\nFigure 11.19: Ten repetitions of 5-fold cross-validation for polynomials of degree 1—10; Auto data set.\n\n\n\n\n\n\n\n\n\nFigure 11.20: Ten repetitions of 10-fold cross-validation for polynomials of degree 1—10; Auto data set.\n\n\n\nThe results of 10-fold cross-validation vary less than those of 5-fold CV. This is the effect of averaging 10 quantities rather than 5. The effect of averaging the results from the folds is stronger than the averaging of observations within the folds. But if training a model is computationally intensive, 5-fold cross-validation is a good solution.\n\n\nLeave-One-Out Cross-validation\nAbbreviated LOOCV, this method takes the random element out of selecting observations into the folds. Instead, each observation is used once as a test set of size 1 and the model is fit to the remaining \\(n - 1\\) observations. The observation is put back and the next observation is removed from the training set.\nLOOCV thus estimates the model \\(n\\) times, each time removing one of the observations. It is a special case of \\(k\\)-fold cross-validation where \\(k = n\\).\nA pseudo-algorithm for LOOCV is as follows:\nStep 0: Set \\(i = 1\\)\nStep 1: Set the index of the hold-out observation to \\(i\\)\nStep 2. Remove observation \\(i\\) and fit the model to the remaining \\(n - 1\\) observations\nStep 3. Compute the loss \\(\\mathcal{l}_{i}\\) for the held-out observation\nStep 4. Put the observation back into the data. If \\(i = n\\), go to Step 5. Otherwise, increment \\(i\\) and return to Step 1.\nStep 5. Compute the LOOCV loss as the average of the \\(n\\) losses: \\(CV\\left( \\mathcal{l} \\right) = \\frac{1}{n}\\sum_{i}^{}\\mathcal{l}_{i}\\)\n\nAdvantages and disadvantage of leave-one-out cross-validation.\n\n\n\n\n\n\nAdvantages\nDisadvantages\n\n\n\n\nNo randomness involved. Identical results upon repetition.\nCan become computationally intensive if fitting a model is expensive and no closed-form expressions (or approximations) are available to compute the loss per observation based on a single fit\n\n\nEvery observation is used in training (\\(n - 1\\) times) and in testing (once)\n\n\n\nA general method that can be applied to any loss function and model\n\n\n\nGood estimate of test error\n\n\n\n\nThe results of LOOCV for the Auto data set are shown in Figure 11.16. LOOCV selects the seventh-degree polynomial.\n\n\n\nLeave-one-out cross-validation for polynomials in the Auto data set.\n\n\nFortunately, the leave-one-out cross-validation error can be computed for some model classes without fitting the model \\(n\\) times. For linear regression models, formulas exist to compute the LOO prediction error from information available after just training the model once on all observations. Wait, what?\nSuppose we are predicting the target value of the \\(i\\)th observation in the LOO step when that observation is not in the training set and denote this predicted value as \\(\\widehat{y}_{-i}\\). The LOO cross-validation error using a squared error loss function is then\n\\[\\frac{1}{n}\\sum_{i = 1}^{n}\\left( y_{i} - {\\widehat{y}}_{- i} \\right)^{2}\\]\nThe sum in this expression is called the PRESS statistic (for prediction sum of squares). The interesting result is that\\({\\widehat{\\ y}}_{- i}\\) can be calculated as\n\\[y_{i} - \\widehat{y}_{- i} = \\frac{y_i - \\widehat{y}_i}{1 - h_{ii}}\\]\nwhere \\(h_{ii}\\) is called the leverage of the \\(i\\)th observation. We will discuss the leverage in more detail in the context of linear model diagnostics. At this point it is sufficient to note that the leverage measures how unusual an observation is with respect to the input variables of the model and that \\(0 &lt; h_{ii} &lt; 1\\).\nThe term in the numerator is the regular residual for \\(y_{i}\\). In other words, we can calculate the leave-one-out prediction error from the difference between observed and predicted values in the full training data by adjusting for the leverage. Since \\(0 &lt; h_{ii} &lt; 1\\), it follows that\n\\[y_{i} - \\widehat{y}_{- i} &gt; y_{i} - \\widehat{y}_i\\]\nPredicting an observation that was not used in training the model cannot be more precise than predicting the observation if it is part of the training set.\n\n\n\nFigure 11.1: Mitscherlich yield equation for plant yield as a function of nitrogen rate fitted to a set of data.\nFigure 11.3: An example of regression to the mean. An extreme observation is more likely to be followed by a less extreme observation, one that falls near the center of the distribution.\nFigure 11.6: The Ladder of Causation according to Pearl and MacKenzie.\nFigure 11.11: Accuracy and precision—the dart board bullseye metaphor.\nFigure 11.18: Example of 5-fold cross-validation for 100 observations. Numbers in the cells represent observation numbers. The records were randomly arranged prior to assigning the folds.",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>General Topics</span>"
    ]
  },
  {
    "objectID": "statlearning/mathstat.html",
    "href": "statlearning/mathstat.html",
    "title": "12  Probability and Statistics",
    "section": "",
    "text": "12.1 Probability\nData are not deterministic; they have an element of uncertainty. Measurement errors, sampling variability, random assignment and selection, incomplete observation, are just some sources of random variability found in data. A solid understanding of probability concepts is necessary for any data professional. To separate signal from noise in data means separating systematic from random effects; to make statements about data requires the quantification of uncertainty.\nSituations in which the outcomes occur randomly are called generically experiments and probability theory is used to model such experiments.",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Probability and Statistics</span>"
    ]
  },
  {
    "objectID": "statlearning/mathstat.html#probability",
    "href": "statlearning/mathstat.html#probability",
    "title": "12  Probability and Statistics",
    "section": "",
    "text": "Sample Space and Events\nThe sample space, denoted \\(\\Omega\\), is the set of all possible outcomes of the experiment. If we model the number of calls queued in a customer service hotline as random, the sample space is the set of non-negative integers, \\(\\Omega = \\{ 0,\\ 1,\\ 2,\\cdots,n\\}\\). On my way to work I pass through two traffic lights that are either red \\((r)\\), yellow \\((y)\\), or green \\((g)\\)at the time I reach the light. The sample space is the set of all possible light combinations:\n\\[\\Omega = \\left\\{ rr,ry,rg,yr,yy,yg,gr,gy,gg \\right\\}\\]\nAn event is a subset of a sample space, denoted with uppercase letters. The event that less than 3 calls are queued is \\(A = \\{ 0,\\ 1,\\ 2\\}\\). The event that both lights are green is \\(A = \\left\\{ gg \\right\\}\\).\nConsider two events, \\(A\\) and \\(B\\). We can construct other events from \\(A\\) and \\(B\\). The union of the events, \\(A \\cup B\\), is the event that \\(A\\) occurs or \\(B\\) occurs or both occur. The intersection of \\(A\\) and \\(B\\), denoted \\(A \\cap B\\), is the event that both \\(A\\) and \\(B\\) occur. The complement of event \\(A\\), denoted \\(A^{c}\\), consists of all events in \\(\\Omega\\) that are not in \\(A\\).\nSuppose \\(B\\) is the event that exactly one light is green in the traffic example, \\(B = \\left\\{ rg,yg,gr,gy \\right\\}\\). Then\n\\[A \\cup B = \\left\\{ gg,rg,yg,gr,gy \\right\\}\\]\n\\[A \\cap B = \\varnothing\\]\n\\[B^{c} = \\left\\{ rr,ry,yr,yy,gg \\right\\}\\]\nThe intersection of \\(A\\) and \\(B\\) in this example is the empty set \\(\\varnothing\\), the set without elements. \\(A\\) and \\(B\\) are said to be disjoint events.\nSet theory teaches us about laws involving events.\n\nLaws of set theory. These are useful in deriving probabilities.\n\n\nLaw\n\n\n\n\n\nCommutative\n\\(A \\cup B = B \\cup A\\)\n\n\n\n\\(A \\cap B = B \\cap A\\)\n\n\nAssociative\n\\((A \\cup B) \\cup C = A \\cup (B \\cup C)\\)\n\n\n\n\\((A \\cap B) \\cap C = A \\cap (B \\cap C)\\)\n\n\nDistributive\n\\((A \\cup B) \\cap C = (A \\cap C) \\cup (B \\cap C)\\)\n\n\n\n\\((A \\cap B) \\cup C = (A \\cup C) \\cap (B \\cup C)\\)\n\n\nDe Morgan’s\n\\((A \\cup B)^{c} = A^{c} \\cap B^{c}\\)\n\n\n\n\\((A \\cap B)^{c} = A^{c} \\cup B^{c}\\)\n\n\n\n\n\nProbability Measures\n\nProperties\nA probability measure is a function \\(\\Pr( \\cdot )\\) that maps from subsets of a sample space \\(\\Omega\\) (from events) to real numbers between \\(0\\) and \\(1\\). We have the following axioms and properties:\n\n\\(\\Pr{(\\Omega) = 1}\\)\n\\(\\Pr{\\left( A^{c}\\  \\right) = 1 - \\Pr(A)}\\)\n\\(\\Pr{(A \\cup B) = \\Pr{(A) + \\Pr{(B) + \\Pr(A \\cap B)}}}\\)\nIf \\(A\\) is a subset of \\(\\Omega\\), denoted \\(A \\subset \\Omega\\), then \\(\\Pr{(A) \\geq 0}\\)\nIf \\(A\\) and \\(B\\) are disjoint, then \\(\\Pr{(A \\cup B) = \\Pr{(A) + \\Pr(B)}}\\)\nIf \\(A\\) and \\(B\\) are disjoint, then \\(\\Pr{(A \\cap B) = 0}\\)\nIf \\(A_{1},\\cdots,A_{n}\\) are mutually disjoint events, then \\(\\Pr{\\left( \\bigcup_{i = 1}^{n}A_{i} \\right) = \\sum_{i = 1}^{n}{\\Pr\\left( A_{i} \\right)}}\\)\n\\(\\Pr{(\\varnothing) = 0}\\)\n\n\n\nConditional probability\nThe conditional probability that event \\(A\\) occurs given that event \\(B\\) has occurred is\n\\[\\Pr{\\left( A|B \\right) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}}\\]\nThis requires that \\(\\Pr{(B) &gt; 0}\\), we cannot condition on something that has zero probability of happening. The idea of the conditional probability is that for purpose of conditioning on \\(B\\), we change the relevant sample space from \\(\\Omega\\) to \\(B\\).\nRearranging the probabilities in this expression, we find that the probability that two events occur together (the events intersect) can be written as\n\\[\\Pr(A \\cap B) = \\Pr{\\left( A|B \\right) \\times \\Pr(B)}\\]\nThis is known as the multiplication law of probabilities. Another way of putting this result is that the joint probability of \\(A\\) and \\(B\\) is the product of the conditional probability given \\(B\\) and the marginal probability of \\(B\\). If \\(\\Pr{(A) &gt; 0}\\), we can also use \\(A\\) to condition the calculation:\n\\[\\Pr(A \\cap B) = \\Pr{\\left( B|A \\right) \\times \\Pr(A)}\\]\nSuppose that the probability of rain \\((A)\\) on a cloudy \\((B)\\) day is \\(\\Pr{\\left( A|B \\right) = 0.3}\\). The probability that it is cloudy is \\(\\Pr(B) = 0.2\\). The probability that it is cloudy and raining is \\(\\Pr{\\left( A|B \\right) \\times \\Pr(B)} = 0.3 \\times 0.2 = 0.06\\). Notice the difference between the event cloudy and raining and the event raining given that it is cloudy.\n\n\nLaw of total probability\nSuppose we divide the sample space into two disjoint events, \\(B\\) and \\(B^{c}\\). Then the probability that \\(A\\) occurs can be decomposed as\n\\[\\Pr{(A) = \\Pr{(A \\cap B) + \\Pr\\left( A \\cap B^{c} \\right)}}\\]\nSubstituting the conditional and marginal probabilities this can be rewritten as\n\\[\\Pr{(A) = \\Pr{\\left( A|B \\right)\\Pr(B) + \\Pr{\\left( A|B^{c} \\right)\\Pr\\left( B^{c} \\right)}}}\\]\nEach of the products on the right-hand side conditions on a different sample space, but since \\(B\\) and \\(B^{c}\\) are disjoint, the entire space \\(\\Omega\\) is covered. For example, the probability that it snows in Virginia is the sum of the probabilities that it snows in Montgomery County and that it snows in the other counties of the state.\nWe can extend the decomposition from two disjoint sets to any number of disjoint sets. If \\(B_{1},\\cdots,\\ B_{n}\\) are disjoint sets and \\(\\bigcup_{i = 1}^{n}B_{i} = \\Omega\\), and all $\\Pr{\\left( B_{i} \\right) &gt; 0},\\ $then\n\\[\\Pr(A) = \\sum_{i = 1}^{n}{\\Pr{\\left( A|B_{i} \\right)\\Pr\\left( B_{i} \\right)}}\\]\n\n\nBayes’ rule\nWhat is known as the Bayes rule, named after English mathematician Thomas Bayes, is on the surface another way of expressing conditional probabilities. Recall the definition of the conditional probability,\n\\[\\Pr{\\left( A|B \\right) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}}\\]\nThe numerator, the probability that events \\(A\\) and \\(B\\) occur together, can be written in terms of a conditional probability as well, \\(\\Pr(A \\cap B) = \\Pr{\\left( B|A \\right)\\Pr(A)}\\). Combining the two results yields Bayes’ rule:\n\\[\\Pr\\left( A|B \\right) = \\frac{\\Pr{\\left( B|A \\right)\\Pr(A)}}{\\Pr(B)}\\]\nThe conditional probability on the left-hand side, \\(\\Pr\\left( A|B \\right)\\), is called the posterior} probability. The marginal probabilities \\(\\Pr(A)\\) and \\(\\Pr(B)\\) are also called the prior** probabilities. The Bayes rule allows us to express the probability of \\(A|B\\) as a function of the probability of \\(B|A\\) and vice versa.\n\\[\\Pr\\left( A|B \\right) = \\frac{\\Pr{\\left( B|A \\right)\\Pr(A)}}{\\Pr(B)}\\]\n\\[\\Pr\\left( B|A \\right) = \\frac{\\Pr{\\left( A|B\\  \\right)\\Pr(B)}}{\\Pr(A)}\\]\nYou can combine Bayes’ formula as given here with the law of total probability to compute the marginal probability in the denominator. This makes it even more evident how Bayes’ rule allows us to reverse the conditioning:\n\\[\\Pr{\\left( B_{j}|A \\right) =}\\frac{\\Pr\\left( A|B_{j} \\right)\\Pr\\left( B_{j} \\right)}{\\sum_{i = 1}^{n}{\\Pr\\left( A|B_{i} \\right)\\Pr\\left( B_{i} \\right)}}\\]\n\n\nExample: Lie Detector Test\n\n\nA lie detector test returns two possible readings, a positive reading \\(( \\oplus )\\) that the subject is lying, or a negative reading \\(( \\ominus )\\) that the subject is telling the truth. The subject of the test is indeed lying \\((L)\\) or is indeed telling the truth \\((T)\\).\nWe can construct from this a number of events:\n\\(\\oplus |\\ L\\): the polygraph indicates the subject is lying and they are indeed lying.\n\\(\\oplus |T\\): the polygraph indicates the subject is lying and they are telling the truth.\n\\(\\ominus |L\\): the polygraph indicates the subject is telling the truth when they are lying.\n\\(T\\): the test subject is telling the truth.\n\\(L\\): the test subject is lying.\nSuppose that we know \\(\\Pr\\left( \\oplus |L \\right) = 0.88\\) and thus \\(\\Pr\\left( \\ominus |\\ L \\right) = 0.12\\). The first probability is the true positive rate of the device. If a person is lying, the probability that the polygraph detects it is 0.88. Similarly, assume we know that the true negative rate, the probability that the lie detector indicates someone is telling the truth when they are indeed truthful, is \\(\\Pr{\\left( \\ominus |T \\right) = 0.86}.\\)\nIf we know the marginal (prior) probabilities that someone is telling the truth on a particular question, say, \\(\\Pr{(T) = 0.99}\\), then we can use Bayes’ rule to ask the question: What is the probability the person is telling the truth when the polygraph says that they are lying, \\(\\Pr{(T| \\oplus )}\\):\n\\[\\Pr{\\left( T \\middle| \\oplus \\right) = \\frac{\\Pr{\\left( \\oplus |T \\right)\\Pr(T)}}{\\Pr{\\left( \\oplus |T \\right)\\Pr(T)} + \\Pr{\\left( \\oplus |L \\right)\\Pr(L)}}} = \\frac{0.14 \\times \\ 0.99}{0.14 \\times 0.99 + 0.88 \\times .01} = 0.94\\]\nDespite the relatively large true positive and true negative rates, 94% of all positive readings will be incorrect—the subject answered truthfully. If the probability that someone lies on a particular question is \\(\\Pr{(T) = 0.5}\\), the chance of a polygraph to indict the innocent is much smaller:\n\\[\\Pr\\left( T \\middle| \\oplus \\right) = \\frac{0.14 \\times \\ 0.5}{0.14 \\times 0.5 + 0.88 \\times 0.5} = 0.13\\]\n\n\n\n\nIndependence\nIndependent and disjoint events are different concepts. Events \\(A\\) and \\(B\\) are disjoint if they cannot occur together. Their intersection is the empty set and thus \\(\\Pr{(A \\cap B) = \\Pr{(\\varnothing) = 0}}\\). Independent events are unrelated events, that means the outcome of one event does not affect the outcome of the other event—the events can occur together, however.\nBeing a sophomore, junior, or senior student are disjoint events; you cannot be simultaneously a junior and a senior. On the other hand, the events rain in Virginia'' andwinning the lottery ticket’’ are independent; they can occur together but whether it rains has no bearing on whether you win the lottery that day and winning the lottery has no effect on the weather.\nFormally, \\(A\\) and \\(B\\) are independent events if \\(\\Pr(A \\cap B) = \\Pr(A){\\times Pr}(B)\\). This should make intuitive sense: if knowing \\(A\\) carries no information about the occurrence of \\(B\\), then the probability that they occur together is the product of the probabilities that they occur separately.\nApplying this to the definition of the conditional probability, the following holds for two independent events:\n\\[\\Pr\\left( A|B \\right) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)} = \\frac{\\Pr{(A)\\Pr(B)}}{\\Pr(B)} = \\Pr(A)\\]\nIn other words, when \\(A\\) and \\(B\\) are independent, the conditional probability equals the marginal probability—the occurrence of \\(B\\) does not alter the probability of \\(A\\).\n\n\n\nRandom Variables\nRandom variables are real-valued functions defined on sample spaces. Recall the sample space of the traffic lights encountered on the way to work:\n\\[\\Omega = \\left\\{ rr,ry,rg,yr,yy,yg,gr,gy,gg \\right\\}\\]\n\\(X\\), the number of green lights is a random variable. \\(X\\) is a discrete random variable since it can take on a countable number of values, \\(X = 0,\\ 1,\\ 2\\). If the nine traffic light configurations in \\(\\Omega\\) are equally likely, then the values of \\(X\\) occur with probabilities\n\\[\\Pr(X = 0) = \\frac{4}{9}\\]\n\\[\\Pr(X = 1) = \\frac{4}{9}\\]\n\\[\\Pr(X = 2) = \\frac{1}{9}\\]\n\nDistribution functions\nThe function \\(p(x) = \\Pr(X = x)\\), that assigns probabilities to the discrete values of the random variable, is known as the probability mass function (p.m.f.). The possible values the random variable can take on are called its support. Discrete random variables can have infinitely large support when there is no limit, for example the number of coin tosses until 5 heads are observed has support \\(5,\\ 6,\\ 7,\\ \\cdots\\). The number of fish caught in a day at a lake has infinite support \\(0,\\ 1,\\ 2,\\ \\cdots\\); it might be highly unlikely to catch 1,000 fish per day, but it is not impossible.\nIf the number of possible values is not countable, the random variable is called continuous. The concept of a probability mass function then does not make sense. Instead, continuous random variables are characterized by their probability density function (p.d.f., \\(f(x)\\)). Probabilities for continuous random variables are calculated by integrating the p.d.f.:\n\\[\\Pr(a &lt; X &lt; b) = \\int_{a}^{b}{f(x)\\ dx}\\]\nThe cumulative distribution function (c.d.f., \\(F(x)\\) is defined for any random variable as\n\\[F(x) = \\Pr{(X \\leq x)}\\]\nIn the case of a discrete random variable, this means summing the probabilities on the support up to and including \\(x\\):\n\\[F(x) = \\sum_{X:X \\leq x}^{}{p(x)}\\]\nFor a continuous random variable, the c.d.f. is the integral up to \\(x\\):\n\\[F(x) = \\int_{- \\infty}^{x}{f(x)\\ dx}\\]\nThe \\(p\\)th quantile of a distribution is the value \\(x_{p}\\) for which \\(F\\left( x_{p} \\right) = p\\). For example, the 0.85th quantile—also called the 85th percentile—is \\(x_{.85}\\) and satisfies \\(F\\left( x_{.85} \\right) = 0.85\\). Special quantiles are obtained for \\(p = 0.25\\), the first quartile, \\(p = 0.5\\), the median (second quartile), and \\(p = 0.75\\), the third quartile.\n\n\nExpected value\nThe expected value of a random variable, \\(\\text{E}\\lbrack X\\rbrack\\), also known as the mean of \\(X\\), is the weighted average of the values of the random variable weighted by the mass or density. For a discrete random variable,\n\\[\\text{E}\\lbrack X\\rbrack = \\sum_{}^{}{x\\ p(x)}\\]\nand for a continuous random variable,\n\\[\\text{E}\\lbrack X\\rbrack = \\int_{}^{}{x\\ f(x)\\ dx}\\]\nTechnically, we need the conditions \\(\\sum |x|p(x) &lt; \\infty\\) and \\(\\int|x|f(x)dx &lt; \\infty\\), respectively, for the expected values to be defined. Almost all random variables satisfy this condition. A famous example to the contrary is the ratio of two normal distributions with mean zero, known as the Cauchy distribution—its mean (and variance) are not defined.\nWe can think of the expected value as the center of mass of the distribution function (density or mass function), the point on which the distribution balances.\nThe Greek symbol \\(\\mu\\) is often used to denote the expected value of a random variable.\nRandomness is contagious—a function of a random variable is a random variable. So, if \\(X\\) is a random variable, the function \\(h(X)\\) is a random variable as well. We can find the expected value of \\(h(X)\\) through the mass or density function of \\(X\\):\n\\[\\text{E}\\left\\lbrack h(X) \\right\\rbrack = \\sum_{x}^{}{h(x)p(x)}\\]\n\\[\\text{E}\\left\\lbrack h(X) \\right\\rbrack = \\int_{- \\infty}^{\\infty}{h(x)f(x)dx}\\]\nNote that the sum and interval are taken over the support of \\(X\\), rather than \\(h(X)\\). You can also compute the expected value over the support of \\(h(X)\\), but then values of \\(h(x)\\) need to be weighted with the probabilities (or densities) of \\(h(X)\\).\nAn important case of the expectation of a function is \\(Y\\) as a linear combination of \\(X\\): \\(Y = aX + b\\):\n\\[\\text{E}\\lbrack Y\\rbrack = \\text{E}\\lbrack aX + b\\rbrack = a\\text{E}\\lbrack X\\rbrack + b\\]\nThe expected value is a linear operator. For \\(k\\) random variables \\(X_{1},\\cdots,\\ X_{k}\\) and constants \\(a_{1},\\cdots,a_{k}\\),\n\\[\\text{E}\\left\\lbrack a_{1}X_{1} + a_{2}X_{2} + \\cdots + a_{k}X_{k} \\right\\rbrack = a_{1}\\text{E}\\left\\lbrack X_{1} \\right\\rbrack + a_{2}\\text{E}\\left\\lbrack X_{2} \\right\\rbrack + \\cdots + a_{k}\\text{E}\\lbrack X_{k}\\rbrack\\]\n\n\nExample: Mean of Binomial Distribution\n\n\nA random variable \\(X\\) with binomial distribution has p.m.f.\n\\[\\Pr(X = x) = \\begin{pmatrix} n \\\\ x \\end{pmatrix}\\pi^{x}(1 - \\pi)^{n - x}\\]\nThe support of \\(X\\) is \\(0,\\ 1,\\ 2,\\ \\cdots,\\ n\\). The term \\(\\begin{pmatrix} n \\\\ x \\end{pmatrix}\\) is known as the binomial coefficient,\n\\[\\begin{pmatrix} n \\\\ x \\end{pmatrix} = \\frac{n!}{x!(n - x)!}\\]\nThe binomial random variable is the sum of \\(n\\) independent Bernoulli experiments. A Bernoulli experiment can result in only two possible outcomes that occur with probabilities \\(\\pi\\) and \\(1 - \\pi\\).\nComputing the expected value of the binomial random variable from the p.m.f. is messy, it requires evaluation of\n\\[\\text{E}\\lbrack X\\rbrack = \\sum_{x = 0}^{n}{\\begin{pmatrix} n \\\\ x \\end{pmatrix}x{\\ \\pi}^{x}}\\ (1 - \\pi)^{n - x}\\]\nIf we recognize the binomial random variable \\(X\\) as the sum of \\(n\\) Bernoulli variables \\(Y_{1},\\cdots,Y_{n}\\) with probability \\(\\pi\\), the expected value follows as\n\\[\\text{E}\\lbrack X\\rbrack = \\text{E}\\left\\lbrack \\sum Y_{i} \\right\\rbrack = \\sum \\text{E}\\left\\lbrack Y_{i} \\right\\rbrack = n\\pi\\]\n\n\n\n\nVariance and standard deviation\nNext to the mean, the most important expected value of a random variable, is the variance, the expected value of the squared deviation of a random variable from its mean:\n\\[\\text{Var}\\lbrack X\\rbrack = \\text{E}\\left\\lbrack \\left( X - \\text{E}\\lbrack X\\rbrack \\right)^{2} \\right\\rbrack = \\text{E}\\left\\lbrack X^{2} \\right\\rbrack -\\text{E}\\lbrack X\\rbrack^{2}\\]\nThe second expression states that the variance can also be written as the difference of the mean of the square of the random variable and the square of the mean of the random variable. The Greek symbol \\(\\sigma^{2}\\) is commonly used to denote the variance. The square is useful to remind us that the variance is in squared units. If the random variable \\(X\\) is a length in feet, the variance is measured in square feet.\nThe square root of the variance is called the standard deviation of the random variable, frequently denoted \\(\\sigma\\). The standard deviation is measured in the same units as \\(X\\).\nFrom the definition of expected values and functions of random values, the variance can be calculated from first principles as the expected value of \\((X - \\mu)^{2}\\), where \\(\\mu = E\\lbrack X\\rbrack:\\)\n\\[\\text{Var}\\lbrack X\\rbrack = \\sum_{x}^{}(x - \\mu)^{2}p(x)\\]\n\\[\\text{Var}\\lbrack X\\rbrack = \\int_{- \\infty}^{\\infty}{(x - \\mu)^2 \\, f(x)dx} \\]\nIt is often easier to derive the variance of a random variable from the following properties. Suppose \\(a\\) and \\(b\\) are constants and \\(X\\) is a random variable with variance \\(\\sigma^{2}\\)\n\n\\(\\text{Var}\\lbrack a\\rbrack = 0\\)\n\\(\\text{Var}\\lbrack X + b\\rbrack = \\sigma\\^{2}\\)\n\\(\\text{Var}\\lbrack aX\\rbrack = a^{2}\\sigma^{2}\\)\n\\(\\text{Var}\\lbrack aX + b\\rbrack = a^{2}\\sigma^{2}\\)\n\nThe first property states that constants have no variability—this should make sense. A constant is equal to its expected value, \\(\\text{E}\\lbrack a\\rbrack = a\\), so that deviations between value and mean are always 0.\nIf \\(X_{1},\\cdots,X_{k}\\) are independent random variables and \\(a_{1},\\cdots,a_{k}\\) are constants, then the variance of \\(a_{1}X_{1} + \\cdots + a_{k}X_{k}\\) is given by\n\\[\\text{Var}\\left\\lbrack a_{1}X_{1} + \\cdots + a_{k}X_{k} \\right\\rbrack = a_{1}^{2}\\text{Var}\\left\\lbrack X_{1} \\right\\rbrack + a_{2}^{2}\\text{Var}\\left\\lbrack X_{2} \\right\\rbrack + \\cdots + a_{k}^{2}\\text{Var}\\lbrack X_{k}\\rbrack\\]\n\n\nExample: Variance of Binomial Distribution\n\n\nWe saw earlier that the sum of \\(n\\) independent Bernoulli random variables with probability \\(\\pi\\) is a Binomial random variable. A Bernoulli(\\(\\pi\\)) random variable \\(Y_i\\) takes on values 1 and 0 with p.m.f.\n\\[\\Pr\\left( Y_{i} = 1 \\right) = \\pi\\]\n\\[\\Pr\\left( Y_{i} = 0 \\right) = 1 - \\pi\\]\nThe mean and variance of \\(Y\\) are \\(E\\left\\lbrack Y_{i} \\right\\rbrack = \\pi\\) and \\(Var\\left\\lbrack Y_{i} \\right\\rbrack = \\pi(1 - \\pi)\\), respectively.\nSince \\(X = \\sum_{i = 1}^{n}Y_{i}\\) and the \\(Y_{i}\\)s are independent,\n\\[\\text{Var}\\lbrack X\\rbrack = \\sum_{i = 1}^{n}{\\text{Var}\\left\\lbrack Y_{i} \\right\\rbrack} = n\\pi(1 - \\pi)\\]\n\n\nIt is sufficient for this result to hold that the random variables are mutually uncorrelated, a weaker condition than mutual independence. We are introducing independence below after discussing the concept of joint distribution functions.\n####Centering and standardization\nA random variable \\(X\\) with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\) is centered by subtracting its mean and standardized (scaled) by dividing by its standard deviation:\n\\[Y = \\frac{X - \\mu}{\\sigma}\\]\nIt is easy to show that \\(Y\\) has mean 0 and variance 1. It is also called the centered-and-scaled version of \\(X\\).\n\n\nCovariance and correlation\nWhereas the variance measures the dispersion of a random variable, the covariance measures how two random variables vary together. The covariance of \\(X\\) and \\(Y\\) is the expected value of the cross-product of two variables centered around their respective means, \\(\\mu_{X}\\) and \\(\\mu_{Y}\\):\n\\[\\text{Cov}\\lbrack X,Y\\rbrack = \\text{E}\\left\\lbrack \\left( X - \\mu_{X} \\right)\\left( Y - \\mu_{Y} \\right) \\right\\rbrack = \\text{E}\\lbrack XY\\rbrack - \\mu_{X}\\mu_{Y}\\]\nSimilar to the variance, the covariance has useful properties when working with random variables:\n\n\\(\\text{Cov}\\lbrack X,a\\rbrack = 0\\)\n\\(\\text{Cov}\\lbrack X,X\\rbrack = \\text{Var}\\lbrack X\\rbrack\\)\n\\(\\text{Cov}\\lbrack aX,bY\\rbrack = ab \\text{Cov}\\lbrack X,Y\\rbrack\\)\n\\(\\text{Cov}\\lbrack aY + bU,cW + dV\\rbrack = ac \\text{Cov}\\lbrack Y,W\\rbrack + bc \\text{Cov}\\lbrack U,W\\rbrack + ad \\text{Cov}\\lbrack Y,V\\rbrack + bd \\text{Cov}\\lbrack U,V\\rbrack\\)\n\nEarlier we gave an expression for the variance of a linear combination of random variables under the assumption that the random variables are independent. We can now generalize the result as follows. If \\(X\\) and \\(Y\\) are random variables with variance \\(\\sigma_{X}^{2}\\) and \\(\\sigma_{Y}^{2}\\), respectively, and \\(a,b\\) are constants, then\n\\[\\text{Var}\\lbrack aX + bY\\rbrack = \\text{Var}\\lbrack aX\\rbrack + \\text{Var}\\lbrack bY\\rbrack + \\text{Cov}\\lbrack aX,bY\\rbrack\\]\n\\[= a^{2}\\sigma_{X}^{2} + b^{2}\\sigma_{Y}^{2} + ab \\text{Cov}\\lbrack X,Y\\rbrack\\]\nIf \\(\\text{Cov}\\lbrack X,Y\\rbrack = 0\\), the random variables \\(X\\) and \\(Y\\) are uncorrelated. The correlation between \\(X\\) and \\(Y\\) is defined as\n\\[\\text{Corr}\\lbrack X,Y\\rbrack = \\frac{\\text{Cov}\\lbrack X,Y\\rbrack}{\\sigma_{X}\\sigma_{Y}}\\]\nThe correlation is often denoted \\(\\rho_{XY}\\) and takes on values \\(- 1 \\leq \\rho_{XY} \\leq 1\\). A zero correlation is a weaker condition than independence of \\(X\\) and \\(Y\\).\n\n\nIndependence\nThe joint behavior of random variables \\(X\\) and \\(Y\\) is described by their joint cumulative distribution function,\n\\[F(x,y) = \\Pr(X \\leq x,Y \\leq y)\\]\nIf \\(X\\) and \\(Y\\) are continuous, this probability is calculated as\n\\[F(x,y) = \\int_{- \\infty}^{x}{\\int_{- \\infty}^{y}{f(u,v)\\ dvdu}}\\]\nThe bivariate probability density is derived from \\(F(x,y)\\) by differentiating,\n\\[f(x,y) = \\frac{\\partial^{2}}{\\partial x\\partial y}F(x,y)\\]\nTwo random variables \\(X\\) and \\(Y\\) are independent if their joint c.d.f.s (or joint p.d.f.s) factor into the marginal distributions. The marginal density function of \\(X\\) or \\(Y\\) can be derived from the joint density by integrating over the other random variable:\n\\[f_{X}(x) = \\int_{- \\infty}^{\\infty}{f(x,y)dy}\\]\n\\[f_{Y}(y) = \\int_{- \\infty}^{\\infty}{f(x,y)dx}\\]\nFinally, we can state that \\(X\\) and \\(Y\\) are independent if \\(f(x,y) = f_{X}(x)f_{Y}(y)\\) (or \\(F(x,y) = F_{X}(x)F_{Y}(y)\\)).\nIf \\(X\\) and \\(Y\\) are independent, then\n\\[\\text{E}\\left\\lbrack g(X)h(Y) \\right\\rbrack = \\text{E}\\left\\lbrack g(X) \\right\\rbrack \\text{E}\\left\\lbrack h(Y) \\right\\rbrack\\]\nFor example, \\(\\text{E}\\lbrack XY\\rbrack = \\text{E}\\lbrack X\\rbrack \\text{E}\\lbrack Y\\rbrack\\) and it follows that the covariance between independent random variables is zero. That is, independent random variables are uncorrelated. The reverse does not have to be true. Lack of correlation (a zero covariance) does not imply independence.",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Probability and Statistics</span>"
    ]
  },
  {
    "objectID": "statlearning/mathstat.html#discrete-univariate-distributions",
    "href": "statlearning/mathstat.html#discrete-univariate-distributions",
    "title": "12  Probability and Statistics",
    "section": "12.2 Discrete (Univariate) Distributions",
    "text": "12.2 Discrete (Univariate) Distributions\n\nBernoulli (Binary) Distribution\nA Bernoulli (or binary) experiment has two possible outcomes that occur with probabilities \\(\\pi\\) and \\(1 - \\pi\\), respectively. The outcomes are coded numerically as \\(Y = 1\\) (with probability \\(\\pi\\)) and \\(Y = 0\\), the subset of the sample space \\(\\Omega\\) that maps to \\(Y = 1\\) is often called the “event” or the “success” outcome of the binary distribution, the complement is called the “non-event” or the “failure” outcome.\nFor example, in the traffic light example the sample space is\n\\[\\Omega = \\left\\{ rr,ry,rg,yr,yy,yg,gr,gy,gg \\right\\}\\]\nA binary random variable can be defined as \\(Y = 1\\) if the first light is green. In other words, the event \\(A = \\{ gr,gy,gg\\}\\) maps to \\(Y = 1\\) and the complement \\(A^{c}\\) maps to \\(Y = 0\\). The p.m.f of the random variable is then given by the Bernoulli(\\(\\pi\\)) distribution as\n\\[p(y) = \\left\\{ \\begin{matrix}\n\\pi & Y = 1 \\\\\n1 - \\pi & Y = 0\n\\end{matrix} \\right.\\ \\]\nSince binary data are often found in studies where events are detrimental—e.g., disease, fraud, death, disapproval—the “event” vernacular is preferred over the “success” vernacular.\nThe mean and variance of the Bernoulli(\\(\\pi\\)) random variable are \\(\\pi\\) and \\(\\pi(1 - \\pi)\\), respectively. Notice that the variance is largest at \\(\\pi = 0.5\\), when there is greatest uncertainty about which of the two events will occur.\n\n\n\nProbability mass function of the Bernoulli(0.7) random variable.\n\n\n\n\nBinomial Distribution\nLet \\(Y_{1},\\cdots,Y_{n}\\) be independent binary experiments with the same event probability \\(\\pi\\). Then \\(X = \\sum_{i = 1}^{n}Y_{i}\\) has a Binomial(\\(n,\\pi\\)) distribution with p.m.f.\n\\[\\Pr(X = x) = \\begin{pmatrix}\nn \\\\\nx\n\\end{pmatrix}\\pi^{x}(1 - \\pi)^{n - x},\\ \\ \\ \\ \\ x = 0,1,\\cdots,n\\]\nThe mean and variance of the Binomial(\\(n,\\pi\\)) variable can be found easily from its definition as a sum of independent Bernoulli(\\(\\pi\\)) variables:\n\\[\\text{E}\\lbrack X\\rbrack = n\\pi\\ \\ \\ \\ \\ \\text{Var}\\lbrack X\\rbrack = n\\pi(1 - \\pi)\\]\n\n\n\nProbability mass function of a Binomial(8,0.3) random variable.\n\n\nThe Bernoulli(\\(\\pi\\)) distribution is the special case of the Binomial(\\(1,\\pi\\)) distribution.\n\n\nGeometric distribution\nThe Binomial(\\(n,\\pi\\)) distribution is the number of events in a series of \\(n\\) Bernoulli(\\(\\pi\\)) experiments. The Geometric(\\(\\pi\\)) distribution also can be defined in terms of independent Bernoulli(\\(\\pi\\)) experiments as the number of trials needed to obtain the first event. There is no theoretical upper bound for the support, as you might need infinitely many binary experiments to realize one event.\nThe p.m.f. of a Geometric(\\(\\pi\\)) random variable, and its mean and variance, are given by\n\\[p(x) = \\pi(1 - \\pi)^{x - 1}\\ \\ \\ \\ \\ \\ x = 1,2,\\cdots\\]\n\\[\\text{E}\\lbrack X\\rbrack = \\frac{1}{\\pi}\\ \\ \\ \\ \\ \\text{Var}\\lbrack X\\rbrack = \\frac{1 - \\pi}{\\pi^{2}}\\]\nThe following figure shows the p.m.f. of a Geometric(0.5) distribution. The probability to observe an event on the first try is 1/2, on the second try is 1/4, on the third try is 1/8, and so forth.\nAn interesting property of Geometric(\\(\\pi\\)) random variables is their lack of memory:\n\\[\\Pr\\left( X &gt; s + t|X &gt; t \\right) = \\Pr(X &gt; s)\\]\nThe probability that we have to try \\(s\\) more times to see the first event is independent of how many times we have tried before (\\(t\\)). To prove this note that \\(\\Pr(X &gt; s)\\) means the first event occurs after the \\(s\\)th try which implies that the first \\(s\\) tries were all non-events: \\(\\Pr{(X &gt; s) = (1 - \\pi)^{s}}\\). The conditional probability in question becomes\n\\[\\Pr\\left( X &gt; s + t|X &gt; t \\right) = \\frac{\\Pr{(X &gt; s + t,\\ X &gt; t)}}{\\Pr{(X &gt; t)}} = \\frac{\\Pr{(X &gt; s + t)}}{\\Pr{(X &gt; t)}} = \\frac{(1 - \\pi)^{s + t}}{(1 - \\pi)^{t}} = (1 - \\pi)^{s}\\]\nBut the last expression is just \\(\\Pr{(X &gt; s)}\\).\n\n\n\nProbability mass function of the Geometric(0.5) distribution.\n\n\nPlease note that there is a second definition of the Geometric(\\(\\pi\\)) distribution in terms of independent Bernoulli(\\(\\pi\\)) experiments, namely as the number of non-events before the first event occurs. The p.m.f. of this random variable is\n\\[p(y) = \\pi(1 - \\pi)^{y}\\ \\ \\ \\ \\ y = 0,1,2,\\cdots\\]\nWith mean \\(\\text{E}\\lbrack Y\\rbrack = \\frac{(1 - \\pi)}{\\pi}\\) and variance \\(\\text{Var}\\lbrack Y\\rbrack = \\frac{(1 - \\pi)}{\\pi}^{2}\\).\n\n\nNegative Binomial Distribution\nAn extension of the Geometric(\\(\\pi\\)) distribution is the Negative Binomial (NegBin(\\(k,\\pi\\))) distribution. In a series of independent Bernoulli(\\(\\pi\\)) trials, the number of experiments until the \\(k\\)th event is observed is a NegBin(\\(k,\\pi\\)) random variable.\nA NegBin(\\(k,\\pi\\)) random variable is thus the sum of \\(k\\) Geometric(\\(\\pi\\)) random variables. The p.m.f., mean, and variance of \\(X \\sim NegBin(k,\\pi)\\) are\n\\[p(x) = \\begin{pmatrix}\nx - 1 \\\\\nk - 1\n\\end{pmatrix}\\pi^{k}(1 - \\pi)^{x - k},\\ \\ \\ \\ \\ x = k,k + 1,\\cdots\\]\n\\[\\text{E}\\lbrack X\\rbrack = \\frac{k}{\\pi}\\ \\ \\ \\ \\ \\text{Var}\\lbrack X\\rbrack = \\frac{k(1 - \\pi)}{\\pi^{2}}\\]\nThe negative binomial distribution appears in many parameterizations. A popular form is in terms of the number of non-events before the \\(k\\)th event occurs. This changes the support of the random variable from \\(x = k,k + 1,\\cdots\\) to \\(y = 0,1,\\cdots.\\) The p.m.f,. mean, and variance of that random variable are\n\\[p(y) = \\begin{pmatrix}\ny + k - 1 \\\\\nk - 1\n\\end{pmatrix}\\pi^{k}(1 - \\pi)^{y},\\ \\ \\ \\ y = 0,1,\\cdots\\]\n\\[\\text{E}\\lbrack Y\\rbrack = \\frac{k(1 - \\pi)}{\\pi}\\ \\ \\ \\ \\text{Var}\\lbrack Y\\rbrack = \\frac{k(1 - \\pi)}{\\pi^{2}}\\]\nThe p.m.f. for a NegBin(5,0.7) in this parameterization is shown in Figure 12.1.\n\n\n\n\n\n\nFigure 12.1: Probability mass function of a NegBin(5,0.7) random variable.\n\n\n\nAn important application of the negative binomial distribution is in mixing models. These are models where the parameters of a distribution are assumed to be random variables rather than constants. This mechanism introduces additional variability into the system and is applied when the observed data appear more dispersed than a distribution permits. This condition is called overdispersion. For example, a binomial random variable has variance \\(n\\pi(1 - \\pi)\\); the variability is a function of the mean \\(n\\pi\\). When the observed data suggest that this relationship between mean and variance does not hold—and typically the observed variability is greater than the nominal variance—one can treat \\(n\\) or \\(\\pi\\) as a random variable. If you assume that \\(n\\) follows a Poisson distribution (see next), then the marginal distribution of the data is also Poisson. If you start with a Poisson distribution and assume that its parameter follows a Gamma distribution, the resulting marginal distribution of the data is negative binomial.\n\n\nPoisson Distribution\nThe Poisson distribution is a common probability model for count variables that represent counts per unit, rather than counts that can be converted to proportions. For example, the number of chocolate chips on a cookie, the number of defective parts per day on an assembly line or the number of fish caught per day can be modeled as Poisson random variables.\nFor the Poisson assumption to be met when modeling event counts over some unit, e.g. time, the rate at which events occur cannot depend on the occurrence of any events and events have to occur independently. For example, if the number of customer calls to a service center increases sharply after the release of a new software product, then the rate of events (calls) per day is not constant across days. This can still be modeled as a Poisson process where \\(\\lambda\\) depends on other input variables such as the time since release of the new product. If one customer’s call makes it more likely that another customer calls into the service center, the Poisson assumption is not valid.\nThe random variable \\(Y\\) has a Poisson(\\(\\lambda\\)) distribution if its probability mass function is given by\n\\[p(y) = \\frac{e^{- \\lambda\\ }\\lambda^{y}}{y!},\\ \\ \\ \\ y = 0,1,\\cdots\\]\nThe mean and variance of a Poisson(\\(\\lambda\\)) variable are the same, \\(\\text{E}\\lbrack Y\\rbrack = \\text{Var}\\lbrack Y\\rbrack = \\lambda\\). Although the random variable \\(Y\\) takes on only non-negative integer value, the parameter \\(\\lambda\\) is a real number.\n\n\n\nProbability mass function of a Poisson(3) random variable.\n\n\nFor larger values of \\(\\lambda\\) the distribution shifts to the right and becomes more symmetric. For small values of \\(\\lambda\\) the mass function is very asymmetric and concentrated at small values of \\(Y\\).\n\n\n\nProbability mass function of a Poisson(0.2) random variable.\n\n\nThis is the basis for the Poisson approximation to Binomial probabilities. In a Binomial(\\(n,\\pi\\)) process, if \\(n \\rightarrow \\infty\\) and \\(\\pi\\) shrinks so that \\(n\\pi\\) converges to a constant \\(\\lambda\\), then the Binomial(\\(n,\\pi\\)) process can be approximated as a Poisson(\\(\\lambda\\)) process. The next figure compares a Binomial(200,0.05) mass function to the p.m.f. of the Poisson(10). At least visually, the histograms are almost indistinguishable.\n\n\n\n\n\n\nFigure 12.2: Poisson approximation to the Binomial, \\(n\\pi \\rightarrow \\lambda\\).\n\n\n\nFor example, the probability that three sixes turn up when three dice are rolled is \\(\\frac{1}{216} = 0.00463\\). If the three dice are rolled 200 times, what is the probability that at least one triple six shows up? If \\(Y\\) is the number of triple sixes out of 200, then the binomial probability is calculated as\n\\[1 - \\begin{pmatrix}\n200 \\\\\n0\n\\end{pmatrix}\\left( \\frac{1}{216} \\right)^{0}\\left( \\frac{215}{216} \\right)^{200} = 0.6046\\]\nand the Poisson approximation is\n\\[1 - \\frac{\\left( \\frac{200}{216} \\right)^{0}}{0!}\\exp\\left\\{ - \\frac{200}{216} \\right\\} = 0.6038\\]\nAs \\(\\lambda \\rightarrow \\infty\\), the Poisson p.m.f. approaches the shape of a Gaussian (normal) distribution. The normal approximation is sometimes made for sufficiently large values of the Poisson parameter, \\(\\lambda &gt; 20\\). Figure 12.3 shows the empirical histogram and density for 10,000 random draws from a Poisson(20) distribution and a Gaussian distribution with the same mean and variance as the Poisson. Some folks recommend the Gaussian approximation for the Poisson for \\(\\lambda &gt; 100\\) or even \\(\\lambda &gt; 1000\\).\n\n\n\n\n\n\nFigure 12.3: Normal approximation to the Poisson distribution for \\(\\lambda = 20\\).\n\n\n\nBecause the events being counted occur independently of each other, a Poisson distribution is divisible. You can think of a Poisson(\\(\\lambda = 5\\)) variable as the sum of five Poisson(1) variables. The result of one variable producing on average 5 events per time units or five variables each producing on average one event per unit is the same. More generally, if \\(Y_{1},\\cdots,Y_{n}\\) are independent random variables with respective Poisson(\\(\\lambda_{i}\\)) distributions, then their sum \\(\\sum_{i}^{}Y_{i}\\) follows a Poisson distribution with mean \\(\\lambda = \\sum_{i}^{}\\lambda_{i}\\).",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Probability and Statistics</span>"
    ]
  },
  {
    "objectID": "statlearning/mathstat.html#continuous-univariate-distributions",
    "href": "statlearning/mathstat.html#continuous-univariate-distributions",
    "title": "12  Probability and Statistics",
    "section": "12.3 Continuous (Univariate) Distributions",
    "text": "12.3 Continuous (Univariate) Distributions\nFor continuous random variables, the probability that the variable takes on any particular value is zero. To make meaningful probability statements we consider integration of the probability density function (p.d.f.) over sets, for example, intervals:\n\\[\\Pr{(a \\leq X \\leq b)} = \\int_{a}^{b}{f(x)dx}\\]\nThe density function satisfies \\(\\int_{- \\infty}^{\\infty}{f(x)dx} = 1\\) and can be obtained by differentiating the c.d.f \\(F(x) = \\Pr{(X \\leq x)}\\); \\(f(x) = \\frac{dF(x)}{dx}\\).\n\nUniform Distribution\nIf a random variable has a continuous uniform distribution on the interval \\(\\lbrack a,b\\rbrack\\), denoted \\(Y \\sim U(a,b)\\), its p.d.f. is given by\n\\[f(x) = \\left\\{ \\begin{matrix}\n\\frac{1}{(b - a)} & a \\leq x \\leq b \\\\\n0 & \\text{otherwise}\n\\end{matrix} \\right.\\ \\]\nThe mean and variance of a \\(U(a,b)\\) random variable are\n\\[\\text{E}\\lbrack Y\\rbrack = \\frac{a + b}{2}\\ \\ \\ \\ \\ \\ \\text{Var}\\lbrack Y\\rbrack = \\frac{(b - a)^{2}}{12}\\]\n\n\nExponential Distribution\nThe exponential distribution is a useful probability model for modeling continuous lifetimes. It is related to Poisson processes. If events occur continuously and independently at a constant rate \\(\\lambda\\), the number of events is a Poisson random variable. The time between the events is an exponential random variable, denoted \\(Y \\sim\\) Expo(\\(\\lambda\\)).\n\\[p(y) = \\lambda e^{- \\lambda y},\\ \\ \\ \\ y \\geq 0\\]\n\\[F(y) = 1 - e^{- \\lambda y},\\ \\ \\ y \\geq 0\\]\n\\[\\text{E}\\lbrack Y\\rbrack = \\frac{1}{\\lambda}\\ \\ \\ \\ \\ \\ \\text{Var}\\lbrack Y\\rbrack = \\frac{1}{\\lambda^{2}}\\]\nLike the discrete Geometric(\\(\\pi\\)) distribution, the Expo(\\(\\lambda\\)) distribution is forgetful,\n\\[\\Pr{(Y &gt; s + t|Y &gt; t)} = \\Pr{(Y &gt; s)}\\]\nand it turns out that no other continuous function has this memoryless property. This property is easily proven using \\(\\Pr(Y &gt; y) = 1 - F(y) = e^{- \\lambda y}\\):\n\\[\\Pr\\left( Y &gt; t + s \\middle| Y &gt; t \\right) = \\frac{\\Pr{(Y &gt; t + s,Y &gt; t)}}{\\Pr{(Y &gt; t)}} = \\frac{Pr(Y &gt; t + s)}{Pr(Y &gt; t)} = \\frac{e^{- \\lambda(t + s)}}{e^{- \\lambda t}} = e^{- \\lambda s}\\]\nThe memoryless property of the exponential distribution makes it not a good model for human lifetimes. The probability that a 20-year-old will live another 10 years is not the same as the probability that a 75-year-old will live another 10 years. The exponential distribution implies that this would be the case. When modeling earthquakes, it might be reasonable that the probability of an earthquake in the next ten years is the same, regardless of when the last earthquake occurred—the exponential distribution would then be reasonable.\nYou don’t have to worry about whether other distributions have this memoryless property in applications where lack of memory would not be appropriate. The exponential distribution is defined by this property, it is the only continuous distribution with lack of memory.\n\n\nGamma Distribution\nThe Expo(\\(\\lambda\\)) distribution is a special case of a broader family of distributions, the Gamma(\\(\\alpha,\\beta\\)) distribution. A random variable \\(Y\\) is said to have a Gamma(\\(\\alpha,\\beta\\)) distribution if its density function is\n\\[f(y) = \\frac{1}{\\beta^{\\alpha}\\Gamma(\\alpha)}y^{\\alpha - 1}e^{- y/\\beta},\\ \\ \\ \\ \\ y \\geq 0,\\ \\alpha,\\beta &gt; 0\\]\nThe mean and variance of a Gamma(\\(\\alpha,\\beta\\)) random variable are given by\n\\[\\text{E}\\lbrack Y\\rbrack = \\alpha\\beta\\ \\ \\ \\ \\ \\text{Var}\\lbrack Y\\rbrack = \\alpha\\beta^{2}\\]\n\\(\\alpha\\) is called the shape parameter of the distribution and \\(\\beta\\) is called the scale parameter. Varying \\(\\alpha\\) affects the shape and varying \\(\\beta\\) affects the units of measurement.\nThe term \\(\\Gamma(\\alpha)\\) in the denominator of the density function is called the Gamma function,\n\\[\\Gamma(\\alpha) = \\int_{0}^{\\infty}{y^{\\alpha - 1}e^{- y}}dy\\]\n\n\n\n\n\n\nTip\n\n\n\nFun fact: if \\(\\alpha\\) is an integer, \\(\\Gamma(\\alpha) = (\\alpha - 1)!\\)\n\n\nThe exponential random variable introduced earlier is a special case of the Gamma family, the Expo(\\(1/\\beta\\)) is the same as the Gamma(1,\\(\\beta\\)).\nAnother special case of the gamma-type random variables is the chi-square random variable. A random variable \\(Y\\) is said to have a chi-squared distribution with \\(\\nu\\) degrees of freedom, denoted \\(\\chi_{\\nu}^{2}\\), if \\(Y\\) is a Gamma(\\(\\frac{\\nu}{2},2\\)) random variable. More on \\(\\chi^{2}\\) random variables below after we introduced sampling from a Gaussian distribution.\n\n\n\nDensities of Gamma(3,1/2) (solid), Gamma(2,2) (dashed) and Gamma(1,1/2) random variables. The Gamma(2,2) is also a \\(\\chi_{4}^{2}\\) distribution. The Gamma(1,1/2) is a Expo(2) distribution.\n\n\n\n\nBeta Distribution\nA random variable has a Beta distribution with parameters \\(\\alpha\\) and \\(\\beta\\), denoted \\(Y \\sim \\text{Beta}(\\alpha,\\beta)\\), if its density function is given by \\[\nf(y) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\, y^{\\alpha-1}\\,(1-y)^{\\beta-1}\\quad 0 &lt; y &lt; 1\n\\] The family of beta distributions takes on varied shapes as seen in Figure 12.4.\n\n\n\n\n\n\nFigure 12.4: Densities of Beta(\\(\\alpha,\\beta\\)) distributions\n\n\n\nThe ratio of Gamma functions is known as the Beta function and the density can also be written as \\(f(y) = y^{\\alpha-1}(1-y)^{(\\beta-1)} / B(\\alpha,\\beta)\\) where \\(B(\\alpha,\\beta) = \\Gamma(\\alpha)\\Gamma(\\beta)/\\Gamma(\\alpha+\\beta)\\).\nThe mean of a \\(\\text{Beta}(\\alpha,\\beta])\\) random variable is \\[\\text{E}[Y] = \\frac{\\alpha}{\\alpha+\\beta}\n\\] and the variance is \\[\n\\text{Var}[Y] = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2 (\\alpha+\\beta+1)} = \\text{E}[Y]\\frac{\\beta}{(\\alpha+\\beta)(\\alpha+\\beta+1)}\n\\]\nThe support of a Beta random variable is continuous on [0,1], which makes it an attractive candidate for modeling proportions, for example, the proportion of time a vehicle is in maintenance or the proportion of disposable income spent on rent.\nThe Beta distribution can also be used for random variables that are defined on a different scale, \\(a &lt; Y &lt; b\\) by transforming to the [0,1] scale: \\(Y^* = (Y-a)/(b-a)\\).\nThe \\(\\text{Beta}(1,1)\\) is a continuous uniform random variable on [0,1].\nSince \\(Y\\) is continuous, we can define the support of the Beta random variable as \\(0 \\le y \\le 1\\) or as \\(0 &lt; y &lt; 1\\). The probability that the continuous random variable takes on exactly the value 0 or 1 is zero. However, in practice you can observe proportions at the extreme of the support; the proportion of income spent on rent by a homeowner is zero.\n\n\nGaussian (Normal) Distribution\nThe Gaussian (or Normal) distribution is arguably the most important continuous distribution in all of probability and statistics. A random variable \\(Y\\) has a Gaussian distribution with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\) if its density function is\n\\[f(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left\\{ - \\frac{1}{2\\sigma^{2}}(y - \\mu)^{2} \\right\\},\\ \\ \\ \\ \\text{-}\\infty &lt; y &lt; \\infty\\]\nThe notation \\(Y \\sim G\\left( \\mu,\\sigma^{2} \\right)\\) or \\(Y \\sim N\\left( \\mu,\\sigma^{2} \\right)\\) is common.\nThe Gaussian distribution has the famous bell shape, symmetric about the mean $\\mu$.\n\n\n\nGaussian distributions: \\(G(4,1)\\) (solid), \\(G(5,4)\\) (dashed), and \\(G(8, {0.75}^{2})\\) (dotted).\n\n\nA special version is the standard Gaussian (standard normal) distribution \\(Z \\sim G(0,1)\\) with density\n\\[f(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{ - \\frac{1}{2}y^{2} \\right\\}\\]\nThe standard normal is also referred to as the unit normal distribution.\nGaussian random variables have some interesting properties. For example, linear combinations of Gaussian random variables are Gaussian distributed. If \\(Y\\sim G\\left( \\mu,\\sigma^{2} \\right)\\), then \\(X = aY + b\\) has distribution \\(G(a\\mu + b,a^{2}\\sigma^{2})\\). As an example, if \\(Y\\sim G\\left( \\mu,\\sigma^{2} \\right)\\), then\n\\[Z = \\frac{Y - \\mu}{\\sigma}\\]\nhas a standard Gaussian distribution. You can express probabilities about \\(Y\\) in terms of probabilities about \\(Z\\):\n\\[\\Pr{(X \\leq x)} = \\Pr\\left( Z \\leq \\frac{x - \\mu}{\\sigma} \\right)\\]\nBecause linear functions of Gaussians are Gaussians, it is easy to establish the distribution of the sample mean \\(\\overline{Y} = \\frac{1}{n}\\sum_{i}^{}Y_{i}\\) in a random sample from a \\(G(\\mu,\\sigma^{2})\\) distribution. First, if we take a random sample from any distribution with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\), then the sample mean \\(\\overline{Y}\\) has mean and variance\n\\[\\text{E}\\left\\lbrack \\overline{Y} \\right\\rbrack = \\mu\\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{Var}\\left\\lbrack \\overline{Y} \\right\\rbrack = \\frac{\\sigma^{2}}{n}\\]\nThis follows from the linearity of the expectation operator and the independence of the observations in the random sample. If, in addition, the \\(Y_{i} \\sim G\\left( \\mu,\\sigma^{2} \\right)\\), then\n\\[\\overline{Y} \\sim G\\left( \\mu,\\frac{\\sigma^{2}}{n} \\right)\\]\nThe sample mean of a random sample from a Gaussian distribution also has a Gaussian distribution. Do we know anything about the distribution of \\(\\overline{Y}\\) if we randomly sample a non-Gaussian distribution? Yes, we do. That is the domain of the central limit theorem.\n\n\nCentral Limit Theorem\n\n\nLet \\(Y_{1},\\cdots,Y_{n}\\) be independent and identically distributed random variables with mean \\(\\mu\\) and variance \\(\\sigma^{2} &lt; \\infty.\\) The distribution of\n\\[\\frac{\\overline{Y} - \\mu}{\\sigma/\\sqrt{n}}\\]\nconverges to that of a standard normal random variable as \\(n \\rightarrow \\infty\\).",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Probability and Statistics</span>"
    ]
  },
  {
    "objectID": "statlearning/mathstat.html#sampling-distributions",
    "href": "statlearning/mathstat.html#sampling-distributions",
    "title": "12  Probability and Statistics",
    "section": "12.4 Sampling distributions",
    "text": "12.4 Sampling distributions\nMany important probability distributions are related to sampling data from Gaussian processes, \\(t\\), \\(\\chi^{2}\\), and \\(F\\) distributions most importantly.\n\nChi-Square distribution\nLet \\(Z_{1},\\cdots,Z_{k}\\) denote independent standard normal random variables \\(G(0,1)\\). Then\n\\[X = Z_{1}^{2} + Z_{2}^{2} + \\cdots + Z_{k}^{2}\\]\nhas p.d.f.\n\\[f(x) = \\frac{1}{2^{\\frac{k}{2}}\\Gamma\\left( \\frac{k}{2} \\right)}x^{\\frac{k}{2}}e^{- x/2},\\ \\ \\ \\ \\ x \\geq 0\\]\nThis is known as the Chi-square distribution with \\(k\\) degrees of freedom, abbreviated \\(\\chi_{k}^{2}\\). The mean and variance of a \\(\\chi_{k}^{2}\\) random variable is \\(\\text{E}\\lbrack X\\rbrack = k\\) and \\(\\text{Var}\\lbrack X\\rbrack = 2k\\).\nThe degrees of freedom can be thought of as the number of independent pieces of information that contribute to the \\(\\chi^{2}\\) variable. Here, that is the number of independent \\(G(0,1)\\) variables. Since the \\(\\chi^{2}\\) variable is the sum of their squared values, the density shifts more to the right as the degrees of freedom increase.\n\n\n\nProbability density function of a \\(\\chi_4^2\\) and a \\(\\chi_8^2\\) random variable. \\(\\chi^2\\) variables are skewed to the right, the density shifts to the right with increasing degrees of freedom.\n\n\n\\(\\chi^{2}\\) distributions are important to capture the sample distributions of dispersion statistics. If \\(Y_{1},\\cdots,Y_{n}\\) are a random sample from a \\(G\\left( \\mu,\\sigma^{2} \\right)\\) distribution, and the sample variance is\n\\[S^{2} = \\frac{1}{n - 1}\\sum_{i = 1}^{n}\\left( Y_{i} - \\overline{Y} \\right)^{2}\\]\nThen the random variable \\(\\frac{(n - 1)S^{2}}{\\sigma^{2}}\\) follows a \\(\\chi_{n - 1}^{2}\\) distribution. It follows immediately that \\(S^{2}\\) is an unbiased estimator of \\(\\sigma^{2}\\):\n\\[\\text{E}\\left\\lbrack \\frac{(n - 1)S^{2}}{\\sigma^{2}} \\right\\rbrack = n - 1\\]\n\\[\\text{E}\\left\\lbrack S^{2} \\right\\rbrack = \\sigma^{2}\\]\n\n\nStudents’ t distribution\nIn the Gaussian case you can also show that \\(\\overline{Y}\\) and \\(S^{2}\\) are independent random variables. This is important because of another distributional result: if \\(Z \\sim G(0,1)\\) and \\(U \\sim \\chi_{\\nu}^{2}\\), and \\(Z\\) and \\(U\\) are independent, then the ratio\n\\[T = \\frac{Z}{\\sqrt{U/\\nu}}\\]\nhas a \\(t\\) distribution with \\(\\nu\\) degrees of freedom. In honor of Student, the pseudonym used by William Gosset for publishing statistical research while working at Guinness Breweries, the \\(t\\) distribution is also known as Student’s \\(t\\) distribution or Student’s distribution for short.\nAs with the \\(\\chi_{\\nu}^{2}\\) distributions, the shape of the \\(t_{\\nu}\\) distribution depends on the degrees of freedom \\(\\nu\\). However, the \\(t_{\\nu}\\) distributions are all symmetric about zero and the degrees of freedom affect how heavy the tails are.\n\n\n\nDensities for \\(t\\) distributions with 2, 4, 10 degrees of freedom, and a standard normal density.\n\n\nAs the degrees of freedom grow, the \\(t\\) density approaches that of the standard normal distribution.\nWe can now combine the results about the sampling distributions of \\(\\overline{Y}\\) and \\(S^{2}\\) to derive the distribution of\n\\[\\frac{\\overline{Y} - \\mu}{S/\\sqrt{n}}\\]\nWe know that \\(\\overline{Y} \\sim G\\left( \\mu,\\frac{\\sigma^{2}}{n} \\right)\\) and that \\(\\frac{(n - 1)S^{2}}{\\sigma^{2}}\\) follows a \\(\\chi_{n - 1}^{2}\\) distribution. Furthermore, \\(\\overline{Y}\\) and \\(S^{2}\\) are independent. Taking ratios as required by the definition of a \\(t\\) random variable yields\n\\[\\frac{\\frac{\\overline{Y} - \\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{S^{2}{/\\sigma}^{2}}} = \\frac{\\overline{Y} - \\mu}{S/\\sqrt{n}} \\sim t_{n - 1}\\]\n\n\nF distribution\nIf \\(\\chi_{1}^{2}\\) and \\(\\chi_{2}^{2}\\) are independent chi-square random variables with \\(\\nu_{1}\\) and \\(\\nu_{2}\\) degrees of freedom, respectively, then the ratio\n\\[F = \\frac{\\chi_{1}^{2}/v_{1}}{\\frac{\\chi_{2}^{2}}{\\nu_{2}}}\\]\nfollows an \\(F\\) distribution with \\(\\nu_{1}\\) numerator and \\(\\nu_{2}\\) denominator degrees of freedom. We denote this fact \\(F\\sim F_{\\nu_{1},\\nu_{2}}\\). The mean and variance of the \\(F\\) distribution are\n\\[\\text{E}\\lbrack F\\rbrack = \\frac{\\nu_{2}}{\\nu_{2} - 2}\\ \\ \\ \\ \\ \\ \\ \\text{Var}\\lbrack F\\rbrack = \\frac{2\\nu_{2}^{2}(\\nu_{1} + \\nu_{2} + 2)}{\\nu_{1}\\left( \\nu_{2} - 2 \\right)^{2}(\\nu_{2} - 4)}\\]\nThe mean exists only if \\(\\nu_{2} &gt; 2\\) and the variance exists onlu if \\(\\nu_{2} &gt; 4\\).\n\n\n\nDensity functions of an \\(F_{4,12}\\) and an \\(F_{12,20}\\) distribution.\n\n\nWhen sampling from a Gaussian distribution we established that \\(\\frac{(n - 1)S^{2}}{\\sigma^{2}}\\) follows a \\(\\chi_{n - 1}^{2}\\) distribution. Suppose that we have two samples, one of size \\(n_{1}\\) from a \\(G(\\mu_{1},\\sigma_{1}^{2})\\) distribution and one of size \\(n_{2}\\) from a \\(G(\\mu_{2},\\sigma_{2}^{2})\\) distribution. If the estimators of the sample variances in the two samples are denoted \\(S_{1}^{2}\\) and \\(S_{2}^{2}\\), then the ratio\n\\[\\frac{\\frac{S_{1}^{2}}{\\sigma_{1}^{2}}}{S_{2}^{2}/\\sigma_{2}^{2}}\\]\nhas an \\(F_{n_{1} - 1,n_{2} - 1}\\) distribution.\n\\(F\\) distributions play an important role in the analysis of variance, where the ratios are ratios of mean squares.\nRecall that the \\(T\\) random variable in the Gaussian case can be written as\n\\[T = \\frac{\\frac{\\overline{Y} - \\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{S^{2}{/\\sigma}^{2}}}\\]\nThe numerator is a \\(G(0,1)\\) variable, so squaring it gives a \\(\\chi_{1}^{2}\\) variable. The square of the denominator is a scaled \\(\\chi_{n - 1}^{2}\\) variable, \\(S^{2}/\\sigma^{2}\\). Also, because \\(\\overline{Y}\\) and \\(S^{2}\\) are independent, the squares of the numerator and denominator are independent. It thus follows that\n\\[T^{2} = \\frac{\\left( \\overline{Y} - \\mu \\right)^{2}}{S^{2}/n}\\]\nfollows an \\(F_{1,n - 1}\\) distribution.\nThis fact is used in software packages that might report the result of an \\(F\\)-test instead of the result of a two-sided \\(t\\)-test. The two are equivalent and the \\(p\\)-values are the same.",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Probability and Statistics</span>"
    ]
  },
  {
    "objectID": "statlearning/linalg.html",
    "href": "statlearning/linalg.html",
    "title": "13  Linear Algebra",
    "section": "",
    "text": "13.1 Basics\nCommand of linear algebra is essential in data science, models and estimators are often expressed in terms of tensors, matrices, and vectors. Using scalar-based arithmetic becomes tedious very quickly as models become more complex. For example, the simple linear regression model and a straight line through the intercept model can be written as\n\\[Y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i}\\]\n\\[Y_{i} = \\beta_{1}x_{i} + \\epsilon_{i}\\]\nUsing scalar algebra, the estimates of the slope are quite different:\n\\[{\\widehat{\\beta}}_{1} = \\frac{\\left( \\sum_{i = 1}^{n}{\\left( y_{i} - \\overline{y} \\right)\\left( x_{i} - \\overline{x} \\right)} \\right)}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}\\]\n\\[{\\widehat{\\beta}}_{1} = \\frac{\\left( \\sum_{i = 1}^{n}{y_{i}x_{i}} \\right)}{\\sum_{i = 1}^{n}x_{i}^{2}}\\]\nThe formulas get messier as we add another input variable to the model. Using matrix—vector notation, the estimator of all the regression coefficients takes the same form, regardless of the size of the model:\n\\[\\widehat{\\boldsymbol{\\beta}} = \\left( \\textbf{X}^{\\prime}\\textbf{X}\\right)^{- 1}\\textbf{X}^{\\prime}\\textbf{Y}\\]\nA scalar is a single real number, a vector is an array of scalars arranged in a single column (a column vector) or a row (a row vector). A matrix is a two-dimensional array of scalars, a tensor is a multi-dimensional array.\nThe order of a vector or matrix is specified as (rows x columns) and is sometimes used as a subscript for clarity. For example,\\(\\textbf{A}_{(3 \\times 5)}\\) denotes a matrix with 3 rows and 5 columns. It can be viewed as a concatenation} of five \\((3 \\times 1)\\) column vectors:\n\\[\\textbf{A}_{(3 \\times 5)}=\\begin{bmatrix}\n\\begin{matrix}\n1 \\\\\n1 \\\\\n1\n\\end{matrix} & \\begin{matrix}\n9.0 \\\\\n3.2 \\\\\n4.1\n\\end{matrix} & \\begin{matrix}\n\\begin{matrix}\n6.2 \\\\\n1.4 \\\\\n- 0.6\n\\end{matrix} & \\begin{matrix}\n1 \\\\\n0 \\\\\n0\n\\end{matrix} & \\begin{matrix}\n0 \\\\\n1 \\\\\n0\n\\end{matrix}\n\\end{matrix}\n\\end{bmatrix}\\]\n\\(\\textbf{a}_{1} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\ \\ \\ \\ \\ \\textbf{a}_{2} = \\begin{bmatrix} 9.0 \\\\ 3.2 \\\\ 4.1 \\end{bmatrix}\\ \\ \\ \\ \\textbf{a}_{3} = \\begin{bmatrix} 6.2 \\\\ 1.4 \\\\ - 0.6 \\end{bmatrix}\\ \\ \\ \\ \\ \\textbf{a}_{4} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\\ \\ \\ \\ \\ \\textbf{a}_{5} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}\\).\nA matrix with as many rows as columns is called a square matrix.\nBold symbols are common, lowercase for vectors and uppercase for matrices, but there are some exceptions. When dealing with vectors of random variables, bold uppercase notation is used for a vector of random variables and bold lowercase notation is used for a vector of the realized values. For example, if \\(Y_{1},\\cdots,Y_{n}\\) is a random sample of size \\(n\\), the vector of random variables is\n\\[\\textbf{Y}_{(n \\times 1)} = \\begin{bmatrix}\nY_{1} \\\\\n\\vdots \\\\\nY_{n}\n\\end{bmatrix}\\]\nand the vector of realized values is\n\\[\\textbf{y}_{(n \\times 1)} = \\begin{bmatrix}\ny_{1} \\\\\n\\vdots \\\\\ny_{n}\n\\end{bmatrix}\\]\nThe difference is significant because \\(\\textbf{Y}\\) is a random variable and \\(\\textbf{y}\\) is a vector of constants. \\(\\textbf{Y}\\) has a multi-variate distribution with mean and variance, \\(\\textbf{y}\\) is just a vector of numbers.\nWe follow the convention that all vectors are column vectors, so that \\(\\textbf{y}_{(n)}\\) serves as a shorthand for \\(\\textbf{y}_{(n \\times 1)}\\).\nThe typical element of a matrix is written as a scalar with subscripts that refer to rows and columns. For example, the statement\n\\[\\textbf{A}= \\left\\lbrack a_{ij} \\right\\rbrack\\]\nsays that matrix \\(\\textbf{A}\\) consists of the scalars \\(a_{ij}\\); for example, \\(a_{23}\\) is the scalar in row 2, column 3 of the matrix.",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "statlearning/linalg.html#special-matrices",
    "href": "statlearning/linalg.html#special-matrices",
    "title": "13  Linear Algebra",
    "section": "13.2 Special Matrices",
    "text": "13.2 Special Matrices\nA few special matrices, common in statistics and machine learning are\n\n\\(\\textbf{1}_{n} = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\), the unit vector of size \\(n\\); all its elements are 1.\n\\(\\textbf{0}_{n} = \\begin{bmatrix} 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\), the zero vector of size \\(n\\); all its elements are 0.\n\\(\\textbf{0}_{(n \\times k)} = \\begin{bmatrix} 0 & 0 & 0 \\\\ \\vdots & \\cdots & \\vdots \\\\ 0 & 0 & 0 \\end{bmatrix}\\), the zero matrix of order \\((n \\times k)\\). All its elements are 0.\n\\(\\textbf{J}_{(n \\times k)} = \\begin{bmatrix} 1 & 1 & 1 \\\\ \\vdots & \\cdots & \\vdots \\\\ 1 & 1 & 1 \\end{bmatrix}\\), the unit matrix of size \\((n \\times k)\\). All its elements are 1.\n\\(\\textbf{I}_{n} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & \\ddots & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\), the identity matrix of size \\((n \\times n)\\) with 1s on the diagonal and 0s elsewhere.\n\nIf the order of these matrices is obvious from the context, the subscripts tend to be omitted.",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "statlearning/linalg.html#basic-operations-on-matrices-and-vectors",
    "href": "statlearning/linalg.html#basic-operations-on-matrices-and-vectors",
    "title": "13  Linear Algebra",
    "section": "13.3 Basic Operations on Matrices and Vectors",
    "text": "13.3 Basic Operations on Matrices and Vectors\nThe basic operations on matrices and vectors are addition, subtraction, multiplication, transposition, and inversion. These are standard operations in manipulating matrix and vector equations. Decompositions such as Cholesky roots, eigenvalue and singular value decompositions are more advanced operations that are important in solving estimation problems in statistics.\n\nTranspose\nThe transpose of a matrix is obtained by exchanging rows and columns. If \\(a_{ij}\\) is the element in row \\(i\\), column \\(j\\) of matrix \\(\\textbf{A}\\), the transpose of \\(\\textbf{A}\\), denoted \\(\\textbf{A}^\\prime\\), has typical element \\(a_{ji}\\). In case of the \\((3\\  \\times 5)\\) matrix shown previously, its transpose is\n\\[\\textbf{A}^\\prime_{(5 \\times 3)} = \\begin{bmatrix}\n\\begin{matrix}\n1 \\\\\n9.0 \\\\\n\\begin{matrix}\n6.2 \\\\\n1 \\\\\n0\n\\end{matrix}\n\\end{matrix} & \\begin{matrix}\n1 \\\\\n3.2 \\\\\n\\begin{matrix}\n1.4 \\\\\n0 \\\\\n1\n\\end{matrix}\n\\end{matrix} & \\begin{matrix}\n1 \\\\\n4.1 \\\\\n\\begin{matrix}\n- 0.6 \\\\\n0 \\\\\n0\n\\end{matrix}\n\\end{matrix}\n\\end{bmatrix}\\]\nThe transpose of a column vector is a row vector:\n\\[\\textbf{a}^{\\prime} = \\begin{bmatrix}\na_{1} \\\\\n\\vdots \\\\\na_{n}\n\\end{bmatrix}^\\prime = \\left\\lbrack a_{1},\\cdots,a_{n} \\right\\rbrack\\]\nTransposing a transpose produces the original matrix, \\(\\left( \\textbf{A}^{\\prime} \\right)^{\\prime}\\ = \\textbf{A}\\).\nA matrix is symmetric if it is equal to its transpose, \\(\\textbf{A}^\\prime = \\textbf{A}\\). Symmetric matrices are square matrices (same numbers of rows and columns). The matrices \\(\\textbf{A}^\\prime\\textbf{A}\\) and \\(\\textbf{A}\\textbf{A}^\\prime\\) are always symmetric. A symmetric matrix whose off-diagonal elements are zero is called a diagonal matrix.\n\n\nAddition and Subtraction\nThe sum (difference) of two matrices is the matrix of the elementwise sums (differences) of their elements. These operations require that the matrices being summed or subtracted have the same order:\n\\[\\textbf{A}_{(n \\times k)} + \\textbf{B}_{(n \\times k)} = \\left\\lbrack a_{ij} + b_{ij} \\right\\rbrack\\]\n\\[\\textbf{A}_{(n \\times k)} - \\textbf{B}_{(n \\times k)} = \\left\\lbrack a_{ij} - b_{ij} \\right\\rbrack\\]\nSuppose, for example, that \\(\\textbf{A}= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\) and \\(\\textbf{B}=\\begin{bmatrix} - 1 & - 2 & - 3 \\\\ - 4 & - 5 & - 6 \\end{bmatrix}\\). Then,\n\\[\\textbf{A}+ \\textbf{B}= \\begin{bmatrix}\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\\]\n\\[\\textbf{A}- \\textbf{B}= \\begin{bmatrix}\n2 & 4 & 6 \\\\\n8 & 10 & 12\n\\end{bmatrix}\\]\nSince addition (subtraction) are elementwise operations, they can be combined with transposition:\n\\[\\left( \\textbf{A}+ \\textbf{B}\\right)^\\prime = \\textbf{A}^{\\prime} + \\textbf{B}^{\\prime}\\]\n\n\nMultiplication\nTwo matrices conform for addition (subtraction) if they have the same order, that is, the same number of rows and columns. Multiplication of matrices requires a different type of conformity; two matrices \\(\\textbf{A}\\) and \\(\\textbf{B}\\) can be multiplied as \\(\\text{AB}\\) (or \\(\\textbf{A}\\text{×}\\textbf{B}\\)), if the number of columns in \\(\\textbf{A}\\) equals the number of columns in \\(\\textbf{B}\\). We say that in the product \\(\\textbf{A}\\text{×}\\textbf{B}\\), \\(\\textbf{A}\\) is post-multiplied by \\(\\textbf{B}\\) or that \\(\\textbf{A}\\) is multiplied into \\(\\textbf{B}\\). The result of multiplying a \\((n \\times k)\\) matrix into a \\((k \\times p)\\) matrix is a \\((n \\times p)\\) matrix.\nBefore examining the typical elements in the result of multiplication, let’s look at a special case, the inner product of two \\((k \\times 1)\\) vectors \\(\\textbf{A}\\) and \\(\\textbf{B}\\), also called the dot product or the scalar product, is the result of multiplying the transpose of \\(\\textbf{A}\\) into \\(\\textbf{B}\\), a scalar value\n\\[\\textbf{A}^\\prime\\textbf{B}= \\left\\lbrack a_{1}, \\cdots,a_{k} \\right\\rbrack\\begin{bmatrix}\nb_{1} \\\\\n\\vdots \\\\\nb_{k}\n\\end{bmatrix} = a_{1}b_{1} + \\cdots a_{k}b_{k} = \\sum_{i = 1}^{k}{a_{i}b_{i}}\\]\nThe square root of the dot product of a vector with itself is the Euclidean \\({(L}_{2})\\) norm of the vector,\n\\[\\left| \\left| \\textbf{a}\\right| \\right| = \\sqrt{\\textbf{a}^\\prime\\textbf{a}} = \\sum_{i = 1}^{k}a_{i}^{2}\\]\nThe \\(L_{2}\\) norm plays an important role as a loss function in statistical models. The vector for which the norm is calculated is then often a vector of model errors.\nNow let’s return to the problem of multiplying the \\((n \\times k)\\) matrix \\(\\textbf{A}\\) into the \\((k \\times p)\\) matrix \\(\\textbf{B}\\) and introduce one more piece of notation: the \\(i\\)th row of \\(\\textbf{A}\\) is denoted \\(\\mathbf{\\alpha}_{i}\\) and the \\(j\\)th column of \\(\\textbf{B}\\) is denoted \\(\\textbf{B}_{j}\\). Now we can finally write the product \\(\\textbf{A}\\text{×}\\textbf{B}\\) as a matrix whose typical element is the inner product of \\(\\mathbf{\\alpha}_{i}\\) and \\(\\textbf{B}_{j}\\):\n\\[\\textbf{A}_{(n \\times k)} \\times \\textbf{B}_{(k \\times p)} = \\left\\lbrack \\boldsymbol{\\alpha}_{i}^\\prime\\ \\textbf{b}_{j} \\right\\rbrack_{(n \\times p)}\\ \\]\nAs an example, let \\(\\textbf{A}= \\begin{bmatrix} 1 & 2 & 0 \\\\ 3 & 1 & - 3 \\\\ 4 & 1 & 2 \\end{bmatrix}\\) and \\(\\textbf{B}= \\begin{bmatrix} 1 & 0 \\\\ 2 & 3 \\\\ 2 & 1 \\end{bmatrix}\\). The product \\(\\textbf{A}\\times\\textbf{B}\\) is a \\((3 \\times 2)\\) matrix with elements\n\\[\\textbf{A}\\times\\textbf{B}= \\begin{bmatrix}\n1 \\times 1 + 2 \\times 2 + 0 \\times 2 & 1 \\times 0 + 2 \\times 3 + 0 \\times 1 \\\\\n3 \\times 1 + 1 \\times 2 - 3 \\times 2 & 3 \\times 0 + 1 \\times 3 - 3 \\times 1 \\\\\n4 \\times 1 + 1 \\times 2 + 2 \\times 2 & 4 \\times 0 + 1 \\times 3 + 2 \\times 1\n\\end{bmatrix} = \\begin{bmatrix}\n5 & 6 \\\\\n- 1 & 0 \\\\\n10 & 5\n\\end{bmatrix}\\]\nHere are a few helpful rules for matrix multiplication:\n\n\\(c\\textbf{A}= \\left\\lbrack ca_{ij} \\right\\rbrack\\)\n\\(c\\left( \\textbf{A}+ \\textbf{B}\\right) = c\\textbf{A}+ c\\textbf{B}\\)\n\\(\\textbf{C}\\left( \\textbf{A}+ \\textbf{B}\\right) = \\textbf{C}\\textbf{A}+ \\textbf{C}\\textbf{B}\\)\n\\(\\left( \\textbf{A}\\textbf{B}\\right)\\textbf{C}= \\textbf{A}(\\textbf{B}\\textbf{C})\\)\n\\(\\left( \\textbf{A}+ \\textbf{B}\\right)\\left( \\textbf{C}+ \\mathbf{D} \\right) = \\textbf{A}\\textbf{C}+ \\textbf{A}\\mathbf{D} + \\mathbf{BC} + \\mathbf{BD}\\)\n\\(\\left( \\textbf{A}\\textbf{B}\\right)^\\prime = \\textbf{B}^\\prime\\textbf{A}^\\prime\\)\n\\(\\left( c\\textbf{A}\\right)^\\prime = c\\textbf{A}^\\prime\\)\n\n\n\nInversion and Rank\nIn scalar algebra, division and multiplication are inverse operations, dividing a non-zero scalar by itself yields the multiplicative identity: \\(\\frac{a}{a} = 1\\). What is the equivalent of this operation for matrices? First, inversion of a matrix does not reduce it to a scalar, the multiplicative identity for matrices is the identity matrix \\(\\textbf{I}\\), a diagonal matrix with 1s on the diagonal. Second, the inversion is only defined for square matrices. If \\(\\textbf{A}\\) is an \\((n \\times n)\\) matrix, the matrix \\(\\textbf{B}\\) for which\n\\[\\textbf{A}\\textbf{B}= \\textbf{I}\\]\nis called the inverse of \\(\\textbf{A}\\), denoted as \\(\\textbf{A}^{- 1}\\).\nInverse matrices do not have to exist, even for square matrices. If \\(\\textbf{A}\\) has an inverse matrix, then \\(\\textbf{A}\\) is called a non-singular matrix. In that case, \\(\\textbf{A}^{- 1}\\textbf{A}= \\textbf{A}\\textbf{A}^{- 1} = \\text{I}\\).\nFor the inverse of a square matrix to exist, for the matrix to be non-singular, the matrix must be of full rank. The rank of a matrix, denoted \\(r(\\textbf{A})\\), is the number of its linearly independent columns. What does that mean? Suppose we are dealing with a \\((n \\times k)\\) matrix \\(\\textbf{B}\\) and its column vectors are \\(\\textbf{B}_{1},\\cdots,\\textbf{B}_{k}\\). A linear combination of the columns of \\(\\textbf{B}\\) is\n\\[c_{1}\\textbf{b}_{1} + c_{2}\\textbf{b}_{2} + \\cdots + c_{k}\\textbf{b}_{k} = q\\]\nIf you can find a set of scalars \\(c_{1},\\cdots,c_{k}\\) such that \\(q = 0\\), then the columns of \\(\\textbf{B}\\) are linearly dependent. If the only set of scalars that yields \\(q = 0\\) is\n\\[c_{1} = c_{2} = \\cdots = c_{k} = 0\\]\nthen the columns of \\(\\textbf{B}\\) are not linearly dependent and the rank of \\(\\textbf{B}\\) is \\(k\\).\nHere are a few more useful results about the rank of a matrix:\n\n\\(r\\left( \\textbf{A}\\right) = r\\left( \\textbf{A}^\\prime \\right) = r\\left( \\textbf{A}^\\prime\\textbf{A}\\right) = r\\left( \\textbf{A}\\textbf{A}^{\\prime} \\right)\\)\n\\(r\\left( \\textbf{A}\\textbf{B}\\right) \\leq \\min\\left\\{ r\\left( \\textbf{A}\\right),r\\left( \\textbf{B}\\right) \\right\\}\\)\n\\(r\\left( \\textbf{A}+ \\textbf{B}\\right) \\leq r\\left( \\textbf{A}\\right) + r(\\textbf{B})\\)\n\nThe first two results are particularly important in statistical models. In models with linear structures, it is common to collect the \\(p\\) input variables in a linear model, including the intercept as a column of ones, into a matrix \\(\\textbf{X}_{(n\\  \\times p + 1)}\\):\n\\[\\textbf{X}_{(n\\  \\times p + 1)} = \\begin{bmatrix}\n1 & x_{11} & \\begin{matrix}\n\\cdots & x_{1p}\n\\end{matrix} \\\\\n\\vdots & \\vdots & \\begin{matrix}\n\\ddots & \\vdots\n\\end{matrix} \\\\\n1 & x_{n1} & \\begin{matrix}\n\\cdots & x_{np}\n\\end{matrix}\n\\end{bmatrix}\\]\nSuppose we want to solve the linear system \\(\\textbf{Y}= \\textbf{X}\\textbf{c}\\) for \\(\\textbf{c}\\). Start by pre-multiplying both sides of the equation with the transpose of \\(\\textbf{X}\\):\n\\[\\textbf{X}^{\\prime}\\textbf{Y}= \\textbf{X}^{\\prime}\\textbf{X}\\textbf{c}\\]\nIf we had an inverse of \\(\\textbf{X}^\\prime\\textbf{X}\\), then we can now pre-multiply both sides with that inverse and isolate \\(\\text{c}\\):\n\\[{\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{\\mathbf{- 1}}\\textbf{X}}^{\\prime}\\textbf{Y}= \\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\textbf{X}^\\prime\\textbf{X}\\textbf{c}= \\textbf{I}\\textbf{c}= \\textbf{c}\\]\nWe have a solution to the system, namely \\({\\textbf{c}=\\left( \\textbf{X}^\\prime\\textbf{X}\\right)}^{- 1}\\textbf{X}^{\\prime}\\textbf{Y}\\), only if the inverse \\(\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\) exists. And that requires this \\((p + 1) \\times (p + 1)\\) matrix is of full rank \\(r\\left( \\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1} \\right) = p + 1\\). This, in turn is equivalent to saying that \\(\\textbf{X}\\) has full rank \\(p + 1\\) because of property (i).\nHere are some useful results about inverse matrices:\n\n\\(\\left( \\textbf{A}^{- 1} \\right)^\\prime = \\left( \\textbf{A}^\\prime \\right)^{- 1}\\)\n\\(\\left( \\textbf{A}^{- 1} \\right)^{- 1} = \\textbf{A}\\)\n\\(\\left( \\textbf{A}\\textbf{B}\\right)^{- 1} = \\textbf{B}^{-1}\\textbf{A}^{-1}\\)\n\\(r\\left( \\textbf{A}^{- 1} \\right) = r(\\textbf{A})\\)\n\nIf the matrix \\(\\textbf{X}\\) is of less than full rank, it is called a rank-deficient matrix. Can we still solve the linear system \\(\\textbf{Y}= \\textbf{X}\\textbf{c}\\)? Not by using a (regular) inverse matrix, but there is a way out, by using a generalized inverse matrix. If a matrix \\(\\textbf{A}^{-}\\) can be found that satisfies\n\\[\\textbf{A}\\textbf{A}^{-}\\textbf{A}= \\textbf{A}\\]\nthen it is called the generalized inverse (or pseudo-inverse or g-inverse) of \\(\\textbf{A}\\). Suppose we can find such a generalized inverse \\(\\left( \\textbf{X}^{\\prime}\\textbf{X}\\right)^{-}\\)f or \\(\\textbf{X}^\\prime\\textbf{X}\\). What if we use that in the solution of the linear system,\n\\[\\textbf{c}= {\\left( \\textbf{X}^{\\prime}\\textbf{X}\\right)^{-}\\textbf{X}}^{\\prime}\\textbf{Y}\\]\nUnfortunately, whereas regular inverses are unique, there are (infinitely) many generalized inverses that satisfy the condition \\((\\textbf{X}^\\prime\\textbf{X})\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-}\\textbf{X}^\\prime\\textbf{X}= \\textbf{X}^\\prime\\textbf{X}\\). So, there will be infinitely many possible solutions to the linear system. Fortunately, it turns out that generalized inverses have some nice properties, for example, \\(\\textbf{X}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-}\\textbf{X}\\) is invariant to the choice of the generalized inverse. Even if the solution \\(\\textbf{c}\\) is not unique, \\(\\textbf{X}\\textbf{c}\\) is unique. This result is important in linear models with rank-deficient design matrices, a condition that is common when the model contains classification variables. While the parameter estimates in such a model are not unique, because we need to use a generalized inverse to derive the estimates, the predicted values are the same, no matter which generalized inverse we choose.\n\n\nDeterminant\nThe rank reduces a matrix to a single scalar value, the number of linearly independent columns of the matrix. Another value that reduces a square matrix to a single scalar is the determinant, written as \\(det(\\textbf{A})\\) or \\(|\\textbf{A}|\\). The determinant has a geometric interpretation which is not that relevant for our discussion. What matters more is that the determinant appears frequently in expressions of multivariate probability distributions and knowing how to manipulate the determinants.\n\n\\(|\\textbf{A}| = |\\textbf{A}^\\prime|\\)\n\\(|\\textbf{I}| = 1\\)\n\\(\\left| c\\textbf{A}\\right| = c^{n}\\mathbf{|A}\\mathbf{|}\\)\nIf \\(\\textbf{A}\\) is singular, then \\(\\left| \\textbf{A}\\right| = 0\\)\nIf each element of a row (column) of \\(\\textbf{A}\\) is zero, then \\(\\left| \\textbf{A}\\right| = 0\\)\nIf two rows (column) of \\(\\textbf{A}\\) are identical, then \\(\\left| \\textbf{A}\\right| = 0\\)\n\\(\\left| \\textbf{A}\\textbf{B}\\right| = \\left| \\textbf{A}\\right|\\ \\left| \\textbf{B}\\right|\\)\n\\(\\left| \\textbf{A}^{- 1} \\right| = 1/|\\textbf{A}|\\)\nIf \\(\\textbf{A}\\) is a triangular matrix, then \\(|\\textbf{A}| = \\prod_{i = 1}^{n}a_{ii}\\)\nIf \\(\\textbf{A}\\) is a diagonal matrix, then \\(|\\textbf{A}| = \\prod_{i = 1}^{n}a_{ii}\\)\n\n\n\nTrace\nThe trace operator, \\(tr(\\textbf{A})\\), applies only to square matrices. The trace of an \\(\\textbf{A}_{(n \\times n)}\\) matrix is the sum of its diagonal elements:\n\\[tr\\left( \\textbf{A}\\right) = \\sum_{i = 1}^{n}a_{ii}\\]\nThe trace plays an important role in statistics in determining expected values of quadratic forms of random variables, for example, sums of squares in linear models. An important property of the trace is its invariance under cyclic permutations,\n\\[tr\\left( \\mathbf{ABC} \\right) = tr\\left( \\mathbf{BCA} \\right) = tr(\\mathbf{CAB})\\]\nprovided the matrices conform to multiplication.\nSome other useful properties of the trace are\n\n\\(tr\\left( \\textbf{A}+ \\textbf{B}\\right) = tr\\left( \\textbf{A}\\right) + tr\\left( \\textbf{B}\\right)\\)\n\\(tr\\left( \\textbf{A}\\right) = tr\\left( \\textbf{A}^\\prime \\right)\\)\n\\(\\textbf{Y}^\\prime\\text{Ay} = tr\\left( \\textbf{Y}^\\prime\\text{Ay} \\right)\\)\n\\(tr\\left( c\\textbf{A}\\right) = c \\times tr\\left( \\textbf{A}\\right)\\)\n\\(tr\\left( \\textbf{A}\\right) = r(\\textbf{A})\\) if \\(\\textbf{A}\\) is symmetric and idempotent (\\(\\textbf{A}\\textbf{A}= \\textbf{A}\\) and \\(\\textbf{A}= \\textbf{A}^\\prime\\))",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "statlearning/linalg.html#random-vectors",
    "href": "statlearning/linalg.html#random-vectors",
    "title": "13  Linear Algebra",
    "section": "13.4 Random Vectors",
    "text": "13.4 Random Vectors\nIf the elements of a vector are random variables, the vector object itself is a random variable. You can think of random vectors as a convenient mechanism to collect random variables. Suppose we draw a random sample \\(Y_{1},\\cdots,Y_{n}\\), then we can collect the \\(n\\) random variables in a single random vector\n\\[\\textbf{Y}= \\begin{bmatrix}\nY_{1} \\\\\n\\vdots \\\\\nY_{n}\n\\end{bmatrix}\\]\n\nExpected Value\nSince each \\(Y_{i}\\) has a probability distribution, a mean (expected value) \\(\\text{E}\\left\\lbrack Y_{i} \\right\\rbrack\\), a variance \\(\\text{Var}\\left\\lbrack Y_{i} \\right\\rbrack\\), and so forth, the same applies to their collection. The expected value (mean) of a random vector is the vector of the expected values of its elements:\n\\[\\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack = \\begin{bmatrix}\n\\text{E}\\left\\lbrack Y_{1} \\right\\rbrack \\\\\n\\vdots \\\\\n\\text{E}\\left\\lbrack Y_{n} \\right\\rbrack\n\\end{bmatrix}\\]\nSuppose that \\(\\textbf{A},\\ \\textbf{B},\\ \\textbf{c}\\) are matrices and vectors of constants, respectively, and that \\(\\textbf{Y}\\) and \\(\\mathbf{U}\\) are random vectors. The following are useful expectation operations in this situations:\n\n\\(\\text{E}\\left\\lbrack \\textbf{A}\\right\\rbrack = \\textbf{A}\\)\n\\(\\text{E}\\left\\lbrack \\mathbf{AYB} + \\mathbf{C} \\right\\rbrack = \\textbf{A}\\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack\\textbf{B}+ \\textbf{C}\\)\n\\(\\text{E}\\left\\lbrack \\mathbf{AY} + \\mathbf{c} \\right\\rbrack = \\textbf{A}\\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack + \\textbf{c}\\)\n\\(\\text{E}\\left\\lbrack \\mathbf{AY} + \\mathbf{BU} \\right\\rbrack = \\textbf{A}\\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack + \\textbf{B}\\ \\text{E}\\lbrack\\mathbf{U}\\rbrack\\)\n\n\n\nCovariance Matrix\nWhile the distribution of \\(Y_{i}\\) is univariate, \\(\\textbf{Y}\\) has a multivariate (\\(n\\)-variate) distribution. The mean of the distribution is represented by a vector. The variance of the distribution is represented by a matrix, the variance-covariance matrix, a special case of a covariance matrix.\nThe covariance matrix between random vectors \\(\\textbf{Y}_{(k \\times 1)}\\) and \\(\\mathbf{U}_{(p \\times 1)}\\) is a \\((k \\times p)\\) matrix whose typical elements are the covariances between the elements of \\(\\textbf{Y}\\) and \\(\\mathbf{U}\\):\n\\[\\text{Cov}\\left\\lbrack \\textbf{Y},\\mathbf{U} \\right\\rbrack = \\left\\lbrack \\text{Cov}(Y_{i},U_{j}) \\right\\rbrack\\]\nThe covariance matrix can be written in terms of expected values of \\(\\textbf{Y}\\), \\(\\mathbf{U}\\), and \\(\\textbf{Y}\\mathbf{U}^\\prime\\)\n\\[\\text{Cov}\\left\\lbrack \\textbf{Y},\\mathbf{U} \\right\\rbrack = \\text{E}\\left\\lbrack \\left( \\textbf{Y}- \\text{E}\\lbrack\\textbf{Y}\\rbrack \\right)\\left( \\mathbf{U} - \\text{E}\\left\\lbrack \\mathbf{U} \\right\\rbrack \\right)^\\prime \\right\\rbrack = \\text{E}\\left\\lbrack \\textbf{Y}\\mathbf{U}^\\prime \\right\\rbrack - \\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack \\text{E}\\left\\lbrack \\mathbf{U} \\right\\rbrack^{\\prime}\\]\nSome useful rules to manipulate covariance matrices are:\n\n\\(\\text{Cov}\\left\\lbrack \\mathbf{AY},\\mathbf{U} \\right\\rbrack = \\textbf{A}\\text{Cov}\\lbrack\\textbf{Y},\\mathbf{U}\\rbrack\\)\n\\(\\text{Cov}\\left\\lbrack \\textbf{Y},\\mathbf{BU} \\right\\rbrack = \\text{Cov}\\left\\lbrack \\textbf{Y},\\mathbf{U} \\right\\rbrack\\textbf{B}^\\prime\\)\n\\(\\text{Cov}\\left\\lbrack \\mathbf{AY},\\mathbf{BU} \\right\\rbrack = \\textbf{A}\\text{Cov}\\left\\lbrack \\textbf{Y},\\mathbf{U} \\right\\rbrack\\ \\textbf{B}^\\prime\\)\n\\(\\text{Cov}\\left\\lbrack a\\textbf{Y}+ b\\mathbf{U},c\\mathbf{W} + d\\textbf{V}\\right\\rbrack = ac\\text{Cov}\\left\\lbrack \\textbf{Y},\\mathbf{W} \\right\\rbrack + bc\\text{Cov}\\left\\lbrack \\mathbf{U},\\mathbf{W} \\right\\rbrack + ad\\text{Cov}\\left\\lbrack \\textbf{Y},\\textbf{V}\\right\\rbrack + bd\\text{Cov}\\lbrack\\mathbf{U},\\textbf{V}\\rbrack\\)\n\n\n\nVariance-covariance Matrix\nThe variance-covariance matrix (or variance matrix for short) of a random vector \\(\\textbf{Y}\\) is the covariance matrix of \\(\\textbf{Y}\\) with itself.\n\\[\\text{Var}\\left\\lbrack \\textbf{Y}\\right\\rbrack = \\text{Cov}\\left\\lbrack \\textbf{Y},\\textbf{Y}\\right\\rbrack = \\text{E}\\left\\lbrack \\left( \\textbf{Y}- \\text{E}\\lbrack\\textbf{Y}\\rbrack \\right)\\left( \\textbf{Y}-\\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack \\right)^\\prime \\right\\rbrack = \\text{E}\\left\\lbrack \\textbf{Y}\\textbf{Y}^\\prime \\right\\rbrack - \\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack \\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack^{\\prime}\\]\nThe diagonal entries of the variance-covariance matrix contain the variances of the \\(Y_{i}\\). The off-diagonal cells contain the covariances \\(\\text{Cov}\\left\\lbrack Y_{i},Y_{j} \\right\\rbrack\\). If the variance matrix is diagonal, the elements of random vector \\(\\textbf{Y}\\) are uncorrelated. Two random vectors \\(\\textbf{Y}\\) and \\(\\mathbf{U}\\) are uncorrelated if their variance matrix is block-diagonal:\n\\[\\text{Var}\\begin{bmatrix}\n\\textbf{Y}_{1} \\\\\n\\textbf{Y}_{2}\n\\end{bmatrix} = \\begin{bmatrix}\n\\text{Var}\\lbrack\\textbf{Y}_{2}\\rbrack & \\textbf{0}\\\\\n\\textbf{0}& \\text{Var}\\lbrack\\textbf{Y}_{1}\\rbrack\n\\end{bmatrix}\\]\nA very special variance-covariance matrix in statistical models is the scaled identity matrix, \\(\\sigma^{2}\\textbf{I}\\). This is the variance matrix of uncorrelated observations drawn from the same distribution—a common assumption for the error terms in models.\nThe rules for working with covariances extend to working with variance matrices:\n\n\\(\\text{Var}\\left\\lbrack \\mathbf{AY} \\right\\rbrack = \\textbf{A}\\ \\text{Var}\\left\\lbrack \\textbf{Y}\\right\\rbrack\\textbf{A}^\\prime\\)\n\\(\\text{Var}\\left\\lbrack \\textbf{Y}+ \\textbf{A}\\right\\rbrack = \\text{Var}\\lbrack\\textbf{Y}\\rbrack\\)\n\\(\\text{Var}\\left\\lbrack \\textbf{A}^\\prime\\textbf{Y}\\right\\rbrack = \\textbf{A}^\\prime\\text{Var}\\left\\lbrack \\textbf{Y}\\right\\rbrack\\textbf{A}\\)\n\\(\\text{Var}\\left\\lbrack a\\textbf{Y}\\right\\rbrack = a^{2}\\text{Var}\\lbrack\\textbf{Y}\\rbrack\\)\n\\(\\text{Var}\\left\\lbrack a\\textbf{Y}+ b\\mathbf{U} \\right\\rbrack = a^{2}\\text{Var}\\left\\lbrack \\textbf{Y}\\right\\rbrack + b^{2}\\text{Var}\\left\\lbrack \\mathbf{U} \\right\\rbrack + 2ab\\ \\text{Cov}\\lbrack\\textbf{Y},\\mathbf{U}\\rbrack\\)\n\nFinally, an important result about expected values of quadratic forms, heavily used to in decomposing variability is\n\\[\\text{E}\\left\\lbrack \\textbf{Y}^\\prime\\mathbf{AY} \\right\\rbrack = tr\\left( \\textbf{A}\\text{Var}\\left\\lbrack \\textbf{Y}\\right\\rbrack \\right) + \\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack^\\prime\\textbf{A}\\ \\text{E}\\lbrack\\textbf{Y}\\rbrack\\]",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "statlearning/linalg.html#matrix-differentiation",
    "href": "statlearning/linalg.html#matrix-differentiation",
    "title": "13  Linear Algebra",
    "section": "13.5 Matrix Differentiation",
    "text": "13.5 Matrix Differentiation\nEstimation of parameters in statistical models often requires minimization or maximization of an objective function. For example, the ordinary least squares (OLS) principle finds the OLS estimator as the function of the data that minimizes the error sum of squares of the model. Maximum likelihood finds estimators of the parameters as the functions of the data that maximizes the joint likelihood (the joint distribution function) of the data.\nThe parameters of the models appear as elements of vectors and matrices. Finding estimators of the parameters thus requires calculus on vectors and matrices. Consider matrix \\(\\textbf{A}\\), whose elements depend on a scalar parameter \\(\\theta\\), \\(\\textbf{A}= \\left\\lbrack a_{ij}(\\theta) \\right\\rbrack\\). The derivative of \\(\\textbf{A}\\) with respect to \\(\\theta\\) is the matrix of the derivatives of the typical elements \\(a_{ij}(\\theta)\\) with respect to \\(\\theta\\). We write this formally as\n\\[\\frac{\\partial\\textbf{A}}{\\partial\\theta} = \\left\\lbrack \\frac{\\partial a_{ij}(\\theta)}{\\partial\\theta} \\right\\rbrack\\]\nThe derivative of a function \\(f(\\boldsymbol{\\theta})\\) with respect to the vector \\(\\boldsymbol{\\theta}_{(p \\times 1)}\\) is the vector of the partial derivatives of the function\n\\[\\frac{\\partial f\\left( \\boldsymbol{\\theta}\\right)}{\\partial\\boldsymbol{\\theta}} = \\begin{bmatrix}\n\\frac{\\partial f\\left( \\boldsymbol{\\theta}\\right)}{\\partial\\theta_{1}} \\\\\n\\vdots \\\\\n\\frac{\\partial f\\left( \\boldsymbol{\\theta}\\right)}{\\partial\\theta_{p}}\n\\end{bmatrix}\\]\nHere are some useful results from vector and matrix calculus where \\(\\textbf{A}\\) and \\(\\textbf{B}\\) are functions of \\(\\theta\\) and vector \\(\\textbf{X}\\) does not depend on \\(\\theta\\):\n\n\\(\\frac{{\\partial ln}\\left| \\textbf{A}\\right|}{\\partial\\theta} = \\frac{1}{\\left| \\textbf{A}\\right|}\\frac{\\partial\\left| \\textbf{A}\\right|}{\\partial\\theta} = tr\\left( \\textbf{A}^{- 1}\\frac{\\partial\\textbf{A}}{\\partial\\theta} \\right)\\)\n\\(\\frac{\\partial\\textbf{A}^{- 1}}{\\partial\\theta} = - \\textbf{A}^{- 1}\\frac{\\partial\\textbf{A}}{\\partial\\theta\\ }\\textbf{A}^{- 1}\\)\n\\(\\frac{\\partial tr\\left( \\mathbf{AB} \\right)}{\\partial\\theta} = tr\\left( \\frac{\\mathbf{\\partial}\\textbf{A}}{\\partial\\theta}\\textbf{B}\\right) + tr\\left( \\textbf{A}\\frac{\\mathbf{\\partial}\\textbf{B}}{\\partial\\theta} \\right)\\)\n\\(\\frac{\\partial\\textbf{X}^\\prime\\textbf{A}^{- 1}\\textbf{X}}{\\partial\\theta} = - \\textbf{X}^\\prime\\textbf{A}^{- 1}\\frac{\\partial\\textbf{A}}{\\partial\\theta}\\textbf{A}^{- 1}\\textbf{X}\\)\n\\(\\frac{\\partial\\textbf{X}^{\\prime}\\mathbf{Ax}}{\\partial\\textbf{X}} = 2\\mathbf{Ax}\\)\n\\(\\frac{\\partial\\textbf{X}^\\prime\\textbf{A}}{\\partial\\textbf{X}} = \\frac{\\partial\\textbf{A}^\\prime\\textbf{X}}{\\partial\\textbf{X}} = \\textbf{A}\\)",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "statlearning/linalg.html#idempotent-matrices",
    "href": "statlearning/linalg.html#idempotent-matrices",
    "title": "13  Linear Algebra",
    "section": "13.6 Idempotent Matrices",
    "text": "13.6 Idempotent Matrices\nThe class of (symmetric) idempotent matrices play an important role in statistical estimation. Idempotent matrices are projection matrices, that means they map a vector from a space to a sub-space.\n\nProjections\nFor example, suppose we want to find a solution for \\(\\boldsymbol{\\beta}\\) in the linear model \\(\\textbf{Y}_{(n \\times 1)} = \\textbf{X}\\boldsymbol{\\beta}_{(p + 1 \\times 1)} + \\mathbf{\\epsilon}\\). The vector \\(\\textbf{Y}\\) is a vector in \\(n\\)-dimensional space \\(\\mathbb{R}^{n}\\) and the model places a restriction on the predicted values \\(\\widehat{\\textbf{Y}} = \\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\): the predicted values are confined to a \\((p + 1)\\)-dimensional sub-space of \\(\\mathbb{R}^{n}\\). Regardless of how we choose the estimator \\(\\widehat{\\boldsymbol{\\beta}}\\), we are dealing with projecting vector \\(\\textbf{Y}\\) onto a sub-space of \\(\\mathbb{R}^{n}\\).\nWe can thus think of the problem of finding the best estimator in this model as the problem of finding the best projection onto the space generated by the columns of \\(\\textbf{X}\\). In that case, why not choose the projection that minimizes the distance between the observed values \\(\\textbf{Y}\\) and the predicted values \\(\\widehat{\\textbf{Y}}\\). This is achieved by projecting \\(\\textbf{Y}\\) perpendicular (orthogonal) onto the sub-space generated by \\(\\textbf{X}\\). In other words, our solution is the vector \\(\\widehat{\\boldsymbol{\\beta}}\\) that satisfies\n\\[\\left( \\textbf{Y}- \\textbf{X}\\widehat{\\boldsymbol{\\beta}} \\right)^\\prime\\textbf{X}\\widehat{\\boldsymbol{\\beta}} = 0\\]\nMultiplying out and rearranging terms yields\n\\[{\\widehat{\\boldsymbol{\\beta}}}^{\\prime}\\textbf{X}^{\\prime}\\textbf{Y}= {\\widehat{\\boldsymbol{\\beta}}}^{\\prime}\\textbf{X}^\\prime\\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\]\nwhich implies that\n\\[\\textbf{X}^{\\prime}\\textbf{Y}= \\textbf{X}^\\prime\\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\]\nIf \\(\\textbf{X}\\) is of full column rank, then \\(\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}\\) exists and we can solve:\n\\[\\widehat{\\boldsymbol{\\beta}}=\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}\\textbf{X}^{\\prime}\\textbf{Y}\\]\n\n\nThe “hat” Matrix\nThe ordinary least squares estimator is the orthogonal projection of \\(\\textbf{Y}\\) onto the sub-space created by the columns of \\(\\textbf{X}\\). To see the projection matrix at work, compute the predicted values, \\(\\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\):\n\\[\\textbf{X}\\widehat{\\boldsymbol{\\beta}}=\\textbf{X}\\left( \\textbf{X}^{\\prime}\\textbf{X}\\right)^{- 1}\\textbf{X}^{\\prime}\\textbf{Y}= \\widehat{\\textbf{Y}}\\]\nThe matrix \\(\\textbf{X}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}\\textbf{X}^{\\prime}\\) has a very special role in regression analysis, it is often called the “hat” matrix and denoted \\(\\textbf{H}\\), because pre-multiplying \\(\\textbf{Y}\\) with \\(\\textbf{H}\\) puts the hats on \\(\\textbf{Y}\\):\n\\[\\textbf{H}\\textbf{Y}= \\widehat{\\textbf{Y}}\\]\nLet’s verify that \\(\\textbf{H}\\) is indeed a projection matrix, which requires that \\(\\textbf{H}\\textbf{H}= \\textbf{H}\\):\n\\[\\textbf{H}\\textbf{H}= \\textbf{X}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}\\textbf{X}^{\\prime} \\times \\textbf{X}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\textbf{X}^\\prime=\\textbf{X}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}\\textbf{X}^{\\prime} = \\textbf{H}\\]\nMatrices with the property that \\(\\textbf{H}\\textbf{H}= \\textbf{H}\\) are called idempotent matrices, these are projection matrices. If, in addition, \\(\\textbf{H}\\) is symmetric, \\(\\textbf{H}^\\prime = \\textbf{H}\\), the matrix is called symmetric idempotent—these are orthogonal projection matrices. (An idempotent matrix that is not symmetric is called an oblique projector.)\nThe hat matrix in the regression model is a symmetric idempotent matrix.\nHere are some results about (symmetric) idempotent matrices that come in handy when working out the properties of estimators in regression-type models:\n\nProjection matrices are typically not of full rank. If an \\((n \\times n)\\) idempotent matrix is of rank \\(n\\), then it is the identity matrix \\(\\textbf{I}\\).\nIf \\(\\textbf{A}\\) is (symmetric) idempotent, then \\(\\textbf{I}- \\textbf{A}\\) is (symmetric) idempotent.\nIf \\(\\mathbf{P}\\) is non-singular, then \\(\\mathbf{PA}\\mathbf{P}^{-1}\\) is an idempotent matrix.\n\nYou can use these properties to show that in the linear regression model with uncorrelated errors and equal variance the variance matrix of the model residuals \\(\\textbf{Y}- \\widehat{\\textbf{Y}}\\) is\n\\[\\text{Var}\\left\\lbrack \\textbf{Y}- \\widehat{\\textbf{Y}} \\right\\rbrack = \\sigma^{2}(\\textbf{I}- \\textbf{H})\\]\n\n\nA Special Case\nConsider the special case where \\(\\textbf{X}= \\textbf{1}_{n}\\), a column vector of ones. The corresponding linear model is \\(\\textbf{Y}= \\textbf{1}\\beta + \\boldsymbol{\\epsilon}\\), an intercept-only model. The hat matrix for this model is\n\\[\\textbf{1}\\left( \\textbf{1}^{\\prime}\\textbf{1}\\right)^{- 1}\\textbf{1}^\\prime = \\frac{1}{n}\\textbf{1}\\textbf{1}^\\prime = \\frac{1}{n}\\textbf{J}= \\begin{bmatrix}\n\\frac{1}{n} & \\cdots & \\frac{1}{n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{1}{n} & \\cdots & \\frac{1}{n}\n\\end{bmatrix}\n\\]a matrix filled with \\(\\frac{1}{n}\\). The projection of \\(\\textbf{Y}\\) onto the space generated by \\(\\textbf{1}_{n}\\) is\n\\[\\frac{1}{n}\\mathbf{JY} = \\begin{bmatrix}\n\\overline{Y} \\\\\n\\vdots \\\\\n\\overline{Y}\n\\end{bmatrix}\\]\nThe predicted values are all the same, the sample mean. In other words, \\(\\beta = \\overline{Y}\\). Since the projector is idempotent, deriving the variance of the predicted value in the iid case is simple:\n\\[\\text{Var}\\left\\lbrack \\textbf{H}\\textbf{Y}\\right\\rbrack = \\textbf{H}\\text{Var}\\left\\lbrack \\textbf{Y}\\right\\rbrack\\textbf{H}^\\prime = \\sigma^{2}\\textbf{H}\\textbf{H}^{\\prime} = \\sigma^{2}\\textbf{H}= \\sigma^{2}\\frac{1}{n}\\textbf{J}=\\begin{bmatrix}\n\\frac{\\sigma^{2}}{n} & \\cdots & \\frac{\\sigma^{2}}{n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\sigma^{2}}{n} & \\cdots & \\frac{\\sigma^{2}}{n}\n\\end{bmatrix}\\]",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "statlearning/linalg.html#multivariate-gaussian-distribution",
    "href": "statlearning/linalg.html#multivariate-gaussian-distribution",
    "title": "13  Linear Algebra",
    "section": "13.7 Multivariate Gaussian Distribution",
    "text": "13.7 Multivariate Gaussian Distribution\n\nDefinition\nA scalar random variable \\(Y\\) has a Gaussian distribution function with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\) if its probability density function is given by\n\\[f(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left\\{ - \\frac{1}{2\\sigma^{2}}(y - \\mu)^{2} \\right\\}\\]\nWe also say that \\(Y\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\). The shorthand expressions \\(Y \\sim G(\\mu,\\sigma^{2})\\) or \\(Y \\sim N(\\mu,\\sigma^{2})\\) are common.\nThe generalization from a scalar random variable \\(Y\\) to a random vector \\(\\textbf{Y}_{(n \\times 1)}\\) with a multivariate Gaussian distribution is as follows. \\(\\textbf{Y}_{(n \\times 1)}\\) has a multivariate Gaussian (normal) distribution with mean \\(\\boldsymbol{\\mu}\\) and variance matrix \\(\\textbf{V}\\), if its density is given by\n\\[f\\left( \\textbf{Y}\\right)=\\frac{\\left| \\textbf{V}\\right|^{- 1/2}}{(2\\pi)^{\\frac{n}{2}}}\\exp\\left\\{ - \\frac{1}{2}\\left( \\textbf{Y}- \\boldsymbol{\\mu}\\right)^\\prime\\textbf{V}^{- 1}\\left( \\textbf{Y}- \\boldsymbol{\\mu}\\right) \\right\\}\\]\nThis is denoted with the shorthand \\(\\textbf{Y}\\sim G_{n}(\\boldsymbol{\\mu},\\textbf{V})\\) or \\(\\textbf{Y}\\sim N_{n}(\\boldsymbol{\\mu},\\textbf{V}\\)\\). If the dimension of the distribution is clear from context, the subscript \\(n\\) can be omitted. A special case is the standard multivariate Gaussian distribution with mean \\(\\textbf{0}\\) and variance matrix \\(\\textbf{I}\\) :\n\\[f\\left( \\textbf{Y}\\right)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}}\\exp\\left\\{ - \\frac{1}{2}\\textbf{Y}^{\\prime}\\textbf{Y}\\right\\} = \\frac{1}{(2\\pi)^{\\frac{n}{2}}}\\exp\\left\\{ - \\frac{1}{2}\\sum_{i}^{n}y_{i}^{2} \\right\\}\\]\nBut this is just the product of the \\(n\\) univariate densities of \\(N(0,1)\\) random variables:\n\\[f\\left( \\textbf{Y}\\right) = f\\left( y_{1} \\right) \\times \\cdots \\times f\\left( y_{n} \\right)\\]\nwhere\n\\[f\\left( y_{i} \\right) = \\frac{1}{(2\\pi)^{1/2}}\\exp\\left\\{ - \\frac{1}{2}y^{2} \\right\\}\\]\nIf the variance matrix is diagonal—that is, the \\(Y_{i}\\) are uncorrelated—the multivariate normal distribution is the product of the univariate distributions. The random variables are independent.\n\n\nProperties\nGaussian distributions have amazing (magical) properties.\n\nLinear combinations are Gaussian\nFor example, a linear combination of Gaussian random variables also follows a Gaussian distribution. Formally, this can be expressed as follows: if \\(\\textbf{Y}\\sim G_{n}\\left( \\boldsymbol{\\mu},\\textbf{V}\\right)\\) and \\(\\textbf{A}\\) and \\(\\textbf{B}\\) are a matrix and vector of constants (not random variables), respectively, then \\(\\mathbf{AY} + \\textbf{B}\\) follows a \\(G(\\textbf{A}\\boldsymbol{\\mu},\\mathbf{AVA})\\) distribution.\nA special case of this result is that if \\(\\textbf{Y}\\sim G_{n}\\left( \\boldsymbol{\\mu},\\textbf{V}\\right)\\), \\(\\textbf{Y}- \\boldsymbol{\\mu}\\) has a \\(G\\left( 0,\\textbf{V}\\right)\\) distribution.\nBecause a linear function of a Gaussian random variable is Gaussian distributed, you can define all multivariate Gaussian distributions as linear transformations of the standard multivariate Gaussian distribution. If \\(\\textbf{Z}\\sim G_{n}(\\textbf{0},\\textbf{I})\\), and \\(\\textbf{V}= \\textbf{C}^\\prime\\textbf{C}\\), then \\(\\textbf{Y}= \\mathbf{C}^\\prime\\textbf{Z}+ \\boldsymbol{\\mu}\\) has a \\(G(\\boldsymbol{\\mu},\\textbf{V})\\) distribution.\n\n\nZero covariance implies independence\nAnother unusual property of Gaussian random variables is that if they are uncorrelated, they are also stochastically independent. We derived this above for the special case of \\(\\textbf{Y}\\sim G(\\textbf{0},\\sigma^{2}\\textbf{I})\\).\nYou cannot in general conclude that random variables are independent based on their lack of correlation. For Gaussian random variables you can. This result can be extended to Gaussian random vectors. Suppose \\(\\textbf{Y}_{(n \\times 1)} \\sim G(\\boldsymbol{\\mu},\\ \\textbf{V})\\) is partitioned into two sub-vectors of size \\(s\\) and \\(k\\), where \\(n = s + k\\). Then we can similarly partition the mean vector and variance matrix:\n\\[\\textbf{Y}_{(n \\times 1)} = \\begin{bmatrix}\n\\textbf{Y}_{1(s \\times 1)} \\\\\n\\textbf{Y}_{2(k \\times 1)}\n\\end{bmatrix},\\ \\ \\boldsymbol{\\mu}= \\begin{bmatrix}\n\\boldsymbol{\\mu}_{1} \\\\\n\\boldsymbol{\\mu}_{2}\n\\end{bmatrix},\\ \\ \\ \\ \\ \\ \\textbf{V}= \\begin{bmatrix}\n\\textbf{V}_{11} & \\textbf{V}_{12} \\\\\n\\textbf{V}_{21} & \\textbf{V}_{22}\n\\end{bmatrix}\\]\nIf \\(\\textbf{V}_{12} = \\textbf{0}\\), then \\(\\textbf{Y}_{1}\\) and \\(\\textbf{Y}_{2}\\) are independent. Also, each partition is Gaussian distributed, for example, \\(\\textbf{Y}_{1} \\sim G(\\boldsymbol{\\mu}_{1},\\ \\textbf{V}_{11})\\). We call the distribution of \\(\\textbf{Y}_{1}\\) the marginal distribution.\nIt follows immediately that each element of \\(\\textbf{Y}\\) follows a (univariate) Gaussian distribution, \\(Y_{i} \\sim G(\\mu_{i},V_{ii})\\)—all marginal univariate distributions are Gaussian.\n\n\nConditionals are Gaussian}\nThe conditional distribution of \\(\\textbf{Y}_{1}\\) given \\(\\textbf{Y}_{2}\\) is also a Gaussian distribution, specifically:\n\\[\\textbf{Y}_{1}|\\textbf{Y}_{2} \\sim G\\left( \\boldsymbol{\\mu}_{1}\\mathbf{+}\\textbf{V}_{12}\\textbf{V}_{22}^{- 1}\\left( \\textbf{Y}_{2} - \\boldsymbol{\\mu}_{2} \\right),\\ \\textbf{V}_{11} - \\textbf{V}_{12}\\textbf{V}_{22}^{- 1}\\textbf{V}_{12}^\\prime \\right)\\]\nThis result plays an important role when predicting Gaussian random variables, for example in mixed models.\nNotice that the variance matrix of the conditional distribution does not depend on the particular value \\(\\textbf{Y}_{2} = \\textbf{Y}_{2}\\) on which the distribution is conditioned. However, the mean of the conditional distribution does depend on \\(\\textbf{Y}_{2}\\) unless \\(\\textbf{V}_{12} = \\textbf{0}\\), a condition established earlier for independence of \\(\\textbf{Y}_{1}\\) and \\(\\textbf{Y}_{2}\\).\n\n\n\nMaximum Likelihood Estimator\nSuppose that we want to find an estimator for \\(\\boldsymbol{\\beta}\\) in the linear model \\(\\textbf{Y}= \\textbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\). If the model errors follow a Gaussian distribution with mean \\(\\textbf{0}\\) and variance \\(\\textbf{V}\\), then \\(\\textbf{Y}\\) follows a Gaussian distribution because it is a linear function of \\(\\boldsymbol{\\epsilon}\\). The probability density function of \\(\\textbf{Y}\\) is\n\\[f\\left( \\textbf{Y}\\right)=\\frac{\\left| \\textbf{V}\\right|^{- 1/2}}{(2\\pi)^{\\frac{n}{2}}}\\exp\\left\\{ - \\frac{1}{2}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)^\\prime\\textbf{V}^{- 1}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right) \\right\\}\\]\nThis joint distribution of the data can be used to derive the maximum likelihood estimator (MLE) of \\(\\boldsymbol{\\beta}\\). Maximum likelihood estimation considers this as a function of \\(\\boldsymbol{\\beta}\\) rather than a function of \\(\\textbf{Y}.\\) Maximizing this likelihood function \\(\\mathcal{l(}\\boldsymbol{\\beta};\\textbf{Y})\\) is equivalent to maximizing its logarithm and working on the log scale is much simpler. The log-likelihood fu1nction for this problem is given by\n\\(ln\\mathcal{\\{ l}\\left( \\boldsymbol{\\beta};\\textbf{Y}\\right\\} = l\\left( \\boldsymbol{\\beta};\\textbf{Y}\\right) = - \\frac{1}{2}\\ln\\left( \\left| \\textbf{V}\\right| \\right) - \\frac{n}{2}\\ln(2\\pi) - \\frac{1}{2}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)^\\prime\\textbf{V}^{- 1}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)\\)\nFinding the maximum of this function with respect to \\(\\boldsymbol{\\beta}\\) is equivalent to minimizing the quadratic form \\(\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)^\\prime\\textbf{V}^{-1}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)\\) with respect to \\(\\boldsymbol{\\beta}\\). Applying the results about matrix differentiation from above we obtain\n\\[\\frac{\\partial}{\\partial\\boldsymbol{\\beta}}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)^\\prime\\textbf{V}^{- 1}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right) = \\frac{\\partial}{\\partial\\boldsymbol{\\beta}}\\left\\{ \\textbf{Y}^\\prime\\textbf{V}^{- 1}\\textbf{Y}- 2\\textbf{Y}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\beta}^\\prime\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\boldsymbol{\\beta}\\right\\}\\]\n\\[= - 2\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}+ 2\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\boldsymbol{\\beta}\\]\nThe derivative is zero when \\(\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}= \\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\boldsymbol{\\beta}\\).\nIf \\(\\textbf{X}\\) is of full column rank, then \\(\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\) is non-singular and its inverse exists. Pre-multiplying both sides of the equation with that inverse yields the solution\n\\[\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}= \\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\]\n\\[\\left( \\textbf{X}^{\\prime}\\textbf{V}^{- 1}\\textbf{X}\\right)^{- 1}\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}= {\\left( \\textbf{X}^{\\prime}\\textbf{V}^{- 1}\\textbf{X}\\right)^{- 1}\\textbf{X}}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\]\n\\[\\left( \\textbf{X}^{\\prime}\\textbf{V}^{- 1}\\textbf{X}\\right)^{- 1}\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}= \\widehat{\\boldsymbol{\\beta}}\\]\nThe maximum likelihood estimator of \\(\\boldsymbol{\\beta}\\) is the generalized least squares estimator\n\\[{\\widehat{\\boldsymbol{\\beta}}=\\left( \\textbf{X}^{\\prime}\\textbf{V}^{- 1}\\textbf{X}\\right)}^{- 1}\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}\\]\nA special case is when the model errors \\(\\boldsymbol{\\epsilon}\\) are uncorrelated. Since the errors are Gaussian distributed, we know that the errors are then independent. The variance matrix \\(\\textbf{V}\\) is then a diagonal matrix\n\\[\\textbf{V}= \\begin{bmatrix}\n\\sigma_{1}^{2} & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\sigma_{n}^{2}\n\\end{bmatrix}\\]\nA further special case arises when the diagonal entries are all the same,\n\\[\\textbf{V}= \\begin{bmatrix}\n\\sigma^{2}\\  & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\sigma^{2}\n\\end{bmatrix} = \\sigma^{2}\\textbf{I}\\]\nWe can write the error distribution in this case as \\(\\boldsymbol{\\epsilon}\\sim G\\left(\\textbf{0},\\sigma^{2}\\textbf{I}\\right)\\) and the model for \\(\\textbf{Y}\\) as \\(\\textbf{Y}\\sim G\\left( \\textbf{X}\\boldsymbol{\\beta},\\sigma^{2}\\textbf{I}\\right)\\). This is known as the *iid** case, the errors are independent and identically distributed. You will encounter the iid assumption a lot in statistical modeling and in machine learning, because it allows you to write multivariate joint distributions as products of univariate distributions and this greatly simplifies matters.\nUnder the iid assumption for the Gaussian linear model we can substitute \\(\\sigma^{2}\\textbf{I}\\) for \\(\\textbf{V}\\) in the formula for \\(\\widehat{\\boldsymbol{\\beta}}\\) and obtain\n\\[{\\widehat{\\boldsymbol{\\beta}}=\\left( \\textbf{X}^{\\prime}\\textbf{X}\\right)}^{- 1}\\textbf{X}^\\prime\\textbf{Y}\\]\nthe ordinary least squares estimator. Notice that \\(\\sigma^{2}\\) cancels out of the formula; the value of the OLS estimator does not depend on the intrinsic variability of the data. However, the variability of the OLS estimator does depend on \\(\\sigma^{2}\\) (and on \\(\\textbf{X}\\)).",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "statlearning/linalg.html#sherman-morrison-woodbury-formula",
    "href": "statlearning/linalg.html#sherman-morrison-woodbury-formula",
    "title": "13  Linear Algebra",
    "section": "13.8 Sherman, Morrison, Woodbury Formula",
    "text": "13.8 Sherman, Morrison, Woodbury Formula\nThis remarkable formula is at the heart of many regression-type diagnostics and cross-validation techniques. A version of this formula was first given by Gauss in 1821. Around 1950, it appeared in several papers by Sherman and Morrison, and Woodbury.\nSuppose we are in a full-rank linear modeling context with design matrix \\(\\textbf{X}_{(n \\times p + 1)}\\), so that the inverse \\(\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\) exists. In diagnosing the quality of a model, we are interested in measuring the prediction error for the \\(i\\)th observation as if the data point had not contributed to the analysis. This is an example of a leave-one-out estimate: remove an observation from the data, redo the analysis, and measure how well the quantity of interest can be computed for the withheld observation.\nIf you do this in turn for all \\(n\\) observations, you must fit the model \\(n + 1\\) times, an overall fit to the training data with \\(n\\) observations, and \\(n\\) additional fits with training data sets of size \\(n - 1\\), leaving out each observation in turn. The computationally expensive part of fitting the linear model is building the cross-product matrix \\(\\textbf{X}^\\prime\\textbf{X}\\) and computing its inverse \\(\\left( \\textbf{X}^pX \\right)^{-1}\\).\nThe Sherman-Morrison-Woodbury formula allows us to compute the inverse of the cross-product matrix based on \\(\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}\\) as if the \\(i\\)th observation had been removed.\nDenote as \\(\\textbf{X}_{-i}\\) the design matrix with the \\(i\\)th observation removed. Then\n\\[\\left( \\textbf{X}_{-i}^\\prime\\textbf{X}_{-i} \\right)^{- 1} = \\left( \\textbf{X}^\\prime\\textbf{X}- \\textbf{X}_{i}\\textbf{X}_{i}^{\\prime} \\right)^{-1}\\  = \\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1} + \\frac{\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-1}{\\textbf{X}_{i}\\textbf{X}_{i}^{\\prime}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)}^{- 1}}{1 - \\textbf{X}_{i}^{\\prime}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\textbf{X}_{i}}\\]\nBecause of this remarkable result, leave-one-out statistics can be calculated easily—without retraining any models—based on the fit to the full training data alone. Note that the quantity in the denominator of the right-hand side is the diagonal value of \\(\\textbf{I}- \\textbf{H}\\), where \\(\\textbf{H}\\) is the hat matrix. If \\(h_{ii}\\) denotes the diagonal values of \\(\\textbf{H}\\), we can write the update formula as\n\\[\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1} + \\frac{\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}{\\textbf{X}_{i}\\textbf{X}_{i}^{\\prime}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)}^{- 1}}{1 - h_{ii}}\\]\nThe leverage values \\(h_{ii}\\) play an important role in the computation of residual, influence, and case-deletion diagnostics in linear models.",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "statlearning/featproc.html",
    "href": "statlearning/featproc.html",
    "title": "14  Feature and Target Processing",
    "section": "",
    "text": "14.1 Standardization",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Feature and Target Processing</span>"
    ]
  },
  {
    "objectID": "statlearning/featproc.html#standardization",
    "href": "statlearning/featproc.html#standardization",
    "title": "14  Feature and Target Processing",
    "section": "",
    "text": "Centering and Scaling\nStandardizing a variable \\(X\\) comprises two steps, centering and scaling. Scaling does what it says, it replaces \\(x_i\\) with \\(c\\times x_i\\) where \\(c\\) is the scaling factor. Centering means shifting the variable so that it is centered at some specific value. The common parameters of standardization are centering at the mean \\(\\overline{y}\\) and scaling with the standard deviation \\(s_x\\): \\[\nz = \\frac{x-\\overline{x}}{s_x}\n\\] The transformed variable has a sample mean of 0 and a sample standard deviation of 1: \\[\n\\overline{z} = 0 \\qquad s_z = 1\n\\] Figure 14.1 shows the effect of standardization of 1,000 observations drawn from a Gamma(3,2) distribution. Centering shifts the data to have mean zero and compresses the dispersion of the data to a standard deviation of one.\n\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n\nnp.random.seed(234)\nx = np.random.gamma(shape=3,scale=2,size=1000)\nx = np.reshape(x,(-1,1))\n\nstder = preprocessing.StandardScaler().fit(x)\n\nstder.mean_\n\narray([6.02121806])\n\nstder.scale_\n\narray([3.47094438])\n\nx_scaled = stder.transform(x)\nplt.figure()\nplt.hist(x,bins=20,color='0.7',alpha=0.7,label='Original')\nplt.hist(x_scaled,color='magenta',alpha=0.5,label='Standardized')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFigure 14.1: Original and standardized data drawn from a Gamma(3,2) distribution.\n\n\n\n\n\nThe mean (center) of the original data is 6.021 and the scale applied by StandardScaler is 3.471. Note that this standard deviation calculation is based on np.std() which by default uses \\(n\\) instead of \\(n-1\\) as the denominator–using \\(n\\) instead of \\(n-1\\) in the calculation of a sample variance yields a biased estimator of the population variance.\n\n\n\n\n\n\nScaling called Standardization\n\n\n\nYou will find references in the literature and in software documentation to “scaling the data”. It is often understood to mean standardization of the data, that is, centering and scaling.\nBe also aware of the default behavior of software packages. The prcomp function for principal component analysis in R, for example, centers the data by default but does not scale by default. The loess function in R normalizes the predictor variables, using scaling to a robust estimate of the standard deviation, but only if there is more than one predictor. The glmnet::glmnet function standardizes the inputs by default (standardize=TRUE) and reports the results on the original scale of the predictors.\n\n\n\n\nRange Scaling\nThis form of scaling of the data scales the data so it falls between a known lower and upper bound, often 0 and 1. Suppose \\(Z\\) is a range-scaled version of \\(X\\) such that \\(z_{\\text{min}} \\le z \\le z_{\\text{max}}\\). \\(Z\\) can be computed by scaling and shifting a standardized form of \\(X\\): \\[\\begin{align*}\n  x^* &= \\frac{x-\\min(x)}{\\max(x)-\\min(x)} \\\\\n  z &= z_{\\text{min}} + x^* \\times (z_{\\text{max}} - z_{\\text{min}})\n\\end{align*}\\]\nThe following Python code scales the Gamma(3,2) sample to lie between 0 and 2.\n\nrange_scaler = preprocessing.MinMaxScaler(feature_range=(0,2)).fit(x)\n\nprint(range_scaler.data_min_)\n\n[0.25157731]\n\nprint(range_scaler.data_max_)\n\n[21.55883927]\n\nx_range_scaled = range_scaler.transform(x)\n\nx_range_scaled.min()\n\n0.0\n\nx_range_scaled.max()\n\n2.0\n\n\nAn advantage of range scaling is that the same range can be applied to training and test data. This makes it easy to process test data prior to applying a model derived from range-scaled data.\n\n\nTo Scale or not to Scale\nSome data scientists standardize the data by default. When should you standardize and when should you not? Analytic techniques that depend on measures of distance to express similarity or dissimilarity between observations, or that depend on partitioning variability, typically benefit from standardized input variables.\nExamples of the former group are \\(K\\)-means clustering and hierarchical clustering. Here, the data are grouped into clusters based on their similarity, which is expressed as a function of a distance measure. Without scaling the data, large items tend to be further apart than small items, distorting the analysis toward inputs with large scale. Whether lengths are measured in meters, centimeters, or inches should not affect the analysis. Principal component analysis (PCA) is an example of a method in the second group, apportioning variability to linear combinations of the input variables. Without scaling, inputs that have a large variance can dominate the PCA to the point of not being interpretable or meaningful. Larger things tend to have larger variability compared to small things.\nBinary variables, are usually not scaled. This includes variables derived from encoding factor variables, see Section 14.3.1.\n\n\n\n\n\n\nTrain and Test Data\n\n\n\nAny preprocessing steps such as standardization, scaling, normalization, are applied separately to train and test data set. In the case of standardization, for example, it means that you split the data into train and test data sets first, then standardize each separately. The training data set is centered and scaled by the means and sample standard deviations in the training data set. The test data set is centered and scaled by the means and standard deviations in the test data set. Both data sets thus have the same properties of zero mean and standard deviation one.\nIf we were to standardize first and split into training and test data set second, neither data set would have zero mean and unit standard deviation. Furthermore, information from the training data (the mean and standard deviation) would leak into the test data set.",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Feature and Target Processing</span>"
    ]
  },
  {
    "objectID": "statlearning/featproc.html#normalization",
    "href": "statlearning/featproc.html#normalization",
    "title": "14  Feature and Target Processing",
    "section": "14.2 Normalization",
    "text": "14.2 Normalization\nNormalization is sometimes (often, actually) described as a transformation of data to normality; the transformed data appears Gaussian distributed. This is not what we consider normalization here. A variable \\(z\\) that is normalized with respect to the \\(L_2\\) norm has the property \\[\n||\\textbf{z}|| = \\sqrt{\\sum_{i=1}^n z_i^2}  = 1\n\\]\nA variable \\(x\\) can be normalized with respect to any norm by dividing each value by the norm of the vector. In the case of \\(L_2\\), \\[\nz_i = \\frac{x_i}{||\\textbf{x}||}\n\\]\nNormalization is a special case of scaling so that the resulting vector has unit length with respect to some norm. Any norm can be used to normalize the data, for example, the \\(L_1\\) norm \\(|\\textbf{x}|\\). Note that scaling by the standard deviation does not yield a vector with unit length, but a vector with unit standard deviation.\nThe following code normalizes the Gamma(3,2) sample with respect to \\(L_2\\).\n\nx_norm_l2 = preprocessing.normalize(x, norm='l2', axis=0)\n\nplt.figure()\nplt.hist(x_norm_l2,bins=20,color='0.7',alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\nFigure 14.2: Normalized data from the Gamma(3,2) distribution.\n\n\n\n\n\nNormalization of the sample from the Gamma(3,2) distribution maintains the general shape of the distribution and changes its scale (Figure 14.2).",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Feature and Target Processing</span>"
    ]
  },
  {
    "objectID": "statlearning/featproc.html#sec-encoding",
    "href": "statlearning/featproc.html#sec-encoding",
    "title": "14  Feature and Target Processing",
    "section": "14.3 Encoding",
    "text": "14.3 Encoding\nData processed by algorithms are represented as numbers, integers and real numbers. The information we collect is often not in the form that can be directly processed by a data science algorithm. Treatments in an experiment could be identified with numeric values, “1”, “2”, “3”, etc. We need to somehow tell the algorithm that these are simply identifying labels and should not be treated as numbers for which differences are meaningful. Treatment “2” is simply different from treatment “1”, it is not necessarily twice as much as treatment “1”. Whether the treatments are labeled “1”, “2”, “3”, or “A”, “B”, “C”, the same analysis must result.\nMany pieces of information come to us as in the form of character strings, either as structured or as unstructured text. A case of structured text data is when we use strings as labels for things. The treatment labels “A”, “B”, “C” are an example. An example of unstructured text is free-form text such as the quote\n\nThe strength of a bureaucracy is measured by its ability to resist change.\n\nAn algorithm that predicts the sentiment of the quote must translate these characters into a numerical representation. The process of transforming data from a human-readable form into a machine-readable form is known as encoding in machine learning. The reverse, translating from a machine-readable to a human-readable representation is known as decoding.\nIn this section we take a somewhat broader view of data encoding.\n\n\nDefinition: Data Encoding and Decoding\n\n\nEncoding of data is the process of translating information into a numerical format for processing by a statistical or machine learning algorithm.\nDecoding is translating information from the numerical format used in analytic processing back into human-readable form.\n\n\nEncoding is more important to us at this stage. When dealing with natural language processing tasks such as sequence-to-sequence modeling, encoding and decoding are important. An example is the translation of text from one language into another using artificiall neural networks. The input text is encoded into numeric format, processed by a network, and the output of that network is decoded by another network into the target language of translation.\n\nFactor Variables\nThe first case of encoding information is the processing of factor input variables. Factors are categorical variables representing levels (or classes, or categories, or states). For example, the variable treatment with levels “A”, “B”, and “C” is a three-level factor variable. When this variable is used in a statistical model, it enters the model through its levels, not through its values. That is why some software refers to the process of encoding factor variables as levelization.\nTo encode factor variables, three decisions affect the representation of the variable and the interpretation of the results:\n\nThe order of the levels\nThe method of encoding\nThe choice of reference level—if any\n\nThe order of the levels is important because it affects the order in which the factor levels enter the model and how the \\(\\textbf{X}\\) matrix is constructed. Consider the following example, which we will use throughout this section, of two factors with 2 and 3 levels, respectively, and a numeric variable \\(X\\).\n\n\n\nTable 14.2: Data with two factors \\(A\\) and \\(B\\) with 2 and 3 levels, respectively, and a numeric variable. To distinguish the values of the variables from subsequent encodings, the values are shown in boldface.\n\n\n\n\n\nY\nA\nB\n\\(X\\)\n\n\n\n\n1\n1\n1\n1\n\n\n2\n1\n2\n0.5\n\n\n3\n1\n3\n0.25\n\n\n4\n2\n1\n1\n\n\n5\n2\n2\n0.5\n\n\n6\n2\n3\n0.25\n\n\n\n\n\n\n\nOne-hot encoding\nIf we choose one-hot encoding, also called standard encoding or dummy encoding, then the \\(k\\) levels of each variable are transformed into \\(k\\) columns of 0/1 values. The value in the \\(j\\)th column is 1 if the variable is at level \\(j\\), 0 otherwise. The one-hot encoding for \\(A\\) and \\(B\\) in Table 14.2 is shown in Table 14.3.\n\n\n\nTable 14.3: One-hot encoding of \\(A\\) and \\(B\\).\n\n\n\n\n\nA\n\\(A_1\\)\n\\(A_2\\)\nB\n\\(B_1\\)\n\\(B_2\\)\n\\(B_3\\)\n\n\n\n\n1\n1\n0\n1\n1\n0\n0\n\n\n1\n1\n0\n2\n0\n1\n0\n\n\n1\n1\n0\n3\n0\n0\n1\n\n\n2\n0\n1\n1\n1\n0\n0\n\n\n2\n0\n1\n2\n0\n1\n0\n\n\n2\n0\n1\n3\n0\n0\n1\n\n\n\n\n\n\n\\(A_1\\) and \\(A_2\\) are the first and second columns of the encoded 2-level factor \\(A\\). So why does the order in which these columns enter a statistical model matter? Suppose our model is a linear model with inputs \\(X\\), a numeric variable, and \\(A\\) (\\(\\textbf{x} = [x, A_1, A_3]\\)): \\[\nf(\\textbf{x}) = \\beta_0 + \\beta_1x + \\beta_2 A_1 + \\beta_2 A_2\n\\] For the six observations in Table 14.2, the \\(\\textbf{X}\\) matrix of the model is \\[\n\\textbf{X} = \\left [\\begin{array}{rrrr}\n1 & 1    & 1 & 0 \\\\\n1 & 0.5  & 1 & 0 \\\\\n1 & 0.25 & 1 & 0 \\\\\n1 & 1    & 0 & 1 \\\\\n1 & 0.5  & 0 & 1 \\\\\n1 & 0.25 & 0 & 1 \\\\\n\\end{array} \\right]\n\\] There is a problem, the first column of \\(\\textbf{X}\\) is the sum of the third and forth column. This perfect linear dependency makes the \\(\\textbf{X}\\) matrix singular, the \\(\\textbf{X}^\\prime\\textbf{X}\\) matrix cannot be inverted, and the ordinary least squares solution cannot be computed.\n\nimport numpy as np\nX = np.array([[1,1,1,0], [1,0.5,1,0], [1,0.25,1,0], [1,1,0,1], [1, 0.5,0,1], [1,0.25,0,1]])\n\nXpX = X.transpose() @ X\nprint(XpX)\n\n[[6.    3.5   3.    3.   ]\n [3.5   2.625 1.75  1.75 ]\n [3.    1.75  3.    0.   ]\n [3.    1.75  0.    3.   ]]\n\nnp.linalg.inv(XpX)\n\nSingular matrix\n\n\nTo remedy this problem many software packages implicitly make one of the levels of a one-hot encoded factor a reference level. For example, R takes the first level of a factor as the reference level. The following code creates a data frame from the data in Table 14.2, declares the numeric variable A as a factor and assigns the second level as the reference level.\n\ndf &lt;- data.frame(y=c(1,2,3,4,5,6),\n                 x=c(1,0.5,0.25,1,0.5,0.25),\n                 A=c(1,1,1,2,2,2))\n\ndf$A &lt;- relevel(as.factor(df$A),ref=2)\ncoef(summary(lm(y ~ x + A, data=df)))\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  6.500000  0.2089277  31.11124 7.296355e-05\nx           -2.571429  0.2857143  -9.00000 2.895812e-03\nA1          -3.000000  0.1781742 -16.83746 4.561983e-04\n\n\nThe analysis now succeeds but the coefficients reported seem to be for a model without the reference level, namely \\[\nf(\\textbf{x}) = \\beta_0 + \\beta_1x + \\beta_2 A_1\n\\]\nYou can easily verify by specifying this model directly:\n\ndf2 &lt;- data.frame(y=c(1,2,3,4,5,6),\n                  x=c(1,0.5,0.25,1,0.5,0.25),\n                  A1=c(1,1,1,0,0,0))\n\ncoef(summary(lm(y ~ x + A1, data=df2)))\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  6.500000  0.2089277  31.11124 7.296355e-05\nx           -2.571429  0.2857143  -9.00000 2.895812e-03\nA1          -3.000000  0.1781742 -16.83746 4.561983e-04\n\n\nSo where did the information about \\(A_2\\) end up? To see how this works, consider the model formula for the cases \\(A_1 = 0\\) and \\(A_1 = 1\\):\n\\[\\begin{align*}\n    A_1 = 0 &: f(\\textbf{x}) = \\beta_0 + \\beta_1 x \\\\\n    A_1 = 1 &: f(\\textbf{x}) = \\beta_0 + \\beta_1 x + \\beta_2 A_1\n\\end{align*}\\]\nBut the value \\(A_1=0\\) corresponds to \\(A_2 = 1\\). The simple linear regression \\(\\beta_0 + \\beta_1 x\\) is the response for the case where \\(A_2 = 1\\). The coefficient \\(\\beta_2\\) now measures the difference between the two levels of \\(A\\), with \\(A = 2\\) serving as the reference level. The model has two intercepts, one for \\(A=1\\), and one for \\(A=2\\), their values are \\(6.5 - 3 = 3.5\\) and \\(6.5\\), respectively.\nYou can also see this from the coefficient estimate in the R output: the sample mean of \\(Y\\) when \\(A = 1\\) is \\(\\overline{y}_{A=1} = 3\\) and \\(\\overline{y}_{A=2} = 6\\). Their difference is \\(\\widehat{\\beta}_2 = -3\\).\nIf the first level is chosen as the reference level, this estimate has the opposite sign:\n\ndf &lt;- data.frame(y=c(1,2,3,4,5,6),\n                 x=c(1,0.5,0.25,1,0.5,0.25),\n                 A=c(1,1,1,2,2,2))\n\ndf$A &lt;- relevel(as.factor(df$A),ref=1)\ncoef(summary(lm(y ~ x + A, data=df)))\n\n             Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept)  3.500000  0.2089277 16.75220 0.0004631393\nx           -2.571429  0.2857143 -9.00000 0.0028958122\nA2           3.000000  0.1781742 16.83746 0.0004561983\n\n\n\\(A=1\\) now serves as the reference level and the coefficient associated with A2 is the difference between the intercept at \\(A=2\\) and at \\(A=1\\). The absolute intercepts of the two level remain the same, \\(3.5\\) for \\(A=1\\) and \\(3.5 + 3 = 6.5\\) for \\(A=2\\).\nWhen a factor has more than two levels, reference levels in one-hot encoding works the same way, the effects of all levels is expressed as the difference of the level with respect to the reference level.\n\n\n\n\n\n\nCaution\n\n\n\nIf a statistical model contains factors and you interpret the coefficient estimates you must make sure to understand the encoding method–including level ordering and reference levels–otherwise you might draw wrong conclusions about the effects in the model. Whn making predictions, the choice of order and reference level does not matter, predictions are invariant, as you can easily verify in the example above.\n\n\nThe issue of a singular cross-product matrix \\(\\textbf{X}^\\prime\\textbf{X}\\) could have been prevented in the model with \\(X\\) and \\(A\\) by dropping the intercept. This model has a full-rank \\(\\textbf{X}\\) matrix \\[\n\\textbf{X} = \\left [\\begin{array}{rrr}\n1    & 1 & 0 \\\\\n0.5  & 1 & 0 \\\\\n0.25 & 1 & 0 \\\\\n1    & 0 & 1 \\\\\n0.5  & 0 & 1 \\\\\n0.25 & 0 & 1 \\\\\n\\end{array} \\right]\n\\]\nEven without an intercept, the problem resurfaces as soon as another factor enters the model. Suppose we now add \\(B\\) in one-hot encoding to the model. The \\(\\textbf{X}\\) matrix gets three more columns:\n\\[\n\\textbf{X} = \\left [\\begin{array}{rrrrrr}\n1    & 1 & 0 & 1 & 0 & 0\\\\\n0.5  & 1 & 0 & 0 & 1 & 0\\\\\n0.25 & 1 & 0 & 0 & 0 & 1\\\\\n1    & 0 & 1 & 1 & 0 & 0\\\\\n0.5  & 0 & 1 & 0 & 1 & 0\\\\\n0.25 & 0 & 1 & 0 & 0 & 1\\\\\n\\end{array} \\right]\n\\] Again, we end up with perfect linear dependencies among the columns. The sum of columns 2–3 equals the sum of columns 4–6. It is thus customary to not drop the intercept when factors are present in the model.\n\n\nAverage effect encoding\nAn encoding that avoids the singularity issue creates only \\(k-1\\) columns from the \\(k\\) levels of the factor and expresses the effect of the levels as differences in the effects from the average effect across all levels. This encoding also relies on the choice of a reference level. Suppose we apply average effect encoding to \\(B\\) and choose \\(B=3\\) as the reference level.\n\nAverage effect encoding for \\(B\\) with reference level \\(B=3\\).\n\n\nB\n\\(B_1\\)\n\\(B_2\\)\n\n\n\n\n1\n\\(1\\)\n\\(0\\)\n\n\n2\n\\(0\\)\n\\(1\\)\n\n\n3\n\\(-1\\)\n\\(-1\\)\n\n\n1\n\\(1\\)\n\\(0\\)\n\n\n2\n\\(0\\)\n\\(1\\)\n\n\n3\n\\(-1\\)\n\\(-1\\)\n\n\n\n\n\nBaseline encoding\nIn this encoding, \\(k-1\\) columns are created for the \\(k\\) levels of the factor. The first level is taken as the baseline and the effects are expressed in terms of differences between successive levels. Baseline encoding of \\(B\\) results in the following encoding:\n\nBaseline encoding for \\(B\\) with reference level \\(B=3\\).\n\n\nB\n\\(B_1\\)\n\\(B_2\\)\n\n\n\n\n1\n\\(0\\)\n\\(0\\)\n\n\n2\n\\(1\\)\n\\(0\\)\n\n\n3\n\\(1\\)\n\\(1\\)\n\n\n1\n\\(0\\)\n\\(0\\)\n\n\n2\n\\(1\\)\n\\(0\\)\n\n\n3\n\\(1\\)\n\\(1\\)\n\n\n\n\n\nExample: Baseline and Average Encoding\n\n\nFor our little data set, we compute the baseline and average effect encoding here. Before doing so, let’s fit a model with factor \\(B\\), and without intercept. The coefficients associated with \\(B\\) are the raw effects of the three levels of \\(B\\). The intercept is automatically added in model formula expressions in R. You can remove it with -1 from the model.\n\ndf &lt;- data.frame(y=c(1,2,3,4,5,6),\n                 x=c(1,0.5,0.25,1,0.5,0.25),\n                 A=c(1,1,1,2,2,2),\n                 B=c(1,2,3,1,2,3))\n\nmodB &lt;- lm(y ~ as.factor(B) -1, data=df)\ncoef(summary(modB))\n\n              Estimate Std. Error  t value   Pr(&gt;|t|)\nas.factor(B)1      2.5        1.5 1.666667 0.19417135\nas.factor(B)2      3.5        1.5 2.333333 0.10183797\nas.factor(B)3      4.5        1.5 3.000000 0.05766889\n\n\nThe coefficients of the factor effects are \\(\\widehat{\\beta}_{B1} =\\) 2.5, \\(\\widehat{\\beta}_{B2} =\\) 3.5, and \\(\\widehat{\\beta}_{B3} =\\) 4.5.\nNext we fit the model using baseline encoding for \\(B\\).\n\nX &lt;- matrix(c(0,1,1,0,1,1,0,0,1,0,0,1),nrow=6,ncol=2)\n\nmodB_baseline &lt;- lm(df$y ~ X)\n\ncoef(summary(modB_baseline))\n\n            Estimate Std. Error   t value  Pr(&gt;|t|)\n(Intercept)      2.5    1.50000 1.6666667 0.1941713\nX1               1.0    2.12132 0.4714045 0.6695150\nX2               1.0    2.12132 0.4714045 0.6695150\n\n\nThe (Intercept) of this model represents the level \\(B=1\\), X1 is the difference of level \\(B=2\\) to level \\(B=1\\) and X2 measures the difference between \\(B=3\\) and \\(B=2\\).\nIf \\(B\\) is expressed in average effect encoding we obtain the following:\n\nX &lt;- matrix(c(1,0,-1,1,0,-1,0,1,-1,0,1,-1),nrow=6,ncol=2)\n\nmodB_ave_eff &lt;- lm(df$y ~ X)\n\ncoef(summary(modB_ave_eff))\n\n                 Estimate Std. Error       t value   Pr(&gt;|t|)\n(Intercept)  3.500000e+00  0.8660254  4.041452e+00 0.02726185\nX1          -1.000000e+00  1.2247449 -8.164966e-01 0.47402139\nX2          -2.110977e-16  1.2247449 -1.723605e-16 1.00000000\n\n\nWe see from the initial analysis that the average effect of \\(B\\) is \\(1/3 \\times (2.5+3.5+4.5) = 3.5\\). The (Intercept) estimate represents this average in the average effect encoding. X1 and X2 represent the differences of \\(B=1\\) and \\(B=2\\) from the average. If we place the \\(-1\\) row at level \\(B=2\\), then X1 and X2 are the differences of \\(B=1\\) and \\(B=3\\) from the average effect of \\(B\\):\n\nX2 &lt;- matrix(c(1,-1,0,1,-1,0,0,-1,1,0,-1,1),nrow=6,ncol=2)\n\nmodB_ave_eff &lt;- lm(df$y ~ X2)\n\ncoef(summary(modB_ave_eff))\n\n            Estimate Std. Error    t value   Pr(&gt;|t|)\n(Intercept)      3.5  0.8660254  4.0414519 0.02726185\nX21             -1.0  1.2247449 -0.8164966 0.47402139\nX22              1.0  1.2247449  0.8164966 0.47402139\n\n\n\n\n\n\n\nContinuous Variables\nWhy would you encode continuous variables, that are already in numerical form?\nSuppose you are modeling a target as a function of time and the measurements are taken in minute intervals. Maybe such a granular resolution is not necessary and you are more interested in seasonal or annual trends. You can convert the time measurements easily into months, seasons, or years, using date-time functions in the software. Should you now treat the converted values as continuous measurements or as categories of a factor?\nWhen working with age variables we sometimes collect them into age groups. Age groups 0-9 years, 10-24 years, and 25-40 years are not evenly spaced and are no longer continuous. Age has been discretized into a factor variable. Other terms used for the process of discretizing continuous variables are binning and quantizing.\nThe advantage of using factors instead of the continuous values lies in the introduction of nonlinear relationships while maintaining interpretability. If you model age as a continuous variable, then \\[\nY = \\beta_0 + \\beta_1 \\text{age} + \\cdots + \\epsilon\n\\] implies a linear relationship between age and the target. Binning age into three groups, on the other hand, leads to the model \\[\nY = \\beta_0 + \\beta_1\\text{[Age=0-9]} + \\beta_2\\text{[Age=10-24]} + \\beta_3\\text{[Age=25-40]} + \\cdots + \\epsilon\n\\] This model allows for different effects in the age groups.\nThe function KBinsDiscretizer in sklearn.preprocessing bins vectors and matrices of continuous variables using different encoding methods. encode='ordinal' returns a vector or matrix of integers identifying the bin a value belongs to. The default binning strategy (strategy='quantile') is to form bins that contain the same number of points.\n\nbinner = preprocessing.KBinsDiscretizer(n_bins=[5], encode='ordinal').fit(x)\n\nx_binned = binner.transform(x)\nx_binned[1:10,]\n\narray([[0.],\n       [2.],\n       [0.],\n       [2.],\n       [4.],\n       [0.],\n       [4.],\n       [4.],\n       [2.]])\n\n\nencode='onehot-dense returns a dense matrix of one-hot encoded variables. The following code uses the uniform binning strategy in which all bins have the same widths.\n\nbinner_q = preprocessing.KBinsDiscretizer(n_bins=[5], \n              encode='onehot-dense',\n              strategy=\"uniform\",\n              subsample=None).fit(x)\n\nx_binned = binner_q.transform(x)\nx_binned[1:10,]\n\narray([[1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 1., 0., 0., 0.]])\n\n\n\n\nUnstructured Text\nIf the values of a variable are structured text in the sense that the values are a limited number of single character strings, the previous encoding methods apply. An example is names of U.S. states or cities, names of active ingredients, medications, places, and so on.\nWhen text is unstructured, it can vary in length and content. How can we encode a sentence such as\n\nThe strength of a bureaucracy is measured by its ability to resist change.\n\nThe encoding itself is just one of possibly many steps in structuring unstructured data.\nParsing and tokenization breaks unstructured text into smaller units (tokens) for further analysis. A token can be a word, a punctuation symbol, an n-gram such as “Covid 19 pandemic”. Entity recognition finds named entities such as places, persons, businesses in text data. Feature extraction pulls relevant characteristics and attributes from text data and forms numeric columns from it. For example, feature extraction finds temperature or precipitation values in weather reports. Normalization transforms unstructured data into a standardized format, removes duplicates, applies consistent rules to spelling, capitalization, etc.\nAfter all that, suppose we have some unstructured data that needs to be encoded.\n\nOne-hot encoding\nOne-hot encoding can be applied to unstructured text as well, but now it is not immediately obvious how many levels to apply in the encoding. Typically, the unstructured text is compared to a dictionary of tokens. The dictionary can be made up of the tokens found in the text to be encoded, or comprise a larger corpus of text. Clearly, dictionaries tend to be large and one-hot encoding of text leads to data matrices with many columns.\nIf the dictionary has 10,000 entries, each word or token is represented by a 10,000-element vector that contains 9,999 zeros and a single one that matches the position of the word in the dictionary. This leads to very high-dimensional problems that are also very sparse.\nTo avoid those problems it is popular to represent words in a lower-dimensional space of real numbers. In other words, rather than choosing 9,999 zeros and a single 1 to represent a particular word, we might choose a 20-dimensional vector of real numbers. This technique is known as word embedding.\n\n\nWord embeddings\nhttps://www.turing.com/kb/guide-on-word-embeddings-in-nlp\nWord embeddings encode words in a low-dimensional space as real-valued vectors. A big advantage of embeddings is that they can be pre-trained and they can capture semantic and syntactic information. Words with similar meaning should have similar representation in the vector space. The distance between two embedding vectors can then be used to measure the similarity between text expressions.\nOne of the most common embeddings is Word2Vec, developed by Google. It measures the semantic similarity of words based on cosine similarity of the embedding vector. Cosine similarity between two \\((n \\times 1)\\) vectors \\(\\textbf{a}\\) and \\(\\textbf{b}\\) is the cosine of their angle, \\[\n\\cos(\\theta) = \\frac{\\sum_{i=1}^n a_i b_i}{\\sqrt{\\sum_{i=1}^n a_i^2}\\sqrt{\\sum_{i=1}^n b_i^2}}\n\\] A cosine similarity of \\(1\\) indicates complete similarity, \\(0\\) indicates lack of correlation, and \\(-1\\) indicates complete dissimilarity.\nWord2Vec is based on two types of neural network models, a continuous bag-of-word (CBOW) model and a skip-gram model. The following Python code trains Word2Vec on a sequence of quotes about change and generates embeddings of length 20. In practical applications the corpus of text you train the model on is much larger, the size of the embedding is 100 or more. You can also download existing Word2Vec embeddings and use those instead of training your own.\n\nfrom gensim.models import Word2Vec\nimport gensim\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize, punkt \n\n\ns = \"The strength of a bureaucracy is measured by its ability to resist change.\\nThe only constant is change.\"\ns = s + \"Change is the law of life. And those who look only to the past or present are certain to miss the future\\n\"\ns = s + \"To improve is to change; to be perfect is to change often\\n\"\ns = s + \"Change before you have to\\n\"\ns = s + \"Not everything that is faced can be changed, but nothing can be changed until it is faced\\n\"\ns = s + \"People who are crazy enough to think they can change the world are the ones who do\"\n\nf = s.replace(\"\\n\", \" \")\n\ndata = []\n \n# iterate through each sentence to tokenize the sentence\nfor i in sent_tokenize(f):\n    temp = []\n    for j in word_tokenize(i):\n        temp.append(j.lower())\n    data.append(temp)\n \n# Create and train a continuous bag-of-words model\nmodel1 = gensim.models.Word2Vec(data, \n                                min_count=1,\n                                vector_size=20, \n                                window=5)\n \nprint(model1.wv[\"strength\"])\n\n[ 0.0170561   0.02584315  0.03140133 -0.01401705  0.03660801  0.01414236\n  0.01435158 -0.01175447 -0.01568154 -0.01181616  0.02141082  0.00037154\n -0.04789285 -0.04828245 -0.03068198 -0.00066577  0.01007916  0.04715109\n  0.02784073 -0.02150913]\n\nprint(model1.wv[\"ability\"])\n\n[ 0.03692275 -0.03363638  0.02789302 -0.04761012 -0.0040024  -0.04343542\n -0.02548623  0.04648114 -0.0093174   0.01458398  0.04536819  0.04468612\n -0.04102621 -0.01507848  0.04946372  0.02549913 -0.00791798 -0.04344965\n  0.01477943 -0.03337966]\n\nprint(model1.wv[\"change\"])\n\n[-0.00833561  0.00161311 -0.02078105 -0.03846731 -0.00756165  0.01230773\n -0.00439966  0.02841706 -0.01395713  0.01146699  0.02746079  0.04177966\n -0.0072043  -0.04605786  0.02223997  0.00267907  0.03758679 -0.00414909\n -0.01355227 -0.04394563]\n\nprint(\"Cosine similarity between 'strength and 'ability' - CBOW : \",\n      model1.wv.similarity('strength', 'ability'))\n\nCosine similarity between 'strength and 'ability' - CBOW :  0.04198855\n\n \nprint(\"Cosine similarity between 'strength and 'change' - CBOW : \",\n      model1.wv.similarity('strength', 'change'))\n\nCosine similarity between 'strength and 'change' - CBOW :  0.20074466\n\nprint(\"Cosine similarity between 'past and 'future' - CBOW : \",\n      model1.wv.similarity('past', 'future'))\n\nCosine similarity between 'past and 'future' - CBOW :  0.2608111\n\n \n# Create and train a skip Gram model\nmodel2 = gensim.models.Word2Vec(data, \n                                min_count=1, \n                                vector_size=20,\n                                window=5, sg=1)\n\nprint(model2.wv[\"strength\"])\n\n[ 1.6670765e-02  2.5719611e-02  3.1497952e-02 -1.4134813e-02\n  3.6302831e-02  1.3883009e-02  1.4506035e-02 -1.1257441e-02\n -1.5849181e-02 -1.1934853e-02  2.1613920e-02  5.8639314e-05\n -4.7993530e-02 -4.8156403e-02 -3.0682029e-02 -4.1891963e-04\n  1.0360058e-02  4.7084000e-02  2.7559893e-02 -2.1798888e-02]\n\nprint(model2.wv[\"ability\"])\n\n[ 0.03672264 -0.03360568  0.02790979 -0.04771556 -0.00409749 -0.04356595\n -0.02543401  0.04678207 -0.00931965  0.0144831   0.04550748  0.04467208\n -0.04109946 -0.0150124   0.04958136  0.02554292 -0.00773899 -0.04348198\n  0.01469189 -0.03355295]\n\nprint(model2.wv[\"change\"])\n\n[-0.01012116  0.00203569 -0.02054695 -0.03874388 -0.00886681  0.01132476\n -0.00387686  0.03110312 -0.01429986  0.01094422  0.0288924   0.04091645\n -0.0075365  -0.04540935  0.02317034  0.00313406  0.03954622 -0.00476322\n -0.0144198  -0.04520554]\n\nprint(\"Cosine similarity between 'strength' and 'ability' - Skip Gram : \",\n      model2.wv.similarity('strength', 'ability'))\n\nCosine similarity between 'strength' and 'ability' - Skip Gram :  0.043875016\n\n \nprint(\"Cosine similarity between 'strength' and 'change' - Skip Gram : \",\n      model2.wv.similarity('strength', 'change'))\n\nCosine similarity between 'strength' and 'change' - Skip Gram :  0.19225661",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Feature and Target Processing</span>"
    ]
  },
  {
    "objectID": "statlearning/featproc.html#transforming-the-target",
    "href": "statlearning/featproc.html#transforming-the-target",
    "title": "14  Feature and Target Processing",
    "section": "14.4 Transforming the Target",
    "text": "14.4 Transforming the Target\nThe previously discussed preprocessing steps can be applied to any variable, input or target variable.\nThe purpose of transforming the target variable is to meet distributional assumptions of the analysis. For example, many statistical estimators have particularly nice properties when the data are Gaussian distributed. A common transformation of \\(Y\\) is thus to morph its distribution so that the transformed \\(Y^*\\) is closer to Gaussian distributed than \\(Y\\). Sometimes transformations just aim to create more symmetry or to stabilize the variance across groups of observations. Transformations change the relationship between (transformed) target and the inputs, it is common to transform targets to a scale where effect are linear. We can then model additive effects on the transformed scale instead of complex nonlinear effects on the original scale of the data.\n\nDiscretizing\nDiscretizing a continuous target variable is commonly done to classify the target. If observations are taken with measurement error, we might not be confident in the measured values but we are comfortable classifying an outcome as high or low based on a threshold for \\(Y\\) and model the transformed data using logistic regression.\n\n\nNonlinear Transformations\nNonlinear transformations of a continuous target variable apply functions to modify the distributional properties. A common device to transform right-skewed distributions (long right tail) of positive values is to take logarithms or square roots. If \\(Y\\) has a log-normal distribution, then the log transformation yields a Gaussian random variable.\nA random variable \\(Y\\) has a log-normal distribution with parameters \\(\\alpha\\) and \\(\\beta\\) if its p.d.f. is given by \\[\np(y) = \\frac{1}{y\\sqrt{2\\pi\\beta}}\\exp\\left\\{-\\frac{(\\log y - \\alpha)^2}{2\\beta} \\right\\}, \\qquad y &gt; 0\n\\] \\(Y\\) has mean \\(\\mu = \\exp\\{\\alpha + \\beta/2\\}\\) and variance \\(\\sigma^2 = (e^\\beta - 1) \\exp\\{2\\alpha + \\beta\\}\\). If \\(Y \\sim \\text{Lognormal}(\\alpha,\\beta)\\), then \\(\\log Y\\) has a Gaussian distribution with mean \\(\\alpha\\) and variance \\(\\beta\\). We can also construct a log-normal variable from a standard Gaussian: if \\(Z \\sim G(0,1)\\), then \\(Y = \\exp\\{\\alpha + \\beta Z\\}\\) has a log-normal distribution. Figure 14.3 shows the histogram of 1,000 random samples from a Lognormal(2,1/2) distribution and the histogram of the log-transformed values.\n\n\n\n\n\n\n\n\nFigure 14.3: Histograms of 1,000 random samples from \\(Y \\sim \\text{Lognormal}(2,1/2)\\) and of \\(\\log Y\\).\n\n\n\n\n\n\nBox-Cox family\nThe Box-Cox family of transformations is a special case of power transformations that was introduced by Box and Cox (1964). Most commonly used is the one-parameter version of the Box-Cox transformation \\[\ny^{(\\lambda)} = \\left \\{\n\\begin{array}{ll}\n\\frac{y^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\ne 0 \\\\\n\\log y & \\text{if } \\lambda = 0\n\\end{array}\\right.\n\\tag{14.1}\\]\nThis transformation applies if \\(y &gt; 0\\). For the more general case, \\(y &gt; -c\\), the two-parameter Box-Cox transformation can be used \\[\ny^{(\\lambda)} = \\left \\{\n\\begin{array}{ll}\n\\frac{(y+c)^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\ne 0 \\\\\n\\log (y+c) & \\text{if } \\lambda = 0\n\\end{array}\\right.\n\\]\n\\(c\\) is simply a value by which to shift the distribution so that \\(y+c &gt; 0\\). The key parameter in the Box-Cox transformation is \\(\\lambda\\).\nThe term \\(- 1\\) in the numerator and the division by \\(\\lambda\\) for \\(\\lambda \\ne 0\\) in Equation 14.1 are linear operations that do not affect the proximity to normality of \\(y^{(\\lambda)}\\). An abbreviated version of Equation 14.1 is\n\\[\ny^{(\\lambda)} = \\left \\{\n\\begin{array}{ll}\ny^\\lambda & \\text{if } \\lambda \\ne 0 \\\\\n\\log y    & \\text{if } \\lambda = 0\n\\end{array}\\right.\n\\tag{14.2}\\]\nFor a given value of \\(\\lambda\\), the transformation is monotonic, mapping large values to large values if \\(\\lambda &gt; 0\\) and large values to small values if \\(\\lambda &lt; 0\\). The transformation thus retains the ordering of observations (Figure 14.4).\n\n\n\n\n\n\n\n\nFigure 14.4: Box-Cox transformations for various values of \\(\\lambda\\)\n\n\n\n\n\nHow do we find \\(\\lambda\\)? If the assumption is that for some value \\(\\widehat{\\lambda}\\) the distribution of \\(y^{(\\widehat{\\lambda})}\\) is Gaussian, then \\(\\lambda\\) can be estimated by maximum likelihood assuming a Gaussian distribution–after all, we are looking for the value of \\(\\lambda\\) for which the Gaussian likelihood of the transformed data is highest.\nThe following Python code simulates 1,000 samples from a Gamma(3,2) distribution and from a Lognormal(2,1) distribution.\n\nnp.random.seed(234)\nx1 = np.random.gamma(shape=3,scale=2,size=1000)\nx1 = np.reshape(x1,(-1,1))\n\n# numpy's lognormal distribution uses the mean and standard deviation\n# of the underlying normal distribution as parameters \nx2 = np.random.lognormal(mean=2,sigma=1,size=1000)\nx2 = np.reshape(x2,(-1,1))\n\nThe PowerTransformer method in sklearn.preprocessing estimates the \\(\\lambda\\) parameter of the Box-Cox transformation. We expect the estimate for the log-normal data to be close to zero, since a log transformation (\\(\\lambda=0\\)) of a log-normal random variable yields a Gaussian random variable.\n\npt = preprocessing.PowerTransformer(method='box-cox', standardize=False)\nx1_boxcox = pt.fit_transform(x1)\npt.lambdas_\n\narray([0.273836])\n\nx2_boxcox = pt.fit_transform(x2)\npt.lambdas_\n\narray([0.01351602])\n\n\nThe estimate of \\(\\lambda\\) to transform the Gamma(3,2) data is \\(\\widehat{\\lambda} = 0.2738\\) and the estimate to transform the log-normal data is \\(\\widehat{\\lambda} = 0.0135\\). The impact on the distribution of the transformed Gamma data is shown in Figure 14.5.\n\n\n\n\n\n\n\n\nFigure 14.5: 1,000 samples from Gamma(3,2) and Box-Cox transformation with \\(\\widehat{\\lambda} = 0.2738\\).\n\n\n\n\n\n\n\nQuantile transformation\nThe Box-Cox transformation does a good job to transform the data closer to a Gaussian distribution in the previous examples. It is not fail safe, however. Data that appear uniform or data with multiple modes do not transform to normality easily with the Box-Cox family.\nHowever, a transformation based on the quantiles of the observed data can achieve this. In fact, you can use quantiles to generate data from any distribution.\nIf \\(Y\\) is a random variable with c.d.f. \\(F(y) = \\Pr(Y \\le y)\\), then the quantile function \\(Q(p)\\) is the inverse of the cumulative distribution function; for a given probability \\(p\\), it returns the value \\(y\\) for which \\(F(y) = p\\).\nNow here is a possibly surprising result: if \\(Y\\) has c.d.f. \\(F(y)\\), then we can think of the c.d.f as a transformation of \\(Y\\). What would its distribution look like? The following R code draws 1,000 samples from a G(\\(2,1.5^2\\)) distribution and plots the histogram of the c.d.f. values \\(F(y)\\).\n\nset.seed(45)\nyn &lt;- rnorm(1000,mean=2,sd=1.5)\nF_yn &lt;- pnorm(yn,mean=2,sd=1.5)    \nhist(F_yn,main=\"\")\n\n\n\n\n\n\n\n\nThe distribution of \\(F(y)\\) is uniform on (0,1)—this is true for any distribution, not just the Gaussian.\nWe can combine this result with the following, possibly also surprising, result: If \\(U\\) is a uniform random variable, and \\(Q(p)\\) is the quantile function of \\(Y\\), then \\(Q(u)\\) has the same distribution as \\(Y\\). This is how you can generate random numbers from any distribution if you have a generator of uniform random numbers: plug the uniform random numbers into the quantile function.\n\nnorm_rv &lt;- qnorm(runif(1000),mean=2,sd=1.5)\nhist(norm_rv)\n\n\n\n\n\n\n\n\nThese are the key results behind quantile transformation, mapping one distribution to another based on quantiles.\nThe QuantileTransformer in sklearn.preprocessing can transform data based on quantiles to a uniform (default) or a Gaussian distribution. The results of transformation to normality for the Gamma(3,2) data are shown in Figure 14.6. These are not very different from the results of the Box-Cox transformation. However, the quantile transformation method always works, even in cases where the Box-Cox family of transformation fails to get close to normality.\n\nnp.random.seed(234)\nquantizer = preprocessing.QuantileTransformer(\n    output_distribution='normal')\n\nx1_trans = quantizer.fit_transform(x1)\n\nplt.figure()\nplt.hist(x1,bins=20,color='0.7',alpha=0.7,label='Original')\nplt.hist(x1_trans,color='magenta',alpha=0.5,label='Quantile transformed')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFigure 14.6: Quantile transformation to normality for Gamma(3,2) data.\n\n\n\n\n\n\n\nTo Transform or not to Transform\nWhy do we not always apply a nonlinear transform to a continuous target variable to achieve greater symmetry, stable variance, greater normality? After all, the results of the transformations are impressive. Figure 14.7 shows a quantile transformation to normality for data from a two-component mixture of Gamma densities. The bimodal distribution maps to a standard Gaussian like magic.\n\n\n\n\n\n\n\n\nFigure 14.7: Quantile transformation to normality for bimodal data from a mixture distribution.\n\n\n\n\n\nWhat are you giving up by analyzing the transformed target instead of the original target?\nWhen the goal of data analytics is testing research hypotheses, we usually observe the target variable of interest and formulate hypotheses about its properties. What do these hypotheses mean in terms of the transformed data? This is not always clear, in particular when highly nonlinear transformations such as quantiles are involved.\nThe parameters of the distribution of \\(Y^*\\), the transformed target, are not estimating the parameters of the distribution of \\(Y\\). The mean of \\(\\log Y\\) is not the log of the mean of \\(Y\\). When the transformation can be inverted, we usually end up with biased estimators. For example, if \\(Y\\) has a Lognormal(\\(\\alpha,\\beta\\)) distribution, then \\(\\overline{Y}\\) is an unbiased estimate of \\[\n\\text{E}[Y] = \\exp\\{\\alpha + \\beta/2\\}\n\\]\nIf we log-transform the data the sample mean of the transformed data \\(y^* = \\log y\\) estimates \\[\n\\text{E}[\\overline{Y}^*] = \\frac{1}{n}\\sum_{i=1}^n \\text{E}[\\log Y_i]\n\\] The exponentiation of this estimator is not an unbiased estimator of \\(\\text{E}[Y]\\).\n\n\n\nFigure 14.4: Box-Cox transformations for various values of \\(\\lambda\\)\nFigure 14.7: Quantile transformation to normality for bimodal data from a mixture distribution.\n\n\n\nBox, G. E. P., and D. R. Cox. 1964. “An Analysis of Transformations.” Journal of the Royal Statistical Society. Series B (Methodological) 26 (2): 211–52. http://www.jstor.org/stable/2984418.",
    "crumbs": [
      "Part III. Statistical Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Feature and Target Processing</span>"
    ]
  },
  {
    "objectID": "ethics/intro.html",
    "href": "ethics/intro.html",
    "title": "15  Introduction",
    "section": "",
    "text": "16 Why Now?\nThe conversations about ethics, bias, and fairness in data science have intensified over the past decade. Yet, data analysis and decisions based on data are not a new phenomenon. What changed? Why is data ethics a hot button issue today?",
    "crumbs": [
      "Part IV. Applied Ethics in Data Science",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ethics/intro.html#cybersecurity",
    "href": "ethics/intro.html#cybersecurity",
    "title": "15  Introduction",
    "section": "17.1 Cybersecurity",
    "text": "17.1 Cybersecurity\nWith respect to the consequences of missteps and unintended consequences, data systems are entering a phase that cybersecurity has gone through previously. Today, the cybersecurity posture of an organization is a top priority. CISOs, Chief Information Security Officers, are common among high-ranking executives, whereas Chief Data Officers and Chief Analytic Officers tend to be found in Director-level positions. That is changing as the strategic importance of data-related activity increases; CDOs and CAOs are on the way up on the corporate ladder.\nInformation security, the security of IT assets, data, and operations, is a modern form of risk management and mitigation. This includes safeguarding information about the organization and its clients (customers).\n\n\nExample: Equifax Data Breach\n\n\nApache Struts is a web application framework to develop Java applications. On March 7, 2017, a patch was released for Struts to fix a security vulnerability. The national credit bureau company Equifax had not updated applications on its website when the vulnerability was exploited on its site on March 12, 2017. The hackers accessed information about Equifax employees and used their credentials to scan databases and extract personal information on over 160 million citizens. The breached information included names, birth dates, addresses, social security numbers, and driver’s license numbers.\nWhile it took only five days from the release of the security fix to an exploit, it took Equifax more than two months to discover the breach and it took until September 2017 to disclose the breach. A week later the Chief Information Officer and Chief Security Officer left the company. Hundreds of lawsuits were filed against Equifax after the breach became public. In 2019, Equifax settled with the Federal Trade Commission for $575 million, $475 million in restitution and $100 million in fines.\nAfter the Equifax data breach, the Chief Information Officer of a major international bank called Apache Struts “the CIO killer.”\nRead more about the Equifax case here including the criticism of the company due to inadequate response, fumbling notification websites, and executives selling stock before the breach was made public.",
    "crumbs": [
      "Part IV. Applied Ethics in Data Science",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ethics/intro.html#model-risk-management",
    "href": "ethics/intro.html#model-risk-management",
    "title": "15  Introduction",
    "section": "17.2 Model Risk Management",
    "text": "17.2 Model Risk Management\nData breaches are not the only source of data-related risk to be mitigated. You can now add the risk from applying models trained on data to the list—models can be incorrect, and they can be misapplied and misused. In financial services, model risk management (MRM) has been a staple for years.\n\n\nDefinition: Model Risk Management (MRM)\n\n\nModel risk management is the management and mitigation of risks that arise from the consequences of decisions based on models that fail to perform.\n\n\nIn general, MRM is concerned with risks from the consequences of decisions based on incorrect, misapplied, or misused models. The FDIC (Federal Deposit Insurance Corporation) provides guidance to banks about effective model risk management. Banks are relying routinely on models for valuations, underwriting credit, determining adequacy of reserves, safeguarding client assets, etc. The use of models invariably presents risk that can lead to financial loss, poor decision making, and damage to reputation. The FDIC notes two primary reasons for model risk:\n\nModels may be fundamentally wrong due to incorrect assumptions, underlying theories, choice of sample design, choice of numerical routines, implementation errors, inadequate or erroneous inputs, and so forth.\nThe model may be fundamentally sound but is used or applied incorrectly. Real-world events might prove the underlying assumptions of a model inappropriate—e.g., a global pandemic. The model might be applied to situations for which it was not developed, for example, to new products or markets.\n\nWe recognize in these points the concepts of data drift and model drift discussed early on. It is also clear that the concept of model risk management is not germane to the financial service industry—it applies everywhere. A key principle of effective MRM is to challenge models effectively, testing and validation play an important part. When models are developed and tested under assumptions that do not reflect their use in real life, model risk increases dramatically. This is what happened to Microsoft Tay.\n\n\nExample: Microsoft Tay\n\n\nMicrosoft released in 2016 a Twitter AI chat bot, called Tay, as in thinking-about-you, that was trained to behave like a teenager in interactions with Twitter users and learn from them. Within less than a day online, Tay had turned into a hateful racist, claiming in offensive tweets that, for example, “Hitler was right”, recommending feminists should all die, and denying the Holocaust. Microsoft blamed Tay’s responses on trolls teaching the AI bot deliberately inflammatory content. Tay was a successful AI project in that the bot learned how to communicate based on who it was communicating with. But the internet can be a dark and awful place and Tay learned awful things from the people that talked to her.\nThe bot was not a successful AI project in that it did not incorporate content moderation and boundaries. The developers probably assumed that users on the internet would interact with the bot in the way they would interact with it. When systems can learn, how do we make sure that they do not learn bad things? Is it ethical to limit what a system can learn? Or is it unethical to expose users to what a system can possibly learn?\nMicrosoft did not break any laws with Tay. The bot was unethical in that it was developed without safeguards that prevented offensive and hurtful responses. AI bots are highly complex pieces of software; the unintended consequences arose from Tay still being good enough for the intended application. It probably worked very well in a sheltered test environment with benign interactions.\nMicrosoft CEO Satya Nadella gave the Tay episode credit for how Microsoft is approaching AI today.\n\n\nThe reasons for model risk stated by the FDIC are somewhat obvious: (i) the model is wrong, (ii) the model is correct but misapplied. We add another source of model risk: models that are trained well, almost too well. The following example reminds us that by merging, processing, and modeling data we can find out more than we should know. While it might not be illegal to have that knowledge, it is unethical to use that knowledge.\n\n\nExample: Target Pregnancy Prediction\n\n\nAn angry father walked into a Minneapolis Target store and demanded to know why the company was sending his teenage daughter coupons for pregnancy- and baby-related items. He wanted to know if Target is trying to encourage his daughter to become pregnant. It turns out Target had indeed sent pregnancy marketing material to her, and it turns out that she was indeed pregnant—unbeknownst to her father at the time he called on the store.\nTarget had developed a predictive model that assigns customers a pregnancy score based on demographic data merged with data from customers who had baby registries at a store, and a set of products that indicated a high probability of a pregnancy when purchased together. The algorithm performed so well that Target could determine the trimester and estimate the due date. This enabled all kinds of targeted, personalized marketing efforts.\nTarget did not run afoul of any laws, they used internal and public data available to them. They developed a model that performed well by statistical standards. But just because you can do something does not mean you should. You can learn something legally about someone and it is still ethically questionable or even inappropriate.",
    "crumbs": [
      "Part IV. Applied Ethics in Data Science",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ethics/intro.html#unintended-consequences",
    "href": "ethics/intro.html#unintended-consequences",
    "title": "15  Introduction",
    "section": "17.3 Unintended Consequences",
    "text": "17.3 Unintended Consequences\nThe law of unintended consequences describes the phenomenon when an action leads to results that are not the intended purpose of the action. Typically, we think of the unintended consequences as detrimental; they can be beneficial a well.\nTechnology, all the inventions of the human mind, are full of unintended consequences. Comic Chuck Nice says that future technology comes with two things: promises and unintended consequences.\nYou can reduce the likelihood of unintended consequences—positive or negative ones—by deeply understanding the systems involved. Greater complexity likely leads to more unintended consequences as complex systems cannot be fully comprehended. Unfortunately, our capabilities through innovation and our ability to build complex systems increases more quickly than our ability to foresee the consequences. In his TED talk, Edward Tenner gives examples of unintended consequences from innovation throughout history. For example, the introduction of HVAC systems in the 1970s raised the temperature of water in these systems to the optimum for reproduction of Legionella bacillus, the bacteria causing Legionnaire disease—an unintended consequence. A bactericide was added to HVAC systems to prevent the disease. In the early 1980s it was observed that computer tape drives were failing in drives close to ventilation ducts. The bactericide contained traces of tin which destroyed the tape heads—an unintended consequence of the safety intervention.\nWhen we build more complex and more opaque systems from data, we should expect more unintended consequences when working with data. While we develop the systems with the best of intentions, we should always ask “What could go wrong?”\n\n\nExample: Facebook’s Year in Review\n\n\nWhen Facebook first rolled out the Year in Review feature, it was intended to digitally scrapbook the previous year based on highly liked and photo-heavy content. Cheerful iconography surrounded images through which Facebook users were prompted to create their digital scrapbook for the year.\nAuthor and designer Eric Meyer called it Inadvertent Algorithmic Cruelty in his blog, when Facebook invited him to celebrate the past year with an image of his daughter, who had died. He writes:\n\nThis inadvertent algorithmic cruelty is the result of code that works in the overwhelming majority of cases, reminding people of the awesomeness of their years, showing them selfies at a party or whale spouts from sailing boats or the marina outside their vacation house.\nBut for those of us who lived through the death of loved ones, or spent extended time in the hospital, or were hit by divorce or losing a job or any one of a hundred crises, we might not want another look at this past year.\n\nThe application is designed with an ideal user in mind, someone who is happy, had a great year, and would like to see a summary. It does not consider the worst-case scenario and lacks empathy for the far-from-ideal users. Meyer concludes:\n\nIf I could fix one thing about our industry, just one thing, it would be that: to increase awareness of and consideration for the failure modes, the edge cases, the worst-case scenarios.\n\n\n\nIt is easy to brush aside unintended consequences as not knowable, the so-called unknown unknowns. They will certainly remain unknowns if you do not want them to be known. Hindsight has always 20:20 vision, but some unintended consequences are in fact knowable and foreseeable. We have to make an effort to think about possible unintended consequences and be creative in imagining what could happen. Data sharing is an interesting case in point. Much good can be done by combining data sources that complement each other—the sum can be more than the parts. Medical research can benefit by combining anonymized data from clinical trials that are conducted in isolation. Data collected in an app with user consent can be made available to a wider audience and make the world a better place—or it can backfire spectacularly.\n\n\nExample: Strava Exercise Maps\n\n\nExercise app Strava opened up its data on Strava.com in 2018 to allow users to find new places to exercise. The move was well-intended. Strava had collected at that time data on more than a billion runs, walks, bicycle rides, and swims all over the world. For someone looking for a place to exercise that information would be highly valuable. Maybe you can find a trail or bike path in your neighborhood that you were not aware of.\nHowever, the move turned into a privacy nightmare. Military researchers discovered that the heatmaps of exercise activity made it possible to study the workouts of military personnel around military bases. Although the data shared online was anonymized, it was possible to de-anonymize the data by requesting information for a specific geographic location. With this method it was even possible to see the names and heart rates of some individuals. If your daily run starts and ends at a military basis, then it is easy to infer that you are somehow connected to the military. By revealing common movements, someone with ill intent could use the information to figure out how a military basis operates.\nThe Guardian determined that by drilling into the data at Strava.com, they were able to reveal the names of 50 U.S. service members stationed at an airbase in Afghanistan. From changes in running profiles one can also glean when service members are transferred.\nRevealing information about military operations was an unintended consequence of the release of information on Strava.com. It was not breach of data, it was done deliberately, with other goals in mind. According to the Guardian, the company argued that the information was already made public by the users who uploaded it. In hindsight, it was a foreseeable consequence of sharing geotagged data.",
    "crumbs": [
      "Part IV. Applied Ethics in Data Science",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ethics/intro.html#measuring-performance",
    "href": "ethics/intro.html#measuring-performance",
    "title": "15  Introduction",
    "section": "17.4 Measuring Performance",
    "text": "17.4 Measuring Performance\nShould we accept a more accurate approach over a less accurate approach? Many would say “Yes”, greater accuracy is better. The more accurate method can have other shortcomings, for example, it may be inscrutable or very difficult to implement and the nod might go to the less accurate method that is easier to implement. In this situation we are making a deliberate choice to prefer one approach over another despite its inferiority with respect to a specific metric. Other considerations also matter.\nBut even if all other things being equal, accuracy is not necessarily the appropriate measure by which to judge the performance of a decision rule. Consider a classification problem with two possible states, “Yes” and “No”. An algorithm is developed to classify an observation based on data. Since the model will not always predict correctly, there are four possible scenarios, commonly displayed in a confusion matrix.\n\n\nDefinition: Confusion Matrix\n\n\nThe confusion matrix in a classification problem is the cross-classification between the observed and the predicted categories. In a problem with two categories, the confusion matrix is a 2 x 2 matrix.\nThe cells of the matrix contain the number of data points that fall into the cross-classification when the model’s decision rule is applied to n observations. If one of the categories is labeled positive and the other is labeled negative, the cells give the number of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions.\n\n\nThe following table shows the layout of a typical confusion matrix for a 2-category classification problem. A false positive prediction, for example, is to predict a positive (“Yes”) result when the true state (the observed state) was negative. A false negative result is when the decision rule assigns a “No” to an observation with positive state.\n\nConfusion matrix for a classification problem with two states.\n\n\n\nObserved Category\n\n\n\n\n\nPredicted Category\nYes (Positive)\nNo (Negative)\n\n\nYes (Positive)\nTrue Positive (TP)\nFalse Positive (FP)\n\n\nNo (Negative)\nFalse Negative (FN)\nTrue Negative (TN)\n\n\n\nBased on the four cells in the confusion matrix we can calculate several statistics.\n\n\n\nTable 17.1: Statistics calculated from a 2 x 2 confusion matrix.\n\n\n\n\n\n\n\n\n\n\nStatistic\nCalculation\nNotes\n\n\n\n\nFalse Positive Rate (FPR)\nFP / (FP + TN)\nThe rate of the true negative cases that were predicted to be positive\n\n\nFalse Negative Rate (FNR)\nFN / (TP + FN)\nThe rate of the true positive cases that were predicted to be negative\n\n\nSensitivity\nTP / (TP + FN) = 1 – FNR\nThis is the true positive rate; also called Recall\n\n\nSpecificity\nTN / (FP + TN) = 1— FPR\nThis is the true negative rate\n\n\n\n\n\n\n\nAccuracy\n(TP + TN) / (TP + TN + FP + FN)\nOverall proportion of correct classifications\n\n\nMisclassification rate\n(FP + FN) / (TP + TN + FP + FN)\nOverall proportion of incorrect classifications, 1 – Accuracy\n\n\nPrecision\nTP / (TP + FP)\nRatio of true positives to anything predicted as positive\n\n\nDetection Rate\nTP / (TP + TN + FP + FN)\n\n\n\nNo Information Rate\n\\(\\frac{\\max(TP+FN,FP+TN)}{TP+TN+FP+FN}\\)\nThe proportion of observations in the larger observed class\n\n\n\n\n\n\nThe model accuracy is measured by the ratio of observations that were correctly classified, the sum of the diagonal cells divided by the total number of observations. The misclassification rate, another common performance measure is the complement of the accuracy.\nThe sensitivity is the ratio of true positives to what should have been predicted as positive. The specificity is the ratio of true negatives to what should have been predicted as negative. They are not complements of each other; they are calculated with different denominators.\nThe problem with focusing solely on accuracy is that the two possible errors, false positives and false negatives might not be of equal consequence. For example, it might not matter how accurate a model is unless it achieves a certain sensitivity—the ability to correctly identify positives.\nConsider the data in the Table 17.2, representing 1,000 observations and predictions.\n\n\n\nTable 17.2: Example of a confusion matrix for 1,000 observations.\n\n\n\n\n\n\nObserved Category\n\n\n\n\n\nPredicted Category\nYes (Positive)\nNo (Negative)\n\n\nYes (Positive)\n9\n7\n\n\nNo (Negative)\n26\n958\n\n\n\n\n\n\nThe classification has an accuracy of 96.7%, which seems impressive. Its false positive and false negative rates are very different, however: FPR = 0.0073, FNR = 0.7429. The model is much less likely to predict a “Yes” when the true state is “No”, than it is to predict a “No” when the true state is “Yes”. Whether we can accept a model with such low sensitivity (100 – 74.29) = 25.71% is questionable, despite the high accuracy. An evaluation of this model should consider whether the two errors, false positive and false negative predictions, are of equal importance and consequence.\nIt is also noteworthy that the accuracy of 96.7% is not as impressive if you check the no-information rate of the data. The proportion of observations in the larger observed class is (958 + 7)/1,000 = 0.965. The accuracy of the decision rule is only slightly larger. In other words, if you were to take a naïve approach and predict all observations as “No” without looking at the data, that naïve decision rule would have an accuracy of 96.5%.\n\n\nExample: COMPAS Tool to Predict Recidivism Risk\n\n\nJudges, parole, and probation officers are increasingly relying on algorithms to determine an offender’s propensity for recidivism—to re-offend. A higher likelihood to re-offend might result in a stiffer sentence. One such algorithm was developed by Northpointe, Inc. and is part of their COMPAS tool (Correctional Offender Management Profile for Alternative Sanctions).\nUpon being booked in jail, a questionnaire of 137 questions is filled out, answered in part by the defendants or by pulling data from criminal records. Questions include things like “How often did you get in fights while at school?”, “How often have you moved in the last 12 months?”, and whether they agree with statements such as “Some people just don’t deserve any respect and should be treated like animals”. The questionnaire is known, but the algorithm that uses the answers to score defendants on the likelihood to re-offend within two years, is proprietary. Two risk scores are produces by the COMPAS tool: the risk of general recidivism and the risk of committing a violent crime in the future.\nThe idea of using algorithms to make key decisions in the legal process, not guided by conscious or unconscious personal bias, is appealing. An accurate prediction of recidivism can make the justice system fairer. But that requires that the algorithm works, it cannot itself be biased. Someone said that even a slightly flawed algorithm is better than a bigoted judge, but the consequence of applying even a slightly biased algorithm at scale, across the nation, in thousands of cases can be disastrous.\nIn 2016, ProPublica investigated the performance of the proprietary algorithm using data from more than 7,000 criminal defendants in Broward County, Florida (Angwin et al. (2016), Larson et al. (2016)). For these defendants the predicted recidivism and whether the individual actually re-offended was known. This allows the investigators to probe how well the COMPAS model is actually doing in predicting outcomes.\nA notebook with data management and analysis using R and Python is available on GitHub, along with the data—made available by ProPublica.\nThe ProPublica analysis uses logistic regression and survival models to compare COMPAS scores among races and genders. The results include confusion matrices for recidivism classification (Low/High) and future violent crime (Low/High). Some of the important findings are:\n\n51% of defendants were Black, 34% White, 8% Hispanic\n81% of defendants were Male, 19% were Female.\nBlack defendants are 45% more likely than white defendants to receive a higher risk score when correcting for seriousness of the crime, previous arrests, and future criminal behavior. The risk score for future violent crime overpredicts recidivism for black defendants by 77% compared to white offenders.\nWomen are 19% more likely to receive a higher risk score than men.\nDefendants under 25 are 2.5 times more likely to receive a higher risk score than middle-aged defendants.\nThe predictive accuracy of the COMPAS model is only 63% (as measured by a Cox proportional hazard model).\nOnly 20% of those predicted to commit violent crimes went on to do so.\nThe false positive rate of the model is 32.3% overall, 44.8% for Blacks, and 23.4% for Whites.\nUnder COMPAS, black defendants are 91% more likely to get a higher score and not go on to commit more crimes over the next two years.\nCOMPAS scores misclassify white re-offenders as low risk 70.4% more often than black re-offenders.\nBlack defendants are twice as likely to be false positives for a higher violent crime score than white defendants.\nWhite defendants are 63% more likely to get a lower violent crime score and go on to commit another crime.\n\nWith an accuracy of just over 60%, the system is barely better than a coin flip, which would have an accuracy of 50%. The racial disparities in the predictions from the model are deeply troubling. In predicting who would re-offend, the algorithms made mistakes for Black and White defendants at similar rates but in very different ways.\n\n\n\n\n\n\n\n\n\nWhite\nBlack\n\n\nLabeled higher risk for recidivism but did not re-offend\n23.5%\n44.8%\n\n\nLabeled lower risk for recidivism and did re-offend\n47.9%\n28.0%\n\n\n\nFor Blacks, the algorithm has a high false positive rate (44.8%). For Whites, the algorithm has a high false negative rate (47.9%). When the algorithm predicts incorrectly, it leads to lighter consequences for Whites and to tougher consequences for Blacks.",
    "crumbs": [
      "Part IV. Applied Ethics in Data Science",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ethics/intro.html#automation-and-scale",
    "href": "ethics/intro.html#automation-and-scale",
    "title": "15  Introduction",
    "section": "17.5 Automation and Scale",
    "text": "17.5 Automation and Scale\nWe automate processes to achieve greater efficiency and productivity, to scale up operations, and to reduce time spent on mundane repetitive tasks. As someone explained,\n\nwhen I do something for the second time, I think about how I did it last time. If I do it for the third time, I think about how to automate it and never do it again.\n\nScaling a process through automation is wonderful if automation makes the process better or if the outcome is inherently good. If automation of tax refunds means that refunds reach taxpayers sooner, no one will complain. If automation of traffic signals means better traffic flow through the city and less time spent staring at brake lights, grumbling is unlikely.\nHowever, if automated decisions are of lesser quality, more biased and potentially harmful, then exercising them at scale can cause great harm.\nA cruel example where this scenario played out is the former “Robodebt” system of the Australian Government.\n\n\nExample: Robodebt Scheme of the Australian Government\n\n\nBetween 2016 and 2019 the government of Australia, through its social services agency Centrelink, wanted to save billions of dollars by recovering social security (welfare) overpayments to recipients. The process previously in place checked for compliance manually and resulted in about 20,000 cases per year. It was based on matching income reported in two different places: the tax office served as the system of record for annual income, and the self-reported bi-weekly income by the welfare recipient. If there was a discrepancy between reported incomes, a staff member would investigate and request that the recipient explain the difference.\nAn automated system, it was thought, could do this more efficiently, issue more debt notices, and hence recover more overpayments. The system would more than pay for itself. Such a system was put in place in 2016. In the first six months in operation, it issued already 169,000 debt notices. But that was not the only difference between the manual and the automated system.\nWhile the manual (human-based) system would issue a notice to the recipient to please explain the difference, the new automated system, when it found a discrepancy between tax-reported and self-reported Centrelink income, would automatically issue a debt notice and expect repayment—hence the name “Robodebt”. The presumption of innocence turned into presumption of guilt. The onus was suddenly placed on the individual to prove that they did not owe the government funds—without human verification or interaction.\nThis is a horrible practice, unethical and possibly illegal, even if the overpayments are correctly assessed. But that is another way in which the automated system went astray. Robodebt had a staggering false positive rate of at least 27%! If you were a social security recipient who received the appropriate payment from the government, you had more than a 1-in-4 chance to be issued a request for overpayment.\nOver the period of operation, 567,000 debts were raised through the use of the Robodebt algorithm.\nAfter the government responsible for Robodebt was voted out of office, the incoming administration killed the program and announced that 470,000 wrongly-issued debts would be repaid in full. The error rate is over 80%!\nIn the article “The Robodebt strategy” in the December 2023 issue of Significance, Trewin, Fisher, and Cressie (2023) note the following main statistical flaws of Robodebt:\na) Using annual tax income to match against bi-weekly self-reported income. Bi-weekly income can fluctuate greatly throughout the year and cannot be simply compared against distributing the annual income evenly.\nb) No documented understanding of error sources with the data or the data matching process.\nc) No sensitivity analysis and inadequate testing of algorithms\nd) No understanding of error rates\ne) No involvement of professional statisticians.\nA system that was supposed to save the government A$300 million cost it A$1.2 billion in repayments alone. The human toll was devastating: some recipients of debt notices had committed suicide.\nThe statistician Noel Cressie called Robodebt\n\na catastrophic program that was legally and ethically indefensible—an example of how technological overreach, coupled with dereliction of duty can amount to immense suffering for ordinary people.\n\nHe noted that Jensen’s inequality tells us that if the algorithm called out overpayments it should have also detected underpayments and issued credits (Cressie (2023)). Robodebt never issued any credits.",
    "crumbs": [
      "Part IV. Applied Ethics in Data Science",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ethics/intro.html#bad-data-flawed-algorithm-poor-implementation",
    "href": "ethics/intro.html#bad-data-flawed-algorithm-poor-implementation",
    "title": "15  Introduction",
    "section": "17.6 Bad Data, Flawed Algorithm, Poor Implementation",
    "text": "17.6 Bad Data, Flawed Algorithm, Poor Implementation\nImplementing a flawed algorithm poorly and feeding it bad data is a recipe for disaster. In the following example, that recipe led to the murder of victims of domestic violence. As a recent audit of the system suggests, at least some of these murders were preventable.\n\n\nVioGén Algorithm to Monitor Gender Violence\n\n\nSince 2007, Spain has been operating the VioGén system, a web application to help Spanish authorities to protect women and children from domestic violence. Based on an external audit by the Eticas Foundation and reported by Politico and others, the system is deeply flawed. (See here for the full report in Spanish).\nThe combination of numerous shortcomings, from bad data, to an opaque algorithm, to lack of human intervention, leads to an underestimation of the risk of violence. Women do not receive the proper protection, some are sent back into a domestic violence environment, and some have paid for it with their lives.\nThe underlying algorithm uses classical statistical models for risk classification, yielding a risk score based on the weighted sum of responses on an intake questionnaire. The system is designed as a recommendation system, assigning a “recommended” score to an individual with the opportunity for adjustment by a human. That human would usually be a police officer who runs the algorithm on the questionnaire answers provided by the victim of domestic violence. The Eticas audit revealed that in 95% of the cases the score returned by the algorithm is not being adjusted. Because of lack of human intervention, the recommendation system becomes the automated decisioning system.\nThe algorithm assigns a case to the risk categories “unappreciated”, “low”, “medium”, “high”, and “extreme”. Only one in seven women who reached out to police for protection actually received protection. Many women reporting violence receive an “unappreciated” risk score; 45% of the cases. Only 3% of women who are victims of gender violence receive a risk score of medium or higher. Furthermore, the system caps the number of higher risk scores at the level at which protection is funded in the budget.\nIs the algorithm biased and is underestimating the risk? Probably so. The algorithm focuses on physical violence and under-values psychological and non-sociological violence, for example, through social media. The lack of transparency of the algorithm does not help. The fact that only 35% of the women are told their risk scores is suspicious.\nWhat is not in doubt are the flaws in the collection of data for the risk prediction algorithm. When a woman makes an official complaint, authorities present them with a standardized set of binary questions assessing 39 risk indicators as “present”/“not present”. Questions explore the severity of previous assaults (e.g., whether weapons were ever used), the features of the aggressor (jealous, bully, sexual abuser, unemployed, drug addict, etc.), the vulnerability of the victim (pregnant, foreign, economically dependent, etc.), and aggravating factors. The questions are blunt and direct, sometimes embarrassing. In the situation of the complaint, the victim of domestic violence is often terrified, under extreme duress, in a state of shock, emotionally charged. That is not the time to fill out a very consequential questionnaire. Police officers are reported to leave victims alone with the questionnaire and pressure them to finish it quickly.\nThe VioGén algorithm and questionnaire have been revised several times, the latest iteration seems to be in 2019. The Eticas Foundation audit was published in 2024. Recent efforts to improve the VioGén system aimed at introducing machine learning to first predict the probability of recidivism and then to adjust that probability based on indicators about the perpetrator. A former employer of mine is involved in this effort; see here and here.\n\n\n\n\n\n\nAngwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. “Machine Bias—Risk Assessment in Criminal Sentencing.” ProPublica, May. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.\n\n\nCressie, N. 2023. “Robodebt Not Only Broke the Law of the Land – It Also Broke Laws of Mathematics.” The Conversation. https://theconversation.com/robodebt-not-only-broke-the-laws-of-the-land-it-also-broke-laws-of-mathematics-201299.\n\n\nLarson, Jeff, Surya Mattu, Lauren Kirchner, and Julia Angwin. 2016. “How We Analyzed the COMPAS Recidivism Algorithm.” ProPublica, May. https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm.\n\n\nLoukides, Mike, Hilary Mason, and D. J. Patil. 2018. Ethics and Data Science. O’Reilly Media. https://resources.oreilly.com/examples/0636920203964/.\n\n\nTrewin, D., N. Fisher, and N. Cressie. 2023. “The Robodebt Tragedy.” Significance 20 (6): 18–21. https://academic.oup.com/jrssig/article/20/6/18/7457250.",
    "crumbs": [
      "Part IV. Applied Ethics in Data Science",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ethics/bias_harm.html",
    "href": "ethics/bias_harm.html",
    "title": "16  Bias and Harm in Algorithms",
    "section": "",
    "text": "16.1 Are Data Biased?\nThe previous section presented examples where things went wrong, where algorithms had unintended consequences and side effects. It is easy to say that the data is biased and thus the fault lies with the data used in training models. The data can be a major source of problems in data analytics. But the answer cannot be as simple as saying “garbage—in—garbage—out”.\nTraining models on data that are not representative of the situation to which the model will be applied is just one possible form of bias, called representation bias. It can occur even if the data was collected perfectly, measured without error, sampled adequately—for a different purpose than it is now used. This is not unlike our statistical understanding of bias, as the (average) difference between an estimator and its target. A perfectly qualified estimator can be biased for one target and unbiased for another target. The sample mean \\(\\overline{Y}\\) of a random sample is an unbiased estimator of the mean \\(\\text{E}[Y] = \\mu\\) of the sampled population and is a biased estimator of the median of the population unless the distribution of \\(Y\\) is symmetric (mean and median are then identical). Context matters.\nRather than taking a 30,000-foot view that “data are biased” or “data can be biased” and harm of data-dependent models is a possible consequence, it is worthwhile to drill deeper and ask what types of biases exist and how unintended consequences and harm flows from the way in which we build, test, and deploy models. Data can be biased. We need to ask in what way bias can be introduced into data, through data, and through processing data. Without this understanding we cannot apply remedies.\nAlgorithms are normally not purposefully built to be unfair or cause harm. Instead, they are the result of processes that lead to algorithms that make unfair or harmful decisions. So how does it happen that an algorithm in Austria determines that women have lower employability than men if all other characteristics are equal? How is it possible that female job seekers are less likely to see advertisements on Google for high-paying jobs than their male counterparts? We know that the algorithm is unfair because we can test it. That is exactly what researchers from Carnegie Mellon did; they put the advertisement algorithm through the ringer with a designed experiment. As reported by the Guardian:\nIn the excellent paper “A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle”, on which this discussion is partly based, Suresh and Guttag (2021) give the following example where the same intervention to correct bias works in one case and fails in another.",
    "crumbs": [
      "Part IV. Applied Ethics in Data Science",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bias and Harm in Algorithms</span>"
    ]
  },
  {
    "objectID": "ethics/bias_harm.html#are-data-biased",
    "href": "ethics/bias_harm.html#are-data-biased",
    "title": "16  Bias and Harm in Algorithms",
    "section": "",
    "text": "One experiment showed that Google displayed adverts for a career coaching service for “$200k+” executive jobs 1,852 times to the male group and only 318 times to the female group. Another experiment, in July 2014, showed a similar trend but was not statistically significant.\n\n\n\n\nExample: Sample More Women\n\n\nThe goal is to build a model to predict the risk of a heart attack. A researcher trains a model on medical records from prior patients at a hospital and observes that the false negative rate of the trained model is higher for women than for men. Assuming that the reason for the poorer predictive performance is a lack of data on women in the records, the researcher adds more data on women who suffered heart attacks. The re-trained model shows improved performance in predicting heart attacks in women.\nIn another application, a hiring model is developed to determine suitability of candidates based on their resume, augmented by candidate ratings supplied by human evaluators. It is observed that the model predicts women to be less likely as suitable candidates compared to men. To correct this shortcoming, more data is collected on women, but the model does not improve. In both scenarios the model builders recognized a deficiency in the model and tried to correct it—that is ethical behavior.\nBut the same intervention, collecting more data on the group for which the model performed poorly, was successful in one instance and pointless in the other. The reason is that the model deficiency is due to different sources of bias. In the medical example, the training data under-represented women, there was not enough information about women in the data to create a model that was as accurate for that group as it was for the group more heavily represented in the data—men.\nModifying the sampling process changed the distribution of genders in the training data. In the hiring example, the bias is not introduced through the representativeness of the sample, but through the human-assigned rating. When the human assessment of candidate suitability disadvantages women over men, adding more data will not correct the model.",
    "crumbs": [
      "Part IV. Applied Ethics in Data Science",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bias and Harm in Algorithms</span>"
    ]
  },
  {
    "objectID": "ethics/bias_harm.html#seven-eight-sources-of-harm",
    "href": "ethics/bias_harm.html#seven-eight-sources-of-harm",
    "title": "16  Bias and Harm in Algorithms",
    "section": "16.2 Seven (Eight) Sources of Harm",
    "text": "16.2 Seven (Eight) Sources of Harm\nSuresh and Guttag (2021) describe seven distinct sources of bias and associate them with steps in the modeling cycle, from data preparation to model deployment.\n\n\n\n\n\n\nFigure 16.1: The seven sources of harm in machine learning models (Suresh and Guttag 2021).\n\n\n\n\nHistorical Bias\nModels do not extrapolate what they learned from the training data to situations that do not reflect the training data. They can create associations only from what is in the training data: the past is prologue.\nA system that reflects accurately how the world is or was can inflict harm, either by unfairly withholding resources (allocation harm) or by perpetuating stereotypes (representational harm).\nHistorical bias, also called pre-existing bias, is rooted in social institutions, practices and attitudes that are reflected in training data. Baking these into the algorithm reinforces and materializes the bias. In the example of the Austrian employability algorithm, the data reflected an accurate representation of a discriminatory labor market, women, especially women with children, are less likely to be found in high-paying jobs. The algorithm trained on this data assigned a lower employability score to women.\nIf you train a model on data from textbooks, then it is more likely to identify engineers as men than as women, since that profession is historically more likely to be depicted by men. Word embeddings are numerical representations of text data used to train language models. Any stereotypes in the source texts on which the embeddings are based will live on in the models developed based on the embeddings.\nThe historical bias might not be evident right away. It can rear its ugly side when the model is applied in an unforeseen way. Google Translate got into trouble when documents were translated from Hungarian into English. Hungarian is a gender-neutral language, so the algorithm had to apply pronouns in the English translation. From the article on DailyMail.com:\n\n‘She’ was placed before domestic chores such as cleaning and sewing, while ‘he’ was used for intellectual pursuits such as being a politician or a professor.\n\n\n‘He teaches. She cooks. He’s researching. She is raising a child. He plays music. She’s a cleaner. He is a politician. He makes a lot of money. She is baking a cake. He’s a professor. She’s an assistant.’\n\nThe researcher who found the issue blamed Google. Google blamed the algorithm on society: the algorithm inadvertently replicated gender biases that exist in the information it scraped from the internet.\nHow can we fix this type of bias?\n\nChange the data. One could revisit the training data in the translation example and change gender pronouns to achieve better balance across attributes. The trained algorithm then would hopefully reflect that balance. Modifying data is considered unethical. There be dragons! What proportion of female pronouns should be used for, say, a profession. The currently prevailing proportion or a desired future state? In Hungary or the U.S? Who decides what that is?\nUse different data. Finding a data source that does not have historical bias baked into it sounds good but might not be possible.\nAdjust the model. Corrections can be made to the inner workings of the model. For example, instead of using gendered pronouns the translation can use gender-neutral language: “they cook, they teach”. Alternatively, the gender pronouns in the translation can be chosen randomly.\n\n\n\nRepresentation Bias\nRepresentation bias bias occurs because of a mismatch between sample, target, and application population. The target population is the universe we have in mind in developing an algorithm. The sample population is the population represented by the training data. The application population is the universe to which the model is applied. It is easy to create mismatches.\nA model developed for house values in Boston probably does not apply in Chicago without re-training on data from Chicago. The target population (Boston) and application population (Chicago) are mismatched.\nSampling bias—also called selection bias—is a form of representation bias where the sampling method is defective and does not represent the target population. A random sample is always representative of the target population but there are many ways in which the training data can deviate from the properties of a random sample from the target population:\n\nAvailable data can misrepresent the target population, for example, if medical data is only available for individuals having a condition.\nRandom sampling that does not reflect the distribution of groups in the target population, for example, drawing random samples from lists of users (website, mobile phone, phone book, tax records) does not accurately represent the overall population (over-/under-coverage).\nMissing values processes that are related to the objective of the study. When recipients in a mental health survey are less likely to respond because of their mental health status, the sample of survey responses is biased (non-response bias).\nSelf-selection: participants who exercise control over their inclusion in a study can bias the data. Folks calling into radio shows or participating in online polls are particularly motivated by issues and their responses overestimate strong opinions in the population.\nSurvivorship bias can occur when the sample focuses on those meeting a selection criterion. For example, studying current companies does not reflect companies that have previously failed.\nDrop-out: subjects drop out of a long-term study for reasons related to the objective of the study. Losing study participants in a longitudinal health study that move out of the area because of a new job probably does not bias the data. Losing study participants because of deteriorating health might bias the data.\n\n\n\nExample: Literary Digest Poll\n\n\nA famous example of sampling bias because of under-coverage and non-response bias was the 1936 poll by the Literary Digest magazine to predict the outcome of the presidential race between incumbent Democratic candidate Franklin D. Roosevelt and Republican candidate Alf Landon. The magazine sampled voters based on its own subscribers, phonebooks and car registration data and predicted that Alf Landon would win the election.\nInstead, Franklin D. Roosevelt defeated Landon in a landslide. Although it had sampled millions of voters, the magazine’s prediction was way off, by more than 19%. The embarrassment contributed to the magazine’s demise by 1938.\nSampling bias due to an inappropriate sampling frame was one factor for the imprecise survey. Subscribers to Literary Digest, phone users and automobile owners were on average wealthier than the average American. Their responses over-estimated the proportion of Republican voters. The second, and primary source of bias was non-response bias: those who disliked Roosevelt were more likely to respond in favor of Landon.\n\n\nIn an earlier chapter we mentioned ImageNet, a large database of images used in computer vision challenges and to benchmark object classification algorithms. ImageNet does not represent a random sample of all images that could be taken. Almost half of its images originate in the U.S., only 1—2% of the images come from China and India. A computer vision model trained on ImageNet data is expected to perform worse classifying objects from images from those geographic areas.\nThis is a special kind of bias. Groups that occur in the data less frequently are predicted with less accuracy, simply because there are fewer training samples in those groups compared to others. In the ImageNet example this is due to the way the data are collected from the internet. If a population is properly sampled randomly, minorities will appear less frequently in the sample than majorities. When model performance is analyzed stratified by groups, the model will be less accurate (have a larger prediction error or mis-classification rate) for groups that contributed fewer observations. Differentiated levels of accuracy are unfair and potentially harmful.\nThe cause of representation bias might not be immediately obvious, while its presence can be very obvious in the results.\n\n\nExample: “Delving” into ChatGPT\n\n\nJeremy Nguyen asked on X (Twitter) why there was such an increase in the recent use of “delve” in medical papers and presented Figure 16.2 as evidence. The tweet appeared in March 2024, so we still had a long way to go in that year to grow the bar on the right.\n\n\n\n\n\n\nFigure 16.2: Use of “delve” in papers on PubMed. Source:JeremyNguyenPhD on X\n\n\n\nUse of “delve” has been on the increase for some time, but in 2023 it took off dramatically. Do medical papers just do more delving since then? It has been noted that ChatGPT responses seem to use certain words more frequently than the internet at large. Examples are “delve”, “explore”, “tapestry”, “testament” and “leverage”. Frequently enough to be seen as a giveaway that text was generated by GPT. GPT 3.5 was released in late 2022 and 2023 is the first full year the world enjoyed ChatGPT. Is Figure 16.2 evidence that (many) medical papers are now being written—or at least augmented—with the help of ChatGPT?\nHow is it possible that GPT, which is trained on a massive corpus of text found on the internet, has such an affinity for “delve” and uses it more frequently than would be consistent with the training data? The reason is a special form of representation bias. ChatGPT is a question-answer application built on top of the foundation large-language model GPT. The ChatGPT training uses a form of reinforcement learning, called reinforcement learning with human feedback (RLFH). That is a fancy way of saying that in order to perform reinforcement learning, the algorithm needs a reward function with which to rate competing actions–that is, competing responses to the prompt. Scoring the quality of a textual response is difficult and this is where human evaluators come in. The human evaluation can alter or correct an initial response from the AI to create a response with a higher reward score. When this altered response is fed back into the algorithm, the model will eventually learn to respond to maximize the reward, and respond in a way similar to the human evaluator.\nThis is the first part of the story. The second part of the story is that human evaluation and feedback in training LLMs is time consuming and costly. That is why tech companies outsource this work to places where anglophonic knowledge workers are cheap. As the Guardian points out in this article, “delve” is much more frequent in business English in Nigeria than in the U.S. or the U.K. Outsourcing the task to provide human feedback to Nigeria creates a feedback loop that makes ChatGPT write more African English than its original training data.\n\n\n\n\nMeasurement Bias\nMeasurement bias—also called detection bias or information bias—is due to systematic errors in the process of data collection. Random data measurement errors do not contribute to measurement bias, they cause greater variability in the data. A scale that is not properly calibrated, on the other hand, will give systematically incorrect readings of weight—that is a biased measurement.\nIncorrect instruments are a source of this bias and insufficient accuracy or limitations of the instruments. Measuring distance that exceed 10 feet with a 10-foot tape measure causes observations to be censored. The true distance is larger than 10 feet, but we do not know the exact value. The accuracy of instruments can vary with its range; being most accurate in the mid-range and less accurate above or below.\nBias in measurements applies to qualitative variables as well, for example, when survey interviewers ask participants to rate their satisfaction for the wrong timeframe.\nThese sources of measurement bias are obvious. Suresh and Guttag also discuss a type of measurement bias that could be called proxy bias. The target variable of interest is often not directly observable. Instead, we use proxies to operationalize the concept of interest. Creditworthiness, for example, is not directly observable, and we use credit scores as the proxy. To model student success, we need to define what we mean by that. The choice of proxy affects the quality of the inference. An oversimplification such as GPA can bias the results because it does not apply to all students, does not capture learning outside of the classroom, does not capture skills not assessed as grades, etc.\nWhile the concept of interest is consistent across groups, e.g., has a disease, the technique for measuring its proxy, the diagnosis, can vary among groups. Regional or cultural differences lead to different interpretations of the same context when annotating images. The same 5-point rating scale is used differently depending on someone’s interpretation of the categories. Does the “best” category mean “best in my experience so far” or “the best one could ever hope for”?\nIn the COMPAS system for predicting recidivism discussed previously, variables like arrested and re-arrested are proxies for criminality. Measurement bias is introduced when certain communities are policed more intensely than others. If the extent to which a higher number of arrests reflects a higher policing intensity is not accounted for, the analysis will be biased and disadvantage the community where policing is higher.\n\n\nAggregation Bias\nAggregation bias occurs when data are grouped or aggregated in a way that ignores or obfuscates important subsets of the data. Trends and associations that we see in aggregated data are not necessarily present in the non-aggregated data. Similarly, trends and associations in non-aggregated data can be missed in aggregated data. For example, working with data at an annual level hides seasonal trends. This can be desirable. It can also lead to bad decisions by not accounting for systematic variation at a smaller level.\nConfusing the units of inference with the units of analysis is known as an ecological fallacy: to assume that what is true for a population is also true for the individual members of the population. One commits an ecological fallacy by concluding that individuals from families in poverty perform less well in school by studying the correlation between average poverty level in schools and school test averages. The analysis is based on aggregates at the school level, an association at that level does not imply an association at the individual level. A proper analysis takes a multi-level approach that models effects at the student, classroom, and school level.\n\n\nLearning Bias\nThis type of bias occurs when the process of training the model introduces bias by emphasizing some criteria more than others. For example, classification models are often trained to minimize the mis-classification rate (maximize the accuracy) on a test data set. Models with a high accuracy are not guaranteed to have a high true positive or true negative rate. If it is important to minimize for a low false positive or a low false negative rate, then the learning algorithm should focus on that criterion (probably at the expense of the overall accuracy).\nIf the purpose of deriving a model is confirmatory inference, then driving the mean-squared prediction error is unlikely to lead to the best model for hypothesis testing.\nTechniques that simplify models such as pruning decision trees or compressing neural networks can amplify performance disparities on data with under-represented groups because the models retain the most frequent features.\n\n\nEvaluation Bias\nEvaluation bias is related to learning bias in that it is caused by the ways in which we evaluate model performance. The use of benchmark data sets to judge the quality of a model is common in computer vision applications. This becomes problematic if the performance of the model against the benchmark is more important than solving the problem in the first place. In particular, if the benchmark data is not without issues. Training a face recognition algorithm against a benchmark data set that under-represents dark-skinned faces encourages models that do not as well for that group than for light-skinned faces.\nEvaluation bias can also occur if we focus model evaluation on individual metrics and compare models on metrics that do not sufficiently differentiate. An example is the use of global goodness-of-fit criteria such as AIC (Akaike’s Information Criterion), BIC (Bayesian Information Criterion). Or Adjusted-\\(R^2\\). These statistics are frequently used to compare models that are not nested—that is, models that cannot be reduced by applying a hypothesis. An example is comparing a random forest and a regression model. Choosing the “best” model based on a single, overall number is straightforward and dangerous. You might end up with the best of a bunch of bad alternatives.\n\n\nDeployment Bias\nThis type of bias occurs when the model is used in a way that does not match the problem the model was intended to solve. This can be due to additional decisions that are made in processing the model output.\nSuppose a model predicts the employability of an individual on a scale from 0—1. The model fulfills its purpose: to provide an indication of employability based on characteristics of an individual. To operationalize the model in the administration of municipal resources for training and employment support, its predictions are classified into three categories, based on cutoffs between 0 and 1:\n\nHighly employable individuals \\(\\rightarrow\\) they do not need any assistance.\nMedium employable individuals \\(\\rightarrow\\) they can receive assistance.\nPoorly employable individuals \\(\\rightarrow\\) they cannot receive assistance.\n\nThe cutoffs are likely chosen based on budgetary considerations, what the city can afford. The way in which the model is deployed will withhold resources from those in the “lost cause” category, and probably unfairly so.\nThe COMPAS recidivism model is another example of deployment bias. Originally intended to predict the risk of predicting a future crime, the scores are used to determine the length of sentences.\n\n\nEmergent Bias\nThis source of bias is not discussed in the paper by Suresh and Guttag, but plays an increasingly important role in the era of models that continuously learn and adjust, such as AI models.\nEmergent bias does not exist when a data product is first released. It emerges because the product changes (evolves) as users interact with it.\nA good example of emergent bias and harm is Microsoft’s Tay chatbot—discussed earlier. It was not a hateful racist when it was first released. Through interaction with users, it was trolled into adopting hateful rhetoric and opinions. Microsoft did not program Tay to be racist, but Microsoft programmed Tay to learn and adapt through interactions.\nEmergent bias is also at work when we like things explicitly—hit the “Like” button—or implicitly—post about a topic—on social media. The algorithms analyzing our engagement are more likely to recommend to us similar topics and add those to our social media feed. A bias bubble emerges that over-emphasizes information we are likely to like.\n\n\n\nFigure 16.1: The seven sources of harm in machine learning models (Suresh and Guttag 2021).\nFigure 16.2: Use of “delve” in papers on PubMed. Source:JeremyNguyenPhD on X\n\n\n\nSuresh, H., and J. Guttag. 2021. “A Framework for Understanding Sources of Harm Throughout the Machine Learning Life Cycle.” https://arxiv.org/pdf/1901.10002.pdf.",
    "crumbs": [
      "Part IV. Applied Ethics in Data Science",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bias and Harm in Algorithms</span>"
    ]
  },
  {
    "objectID": "ethics/privacy.html",
    "href": "ethics/privacy.html",
    "title": "17  Personal Information and Personal Data",
    "section": "",
    "text": "17.1 Privacy and Confidentiality\nWorking with data requires compliance with data privacy regulations, an increasingly complex field. It is our job to safeguard all information, not just information about individuals. In the everyday use of the words, we might not make a distinction between privacy and confidentiality. Legally, and technically, there is an important distinction. Privacy is about people; it is about the individual’s rights regarding their information. Confidentiality is about an individual’s data and how it is protected from others.\nConfidentiality can be required explicitly: confidentiality clauses in employee agreements can limit information we are allowed to share about our employer. In the course of employment, we are learning trade secrets and proprietary information about products that constitute a competitive advantage. Data collection in research projects follows detailed protocols to ensure integrity and trust in the information. Non-disclosure agreements (NDAs) place restrictions on information that can be shared.\nBesides explicit confidentiality agreement, there is an expectation of confidentiality beyond privacy. Many consumers understand that organizations have access to their data. Their concern is how the organization uses the data, and ensures that it remains a secret. An organization that meets the letter of data privacy regulations can still run afoul of the confidentiality expectations of its employees and customers.\nThe Code of Conduct of the Data Science Association has this to say about confidential information:\nSpecial safeguards and regulations apply to personal data and personal information—information that is about people or relatable to people. This is the realm of data privacy. Not safeguarding personal information can lead to harm for both sides: identity theft, fraud, violation of rights, etc., for the individual whose personal data is mishandled; fines, legal action, loss of license, embarrassment, loss of business, etc., for the individual mishandling the personal information.",
    "crumbs": [
      "Part IV. Applied Ethics in Data Science",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Personal Information and Personal Data</span>"
    ]
  },
  {
    "objectID": "ethics/privacy.html#privacy-and-confidentiality",
    "href": "ethics/privacy.html#privacy-and-confidentiality",
    "title": "17  Personal Information and Personal Data",
    "section": "",
    "text": "(a) Confidential information is information that the data scientist creates, develops, receives, uses or learns in the course of employment as a data scientist for a client, either working directly in-house as an employee of an organization or as an independent professional. It includes information that is not generally known by the public about the client, including client affiliates, employees, customers or other parties with whom the client has a relationship and who have an expectation of confidentiality. The data scientist has a professional duty to protect all confidential information, regardless of its form or format, from the time of its creation or receipt until its authorized disposal.\n\n\n(b) Confidential information is a valuable asset. Protecting this information is critical to a data scientists reputation for integrity and relationship with clients, and ensures compliance with laws and regulations governing the client’s industry.\n\n\n…\n\n\n(h) A data scientist shall protect client confidential information after termination of work for the client.\n\n\n(i) A data scientist shall return any and all confidential information in possession or control upon termination of the data scientist - client relationship and, if requested, execute an affidavit affirming compliance with obligations relating to confidential information.\n\n\n\n\n\nExample: Henrietta Lacks and the HeLa Cell Line\n\n\nHenrietta Lacks was born on August 1, 1920, in Roanoke, VA. She is probably one of the most important medical pioneers of the 20th century, with contributions that study the effect of toxins, drugs, hormones, and viruses on the growth of cancer cells. Not through her own medical work, but through the cells that were taken from her—without consent.\nMrs. Lacks had moved to Maryland and in 1951 visited Johns Hopkins Hospital in Baltimore complaining of vaginal bleeding. A malignant tumor was found on her cervix, and she started radiation therapy. On October 4, 1951, the cervical cancer took her life.\nCancer cells retrieved from her through a biopsy were sent to the tissue lab of Dr. George Gey. To his astonishment, Dr. Gey found that Mrs. Lacks’ cells behaved differently than cancer cells from other patients. While other cells would die within days of culturing, Mrs. Lacks’s cells continued to live and doubled about once a day.\nThe cells, called the HeLa cell line, is the first immortalized human cell line and continues to live to today. It has been used in countless medical research projects. As was practice at the time, no consent was required from the patient to culture the cells, and to distribute them. This would be unthinkable today. Even so, it was an unethical practice back then.\nAfter her death, Dr. Gey instructed his lab assistant to collect more cells while her body was in the autopsy facility.\nJohns Hopkins claims to have never sold or profited from the HeLa cell line and has given them away freely and widely for scientific research. Johns Hopkins admits that it could have done more to work with the Lacks family. They were not informed until 1975 about the existence of the HeLa cell line—another disturbing aspect about handling medical information without consent. The Lacks family found out about the cells because researchers contacted them and asked for blood samples hoping to use those to distinguish HeLa cells from other cell lines and through a conversation at a dinner party.\nMore than 10,000 patents involve HeLa cells. They were used to develop the polio vaccine, and in everything from AIDS research to gene mapping to testing cosmetics.\nIn October 2023, a statute of Henrietta Lacks was dedicated at Lacks Plaza in Roanoke. Small-scale statutes were dedicated at the Fralin Biomedical Research Institute at VTC, Virginia Tech Carilion School of Medicine, and Carilion Clinic. HeLa cells live at Fralin Biomedical Research Institute and in labs all over the world.",
    "crumbs": [
      "Part IV. Applied Ethics in Data Science",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Personal Information and Personal Data</span>"
    ]
  },
  {
    "objectID": "ethics/privacy.html#data-privacy-regulations",
    "href": "ethics/privacy.html#data-privacy-regulations",
    "title": "17  Personal Information and Personal Data",
    "section": "17.2 Data Privacy Regulations",
    "text": "17.2 Data Privacy Regulations\nSince the late 2010s, data privacy regulations have appeared in many parts of the world. They are often similar but frequently different enough to create a messy landscape of laws that can be difficult to navigate. The most important regulations are\n\nGDPR: the General Data Protection Regulations of the European Union\nCCPA: the California Consumer Privacy Act\nCPRA: the California Privacy Rights Act\nVCDPA: the Virginia Consumer Data Protection Act\nCPA: the Colorado Privacy Act\n\nThis article in Bloomberg Law provides a good overview of the four pieces of legislation.\n\n\n\n\n\n\nFigure 17.1: Basics of major data privay regulations. Source.\n\n\n\n\n\n\n\n\n\nFigure 17.2: Data protected by major data privay regulations. Source.\n\n\n\nThere is substantial overlap between the regulations, and also important differences. For example, the U.S. laws, which are based on bottom-up common law, are opt-out laws, whereas the top-down GDPR is an opt-in law. Consider consent, for example. An opt-out law assumes that consent is automatically given unless someone opts out. Under GDPR, consent is automatically withheld unless someone opts in explicitly (Figure 17.1).\nThe type of personal data protected under the regulations differs as well. The California statutes extend personal data to data about a household. When modeling consumer behavior, for example, modeling at the aggregate household level does not relieve you from worrying about personal data privacy of California residents. The U.S. laws exclude publicly available data from the definition of personal data, while GDPR includes publicly available data (Figure 17.2).\n\n\n\n\n\n\nFigure 17.3: Who must comply with major data privay regulations. Source.",
    "crumbs": [
      "Part IV. Applied Ethics in Data Science",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Personal Information and Personal Data</span>"
    ]
  },
  {
    "objectID": "ethics/privacy.html#what-is-personal-data",
    "href": "ethics/privacy.html#what-is-personal-data",
    "title": "17  Personal Information and Personal Data",
    "section": "17.3 What is Personal Data?",
    "text": "17.3 What is Personal Data?\nThings get complicated quickly because the definition of personal data varies and because there are many regulations to pay attention to. The General Data Protection Regulations (GDPR) of the European Union applies a very broad and non-prescriptive definition of personal information. HIPAA, The U.S. Health Insurance Portability and Accountability Act defines items specifically as PII, personally identifiable information. The primary focus of HIPAA is to protect personal information related to health. The National Institute of Standards and Technology, NIST, applies a different definition to PII. For example, your computer’s IP address is not PII under NIST standards but is personal data under GDPR. In 2011, the California State Supreme Court ruled that a person’s ZIP code is PII. The California Consumer Privacy Protection Act (CPPA) distinguishes personal information such as internet browsing history from sensitive personal information, for example, an account log-in or union membership.\nWe can never cover all the bases and thus recommend taking a conservative stance and a broad definition of personal information. If in doubt, consider a piece of data that can be linked to a person as personal information that is protected and needs to be safeguarded. The broad coverage of GDPR is a good place to start. In addition, you need to know the data privacy regulations that apply in a particular domain or to data from a specific group. The following are some federal laws that govern specific types of data:\n\nThe Privacy Act of 1974 governs how federal agencies can collect and use data about individuals in its system of records.\nThe Health Insurance Portability and Accounting Act (HIPAA) governs the collection of health information.\nThe Children’s Online Privacy Protection Act (COPPA) governs the collection of information about minors.\nThe Gramm Leach Bliley Act (GLBA) governs personal information collected by banks and financial institutions.\nThe Fair Credit Reporting Act (FCRA), which regulates the collection and use of credit information.\nThe Family Educational Rights and Privacy Act (FERPA), which protects the privacy of student education records.\n\nThe GDPR defines personal data in Article 4:\n\n‘personal data’ means any information relating to an identified or identifiable natural person (‘data subject’);\n\nThere is a lot to unpack here. What does “relating to” mean? What is the difference between an “identified” and an “identifiable” natural person?\nAn identifiable person can be identified by reference to a name, ID, location data, or by one or more other attributes specific to the identity of that person, such as genetic, mental, physical, cultural, or social information. To be identifiable, it is sufficient that you can distinguish them from other individuals. Personal data is not just what makes us unique, it is sufficient for the data to distinguish us from others. Your online shopping cart makes you an identifiable person because it can be used to distinguish you from people with a different shopping cart. It does not matter that there could be other shoppers whose cart looks exactly like yours.\nThe name is an identifier, but it might not make you identifiable—there are many “John Smiths”. It is still personal information.\nWhat is the meaning of “relating to”? A careful reading of the GDPR definition of personal data suggests that identifying data might not be personal data if it does not relate to the person. Wait, what? A few examples will make this clearer:\n\nInformation about an individual or about their activities is obviously personal data, it explicitly relates to the individual. Your birthday is your personal data. However, March 28, 2003, is just a date. It is not personal data until it is someone’s birthday, anniversary, wedding day, arrest day, etc.\nConsider a job listing that includes salary information. The salary is not personal information, it relates to no specific individual. As soon as the position is filled, the salary information becomes personal information because it now relates to the new employee.\nAn organization has an org chart with job titles. The job title in the org chart is not personal data until it is related to a specific individual or a group of employees.\nData about a house is not, by itself, personal data. But an individual’s tax record or utility bill are personal data. As soon as the information about the house is tied to an owner, it becomes personal data—it now relates to the owner. You can conduct analyses of house values in a geographical area. The houses in the study are selected because of their properties—four bedrooms and a double garage—not because they are occupied by a specific entity. But if the data about the house is used with respect to an individual, then it is personal data.\nA factory collects information about a machine on the factory floor. This is not personal data if the purpose of analysis is to understand the operating characteristics of the machine. If the data is used to assess the productivity of the machine operator, then it is personal data.\n\nThe definition of personal data in GDPR is non-prescriptive. The regulations do not explicitly state that a social security number is personal information—it is. Whether information is personal data depends on the context. Does it relate to distinguishable individuals? If so, it is personal data. GDPR is adaptable because of this approach, if we determine that an IP address is personal data in a context, then a MAC address is as well. The downside of a non-prescriptive definition is that context matters, and we need to know how to apply it.\nData does not have to be correct to constitute personal data. A social-security number entered into a database is still personal data, even if someone fat-fingered some digits.\n\n\nExample: Inaccurate Personal Data\n\n\nJohn and William live in an apartment building; John lives on the ground floor and William lives on the top floor. They both wear glasses.\nThe landlord receives a complaint that the man wearing glasses who lives on the ground floor has engaged in bad behavior. However, the complaint actually is about William’s activity.\nThe landlord records the information about the behavior relating to John. The information is inaccurate, but it is nevertheless personal data related to John. At the same time, this is also personal data about William, even though it’s been recorded about John.",
    "crumbs": [
      "Part IV. Applied Ethics in Data Science",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Personal Information and Personal Data</span>"
    ]
  },
  {
    "objectID": "ethics/privacy.html#safeguarding-personal-information",
    "href": "ethics/privacy.html#safeguarding-personal-information",
    "title": "17  Personal Information and Personal Data",
    "section": "17.4 Safeguarding Personal Information",
    "text": "17.4 Safeguarding Personal Information\nIf you google how to safeguard personal information you get a wealth of similar responses—almost all of them are about how you can safeguard your personal information: use strong passwords, use two-factor authentication, don’t overshare online, turn off location services, watch out for phishing attempts, don’t follow unknown or suspicious links, keep your social security card in a safe place at home, …\nBut how do we safeguard other’s personal information that makes it into data sets we work with? There are a number of principles you can follow as an ethical data scientist. These (and more) are enshrined as the data protection principles of GDPR.\n\nLawful, Fair, and Transparent\nAct in the best interest of the data subjects, always on a legal basis and with best intentions. That means you had consent to collect and analyze the data and the scope of analysis does not exceed what can reasonably be expected.\n\n\nData Minimization\nOnly collect and keep the amount of personal data that is needed, in terms of the number of records and the number of variables. If you gather data to deliver a newsletter electronically, then all you need is the name and email address. Maybe you do not even need to know the name of the subscriber. Having their job title might be an interesting piece of information but it is not necessary to send out the newsletter—it should not be collected.\nSuppose you analyze data on houses to develop a predictive model for residential real estate prices, and the data set contains information about the owners of the properties. This information is not necessary for the purpose and should be removed from the data frame you work with.\n\n\nConfidentiality\nOnly people who should have access to data are working with the data. If you are in control of data, you have a responsibility to protect the personal data you are processing. A common means of violating this principle is sharing data with others not knowing that the data contains personal information. If you receive a request for access to data that you have access to, ask yourself whether the requestor is authorized to use the data. If in doubt, put security concerns ahead of trying to be helpful.\n\n\nStorage Limitation\nDo not store personal data you do not need anymore. This is related to the data minimization principle, do not keep what you do not need.\nDestroying data is trickier than you might think. There can be multiple copies of records, on multiple devices, including backups in the cloud, backups on tapes, etc. The same information can be stored at different levels of data preparation, for example in different layers of a data lake with medallion architecture. You can destroy data by removing it from storage devices using erasure, degaussing (destroying the magnetic field), or overwriting. Or you can destroy the storage device, for example by shredding hard drives.\nThe GDPR include a right to data deletion, any data subject can ask that their personal data be deleted. You need to know what personal data you have on someone, where it is, and how to destroy that data.\nIf you gather email addresses for the purpose of sending an electronic newsletter, and you decide to stop sending newsletter, you have to delete the email addresses because the purpose for which the personal data was given no longer exists.\n\n\nPurpose Limitation\nUse the data only for the purpose it was intended for. If the purpose expands or changes you need to re-secure consent. You cannot reuse personal data for other purposes. If you tell an online user that you store their IP address to document their consent, then you cannot use the IP address to send them marketing material.\n\n\nAnonymization and Pseudonymization\nAnonymization: remove personal information from a data set. The anonymized data should eliminate direct re-identification, for example, by combining other information in the data set.\nPseudonymization: pseudonymizing data is a weaker safeguard than anonymizing it. Pseudonymization replaces the original value of personal information with a mask or by encryption. If the pseudonymization is reversible, for example with an encryption key, the data is still considered personal information.\n\n\n\nFigure 17.1: Basics of major data privay regulations. Source.\nFigure 17.2: Data protected by major data privay regulations. Source.\nFigure 17.3: Who must comply with major data privay regulations. Source.",
    "crumbs": [
      "Part IV. Applied Ethics in Data Science",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Personal Information and Personal Data</span>"
    ]
  },
  {
    "objectID": "integ/coding_practices.html",
    "href": "integ/coding_practices.html",
    "title": "18  Coding Best Practices",
    "section": "",
    "text": "18.1 Version Control\nVersion control refers to the management and tracking of changes in digital content; mostly files and mostly code. Any digital asset can be placed under version control. Even if you are working (mostly) by yourself, using a version control system is important. Employers consider it a non-negotiable skill and you do not want to stand out as the applicant who does not know how to use Git. The benefits of version control systems are many, even the solo programmer would be remiss not to use it.\nWhat does a version control system like Git do for you:\nThe list goes on and on. The main point is that these capabilities and benefits are for everyone, whether you work on a project alone or as a team member.\nThere are many version control systems, Git, Perforce, Beanstalk, Mercurial, Bitbucket, Apache Subversion, AWS CodeCommit, CVS (Concurrent Versions System, not the drugstore chain), and others.\nThe most important system today is Git. GitHub and GitLab are built on top of Git. What is the relationship? Git is a local version control system, it runs entirely on the machine where it is installed and manages file changes there. GitHub and GitLab are cloud-based systems that allow you to work with remote repositories. In addition to supporting Git remotely, GitHub adds many cool features to increase developer productivity. The files for the pages you are reading are managed with Git and stored in a remote repository on GitHub (the URL is https://github.com/oschabenberger/oschabenberger-github.io-bn). GitHub also hosts the web site for the text through GitHub Pages. GitHub Actions can be set up so that the web site (the book) automatically rebuilds if any source files changes. And all of that comes with the free capabilities of GitHub.",
    "crumbs": [
      "Part V. Integration of Data Science Solutions",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Coding Best Practices</span>"
    ]
  },
  {
    "objectID": "integ/coding_practices.html#sec-repro-vc",
    "href": "integ/coding_practices.html#sec-repro-vc",
    "title": "18  Coding Best Practices",
    "section": "",
    "text": "It keeps track of files and their changes over time.\nIt saves changes to files without duplicating the contents, saving space in the process.\nIt groups content in logical units (branches) that are managed together. For example, all files associated with a particular build of a software release are kept in a branch.\nIt is a time machine, allowing you to reconstruct a previous state of the project and to see the complete history of the files.\nIt is a backup machine, making sure you have access to older versions of files and that changes do not get lost.\nIt allows you to perform comparisons between versions of files and to reconcile their differences.\nIt allows you to safely experiment with code without affecting code others depend on.\nIt allows you to see which parts of a project are worked on most/least frequently.\nIt is a collaborative tool, that reconciles changes to files made by multiple developers. Version control systems allow you to submit changes to someone else’s code.\nBy supporting modern continuous integration/continuous deployment (CI/CD) principles, a version control system can automate the process of testing and deploying software.\n\n\n\n\n\n\n\n\nTip\n\n\n\nOh how I wish there were easily accessible version control systems when I did my Ph.D. work. It involved a lot of programming algorithms and the analysis of real data sets. Developing the code took months to years and went through many iterations. I made frequent backups of the relevant files using really cool storage technology using special 1GB-size cartridges and a special reader. There were disks labeled “January 1993”, “March 1993”, “December 1993”, “Final”, “Final-V2”, and so forth. The storage technology was discontinued by the manufacturer and the cartridges are useless today. I am not able to access the contents even if the bits have not rotted on the media by now.\nTo study how the algorithm I needed to write for the dissertation evolved over time, I would have to go through all the backups and compare files one by one. A version control system will show me the entire history of changes in one fell swoop.\nUsing a cloud-based version control system would have avoided that headache. Alas, that did not exist back then.\n\n\n\n\n\nA Crash Course in Git\nGit is installed on your machine, it is a local tool for versioning files. You can perform all major Git operations (clone, init, add, mv, restore, rm, diff, grep, log, branch, commit merge, rebase, etc.) without an internet connection. The collaborative aspect of version control comes into play when you use a Git service provider such as GitHub or GitLab. Besides making Git a tool for multi-user applications, using GitHub or GitLab also gives you the ability to work with remote repositories; you can push your local changes to a server in the cloud, making it accessible to others and making it independent of the local workstation. Just because you push a repository to GitHub does not necessarily give everyone on the internet access to it—you manage whether a repository is private or public.\n\nInstalling Git\nThere are several ways to get Git on your machine, see here. On MacOS, installing the XCode Command Line tools will drop git on the machine. To see if you already have Git, open a terminal and check:\n➜ which git\n/usr/bin/git\nThe executable is installed in /usr/bin/git on my MacBook.\n\n\nBasic configuration\nThere are a million of configuration options for Git and its commands. You can see the configuration with\n➜ git config --list\nTo connect to GitHub later, add your username and email address to the configuration:\n➜ git config --global user.name \"First Last\"\n➜ git config --global user.email \"first.last@example.com\"\nYou can have project-specific configurations, simply remove the --global option and issue the git config command from the project (repository) directory.\n\n\nRepositories\nA repository is a collection of folders and files. Repositories are either cloned from an existing repository or initialized from scratch. To initialize a repository, change into the root directory of the project and issue the git init command:\n➜ cd \"STAT 5014\"\n➜ STAT 5014 pwd\n/Users/olivers/Documents/Teaching/Data Science/STAT 5014\n➜ STAT 5014 git init\nInitialized empty Git repository in /Users/olivers/Documents/Teaching/Data Science/STAT 5014/.git/\n➜ STAT 5014 git:(main)\nTo get help on git or any of the git commands, simply add --help:\n➜ git --help\n➜ git status --help\n➜ git add --help\n\n\nStages of a file\nA file in a Git repository goes through multiple stages (Figure 18.1). At first, the file is unmodified and untracked. A file that was changed in any way is in a modified state. That does not automatically update the repository. In order to commit the change, the file first needs to be staged with the git add command.\nWhen you issue a git add on a new file or directory, it is being tracked. When you clone a repository, all files in your working directory will be tracked and unmodified.\n\n\n\n\n\n\nFigure 18.1: The lifecycle of a file in Git. Source\n\n\n\nA file that is staged will appear under the “Changes to be committed” heading in the git status output.\nOnce you commit the file it goes back into an unmodified and tracked state.\n\nTracking files\nTo track files in a repository, you need to explicitly add them to the file tree with git add. This does not commit the file or push the file into a branch or a remote repository, it simply informs Git which files you care about.\n➜ git add LeastSquares.R\n➜ git add *.Rmd\n➜ git add docs/\nThe previous commands added LeastSquares.R, all .Rmd files in the current directory, and all files in the docs subfolder to the Git tree. You can see the state of this tree any time with\n➜ git status\ngit status shows you all files that have changed as well as files that are not tracked by Git and are not ignored. For example, after making some changes to the quarto.yml and to reproducibility.qmd files since the last commit, the status of the repository for this material looks as follows:\n➜  StatProgramming git:(main) ✗ git status\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    modified:   _quarto.yml\n    modified:   docs/reproducibility.html\n    modified:   docs/search.json\n    modified:   reproducibility.qmd\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    .DS_Store\n    .gitignore\n    .nojekyll\n    .python-version\n    StatProgramming.Rproj\n    _book/\n    ads.ddb\n    customstyle.scss\n    data/\n    debug_ada.R\n    debug_ada.Rmd\n    images/\n    latexmacros.tex\n    sp_references.bib\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nTwo more files have been noted by Git as modified, docs/reproducibility.html and docs/search.json. These files are generated by Quarto when the content of the modified files is being rendered. They will be added to the next commit to make sure the website is up to date and not just the source (.qmd) files.\ngit add can be a bit confusing because it appears to perform multiple functions: to track a new file and to stage a file for commit. If you think of git add as adding precisely the content to the next commit, then the multiple functions roll into a single one.\n\nAn ignored file is one for which you explicitly tell Git not to worry about. You list those files in a .gitignore file. (You can have multiple .gitignore files in the directory hierarchy, refer to the Git documentation on how they interact. The typical scenario is a .gitignore file in the root of the repository.)\nThe contents of the following .gitignore file state that all .html files should be ignored, except for foo.html. Also, StatLearning.Rproj will be ignored.\n➜ cat .gitignore\n*.html\n!foo.html\nStatLearning.Rproj\nFiles that are listed in .gitignore are not added to the repository and persist when a repository is cloned. However, if a file is already being tracked, then adding it to .gitignore does not untrack the file. To stop tracking a file that is currently tracked, use\ngit rm --cached filename \nto remove the file from the tree. The file name can then be added to the .gitignore file to stop the file from being reintroduced in later commits.\nFiles that you want to exclude from tracking are often binary files that are the result of a build or compile, and large files. Also, if you are pushing to a public remote repository, make sure that no files containing sensitive information are added.\n\n\nCommitting changes\nOnce you track a file, Git keeps track of the changes to the file. Those changes are not reflected in the repository until you commit them with the commit command. A file change will not be committed to the repository unless it has been staged. git add will do that for you.\nIt is a good practice to add a descriptive message to the commit command that explains what changes are committed to the repository:\n➜ git commit -m \"Early stopping criterion for GLMM algorithm\"\nIf you do not specify a commit message, Git will open an editor in which you must enter a message.\n\n\n\n\n\n\nTip\n\n\n\nIf Git opens an editor for you and this is the first time you find yourself in vi or vim, you might struggle with those editors. To set a different default editor on MacOS or Linux, set the EDITOR environment variable.\n➜ echo $EDITOR\ntells you whether a default text editor has been set.\n\n\nSince only files that have been added with git add are committed, you can ask Git to notice the changes to the files whose contents are tracked in your working tree and do corresponding git adds for you by adding the -a option to the commit:\n➜ git commit -a -m \"Early stopping criterion for GLMM algorithm\"\nWhat happens when you modify a file after you ran git add but before the net commit? The file will appear in git status as both staged and ready to be committed and as unstaged. The reason is because Git is tracking two versions of the file now: the state it was in when you first ran git add and the state it is in now, which includes the modifications since the last git add. In order to stage the most recent changes to the file, simply run git add on the file again.\n\n\nRemote repositories\nThe full power of Git comes to light when you combine the local work in Git repositories with a cloud-based version control service such as GitHub or GitLab. To use remote repositories with Git, first set up an account, say with GitHub.\nThe Git commands to interact with a remote repository are\n\ngit pull: Incorporates changes from a remote repository into the current branch. If the current branch is behind the remote, then by default it will fast-forward the current branch to match the remote. The result is a copy of changes into your working directory.\ngit fetch: Copies changes from a remote repository into the local Git repository. The difference between fetch and pull is that the latter also copies the changes into your working directory, not just into the local repo.\ngit push: Updates remote references using local references, while sending necessary objects.\ngit remote: Manage the set of remote repositories whose branches you track.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have used other version control systems, you might have come across the terms pushing and pulling files. In CVS, for example, to pull a file means adding it to your local checkout of a branch, to push a file means adding it back to the central repository.\nWith Git, push and pull command only come into play when you work with remote repositories. As long as everything remains on your machine, you do not need those commands. However, most repositories these days are remote, so the initial interaction with a repository is often a clone, pull, or fetch.\n\n\nStart by creating a new repository on GitHub by clicking on the New button. You have to decide on a name for the repository and whether it is public or private. Once you created a remote repository, GitHub gives you alternative ways of addressing it, using https, ssh, etc.\n\n\n\n\n\n\nTip\n\n\n\nDepending on which type of reference you use on the command line, you also need different ways of authenticating the transaction. GitHub removed passwords as an authentication method for command-line operations some time ago. If you use SSH-style references you authenticate using the passphrase of an SSH key registered with GitHub. If you use https-style references you authenticate with an access token you set up in GitHub.\n\n\nBack on your local machine you manage the association between the local repository and the remote repository with the git remote commands. For example,\n➜ git remote add origin git@github.com:oschabenberger/oschabenberger-github.io-bn.git\nassociates the remote repository described by the ssh syntax git@github.com:oschabenberger/oschabenberger-github.io-bn.git with the local repository. Using html syntax, the same command looks like this:\n➜ git remote add origin https://github.com/oschabenberger/oschabenberger-github.io-bn\nGitHub provides these strings to you when you create a repository.\nTo update the remote repository with the contents of the local repository, issue the git push command:\n➜ git push",
    "crumbs": [
      "Part V. Integration of Data Science Solutions",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Coding Best Practices</span>"
    ]
  },
  {
    "objectID": "integ/coding_practices.html#structure-and-organization",
    "href": "integ/coding_practices.html#structure-and-organization",
    "title": "18  Coding Best Practices",
    "section": "18.2 Structure and Organization",
    "text": "18.2 Structure and Organization\n\nNaming\n\nChoose names for variables and functions that are easy to understand. Variable and function names should be self explanatory. Most modern programming languages and tools no longer limit the length of function or variable names, there is no excuse for using a1, a2, b3 as variable names. Use nouns for names of variables and objects that describe what the item holds; for example, originalData and randomForestResult instead of d and out.\nStick with a naming convention such as snake_case, PascalCase, or camelCase. In snake_case, spaces between words are replaced with an underscore. In camelCase, words are concatenated and the first letter of the word is capitalized. PascalCase is a special case where the first letter of the entire name is also capitalized; camelCase is ambivalent about capitalizing the first letter of the name. The following are examples of names in camelCase.\n\naccountBalance\nthisVariableIsWrittenInCamelCase\nitemNumber\nsocialSN\nMasterCard\nAn issue with camelCase is that it is not entirely clear how to write names in that style that contain other names or abbreviations, for example, is it NASAAdminFiles or NasaAdminFiles? I am not sure it really matters.\nsnake_case is popular because it separates words with underscores—mimicking white space—while producing valid names for computer processing. The following are examples of names in snake_case:\naccount_balance\nACCOUNT_BALANCE\nhome_page\nitem_Number\nUsing upper-case letters in snake_case is called “screaming snake case”, situations where I have seen it used are the definition of global constants or macro names in C. kebab case is similar to snake case but uses a hyphen instead of an underscore. Here are examples of names in kebab case:\naccount-balance\nhome-page\nitem-Number\nAlthough it might look nice, it is a good idea to avoid kebab case in programs. Imagine the mess that ensues if the hyphen were to be interpreted as a minus sign! While the compiler might read the hyphen correctly, the code reviewer in the cubicle down the hall might think it is a minus sign.\nDo not assign objects to existing names, unless you really want to override them. This goes in particular for internal symbols and built-in functions. Unfortunately, R does not blink and allows you to do things like this:\nT &lt;- runif(20)\nC &lt;- summary(lm(y ~ x))\nThese assignments override the global variable whose value is set to TRUE for logical comparison and the function C() that defines contrasts for factors. If in doubt whether it is safe to assign to a name, check in the console whether the name exists or request help for it\n?T\n?C()\n\n\nComments\nUnless you write a literal program use comments throughout to clarify why code is written a certain way and what the code is supposed to accomplish. Even with literal programs, comments associated with code are a good practice because the code-portion of the literal program can get separated from the text material at some later point.\nComments frequently are intended by programmers to leave themselves some notes, for example, about functions yet to be written or to be refactored later. Make it clear with a “TODO” at the beginning of the comment where those sections of the program are and make the TODO comment stand out visually from other comments.\nIt is a good practice to have a standardized form for writing comments. For example, you can have a standard comment block at the beginning of functions. Some organizations will require you to write very detailed comment blocks that explain all inputs and outputs down to length of vectors and data types.\n# -------------------------------------------------------------------\n# TODO: Add check whether it is safe to perform the division before\n#       returning from the function. Variances can be zero or near zero.\n# -------------------------------------------------------------------\n\n# ###################################################################\n# Function: getIRLSWeights\n# Purpose: retrieve the vector of weights for iteratively \n#          reweighted least squares\n# Arguments:\n#       eta: numeric vector of linear predictors\n#       link: character string describing the link function\n#       dist: character string describing the distribution of the response \n#\n# Returns: the vector of weights, same length as the eta vector (input)\n#\n# Notes: \n#   For efficiency, eta is not checked for NULL or missing values. \n#   These checks are in the deta_dmu() and get_var() functions.\n# ###################################################################\n\ngetIRLSWeights &lt;- function(eta, link=\"log\", dist=\"poisson\") {\n    var_z &lt;- deta_dmu(eta,link)^2 * get_var(get_mu(eta,link),dist)\n    return (1/var_z)    \n}\nIf you program in Python, you would add docstrings to the function. This also serves as the help information for the user.\n\n\nWhitespace\nJudicious use of whitespace makes code more readable. It helps to differentiate visually and to see patterns. Examples are indentation (use spaces, not tabs), alignment within code blocks, placement of parentheses, and so forth.\nWhich of the following two functions is easier to read? It does not matter for the R interpreter but it matters for the programmer.\nget_z &lt;- function(y, eta, link) {\nif (is.null(y) || is.null(eta)) {\nstop(\"null values not allowed\") }\nif (anyNA(y) || anyNA(eta)) {\nstop(\"cannot handle missing values\") }\nz &lt;- eta + (y - get_mu(eta,link)) * deta_dmu(eta,link)\nreturn(z)\n}\n\nget_z &lt;- function(y, eta, link) {\n    if (is.null(y) || is.null(eta)) {\n        stop(\"null values not allowed\")\n    }\n    if (anyNA(y) || anyNA(eta)) {\n        stop(\"cannot handle missing values\")\n    }\n    z &lt;- eta + (y - get_mu(eta,link)) * deta_dmu(eta,link)\n    return(z)\n}\nThe following code uses indentation to separate options from values and to isolate the function definition for handling the reference strip. The closing parenthesis is separated with whitespace to visually align with the opening parenthesis of xyplot.\nxyplot(diameter ~ measurement | Tree, \n       data    = apples,\n       strip   = function(...) {\n           # alter the text in the reference strip   \n           strip.default(..., \n                         strip.names  = c(T,T), \n                         strip.levels = c(T,T),\n                         sep          = \" \")\n       },\n       xlab    = \"Measurement index\",\n       ylab    = \"Diameter (inches)\",\n       type    = c(\"p\"),\n       as.table= TRUE,\n       layout  = c(4,3,1)\n      )\nWith languages such as Python, where whitespace is functionally relevant, you have to use spacing within the limits of what the language allows.\n\n\nFunctions\n\nR\nIn R almost everything is a function. When should you write functions instead of one-off lines of code? As always, it depends; a partial answer hides in the question. When you do something only once, then writing a bunch of lines of code instead of packaging the code in a function makes sense. When you write a function you have to think about function arguments (is the string being passed a single string or a vector?), default values, return values, and so on.\nHowever, many programming tasks are not one-offs. Check your own code, you probably write the same two or three “one-off” lines of code over and over again. If you do it more than once, consider writing a function for it. If you do a substantial task over and over, consider writing a package.\n\n\n\n\n\n\nNote\n\n\n\nIf you are interested in writing an R package, check out this chapter in the book by Peng, Kross, and Anderson (2020).\n\n\nFunction names should be verbs associated with the function purpose, e.g., joinTables(), updateWeights(). For functions that retrieve or set values, using get and set is common: getWeights(), setOptimizationInput().\nThe comment block for function should document the function purpose, required arguments, and returns.\nSome argue that it is good coding practice to have default values on function arguments. For example,\naddNumbers &lt;- function(a=1, b=2) {return(a+b)}\ninstead of\naddNumbers &lt;- function(a, b) {return(a+b)}\nAdding defaults ensures that all variables are initialized with valid values and simplifies calling the function. On the other hand, it can mask important ways to control the behavior of the function. Users will call a function as they see it being used by others and not necessarily look at the function signature. Take the duckload() function:\n\nduckload &lt;- function(tableName, whereClause=NULL, dbName=\"ads.ddb\") {\n    if (!is.null(tableName)) {\n        if (!(\"duckdb\" %in% (.packages()))) {\n            suppressWarnings(library(\"duckdb\"))\n            message(\"duckdb library was loaded to execute duckload().\")\n        }\n        con &lt;- dbConnect(duckdb(), dbdir=dbName, read_only=TRUE)\n        query_string &lt;- paste(\"SELECT * from \", tableName)\n        if (!is.null(whereClause)) {\n            query_string &lt;- paste(query_string, \" WHERE \", whereClause)\n        }\n        df_ &lt;- dbGetQuery(con, query_string)\n        dbDisconnect(con)\n        return (df_)\n    } else {\n        return (NULL)\n    }\n}\n\nWould you know from the following usage pattern that you can pass a WHERE clause to the SQL string?\n\nduckload(\"apples\")\n\nIf the function arguments had no defaults, the function call would reveal its capabilities:\n\nduckload(\"apples\", whereClause=NULL, dbName=\"ads.ddb\")\n# or\nduckload(\"apples\",NULL,\"ads.ddb\")\n\nOther good practices to observe when writing functions:\n\nAlways have an explicit return argument. It makes it much easier to figure out where you return from the function and what exactly is being returned.\nCheck for NULL inputs\nCheck for missing values unless your code can handle them.\nHandle errors (see below)\nPass through variable arguments (...)\nIf you return multiple values, organize them in a list or a data frame. Lists are convenient to collect objects that are of different types and sizes into a single object. The following function returns a list with three elements,\n\n\niterationWeight &lt;- function(Gm,wts,method=\"tree\") {\n    pclass &lt;- predict(Gm,type=\"vector\")\n    misclass &lt;- pclass != Gm$y\n    Em &lt;- sum(wts*misclass)/sum(wts)\n    alpha_m &lt;- log((1-Em)/Em)\n    return (list(\"misclass\"=misclass,\"Em\"=Em,\"alpha_m\"=alpha_m))\n}\n\n\nError Handling\nThink of a function as a contract between you and the user. If the user provides specified arguments, the function produces predictable results. What should happen when the user specifies invalid arguments or when the function encounters situations that would create unpredictable results or situations that keep it from continuing?\nYour opportunities to handle these situations include issue warning messages with warning(), informational messages with message(), stopping the execution with stop() and stopifnot() and try-catch-finally execution blocks. In general, stopping the execution of a function with stop or stopifnot is a last resort if the function cannot possibly continue. If the data passed are of the wrong type and cannot be coerced into the correct data type, or if coercion would result in something nonsensical, then stop.\nIn the event that inputs are invalid and you cannot perform the required calculations, could you still return NULL as a result? If so, do not stop the execution of the function. You can issue a warning message and then return NULL. Warning messages are also appropriate when the function behavior is changing in an unexpected way. For example, the input data contains missing values (NAs) and your algorithm cannot handle them. If you process the data after omitting missing values, then issue a warning message if that affects the dimensions of the returned objects.\nKeep in mind that R is used in scripts and as an interactive language. Messages from your code are intended for human consumption so they should be explicit and easy to understand. But avoid making your code too chatty.\nTo check whether input values have the expected types you can use functions such as\n\nis.numeric()\nis.character()\nis.factor()\nis.ordered()\nis.vector()\nis.matrix()\n\nand to coerce between data types you can use the as. versions\n\nas.numeric()\nas.character()\nas.factor()\nas.ordered()\nas.vector()\nas.matrix()\n\ntryCatch() is the R implementation of the try-catch-finally logic you might have seen in other languages. It is part of the condition system that provides a mechanism for signaling and handling unusual conditions in programs. tryCatch attempts to evaluate expression expr, and if it succeeds, executes the code in the finally block. You can add erorr and warning handlers with the error= and warning= options.\n\ntryCatch(expr,\n         error = function(e){\n           message(\"An error occurred:\\n\", e)\n         },\n         warning = function(w){\n           message(\"A warning occured:\\n\", w)\n         },\n         finally = {\n           message(\"Finally done!\")\n         }\n        )\n\ntryCatch is an elegant way to handle conditions, but you should not overdo it. It can be a drag on performance. For example, if you require input to be of numeric type, then it is easier and faster to check with is.numeric than to wrap the execution in tryCatch.\n\n\nDependencies\nIt is a good idea to check dependencies in functions. Are the required packages loaded? It is kind of you to load required packages on behave of the caller rather than stopping execution. If you do, issue a message to that effect. See duckload() above for an example.\nInstalling packages on behalf of the caller is a step too far in my opinion, since you are now changing the R environment. You can check whether a package is installed with require. The following code stops executing if the dplyr package is not installed.\n\ncheck_pkg_deps &lt;- function() {\n    if (!require(dplyr))\n        stop(\"the 'dplyr' package needs to be installed first\")\n}\n\nrequire() is similar to library(), but while the latter fails with an error if the package cannot be loaded, require returns TRUE or FALSE depending on whether the library was loaded and does not throw an error if the package cannot be found. Think of require as the version of library you should use inside of functions.\n\n\nDocumentation\nComments in code are not documentation. Documentation is a detailed explanation of the purpose of the code, how it works, how its functions work, their arguments, etc. It also includes all information someone would want to need to take over the project. In literal programs you have the opportunity to write code and documentation at the same time. Many software authoring frameworks include steps in programming that generate the documentation. For example, to add documentation to an R package, you need to create a “man” subdirectory that contains one file per function in the special R Documentation format (.Rd). You can see what the files look like by browsing R packages on GitHub. For example, here is the repository for the ada package.\nAt a minimum a README file in Markdown should accompany the program. The file has setup instructions and use instructions someone would have to follow to execute the code. It identifies author, version, major revision history, and details on the functions in the public API—those functions called by the user of the program.\nThere are great automated documentation systems such as doxygen which annotate the source code in such a way that documentation can be extracted automatically. An R package for generating inline documentation that was inspired by doxygen is roxygen2.\n\n\n\nPython\n\n\n\nFigure 18.1: The lifecycle of a file in Git. Source\n\n\n\nPeng, Roger D., Sean Kross, and Brooke Anderson. 2020. Mastering Software Development in r. https://bookdown.org/rdpeng/RProgDA/.",
    "crumbs": [
      "Part V. Integration of Data Science Solutions",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Coding Best Practices</span>"
    ]
  },
  {
    "objectID": "integ/tools.html",
    "href": "integ/tools.html",
    "title": "19  Data Science Tools",
    "section": "",
    "text": "19.1 Tech Stacks\nBecause the “T-shaped” role of the data scientist touches many disciplines and systems it can take many tools to do the work. From project management tools like JIRA or Asana to machine learning frameworks like TensorFlow, CNTK, PyTorch, or Keras, to multiple programming languages (R, Python, Scala, Java, JavaScript, C++, Go), multiple databases, visualization tools, cloud providers, ETL and ELT tools, DevOps tools, ModelOps tools, dashboard builders. The list goes on and on. The abundance of software tools and frameworks available to data scientists can be overwhelming—and it is growing every day.\nA few comments and recommendations:\nEmployers will have preferences and standards you must comply with. Expertise with one tool makes switching to another tool easier. If your employer is an AWS shop you will not convince them to switch to Google BigQuery. If you have basic SQL skills, you will be able to move from BigQuery to Amazon Redshift. If you are familiar with business intelligence tools like Tableau or Alteryx, adding Power BI to your toolbox is not a problem.\nYou will also find that organizations have invested heavily in systems in the past and are slow to move on although more modern and better performing options are available. For example, many companies have built data lakes and machine learning environments based on the Hadoop ecosystem. Although superseded by cloud-based object storage, migrating from a Hadoop cluster is costly and time consuming. Hadoop-based tools such as Hive, Impala, Kudu, Pig, Mahout, Sqoop, and Zookeeper will be around for a while longer. Be ready to work with tools that might not be on the Favorites list.",
    "crumbs": [
      "Part V. Integration of Data Science Solutions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#tech-stacks",
    "href": "integ/tools.html#tech-stacks",
    "title": "19  Data Science Tools",
    "section": "",
    "text": "You do not need to be an expert at every tool in order to be an expert data scientist.\nYou can and should develop your own preferred tech stack.\nEvery tool has pros and cons, and everyone has preferences.\nLook at job postings to learn about the tech stack of employers. Some companies have published or described their tech stack online. This will give you an idea of the tools you will encounter at those employers and how companies make choices about their data stack.\nCheck Reddit, Stack Overflow, Stack Exchange, GitHub, etc. The degree of activity on these sites is a good indicator for the relevance of a tool.\nIn an organization with a small data science team, you will have a different set of tasks and tools than as part of a larger data science team with specialists in data engineering, visualization, etc.\nIn research settings the tech stack is smaller since the number of data science tasks is smaller.\nYou will find tools built in-house in larger companies. Many organizations use a blend of internal and 3rd-party tools. Some companies have their own forks of open-source software projects.\nBe ready to adapt and learn new tools, technologies, languages.\n\n\n\n\nExample Tech Stacks\nHere are some examples of tech stacks at companies. These include frontend and backend tools as well as data tools. We dive a bit deeper into the data engineering stack at Meta (formerly Facebook) in the next section.\nIn the video below from 2023, Chris Wiggins, Chief Data Scientist of the New York Times discusses the evolution of the data science tech stack at the New York Times:\n\nLike many other organizations, their data science tech stack is changing, the company is iterating to find what works best for them. They move from “write your own MapReduce jobs against S3 buckets” to “jobs in Hive and Pig” to “started our own Hadoop instance” to “all in BigQuery and GCP tech stack”: code in Python leveraging scikit-learn, when necessary code in Go, data are read from BigQuery, model output is pushed back to BigQuery, sometimes hosting an API, scheduling using Airflow instance on GCP, containerized.\nWhat are the takeaways:\n\nMoving away from Hadoop\nNot afraid of changing cloud providers (from AWS S3 to Google Cloud Platform and BigQuery)\nReading from and writing back to the same data platform: minimizing data movement, single format, SQL access\nAt every stage the tech stack was pretty modern and the company is not afraid to change—good attributes.\n\nHere are the tech stacks of some other companies:\n\nGoogle: Python, Java, AngularJS, Golang, C++, Dart, Preact, K8s, Android Studio, Bazel\nFacebook: React, PHP, GraphQL, Cassandra, Memcached, Presto, Flux, Tornado, RocksDB, Jenkins, Chef, Phabricator, Datadog, Confluence\nNetflix: Python, Node.js, React, Java, MySQL, PostgreSQL, Flask, AWS (S3, EC2, RDS, DynamoDB, EMR, CloudTrail), Cassandra, Oracle, Hadoop, Presto, Pig, Atlas-DB, GitHub, Jenkins, Gradle, Sumo Logic.\nUber: Python, jQuery, Node.js, React, Java, MySQL, NGINX, PostgreSQL, MongoDB, Redis, Amazon EC2, Kafka, Golang, Cassandra, Apache Spark, Hadoop, AresDB, Terraform, Grafana, Prometheus, Zookeeper. Also see this article on data science at Uber.\nShopify: Python, React, MySQL, NGINX, Redis, GraphQL, Kafka, Goang, Memcached, Apache Spark, Hadoop, dbt, Apache Beam, ElasticSearch, GitHub, Docker, K8s, Datadog, Chef, Zookeeper\nUdemy: Python, jQuery, Node.js, React, MySQL, NGINX, CloudFlare, AngularJS, Redis, Django, Spring Boot, Kafka, Kotlin, Memcached, ElasticSearch, GitHub, Docker, Jenkins, K8s, PyCharm, Ansible, Terraform, Sentry, Datadog\n\n\n\nThe Meta Data Engineering Stack\nWe picked the stack for data engineering at Meta (fka Facebook) for a deeper examination because their team provided a detailed article that discusses some of the characteristics of data engineering at large, modern companies (Meta 2023):\n\nVery large data warehouses\nComplex data pipelines\nA mix of commercial and open-source tools\nA mix of in-house and 3rd-party tools\n\nThe main data warehouse for analytics consists of a collection of millions of Hive tables stored in ORC (Optimized Row Columnar) format (see Section 4.4.4). Meta maintains its own fork of ORC, which suggests that they optimized the file format for their use cases.\nThe data warehouse is so large that it cannot be stored in one data center. The data are partitioned geographically and logically into namespaces—groups of tables that are likely used together. Tables in the same namespace are located together in the same data center location to facilitate merges and joins without sending data across geographies. If data needs to be accessed across namespaces, the data are replicated to another namespace so that they can be processed at the same location.\nYou really have a lot of data if the analytic data needs to be spread across multiple data centers in multiple geographies. The total size of the Meta data warehouse is measured in exabytes (millions of terabytes).\nMeta has a strict data retention policy, table partitions older than the table’s retention time are deleted or archived following anonymization of the data.\nTo find data in such a massive data warehouse, Meta developed its own tool, iData, to search for data by keyword. The iData search engine returns tables ranked by relevance, considering data freshness, number of uses, and number of mentions in posts of the table.\nTo query the data warehouse, Meta uses Presto and Spark. Presto is an open-source SQL querying engine originally developed by Meta. After open-sourcing Presto, Meta maintains its own internal fork. SQL (Presto SQL or Spark SQL) is key for querying the data at Meta. Presto is used for most day-to-day queries; a light query at Meta’s scale scans through a few billion rows of data. Spark is used for the heavy workloads.\nData exploration and analysis are based on internal tools, Daiquery is the internal tool for querying and visualizing any data source. Bento is an internal implementation of Jupyter notebooks for Python and R code.\nDashboards are created with another internal tool, Unidash.\nData pipelines are written in SQL, wrapped in Python, and orchestrated with Dataswarm, a predecessor of Airflow.\nVSCode is the IDE of choice for developing data pipelines and has been enhanced with custom plugins developed internally. For example, a custom linter checks SQL statements. On Save the internal VSCode extension generates the directed acyclic graph for the pipeline. The data engineer can then schedule a test run of the pipeline using real data, writing the result to a temporary result table.",
    "crumbs": [
      "Part V. Integration of Data Science Solutions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#sec-tools-eda-bi",
    "href": "integ/tools.html#sec-tools-eda-bi",
    "title": "19  Data Science Tools",
    "section": "19.2 Exploratory Data Analysis and Business Intelligence",
    "text": "19.2 Exploratory Data Analysis and Business Intelligence\nBusiness Intelligence (BI) is the processing of organizational data and presenting it in reports and on dashboards. The goal is to help an organization’s operations by using relevant data. Key functions are to monitor, report, and analyze the business operations. BI overlaps with Exploratory Data Analysis (EDA) in that it is highly descriptive, relying on visualizations and summarizations to inform about what is and has been happening.\nHere is a list of some BI tools you will encounter in practice (in no particular order):\n\nMicrosoft Power BI\nTableau (acquired by Salesforce)\nHeap Analytics\nMetabase\nMode (acquired by ThoughtSpot)\nThoughtSpot\nQlik\nSisense\nSAP BusinessObjects\nOracle BI\nTIBCO Spotfire\nAWS QuickSights\nLooker (Looker Studio)\nDOMO\nIBM Cognos Analytics\nMicroStrategy\nYellowfin\nSAS Augmented Analytics & BI\nJMP",
    "crumbs": [
      "Part V. Integration of Data Science Solutions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#data-engineering",
    "href": "integ/tools.html#data-engineering",
    "title": "19  Data Science Tools",
    "section": "19.3 Data Engineering",
    "text": "19.3 Data Engineering\nData engineers build pipelines that help to collect, merge, cleanse, prepare, and transform data for subsequent analytics. Data engineering defines, creates, and maintains the infrastructure that enables modern data analytics. Key steps in the data engineering workflow are pipelining, data replication, change-data-capture (CDC), ETL (Extract-Transform-Load) and/or ELT (Extract-Load-Transform).\nHere is a list of some common tools used in data engineering.\n\nDbt Labs. The “t” in dbt is the “T” in ELT. Dbt is a SQL-based data engineering tool that assumes the data is already loaded into the target system. It transforms data where it lives.\nFivetran. An ELT and data-movement platform with extensive data replication and change-data-capture capabilities.\nCData. Data connectivity, data movement, data sync (CDC).\nSpark. An engine for distributed big-data analytics with interfaces for Python (pySpark), SQL (spark-sql), Scala, Java, and R (sparkR)\nDask. A parallel-processing framework for Python\nApache Airflow. A Python-based tool to create and manage workflows. Often used to pipeline data.\nPrefect. Workflow orchestration for data engineers and ML engineers.\nApache Kafka. Open-source distributed event-streaming platform that is frequently used to move data through streaming pipelines.\nMatillion. Build and orchestrate data pipelines.\nDatabases (see Section 4.1)\nElasticSearch. Distributed search and analytics engine.\nPresto. An open-source, distributed SQL engine for analytic queries\nRedis. An open-source, in-memory data store. Often used as a memory cache.",
    "crumbs": [
      "Part V. Integration of Data Science Solutions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#data-visualization",
    "href": "integ/tools.html#data-visualization",
    "title": "19  Data Science Tools",
    "section": "19.4 Data Visualization",
    "text": "19.4 Data Visualization\n\nPython-based\npandas, matplotlib, seaborn, plotly, Vega-altair, plotnine (ggplot)\nR-based\ntidyverse (dplyr, tidyr, ggplot2, shiny)\nMany of the tools listed in Section 19.2.",
    "crumbs": [
      "Part V. Integration of Data Science Solutions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#data-analytics-and-machine-learning",
    "href": "integ/tools.html#data-analytics-and-machine-learning",
    "title": "19  Data Science Tools",
    "section": "19.5 Data Analytics and Machine Learning",
    "text": "19.5 Data Analytics and Machine Learning\n\nCloud Service Providers\n\nAWS SageMaker\nAzure Synapse Analytics\nGoogle BigQuery ML\n\n\n\nLanguages & Packages\n\nPython-based\n\nnumpy, scipy, pandas, polars, statsmodels, scikit-learn\nPyspark\n\nScala-based\n\nSpark\n\nR-based\n\nBasic modeling capabilities are built into the language\ndplyr, tidyr, caret, gam, glmnet, nnet, KernLab, E1071, RandomForest, tree, gbm, xgboost, lme4, boot, …. See the CRAN overview for Machine Learning and Statistical Learning\nsparkR\n\nJava: mostly used to put models into production and to build applications rather than model building.\n\nDeeplearning4j: open-source toolkit for Java to deploy deep neural nets.\nND4J: n-dimensional array objects for scientific computing\n\nGolang: Go is used mainly as a language for managing and orchestrating backend architecture but is finding more applications in data orchestration.\n\n\n\nCommercial Offerings\n\nAlteryx\nKNIME\nDomino Data Lab\nDataRobot\nDataiku\nH20.ai\nRapidMiner (acquired by Altair)\nMindsDB\nDatabricks\nSAS Viya\nJMP Pro\nMATLAB",
    "crumbs": [
      "Part V. Integration of Data Science Solutions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#deep-learning",
    "href": "integ/tools.html#deep-learning",
    "title": "19  Data Science Tools",
    "section": "19.6 Deep Learning",
    "text": "19.6 Deep Learning\n\nTensorFlow\nKeras\nTorch\nPyTorch\nMicrosoft Cognitive Toolkit (CNTK)\nOpenAI\nOpenCV\nViso Suite from viso.ai\nDeepLearningKit for Apple tvOS, iOS, OS X\nH2O.ai\nCaffe from Berkeley AI Research",
    "crumbs": [
      "Part V. Integration of Data Science Solutions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#ides-and-developer-productivity",
    "href": "integ/tools.html#ides-and-developer-productivity",
    "title": "19  Data Science Tools",
    "section": "19.7 IDEs and Developer Productivity",
    "text": "19.7 IDEs and Developer Productivity\n\nIPython: a command shell for interactive computing\nJupyterLab and Jupyter Notebook\nSpyder: IDE for Python\nVSCode: a code editor with IDE-like plugins\nVisual Studio (an IDE)\nDataSpell from JetBrains\nPyCharm: a commercial IDE for Python from JetBrains\nGoogle Colab(oratory)\nGit, GitLab, GitHub\nGitHub Copilot\nRStudio (free open-source edition)",
    "crumbs": [
      "Part V. Integration of Data Science Solutions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#cloud-computing-devops",
    "href": "integ/tools.html#cloud-computing-devops",
    "title": "19  Data Science Tools",
    "section": "19.8 Cloud Computing & DevOps",
    "text": "19.8 Cloud Computing & DevOps\n\nAWS (Amazon Web Services)\nMicrosoft Azure\nGCP (Google Cloud Platform)\nDocker\nKubernetes (K8s)\nFly.io\nCloudflare\nJenkins (CI/CD)\nAnsible",
    "crumbs": [
      "Part V. Integration of Data Science Solutions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#web-development",
    "href": "integ/tools.html#web-development",
    "title": "19  Data Science Tools",
    "section": "19.9 Web Development",
    "text": "19.9 Web Development\n\nSvelte\nVue.js\nAngular\nReact\nD3.js (data visualization)\nLaravel (PHP based)\nGraphQL (Graphene, Apollo etc.)\nNodeJS\nFlask\nDjango\nHeroku\nVercel, Next.js\nNetlify\nAWS Amplify\nAWS Lambda\nMongoDB Realm\nFirebase\nDigitalOcean",
    "crumbs": [
      "Part V. Integration of Data Science Solutions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#programming-languages",
    "href": "integ/tools.html#programming-languages",
    "title": "19  Data Science Tools",
    "section": "19.10 Programming Languages",
    "text": "19.10 Programming Languages\n\nR\nPython\nSQL\nScala\nJulia\nPHP\nHTML\nCSS\nC/C++\nGo\nRust\nJava\nJavaScript\nTypeScript\n\n\n\n\n\nMeta, Analytics at. 2023. “Data Engineering at Meta: High-Level Overview of the Internal Tech Stack.” Medium. https://medium.com/@AnalyticsAtMeta/data-engineering-at-meta-high-level-overview-of-the-internal-tech-stack-a200460a44fe.",
    "crumbs": [
      "Part V. Integration of Data Science Solutions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "From the project part\nData Scientist: The Sexiest Job of the 21st Century by Thomas H. Davenport and DJ Patil, Harvard Business Review, 2012\nBig Data is Dead by Jordan Tigani, 2023\nIs Data Scientist Still the Sexiest Job of the 21st Century by Thomas H. Davenport and DJ Patil, Harvard Business Review, July 2022\nA Very Short History Of Data Science by Gil Press, Forbes, 2013\nThe History of Data Science and Pioneers You Should Know\nThe Future of Data Analysis, John W. Tukey, 1962\nScience and Statistics, George, E.P. Box, Journal of the American Statistical Association, 1976, 71:3\nThink like a Data Scientist, Brian Godsey, Manning Publications, 2017\nData Science 101: Life Cycle of a Data Science Project, Abraham Musa, Medium, 2021\nSolving the Last Mile Problem for Data Science Project Success, Bill Waid, Forbes, 2019\nThe Ultimate Guide to Deploying Machine Learning Models, Luigi Patruno, 2020\nMeet Airbnb’s official party pooper, who reduced partying by 5% in two years, CNBC, Sept. 19, 2023\nLectures and conferences on mathematical statistics and probability. Jerzy Neyman, 1952",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "references.html#from-the-data-part",
    "href": "references.html#from-the-data-part",
    "title": "References",
    "section": "From the data part",
    "text": "From the data part\nJunk Charts Trifecta Checkup: The Definite Guide, by Kaiser Fung.\nFundamentals of Data Visualization, by Claus O. Wilke, O’Reilly Media, 2019.\nThe science behind data visualization, by Graham Odds on Creative Blog, August 8, 2013\nChoosing a good chart, by Andrew Abela, blog, The Extreme Presentation Method, Sept. 15, 2020.\nThe Grammar of Graphics, by Leland Wilkinson, Springer Verlag, 2005\nDesigning Against Bias in Machine Learning and AI by David Corliss, AMSTAT News, September 2023\nASA Ethical Guidelines for Statistical Practice, American Statistical Association\nEthics and Data Science by Mike Loukides, Hilary Mason, and DJ Patil, O’Reilly Media, 2018.\nA Guide for Ethical Data Science, Royal Statistical Society and Institute and Faculty of Actuaries\nBig data ethics and 10 controversial data science experiments, by Sabrina Dominquez, Data Science Dojo, May 2018\nStrava’s heatmap was a ‘clear risk’ to security, UK military warned, by Matt Burgess, Wired Magazine, April 2018\nThe Robodebt tragedy, by Trewin, D., Fisher, N., and Cressie, N. Significance, December 2023\nRobodebt not only broke the laws of the land—it also broke the laws of mathematics, by Noel Cressie, The Conversation, March 16, 2023\nUniversity lecturer slams ‘sexist’ Google Translate as gender neutral languages are translated into English, Daily Mail.com, March 24, 2021\nRegression to the mean: what it is and how to deal with it, by Barnett, A.G., van der Pols, J.C., and Dobson, A.J., International Journal of Epidemiology, 34(1), 2005.\nAn Introduction to Statistical Learning, 2nd ed., by James, G., Witten, D., Hastie, T., and Tibshirani, R., Springer Verlag, 2023\nLectures and conferences on mathematical statistics and probability. Neyman, Jerzy, Washington, Graduate School, U.S. Dept. of Agriculture, 1952\nThe Book of Why, Judea Pearl and Dana MacKenzie, Basic Books, New York, 2018\nCausality, 2nd ed., Judea Pearl, Cambridge University Press, 2009\nMatrices with Applications in Statistics, 2nd ed., Franklin A. Graybill, Wadsworth International Group, Belmont, CA., 1983\nTheory and Application of the Linear Model, Franklin A. Graybill, Duxbury Press, North Scituate, Massachusetts, 1976\nAdjustments of an inverse matrix corresponding to changes in the elements of a given column or a given row of the original matrix. Sherman, J., and Morrison, W.J. Ann. Math. Stat., 20, 621, 1949\nInverting modified matrices. Woodbury, M., Memorandum No. 42, Statistical Research Group, Princeton University, 1950.\nMathematical Statistics and Data Analysis, 2nd ed. John A. Rice, Duxbury Press, Belmont, CA, 1995",
    "crumbs": [
      "References"
    ]
  }
]