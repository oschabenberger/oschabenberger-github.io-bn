[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Data Science",
    "section": "",
    "text": "Preface\nSince data science appeared on the scene some decades ago it has been subject to constant change. In the beginning many were not quite sure what to make of data science. How was it a scientific discipline and how was it different from statistics or data analytics? Is the scientific method involved? Is the data scientist not another name for the business analyst?\nThe dust has settled and we have a much better handle on what data scientists do, the skills they need, and how the role differentiates from other data professionals. On the other hand the data profession in general has evolved and keeps expanding. At first, a data scientist was the all-round unicorn that was expert in engineering data, modeling data, writing code, and implementing data solutions. Those individuals were difficult to find and difficult to train.\nIn smaller organizations the workload of the data scientist continues to be fluid, simply because the number of folks who can perform data-related tasks is limited. In larger organizations with dedicated data teams you can now find data engineers, data analysts, business analysts, machine learning engineers, AI engineers, statistical programmers, and so on. Where does a data scientist fit into this picture and what are the skills a data scientist should have to be and to remain competitive in this landscape?\nMy personal journey to data science led from forestry to forest biometry to statistics to analytical software development. Like me, many will come to data science from a non-statistical and non-computational path. As educators we need to think about how to ensure the foundations of the discipline are being taught. Assuming that everyone who strives for a career in data science comes to the discipline with a solid background in statistics, applied mathematics, and computer science is not helpful. We have to expect varied backgrounds and strengths. Data science, like no other discipline I encountered over three decades working with data, combines technical and non-technical skills, combines hard and “soft” (human) skills, and has a focus on solving real-world problems.\nIt is OK to come to data science with strengths and weaknesses in different aspects of statistics, mathematics, computer science, and domain knowledge. Those are the foundation disciplines. Your foundation lies in a genuine interest in these disciplines, the curiosity and drive to fill in the gaps, a growth mindset, and a desire for lifelong learning. If you hate math and are not willing to learn the linear algebra concepts necessary to manipulate data science models—forget it. If you have not programmed before and think that you can pass that off to some software engineer—forget it. If you are afraid of public speaking and are not willing to work on improving communication skills—forget it.\n\nOn the other hand …\nIf you are excited about learning principles of data engineering and statistical learning, about communicating data concepts to colleagues, customers and executives, if you are aching to write better software, and love managing complex projects, then welcome! Data science has been waiting for you.\n\nThe origin for this material lies in recognizing how data science as a discipline has evolved and continues to change. The foundational disciplines remain the same, but the demands on data scientists today are much more varied and complex than some courses and certifications would lead you to believe.\nIn my own practical experience I have witnessed the transition from creating data/analytic teams in organizations—because that is what everyone else was doing—to measuring the success and justification of those teams in terms of value created for the organizations. Many data science projects are faltering, study after study reveals that the majority of data science projects do not succeed. There are many reasons for this sobering reality, how we educate and train for data science careers definitely plays a part.\nWe refer generically to organizations that practice data science. That can be an academic department at a university, a consulting company, a non-profit, a local, state, or federal government agency, a university IT department, and so on. In other words, almost every type of organization today engages with data and practices some form of data science. Despite this diversity of organizations practicing data science, the vast majority of jobs are found in commercial settings, sometimes lumped together as industry. Data science solves real-world problems, the problems companies are interested in solving are those that support the business model. Data for Good, data science in pursuit of making a better world, is a noble goal. A discretionary goal for most companies that places behind the primary goal of growing the company value and satisfying share holders. The reality of most jobs is Data for Money.\n\nUnderstanding data science begins with recognizing data science as a team sport. It is not about slinging code and developing a cool model. As someone said,\n\na data science model in a Jupyter notebook has $0.0 value.\n\nUnderstanding and applying all the classical and modern methods of data analysis is table stakes. But your random forest is not going to be any better than your colleague’s random forest. You are probably using the same software and are tuning the same hyperparameters. Everyone uses TensorFlow or CNTK or PyTorch to implement deep learning. You can implement LeNet-5 or AlexNet for image classification with a few lines of R of Python code—if you are not sure how, check Stack Exchange or ask a large language model for help.\nWhat are the differentiating factors in this competitive field?\n\nUnderstanding how to take a business problem, research, or policy question and turn it into a data project, executing the data project as a team and translating it back into a business, research, or policy solution.\nBeing able to communicate with business, information technology, marketing, finance, and legal to define, develop and implement a data-driven solution.\nBeing at times evangelist, programmer, project manager, instructor, presenter, and implementer.\nUnderstanding the ethical, security, and privacy issues associated with data and data-driven decisions.\nCultivating a desire to learn as new tools and methods affect what can be done with data—GPT anyone?\n\n\nLifelong learning is a mindset that has worked in my favor for decades. It is a growth mindset that builds on curiosity to push your boundaries further out. Faced with a formidable task you don’t know how to accomplish you say to yourself\n\nI don’t know how to do this. Not yet!\n\nContrast this with a fixed mindset that views your abilities as limited and confines you to a comfort zone, not pushing against the boundaries. With a fixed mindset, “Not yet!” turns into “Not now” or “Not ever”.\nA growth mindset and a joy of learning serves you well in data science. Any of the technology revolutions of the last decades had a profound effect on the way we collect, store, analyze, and use data: the internet, social media, cloud computing, artificial intelligence. You cannot just sit out any of these trends, hoping that they will blow over. You have to engage, figure out what they are about, what works and what does not, and how they can help your organization be better at working with data. It takes an ongoing commitment to be curious, to be critical, and to learn.\nWe might not know what the next disruption looks like, but we know it is coming. A few years ago, large language models were an interesting novelty, today they have changed our view of what is possible with data. At the same time, principal component analysis (PCA), one of the oldest statistical methods, continues to be one of the most important tools for summarization, visualization, and dimension reduction in statistics and machine learning. This tension between old and new is what makes data science such an exciting field.\nLet’s go.\n\nThis material was written in Quarto, because it handles multiple programming languages in the same document, works in RStudio like RMarkdown on steroids, incorporates \\(\\LaTeX\\) beautifully, and creates great-looking and highly functional documents. To learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 How to Use This Material\nThis material is intended for a regular 3-credit hour course at the graduate level. At Virginia Tech, it is a first-semester core course in the Master of Science in Data Science program.\nIf you are self-studying this material, concentrate on the areas where you have a gap. The chapters do not have to be read sequentially. If the terms in the opening chapters are unfamiliar to you, maybe you need to work through Module VIII Review Topics. If your background is in data engineering, Module III Data Engineering will be very familiar, spend your time elsewhere.\nThe material is organized in modules. Module I discusses the history of data science as a cross-disciplinary activity that draws on mathematics, statistics, and computer science and applies the “mixture” to solve real-world problem in a specific domain.\nThis problem solution is viewed as an end-to-end process that starts with a business, research, or policy question and ends with implementation of a solution that uses data. The data science project life cycle, as we call it, describes the iterative process of working through these projects as a team sport.\nThe modules that follow cover specific steps in the data science project lifecycle more deeply.\nModule II introduces you to the discovery phase of the cycle where you develop business understanding and translate it into data science activities.\nModule III introduces working with data. We discuss various data sources and formats and how to access them. Not everything comes as a CSV file. Then we go on a “first date” with the data using profiling tools that can highlight problems with data quality. Summarizing and visualizing data and analysis results are major activities that follow once the data is properly pre-processed. We would be remiss not covering SQL, the structured querying language that is the backbone of many database management systems. Paraphrasing an employer:\nModule IV Modeling Data takes up much space since this is a key activity of data scientists. After discussing some general concepts such as types of learning, types of models, bias-variance tradeoff, correlation and causation, and others you get an overview of the various models you build and deploy in data science projects.\nThe goal is to introduce these topics and to develop the vernacular of data science and your thinking as a data scientist. Since the material is foundational we cannot cover models and modeling to the extent necessary for a data science degree. This is left to additional material, for example, the Statistical Learning methods course.\nModule V Evaluation & Communication presents fundamental concepts and ideas about communicating as a member of a data science project team. The material is a pre-cursor to in-depth coverage in the Communications course.\nIn Module VI Operationalization you learn what it takes to take a data solution from the playpen to production.\nModule VII Applied Ethics in Data Science dives into an increasingly important topic: working with data in a manner that does not perpetuate stereotypes, avoids harm, and honors privacy concerns. We cover many case studies and examples where things have gone wrong, due to unintended consequences, focus on the wrong performance metrics, bad data, or for other reasons.\nHow algorithms can introduce bias and cause harm is the subject of the following chapter. Separate chapters are dedicated to concerns around personal information and data privacy and to ethical considerations in generative artificial intelligence.\nThe final module of the material, Review Topics contains material that we consider pre-requisites for the Foundation material.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#how-to-use-this-material",
    "href": "intro.html#how-to-use-this-material",
    "title": "1  Introduction",
    "section": "",
    "text": "there is much we do not agree on in this field, but we agree that at some point you need to access data in a database and you need to use version control: learn SQL and learn Git.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "proj/history.html",
    "href": "proj/history.html",
    "title": "2  History and Evolution of Data Science",
    "section": "",
    "text": "2.1 What is Data Science?\nIt is revealing that there is no single agreed-upon definition of data science. The lack of a clear definition is understandable if one considers that,\nWe can begin to develop an understanding of data science by considering the three foundational disciplines: statistics, mathematics, and computer science. What problems associated with data required their combination and could not be solved by the discipline in isolation? Maybe we should put the question slightly differently: What problems are we able to solve at the intersection of these disciplines?\nFirst, data is information collected about the real world. Data science then is concerned with real-world problems and phenomena. We test hypotheses about real-world problems, but we do not solve hypothetical problems or prove theorems. Typical questions addressed by data scientists based on data and the types of analysis invoked to answer the question are shown in Table 2.1.\nStatistics is concerned with describing the world and with drawing conclusions about the world using incomplete information. The information—data—is incomplete for various reasons. We might have drawn a sample from a population rather than observed every entity. We might have assigned treatments at random to subjects in an experiment in order to balance out the influence of variables not controlled in the experiment. We have incomplete information about the input variables that relate to an output variable. We have incomplete information about the true relationship between inputs and outputs.\nComputer Science is concerned with computation, information, and automation. Key to computer science are data structures—efficient ways to store and process information—and algorithms—specifications to solve a specific problem through computation. Because today data is almost always stored in digital form it is now immediately accessible to processing via computation. A remarkable contribution of computer science to data science over the last decades is to broaden its view from methods and algorithms to organize and store data to methods and algorithms that draw conclusions from the data through computation. Computer science has discovered data as a source of learning, not just as a medium of storage and processing.\nThe algorithmic approach to understanding data rather than a probabilistic approach is one of the great contributions of computer science to data science. Another major contribution is software engineering and software development. Data science projects are software projects, they involve the use of software tools and create code written in languages such as Python, SQL, R, Julia, Scala, Java, JavaScript, and others.\nMathematics is concerned with the study of numbers, formulas (algebra), shapes and structures (geometry), and patterns (analysis). Statistics is often considered a form of applied mathematics. Relationships between inputs and outputs in data science are often modeled through continuous functions, their properties are studied through linear algebra, and they are related to data through differentiation, integration, and numerical analysis.\nRather than thinking of data science as a new domain of knowledge, we think of it as a set of skills that are applied to a subject matter domain (=area of expertise). This view is informed less by a scientific discipline around data than the recognition that today all subject matter domains, including the sciences, are using data to answer questions.\nWhich skills are most important? Is it the hard technical skills in statistics, machine learning, software development? Or is it the ability to communicate across organizational functions in a team-based environment? Or is it the ability to understand and analyze real-world problems in a specific domain? A data scientist will eventually need to acquire all those skills, but they are not interchangeable. Subject matter skills can be acquired by working in a particular domain. Working on data science problems in financial services, for example, you will pick up the specifics and idiosyncrasies of credit and debit card transactions, anti-money laundering, electronic payments, and so on.\nDomain-specific knowledge takes the least time to learn to contribute to solving data science problems. Learning the statistical and mathematical foundations and how to develop good software is the more difficult task. When you join an organization as a data scientist, you will be surrounded by people who understand the domain inside and out—it is what they do. You, however, might be the only person who understands how to apply machine learning to forecast data and who knows how to write analytical software that works.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History and Evolution of Data Science</span>"
    ]
  },
  {
    "objectID": "proj/history.html#what-is-data-science",
    "href": "proj/history.html#what-is-data-science",
    "title": "2  History and Evolution of Data Science",
    "section": "",
    "text": "data science is a young discipline, its boundaries—what is in and what is out—are still evolving.\ndata science is a cross-functional discipline that combines domain expertise with foundations in statistics, mathematics, and computer science. Views of data science differ based on how these components are weighted.\ndata science is defined by what data scientists do. That encompasses everything from finding data, handling data, processing data at scale, to applying statistics and machine learning, to writing code and implementing algorithms to interpreting, visualizing, and communicating data and results.\n\n\n\n\n\n\nTable 2.1: The relationship between data analysis types and the questions they address.\n\n\n\n\n\nType of Analysis\nType of Question\n\n\n\n\nDescription\nWhat is and what has been?\n\n\nPrediction\nWhat will be?\n\n\nClassification\nWhat category does this item belong to?\n\n\nHypothesis Testing\nWhat can I say about X?\n\n\nPrescription\nWhat should I do?\n\n\nClustering\nWhich things are similar?\n\n\nAssociation\nWhich things occur together?\n\n\nOptimization\nWhat is the best way to do something?\n\n\nGeneration\nWhat novel content is there?\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Data Science\n\n\nAt the intersection of the foundation disciplines, performing data science means drawing conclusions from data about real-world problems using computation and automation in the presence of uncertainty.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History and Evolution of Data Science</span>"
    ]
  },
  {
    "objectID": "proj/history.html#the-sexiest-job-and-john-w.-tukey",
    "href": "proj/history.html#the-sexiest-job-and-john-w.-tukey",
    "title": "2  History and Evolution of Data Science",
    "section": "2.2 The Sexiest Job and John W. Tukey",
    "text": "2.2 The Sexiest Job and John W. Tukey\nIn 2009, Hal Varian, Chief Economist at Google declared that the sexy job in the next ten years would be statisticians. As a statistician, I agreed of course. In 2012, Thomas Davenport and DJ Patil published “Data Scientist: Sexiest Job of the 21st Century” in Harvard Business Review (Davenport and Patil 2012). These articles were not describing the work of the typical statistician, but a more general approach to extracting information from large data sets and presenting the insights to others. A new kind of profession was emerging in response to a greater abundance of data in the world that did not fit neatly into existing categories like mathematician, statistician, business analyst, or “quant”. Varian said,\n\nThe ability to take data—to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it—that’s going to be a hugely important skill in the next decades…\n\nThe approach seemed fresh in placing the data at the center of the investigation rather than the traditional approach of choosing the problem first and collecting data second to answer the problem. It emphasized learning from data through visualization and communicating about data—data literacy. What can we learn from the data we have, what do the data tell us? Alas, this shift in focus was not new, it was first proposed by a famous statistician, John W. Tukey.\n\n\n\n\n\n\nData and Datum\n\n\n\n\n\nYou might have noticed that data is used in the plural form, “data are…”. This is grammatically correct since data is the plural form of datum. It is common to use data as a singular noun, “data is…”. I might fall into that trap every now and then but prefer to use the plural form. After all, using data as singular would be wasting a perfectly good noun: datum.\n\n\n\nTukey is known to statisticians as the founder of exploratory data analysis (EDA), as compared to confirmatory data analysis. His famous book “Exploratory Data Analysis” (Tukey 1977) established the idea that much can be learned by using the data itself and that data can suggest hypotheses to test. In confirmatory analysis, on the other hand, you start with a hypothesis and then collect data to verify whether the hypothesis might be true.\nMuch of the statistical work leading up to this point had been confirmatory, based on the concept that data are the realization of a data-generating mechanism—usually a random process. Hypotheses are tested by capturing this mechanism in a statistical model, deriving estimates for the parameters of the model. Once the statistical model is accepted as the abstraction of the data-generating mechanism it becomes the lens through which the problem of interest is viewed. The model is applied to test hypotheses and to calculate predictions along with measures of uncertainty.\nIn 1962, John W. Tukey published a pre-cursor to EDA, “The Future of Data Analysis”, in which he laid the foundation of modern data science, he called it data analysis, and argued why this is a scientific discipline (Tukey 1962).\n\nFor a long time I have thought I was a statistician, interested in the inference from the particular to the general. But as I have watched mathematical statistics evolve, I have had cause to wonder and to doubt.\nAll in all, I have come to feel that my central interest is in data analysis, which I take to include, among other things: procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise and more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.\n\nTukey asked that we give up familiar constraints and look for stories and meaning in data sets. Data analysis can precede probability models; for example, one can identify an interesting function of the data and make progress by asking what the function might reasonably be estimating.\nThis was a liberating view that contrasted against the rigor of probability models and the search for optimal estimators in favor of putting the data first, accepting “good enough”, giving advice when there is reasonable evidence for the advice to be sound, and being prepared that in a reasonable fraction of cases that advice will be wrong.\n\nData analysis must progress by approximate answers, at best, since its knowledge of what the problem really is will at best be approximate. Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History and Evolution of Data Science</span>"
    ]
  },
  {
    "objectID": "proj/history.html#data-mining-and-data-science",
    "href": "proj/history.html#data-mining-and-data-science",
    "title": "2  History and Evolution of Data Science",
    "section": "2.3 Data Mining and Data Science",
    "text": "2.3 Data Mining and Data Science\nData mining is the process of searching large data sets for trends and patterns using computers and automation. We can see a direct link to data analysis in the sense of Tukey, working with a data set to extract interesting patterns in data, to discover new information in the data, to classify observations, identify associations, detect outliers or anomalies, and to identify groups of similar observations (clustering).\nWith the dramatic growth of data sets in rows and columns, it became impossible to conduct this type of exploratory discovery (in the sense of Tukey) manually. There was also growing interest in using special tools and algorithms to discover hidden nuggets of information and relationships in data sets that would otherwise go undetected. Data mining relies on computing power and automation to find patterns in data at scale.\nThe overall process of discovering knowledge from data was formalized as Knowledge Discovery in Databases (KDD). Data mining is the part of the KDD process that uses algorithms to discover patterns.\nThis approach is not without problems or critics. The statistical viewpoint likened the approach to “data dredging” or “data fishing”: looking for relationships even if they are meaningless and then forming hypotheses about why the relationships exist. For example, associations and correlations can be spurious, caused by latent or mediating variables. A strong correlation is not evidence of a causal relationship and might lead to the formulation of bad hypotheses or poor decisions.\n\n\nExample: Chocolate Consumption and Nobel Laureates\n\n\nMesserli (2012) published in 2012 in the New England Journal of Medicine an article that relates the chocolate consumption per capita to the number of Nobel prize winners in various countries, a highly significant statistical relationship that explains almost 2/3 of the country-to-country variation in Nobel laureates (Figure 2.1).\n\n\n\n\n\n\n\n\nFigure 2.1: Relationship between chocolate consumption and number of Nobel laureates from Messerli (2012).\n\n\n\n\n\nIf the relationship were causal—which it is not—an increase of 0.4 kg per year per capita would produce one additional Nobel laureate. In the U.S. that amounts to an extra 125 million kg of chocolate per year. If the relationship between chocolate consumption and number of Nobel laureates is not causal, how can we then explain the obvious relationship seen in the figure? Could it be explained by chocolate consumption improving cognition which creates a fertile ground from which Nobel laureates sprout? A look at how the data was collected sheds light and casts doubt on the study: only four years of chocolate consumption were considered on a l imited number of chocolate products and no data prior to 2002 was used. The number of laureates is a cumulative measure that spans a much longer time frame.\nIt appears that the data were organized in such a way as to suggest a relationship between the variables.\n\n\nAnother criticism of the data mining approach is that traditional statistical decisioning based on \\(p\\)-values is not adequate. Many statistical tests are overpowered by very large data sizes, making even small differences statistically significant. The repeated application of statistical tests across hundreds or thousands of variables leads to inflated error rates unless multiplicity adjustments are made.\n\n\nExample: Market Basket Analysis\n\n\nMarket basket analysis uses Association Rule Mining (ARM) to find associations between items that occur in databases. An application is to identify items that are purchased together, for example, customers who buy whole milk might be more likely to also purchase yogurt and cereals as compared to a completely random choice of products.\nThe name market basket analysis stems from this application, but ARM has many other use cases. Association rules can be used in diagnosing medical conditions based on co-occurrence of symptoms, in text analytics to extract meaning of documents based on the co-occurrence of words, in survey analysis to find associations between answers to different questions.\nA rule is represented as a logical \\(\\{A\\}\\Rightarrow\\{B\\}\\), where \\(\\{A\\}\\) is a set of items called the antecedent (head) of the rule and \\(\\{B\\}\\) is a set of items called the consequent (body) of the rule. ARM discovers rules such as \\(\\{\\text{whole milk}\\}\\Rightarrow\\{\\text{yogurt, cereals}\\}\\) and arranges them by measures of rule quality such as support, confidence, and lift.\nThe support of an item set is the frequency with which its items appear, the confidence is a measure of predictability of the rule, and the lift measures how much more likely the item set appears compared to a random allocation of items.\nWhen applied to large databases the number of possible association rules is astronomical. Suppose there are 500 items in a store. There are more than 62 million rules with just two items in the antecedent and a single item in the consequent. If all associations between items are completely random, at a 1% Type-I error rate we would declare over 600,000 associations as “significant”.\n\n\nThis criticism is valid, of course, if the results of data mining are used in a confirmatory fashion. But when one puts data first, not a probability model that might have created the data, then data mining techniques are incredibly useful and necessary to help us learn about data, to formulate hypotheses, and to plot the path of inquiry—rather than to confirm a result. Data mining is part of the data science methodology and not a separate discipline. The Data Science Process Alliance, concerned with project management in data science, uses the cross industry standard process for data mining (CRISP-DM) as the foundation for the data science process—data mining and data science today are intertwined.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History and Evolution of Data Science</span>"
    ]
  },
  {
    "objectID": "proj/history.html#an-odyssey",
    "href": "proj/history.html#an-odyssey",
    "title": "2  History and Evolution of Data Science",
    "section": "2.4 2001: An Odyssey",
    "text": "2.4 2001: An Odyssey\nTwo influential papers appeared in 2001 that paved the way from statistics to data science.\nIn “Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics”, William S. Cleveland makes the case to enlarge the major focus areas in statistics to a field called data science (Cleveland 2001). The new discipline places more emphasis on\n\nMultidisciplinary projects. Collaboration with domain experts is a source of innovation. Data is the engine for invention in data science.\nModel building. Effort should increase toward methods for building models compared to formal, mathematical-statistical inference in models.\nComputational methods and computing with data that includes databases, networks, and analytical software.\nCommunication and pedagogy.\n\nIn “Statistical Modeling: The Two Cultures”, Leo Breiman contrasts statistical (data) modeling and algorithmic modeling (Breiman 2001). The former assumes that the data are generated by a stochastic data model. According to Breiman, 98% of statisticians subscribe to this approach. Algorithmic modeling, on the other hand, makes no assumption about the underlying data model, treats the data mechanism as unknown, and is more common in fields outside of statistics. In Breiman’s words\n\nPerhaps the damaging consequence of the insistence on data models is that statisticians have ruled themselves out of some of the most interesting and challenging statistical problems that have arisen out of the rapidly increasing ability of computers to store and manipulate data. These problems are increasingly present in many fields, both scientific and commercial, and solutions are being found by nonstatisticians.\n\nThe goal of algorithmic models is more predictive accuracy than confirmatory inference and hypothesis testing. The model is supposed to approximate an unknown relationship between inputs and outputs well enough to provide satisfactory accuracy in predicting outputs of previously unseen inputs. Neural networks and decision trees are examples of algorithmic tools that found rapid adoption outside of statistics. Machine learning as it emerged from computer science is a manifestation of algorithmic modeling.\nIn data modeling, theory focuses on the probabilistic properties of the model and of quantities derived from it. In algorithmic modeling, the focus is on the properties of the algorithm itself: starting values, optimization, convergence behavior, parallelization, hyperparameter tuning, and so on.\nBreiman’s article was widely discussed—the invited comments by leading statisticians at the end of the paper give a sample of opinions.\nMeanwhile, computer scientists had realized that data is not just an abstract concept, that data is more than information to be structured, stored, secured, and transmitted. They realized that data had intrinsic value; processing data can derive insight from data. That knowledge filled a void left by statisticians with limited knowledge of computing environments. Computer scientists, on the other hand, had limited knowledge about how to approach the analysis of data. The fields were ripe for a merger.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History and Evolution of Data Science</span>"
    ]
  },
  {
    "objectID": "proj/history.html#the-big-data-era",
    "href": "proj/history.html#the-big-data-era",
    "title": "2  History and Evolution of Data Science",
    "section": "2.5 The Big Data Era",
    "text": "2.5 The Big Data Era\nAn early use of the term Big Data was in a 1997 paper by Michael Cox and David Ellsworth in the Proceedings of the IEEE 8th Conference on Visualization (Cox and Ellsworth 1997). The authors noted that data set sizes have grown bigger to the point that they do not fit in main memory and termed the problem big data.\nDid we really need another term to describe increasing data size? Whether data fits into main memory of a computer depends on the size of the data and the size of the memory. And since the invention of digital computers both have constantly increased. There had to be more to it than just an increase in the volume of data.\nIndeed, in the 2000s a new class of applications and uses of data emerged as several developments in data and computing coalesced:\n\nMore and more data were now captured in digital form, the amount of data generated in digital form increased sharply.\nA growing interest in the analysis of rich data types such as text, documents, audio, and video.\nA continuum of data in motion; from data at rest in the cloud or data center to streaming data.\nLarge-scale data collection enabled through the internet.\nA shorter shelf life of new kinds of data such as behavioral data (online shopping, social media activity, web browsing, behavioral data) compared to more stable demographic data (age, ZIP code).\nThe beginnings of data-driven application where the data defines how the system operates.\nGreater automation of data-processing.\n\nThe Big Data phenomenon was never just about the size of the data alone, or even the “3Vs”, volume, velocity, and variety. Big Data was about doing something different with data than had been done before, using different data types, different model types, different algorithms, and different computing environments. Building a recommendation system at Netflix or Amazon from data on millions of customers and items viewed or bought is an example of this new type of applications.\nBig Data was about reaching the limits of the ability to store, access, and process data. The volume of data can be a limiting factor, of course and call for specialized approaches. Traditional statistical modeling with hypothesis testing might be useful when deciding about 20 possible input variables. What happens when there are 30,000 potential input variables? Computing \\(p\\)-values in a random sample of size 100 makes sense, but when the data set contains 10 million observations the statistical test is so powerful that nearly any hypothesis can be rejected. What should be used instead? How do you engage multiple computers to solve massive analytic problems in parallel?\nIf the definition of Big Data is “whatever does not fit on a single machine” and needs some form of distributed computing, then the frontier is continuously receding. Today (2023), a storage-optimized instance in the Amazon Elastic Compute Cloud (AWS EC2, i4g.16xlarge), features 64 cores (vCPUs) and 512 GB of memory. AWS launched EC2 in 2006 with single-core machines that featured 2 GB of RAM. That is an increase of more than two orders of magnitude. And, in case that is not powerful enough, p5.48xlarge instances feature 192 cores and 2 TB of memory. A lot of Big Data work can be done on a single machine today.\nJordan Tigani, CEO and founder of database company MotherDuck and founding engineer of Google BigQuery has captured this development in the blog “Big Data Is Dead” (Tigani 2023):\n\nThe world in 2023 looks different from when the Big Data alarm bells started going off. The data cataclysm that had been predicted hasn’t come to pass. Data sizes may have gotten marginally larger, but hardware has gotten bigger at an even faster rate. Vendors are still pushing their ability to scale, but practitioners are starting to wonder how any of that relates to their real world problems.\n\nOf course, just because the amount of data being generated is increasing doesn’t mean that it becomes a problem for everyone; data is not distributed equally. Most applications do not need to process massive amounts of data. This has led to a resurgence in data management systems with traditional architectures.\n\nThe amount of data an organization stores is typically much greater than the amount of data being analyzed: because more recent data is more important, because data are being processed for analytics, and so on.\nWhatever we think of the term Big Data, the era has contributed to the rise of data science as a discipline. New data-driven applications with new types of data challenged us to approach data analytics in a new way. Algorithmic approaches that put the problem first (I need to predict customer churn) are winning in this environment over data modeling approaches that first build an abstraction of the data mechanism based on probability principles.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History and Evolution of Data Science</span>"
    ]
  },
  {
    "objectID": "proj/history.html#data-science-today",
    "href": "proj/history.html#data-science-today",
    "title": "2  History and Evolution of Data Science",
    "section": "2.6 Data Science Today",
    "text": "2.6 Data Science Today\nThe people who identify as data scientists often have unusual career paths; they do not necessarily come to data science through one of the foundational disciplines. They turn out to be problem solvers with an acumen for technology, programming, and communication.\nBig Data created an epic wave and data scientists are the people who know how to surf. Thanks to the Big Data wave, tools and techniques to process and analyze large and complex datasets are now standard in data science. Knowledge in data storage, parallel computing, distributed systems, scalable data processing frameworks, and cloud computing is part and parcel of data science.\nTen years after they published “Data Scientist: Sexiest Job of the 21st Century”, Thomas Davenport and DJ Patil revisited in a 2022 Harvard Business Review article whether the assessment still holds (Davenport and Patil 2022). They conclude that over the last 10 years there has been continuity in the developments but also important changes.\nThe job of the data scientist continues to gain importance as the amount of data, analytics, and AI in business, government, and society is bound to only increase. A decade ago, the data scientist was a unicorn who combined rare skills in statistics, machine learning, and coding to wrangle information from large datas ets. Thanks to many graduate and undergraduate programs, online courses and certifications, there is today a more structured approach to acquire data science skills. While these skills continue to range widely, there is also more differentiation of the data scientist role against other professions. In response to specialization and the need to fill a gap in managing data in the data science project lifecycle, AI engineers, ML engineers, and data engineers are on the rise. In the words of Davenport and Patil:\n\nWe expect to see continued differentiation of responsibilities and roles that all once fell under the data scientist category. Companies will need detailed skill classification and certification processes for these diverse jobs, and must ensure that all of the needed roles are present on large-scale data science projects. Professional data scientists themselves will focus on algorithmic innovation, but will also need to be responsible for ensuring that amateurs don’t get in over their heads. Most importantly, data scientists must contribute towards appropriate collection of data, responsible analysis, fully-deployed models, and successful business outcomes.\n\nAmong the important changes in technology in the past decade the authors list\n\nCloud computing and cloud data storage (data lake, data warehouse, lakehouse)\nAuto machine learning and citizen data science\nLarge language models\nModel operations\nEthics of data science\n\nTwenty years after the arrival of Big Data, data science continues to evade a precise definition because it is defined by those who practice it, rather than by a list of activities. Hacker’s art, statistics, programming, visualization, modeling, subject-matter experience, and communication to solve problems with data through computation are key ingredients. Data science will continue to evolve as its ingredients change and with the emergence of new methods and tools. Most recently, since late 2022, large language models based on transformer technology created a sea change in natural language understanding that is believed to disrupt many occupations. Data scientists embrace such disruptions to draw better conclusions from data.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History and Evolution of Data Science</span>"
    ]
  },
  {
    "objectID": "proj/history.html#data-science-and-data-in-science",
    "href": "proj/history.html#data-science-and-data-in-science",
    "title": "2  History and Evolution of Data Science",
    "section": "2.7 Data Science and Data in Science",
    "text": "2.7 Data Science and Data in Science\nThe majority of data science projects and applications take place in non-academic settings, often lumped together under the generic term industry. Most data science jobs are thus found in industry.\nThere is a difference between data science in research settings and commercial settings. The boundaries typically align with academia and industry, but it is not unusual for commercial enterprises to have Research & Development divisions that engage in scientific research in support of the enterprise’s mission and products.\nRather than pitting industry against academia, consider how data science projects differ between research and commercial settings. There is a lot of hype about how organizations become more data driven, replacing decisions made on gut feeling with hard evidence based on data—the data science team is at the heart of that digital transformation, disrupting the old ways of doing business with novel approaches based on machine learning and AI. In most situations, it is just that: hype.\nIn research and commercial settings, the goal is to provide value through using data. How that value is measured is fundamentally different. Research adds value by increasing our body of knowledge. Commercial enterprises are interested in increasing the value of the company.\nMost data science jobs in commercial settings exist not to disrupt the business model but to support an existing business by solving the current data-related problems. The problems tend to be narrow and specific with result metrics that are tied to the objectives of the enterprise. Objectives such as “increase revenue”, “retain customers”, “increase productivity”, “reduce cost” are universal and boring. Because questions are not open ended and the time to find a solution is of the essence, there is a tendency to reuse rather than to develop new approaches and/or algorithms. Solutions have merit if they meet the goals and can be implemented; they must make a difference in the practice of the business.\nIn research settings, on the other hand, questions are more open-ended without a clear solution path. The goal of research is to find answers to new questions. Time is allotted to deep thinking and to uncovering mechanisms. This often involves novel approaches, custom algorithms implemented by the research team in specialized software. Solutions are developed only as necessary to prove the result of the research. They do not have to be implemented or practical. A research team might find a better active ingredient to fight disease and leave the task of productizing a drug to someone else.\nResearch projects have carefully designed protocols about data, what data to use, how to measure attributes, how to sample observations. Experiments are designed to collect data under controlled conditions in order to draw cause-and-effect conclusions.\nIn commercial settings you are more likely to work with the data that is collected as a by-product of the normal business operations. True experimentation with data collected under controlled conditions is rare, sometimes confined to A/B testing of one data product against another. It is thus common in commercial situations to be working with data collected for a different purpose. The data in the enterprise CRM (customer relationship management) system was collected to track interactions with customers, the data in the Salesforce instance tracks revenue opportunities, and ServiceNow records IT assets and tickets. Is that the best data to optimize digital marketing campaigns? Weak correlations, low signal-to-noise ratios, poor data quality are common problems.\nThe goal of this section is not to put a damper on working as a data scientist in commercial enterprises. It is a good idea to ask what role data and software play in the organization. If the core business and the core product is built on data and software, then it is likely that data science contributes directly to the mission and the bottom line of the company—it is essential to the business. In many organizations the core business is not built on data and technology—at least not yet. A bank’s core business is to deposit and lend money. A retailer’s core business is to sell goods. An insurance’s core business is risk assessment and underwriting. Data science supports the core functions through traditional data functions such as reporting, data analysis, forecasting—it is important but not essential. Increasingly, however, these core functions are becoming the focus of data science itself providing a fertile ground for interesting and novel approaches: micro-lending (banks), recommendation systems (retailers), reinforcement learning for underwriting (insurance). At this point data science helps transform a traditional business model as its products become essential to operating the business.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History and Evolution of Data Science</span>"
    ]
  },
  {
    "objectID": "proj/history.html#sec-history-cholera",
    "href": "proj/history.html#sec-history-cholera",
    "title": "2  History and Evolution of Data Science",
    "section": "2.8 A Case Study in Data Science History: The 1854 Cholera Outbreak in Soho, London",
    "text": "2.8 A Case Study in Data Science History: The 1854 Cholera Outbreak in Soho, London\n\nTwo Snows\nIn 1854, a severe outbreak of cholera occurred near Broad Street in Soho, London, killing over 600 people. The outbreak was studied by John Snow, considered one of the founders of modern epidemiology. No, not the Jon Snow you might be thinking of, Lord Commander of the Night Watch (Figure 2.2 (b)), but the John Snow in Figure 2.2 (a).\n\n\n\n\n\n\n\n\n\n\n\n(a) John Snow. Source: Wikipedia\n\n\n\n\n\n\n\n\n\n\n\n(b) Jon Snow. Source: Wikipedia\n\n\n\n\n\n\n\nFigure 2.2: Famous John/Jon Snows\n\n\n\n\n\nVisualization\nFigure 2.3 shows the map drawn by John Snow, recording the number of cholera cases with stacked bars at the location where cholera cases occurred (click on the map to zoom in). Also shown on the map as black circles and annotated as “PUMP” are the public water pumps throughout the city. The high number of cholera cases on Broad Street stands out, and they seem to be clustered near the location of the Broad Street Pump (Snow 1855, 46).\n\n\n\n\n\n\nFigure 2.3: John Snow’s original cholera map from Soho, London in 1854.\n\n\n\nCholera had been a major problem in the city, thousands had died during previous outbreaks. The prevailing theories of the cause of cholera were (i), airborne particles, called miasma that rose from decomposing organic material and (ii), an as of yet unidentified germ. According to the miasma theory, cholera is contracted by breathing bad air. John Snow adhered to the germ theory and believed that it was transmitted through contaminated water.\n\n\nData Validation\nTalking to residents, Snow identified the public water pump on Broad Street to be the source of the outbreak. He failed to identify the germ under the microscope but came to the conclusion based on the pattern in the data and conversations with residents. Investigating on the ground, he found that nearly all deaths were in the vicinity of the Broad Street Pump or by people who had consumed water from the pump (Snow 1855, 47):\n\nIt will be observed that the deaths either very much diminished, or ceased altogether, at every point where it becomes decidedly nearer to send to another pump than to the one in Broad street. It may also be noticed that the deaths are most numerous near to the pump where the water could be more readily obtained.\n\nHe persuaded local authorities to remove the handle from the pump to prevent access to the water. The mortality rate declined after that, but it is believed that the outbreak was already in decline as people had fled the area.\n\n\n\n\n\n\nRemoving the handle\n\n\n\n“Removing the handle” is now a term in epidemiology for the removal of a harmful agent from the environment. When epidemiologists look for simple answers to questions about epidemics, they ask “Where is the handle to this pump?” (Adhikari, DeNero, and Wagner 2022).\n\n\nThere were some data points (outliers?) that did not agree with the hypothesis that proximity to the Broad Street pump resulted in more cholera incidences. At the intersection of Broad Street and New Street was the Lion Brewery; there were no cholera cases at the brewery although it used water from the Broad Street pump. It turns out that the workers there were protected from cholera by virtue of a daily beer allowance. The cholera bacteria is killed in the brewing process making the beer safe to drink. Drinking beer instead of the contaminated water saved the workers from cholera. What appears as an outlier to the model actually reinforces it.\nAdhikari, DeNero, and Wagner (2022) discuss other data points that appeared initially as anomalies and ended up implicating the public pump on Broad Street:\n\nThere were deaths in houses closer to the Rupert Street pump than the Broad Street pump. It was more convenient for those residents to use the Broad Street pump due to the street layout.\nDeaths in houses several blocks away from the Broad Street pump were linked to children who drank from the Broad Street pump on their way to school.\nJohn Snow was initially puzzled by two isolated deaths in the Hampstead area, far from Soho. He learned that the deceased had once lived in the Broad Street area. Because they liked the taste, they had water from the Broad Street pump delivered to Hampstead every day.\n\nThe source of the contamination was ultimately determined to be a leak from a nearby cesspit into the well from which the Broad Street pump drew water. Clothing from a cholera patient had been discarded into the pit and leakage from the pit had contaminated the well.\n\n\nToward Causality\nThe evidence that the contaminated well water at the Broad Street pump caused the high rate of cholera in that neighborhood and among those who consumed the water is strong. Is it conclusive, however? Have we ruled out any other explanation beyond a reasonable doubt?\nThere could be other explanations for the higher cholera incidence rate in the Broad Street neighborhood compared to other areas of London. Maybe the diet is different among the residents of that poorer area. Maybe their occupations expose them to harmful agents at work. Maybe there is something different in the way their houses were constructed.\nWhile we know today that the bacterium Vibrio cholerae causes cholera, that discovery was not made until 1883 and John Snow had failed to identify a “germ” when he studied the Broad Street pump water. The prevailing miasma theory of infection from airborne particles also did not support Snow’s findings. While his data, visualization, and analysis showed a strong association between cholera and proximity to the Broad Street pump, a deeper analysis was necessary to convince his contemporaries.\nTo establish cause and effect and prove that a variable causes an outcome, modern science would design and run an experiment, provided it is ethically and technically possible. In such an experiment one would control for all other factors except the one hypothesized to cause the outcome. One method of controlling these confounding factors is by randomly assigning the conditions of interest to people and to observe what happens. Exposing folks deliberately to contaminated water that could harm or even kill them is not justified. Fortunately, John Snow found a real-life experiment with perfect conditions to establish cause and effect between cholera and water contamination.\nHe studied the cholera incidences among recipients of water from two water supply companies. The Lambeth company used water from the River Thames drawn upriver from sewage discharge and the Southwark & Vauxhall company drew water below the discharge. Snow also established that for all intents and purposes the households receiving water from either company were indistinguishable; in statistical terms they were comparable. The only thing that differentiated the two groups was the water supplier. Snow (1855, 75) wrote\n\nIn many cases a single house has a supply different from that on either side. Each company supplies both rich and poor, both large houses and small; there is no difference in the condition or occupation of the persons receiving the water of the different companies…\n\nAs there is no difference whatever either in the houses or the people receiving the supply of the two Water Companies, or in any of the physical conditions with which they are surrounded, it is obvious that no experiment could have been devised which would more thoroughly test the effect of water supply on the progress of Cholera than this, which circumstances placed ready made before the observer.\n\nTable 2.2 is based on Snow (1855, 80) and covers the period from January 1 to December 12, 1853 leading up to the 1854 outbreak. The cholera death rate on a per 10,000 house basis is almost 14 times higher in households supplied by the Southwark & Vauxhall water company compared to those who received their water from the Lambeth company.\n\n\n\nTable 2.2: Cholera incidences and rates for two water supply companies leading up to the 1854 outbreak.\n\n\n\n\n\n\n\n\n\n\n\nWater Supplier\nNo. of Houses\nCholera Deaths\nDeaths per 10,000 Houses\n\n\n\n\nSouthwark & Vauxhall\n40,046\n286\n71\n\n\nLambeth\n26,107\n14\n5\n\n\n\n\n\n\nIn all of London there were 563 deaths from cholera in the same period. In other words, 50% of the deaths took place among customers of the Southwark & Vauxhall company. Ouch.\nFor the first seven week period of the 1854 outbreak, Snow (1855, 86) recorded the death rates in Table 2.3.\n\n\n\nTable 2.3: Cholera incidences and rates during the first seven weeks of the outbreak. The death rate in the rest of London was reported as 59 in Table IX of Snow (1855), but calculates to 55 deaths per 10,000.\n\n\n\n\n\n\n\n\n\n\n\nWater Supplier\nNo. of Houses\nCholera Deaths\nDeaths per 10,000 Houses\n\n\n\n\nSouthwark & Vauxhall\n40,046\n1,263\n315\n\n\nLambeth\n26,107\n98\n37\n\n\nRest of London\n256,423\n1,422\n\\(55^*\\)\n\n\n\n\n\n\nIn statistical terms, such a difference in the death rates is highly significant, meaning that if there is no difference in the water quality between the suppliers, such a discrepancy would virtually never happen. The only reasonable explanation for the higher death rate, since differences between the groups receiving the water have been ruled out, is the quality (contamination) of the Southwark & Vauxhall water.\nCase closed!\n\n\n\nFigure 2.1: Relationship between chocolate consumption and number of Nobel laureates from Messerli (2012).\nFigure 2.3: John Snow’s original cholera map from Soho, London in 1854.\n\n\n\nAdhikari, Ani, John DeNero, and David Wagner. 2022. Computational and Inferential Thinking: The Foundations of Data Science. 2nd Ed. https://inferentialthinking.com/chapters/intro.html.\n\n\nBreiman, Leo. 2001. “Statistical Modeling: The Two Cultures.” Statistical Science 16 (3): 199–231.\n\n\nCleveland, William S. 2001. “Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics.” International Statistical Review / Revue Internationale de Statistique 69 (1): 21–26. http://www.jstor.org/stable/1403527.\n\n\nCox, M., and D. Ellsworth. 1997. “Application-Controlled Demand Paging for Out-of-Core Visualization.” In Proceedings. Visualization ’97 (Cat. No. 97CB36155), 235–44. https://doi.org/10.1109/VISUAL.1997.663888.\n\n\nDavenport, Thomas H., and D. J. Patil. 2012. “Data Scientist: The Sexiest Job of the 21st Century?” Harvard Business Review. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century.\n\n\n———. 2022. “Is Data Scientist Still the Sexiest Job of the 21st Century?” Harvard Business Review. https://hbr.org/2022/07/is-data-scientist-still-the-sexiest-job-of-the-21st-century.\n\n\nMesserli, F. H. 2012. “Chocolate Consumption, Cognitive Function, and Nobel Laureates.” New England Journal of Medicine 367: 1562–64.\n\n\nSnow, John. 1855. On the Mode of Communication of Cholera, 2nd. Ed. John Churchill, London. https://archive.org/stream/b28985266#page/n3/mode/2up.\n\n\nTigani, Jordan. 2023. “Gig Data Is Dead.” Motherduck blog. https://motherduck.com/blog/big-data-is-dead/).\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67.\n\n\n———. 1977. Exploratory Data Analysis. Pearson.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>History and Evolution of Data Science</span>"
    ]
  },
  {
    "objectID": "proj/lifecycle.html",
    "href": "proj/lifecycle.html",
    "title": "3  The Data Science Project Lifecycle",
    "section": "",
    "text": "3.1 Introduction\nA data scientist finds an interesting set of data, builds a clever analytical model, and changes the world.\nThis sounds great; unfortunately, data science projects do not unfold this way in the real world. At least not most of them; almost none of them. In this section we examine the typical structure of data science projects in non-research settings—that is, a project to solve a specific problem in an organization. Let’s call it the business problem, acknowledging that such problems arise in industry, government, and non-profit organizations alike. The next chapter takes a closer look at data science in research settings.\nData science models play a big part in this, but it is not where we start. Building a model is just one step in the lifecycle of a data science project.\nProjects involve many steps, diverse teams, and projects can fail at any step. The model building phase is not where most unsuccessful projects go off the rails. Industry analyst firm Gartner estimated in 2016 that about 60% of data science projects are failing and admitted two years later that was an underestimate; the real failure rate was closer to 85%. That is a sobering statistic, and many reasons are cited for the failures:\nThe most important reason for failure of data science projects is called the “last-mile problem” of data science: the struggle to deploy the result of the data analysis (models) into processes and applications where they are used by the organization and are serving the intended end user. It is one thing to build from historical data a model that can predict customer churn accurately. If the goal is to provide a customer service representative with a real-time prediction of a customer’s tendency to cancel a service and to recommend an action, e.g., to give a discount, the project is not complete until the model is incorporated into the system through which customers and representatives interact.\nIt is best to recognize the reality that data science projects will likely fail unless they are carefully planned and executed. Successful projects combine technical skills with soft skills such as communication, trust building, transparency, and empathy. Successful projects are a team sport that incorporate collaboration and communication throughout the project cycle with data scientists taking on multiple roles. The data scientist is at times evangelist, planner, presenter, data wrangler, modeler, software developer, engineer, facilitator, implementer, storyteller, auditor.\nIt is often said that data scientists spend 80% of their time wrangling and cleaning data. That is not true, but the narrative reveals that a data scientists’ job entails much more than the technical aspects of building models on prepared data sets. A good portion of the job is to meet people, interact about project requirements, communicate insights, manage projects, etc. In the terminology of people management, data scientists are “T-shaped people” who combine depth of expertise with broad cross-functional competencies (Figure 3.1).\nThis allows data scientists to understand and communicate about many disciplines and systems (the horizontal bar of the generalist) combined with deep expertise in at least one discipline or system, e.g., cloud-based machine learning (the vertical bar of the expert). As a data scientist you work both independently and collaboratively.\nThe associated skill sets are exercised during the key stages in the data science project lifecycle.\nOur methodology calls out seven phases in the project: discovery, data engineering, model planning, model building, communication, deployment, and monitoring (Figure 3.2).\nOther process models for data science exist, for example CRISP-DM, the cross-industry standard process for data mining is the foundation of the data science process model according to the Data Science Process Alliance. CRISP-DM is shown in Figure 3.3.\nThe process models use similar terms and stages, what matters is not the labels but the general flow that results in a data-driven solution to a real-world problem, that the stages are iterative, and that a project does not start with modeling data. To deliver a production data project that is used repeatably by an organization requires many steps before and after the data modeling phase.\nThe modules of this book are organized according to the flows in Figure 3.2 and Figure 3.3. During the discovery phase we develop an understanding of the business problem and whether it can be solved with data technology. The data engineering phase develops our understanding of available and needed data sources and how to prepare the data for the modeling phase. Modeling comprises planning and building of statistical, machine learning, and AI models to address the core of the business problem. The models need to be evaluated against metrics that map back to the business problem. Models that perform well on statistical grounds but do not move the needle on key business metrics are not helpful. This step also involves communicating the data science solutions to stakeholders outside of the immediate team. If the project is accepted at this stage, the model needs to be integrated and deployed and its performance monitored.\nIn the following paragraphs we introduce the phases of the project life cycle a bit further, then dedicate entire modules to the steps.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Science Project Lifecycle</span>"
    ]
  },
  {
    "objectID": "proj/lifecycle.html#introduction",
    "href": "proj/lifecycle.html#introduction",
    "title": "3  The Data Science Project Lifecycle",
    "section": "",
    "text": "Difficulties inherent in integrating data-driven solutions with existing processes and applications\nManagement resistance and internal politics about project ownership, budget, etc.\nLack of skills\nLack of quality data\nSecurity and governance challenges\n\n\n\n\n\n\n\n\n\n\nFigure 3.1: The concept of “T-shaped” competencies, combining deep expertise in one area with breadth in multiple disciplines and systems.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: The stages of the data science project lifecycle.\n\n\n\n\n\n\n\n\n\n\nFigure 3.3: The CRISP-DM model for the data science process.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Science Project Lifecycle</span>"
    ]
  },
  {
    "objectID": "proj/lifecycle.html#discovery-and-business-understanding",
    "href": "proj/lifecycle.html#discovery-and-business-understanding",
    "title": "3  The Data Science Project Lifecycle",
    "section": "3.2 Discovery and Business Understanding",
    "text": "3.2 Discovery and Business Understanding\nDuring the Discovery phase the business problem is identified and defined with enough specificity to facilitate answering the question: “Does the problem translate to data science?” A problem statement such as “We need to increase revenue” is not specific enough. The question “does a modified user interface for the checkout process increase online sales?” is more specific; it provides information about the target (online sales) and how it is related to measurable changes.\nThe question “Can wildlife species be identified with at least 80% accuracy from drone video?” is very specific, it identifies the goal (wildlife identification), the methodology (drone video), and what success looks like (accuracy greater than 80%). Many questions still need to be addressed before this results in a successful data science project. How will the video be taken (geographic location, flight height, type of camera, time of day, etc.)? Will wildlife be identified based on tracks, signs of habitation, animal images? Are there existing models for object detection that can be used directly or enhanced via transfer learning?\nThe data scientist can help to increase specificity of the problem formulation by asking the right questions. Domain experts think and communicate in terms of subject matter. Data scientists can link the problem to data and analytic methods where that is possible. To measure the level of financial risk exposure the Chief Risk Officer of a bank is interested in forecasting VaR, the value at risk in their investment portfolio for the next two weeks. The data scientist can translate this into a simulation study: based on historical data of investment position values states of the market are simulated and the portfolio is evaluated against the simulated future market states. VaR is calculated as the 95th percentile of the distribution of simulated values.\nData science is powerful but not all questions are solved by analyzing data. The best approach might be to build a handcrafted expert system by translating existing knowledge and logic into software (think “TurboTax”) or to develop a piece of hardware. We need to be honest and transparent if data science cannot provide what is asked for. The adage that if you have only a hammer then everything looks like a nail applies here.\nThe results of a data science project need to be relevant and solve a real-world problem. A client once conveyed the following anecdote: he was approached by the data science team that proudly demonstrated a new algorithm to predict a customer’s age based on their online behavior. The executive responded “That’s cool. But we do know their date of birth.”\n\n\n\nThe recommender system no one needs.\n\n\nDuring the Discovery phase we are beginning to deepen our data understanding. What type of data might be needed to solve the problem, and what data sources are available to us? Exploratory data analysis and visualizations inform us about the shape of the data, the distribution of the variables and possible relationships. Data quality issues will surface during this phase.\nIt is important to carefully document what you find and the decisions that are being made. Why are certain data included or excluded? How are variables defined, what metadata is available? What data quality issues have surfaced (outliers, missing values, inconsistent variable definitions, etc.)? This information is important during the next stage when we use data engineering to prepare data for analysis. Just because two tables have a field customer_id does not imply that we can join the tables on that field. Missing values can be replaced by actual or computed values, but such data imputation is not without pitfalls either.\nDuring the Discovery phase it is also not too soon to consider the ethical concerns and implications of the project. If data will be used or collected from individuals, do we have the consent to do so? Are there potential biases that could lead to poor outcomes by perpetuating stereotypes or by withholding resources? As algorithms and models are becoming more complex, less interpretable, and more easily automated, our need for transparency increases. Data scientists play an important role in recognizing and avoiding the introduction or continuation of bias in data-based decision making. We are devoting an entire chapter to the important topic of ethical decisioning with data.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Science Project Lifecycle</span>"
    ]
  },
  {
    "objectID": "proj/lifecycle.html#data-engineering",
    "href": "proj/lifecycle.html#data-engineering",
    "title": "3  The Data Science Project Lifecycle",
    "section": "3.3 Data Engineering",
    "text": "3.3 Data Engineering\nIf the real-world problem translates into a data science activity, we progress to the next stage. In the CRISP-DM methodology, this phase comprises two steps: data understanding and data preparation. We combine these into a single stage.\nDuring the Data Engineering stage the translation of the business understanding into a data science activity is made concrete by identifying, collecting, and working with the data that will be used to build the data solution. Are the data available to us? What is their size and quality? Are additional attributes required, if so, where do they come from? Do we need to collect more data, or can we obtain data from a 3rd party source (for free, for fee)?\nHow is raw data converted into usable form? How is data from multiple sources integrated, cleansed, and processed? It is not uncommon these days to have data stored in multiple clouds as well as on-premises systems—how do these data come together for analysis in one place?\nIn data science projects that go beyond reporting and building dashboards, the projects that involve data scientists building models to cluster, predict or classify, it is unusual to work directly with the raw data. Instead, data will be organized, cleansed, and stored to fit the subsequent modeling needs. How will the data be prepared for analysis and where is that data stored? Traditionally, analytic tables are two-dimensional arrays of rows and columns, where columns have simple data types such as integers, doubles, or strings. Increasingly, data can be served to analytics as JSON structures, in Parquet format, or in feature stores designed for machine learning. Data engineers and data scientists need to agree on the “best” format considering efficiency, tooling, cost, etc.\nThe answer and approach to many of these questions depends on what you want to do with the data. We are looking down the road to the modeling phases and it is a great time to emphasize that the data science project cycle is not a single clockwise motion but an iterative process.\nWhat data sources you consider is informed by the types of models you are imagining (Model Planning and Model Building stages). On the other hand, the types of data that can be made available at sufficient quantity and quality will inform the types of models you can plan for.\n\n\n\n\n\n\n\nThe data science project cycle is iterative. You might have to move back to an earlier stage based on information discovered at a later stage.\n\n\nIt is often stated that 80% of the data science project is data engineering and data preparation. That is not true. What is true is that you should spend effort here because not having the right data prepared in the right way can invalidate the subsequent steps and jeopardize the project. It is also true that in smaller organizations you might not find dedicated data engineering teams that could do the heavy lifting in preparing high-quality data for analysis; some of that work then falls to the data scientist. It is also true that many data scientists find themselves spending more time engineering data than they hoped or expected.\nImagine that you find a very strong relationship between patient well-being and a health metric in a long-term study only to find out when you communicate the results that the definition of the health metric was changed half-way through the study.\nImagine that you fail to find evidence for the effects of climate change in a multi-national study and discover later that during merging of data from different countries measurements in imperial and metric units were intermingled and that temperatures in degrees Fahrenheit were not converted to degrees Celsius.\nWhen data are missing, we must ask questions about why the data points are unobserved. Are entire records missing, for example, when a group of the population was not included in the data collection? Were measurements unobservable because of limited precision of the instruments? Did subjects drop out of the study, if so, is the dropout related to the purpose of the study? Is the reason why survey recipients did not answer questions related to the study itself?\nSome typical tasks during the Data Engineering stage involve\n\nData Sources: Final selection of the data sources.\nData Exploration: Profiling and exploring data tells us what we are dealing with, identifies relationships and patterns, and suggests further questions to ask of the data.\nData Quality: The result of this is clean and consistent data with known properties. Modifications to raw data are documented. Any remaining data quality issues are documented.\nData Integration: Merging and joining of data sets, creation of new attributes (feature engineering), re-formatting of data, renaming of columns.\nData Delivery: The result of data preparation is made available in an agreed-upon format, for example, as Parquet files stored in S3 or as tables in a database.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Science Project Lifecycle</span>"
    ]
  },
  {
    "objectID": "proj/lifecycle.html#modeling",
    "href": "proj/lifecycle.html#modeling",
    "title": "3  The Data Science Project Lifecycle",
    "section": "3.4 Modeling",
    "text": "3.4 Modeling\n\nWhat is in a Model?\nThe term model is pervasive in our field, and we come to this conversation with different notions of what constitutes a model. Before going any further in the discussion, we’ll discuss the concept of a model in the context of data science.\n\n\n\n\n\n\n\n\nFigure 3.4: A simple representation of a model that processes inputs with algorithmic logic and produces output.\n\n\n\n\n\nFrom the 30,000 foot view a model is simply a mechanism to process some input and produce a corresponding output (Figure 3.4).\nThe input to drive the model algorithm is almost always some form of data. The algorithm that processes the inputs can be based on data, but that is not necessarily so. Suppose the problem we are trying to solve is to ascertain an individuals annual federal income tax. The problem is solved with a model that takes as input the individuals financial situation. This information is typically known without error as information about income, property taxes, expenses, etc. is well documented. The algorithm processing this input is a translation of the relevant information in the federal income tax laws into machine instructions. The output is the amount of money owed to the government or expected as a refund.\nNow suppose that for some reason the input data in the tax problem is not known without error. For example, income from tips, medical expenses or charitable contributions might be best guesses rather than exact amounts. Income data could be noisy because foreign income is converted at fluctuating exchange rates. If the input data is the realization of stochastic (random) influences, should we modify the algorithm?\nWhen the input data to an algorithm is the result of observing random variation, we are looking to the algorithms of the model to find the signal in the data, to de-noise it. The signal located in the data is then transformed into the model output. Most models we build in data science are of this kind because the data we work with is inherently noisy. The reasons for the random variations are many: selecting observation from a larger population at random, applying treatments to randomly chosen experimental units, variations in measurement instruments and procedures, variations in the environment in which a phenomenon is observed, and so on. The algorithms we use depend on the goals of the analysis, properties of the data, assumptions we are willing to make, attributes we look for in competitive models, and personal preferences.\nMuch of data science methodology is to select the right approach (algorithm) based on input data, learning methodology (supervised, unsupervised, semi-supervised, self-supervised) and analysis goal (prediction, recommendation, classification, clustering, dimension reduction, sequential decisioning), to train the model, and to deploy the model. Figure 3.5 is an attempt at structuring the input, algorithm, and output components of a model in the data science context. The diagram is complex and woefully incomplete and is intended to give you an idea of the diversity of methods and the many ways we can look at things. For example, in discussing input data we could highlight how data are stored, how fast it is moving, the degree to which the data is structured, the data types, and so forth. There are many other categorizations of data one could have listed.\nThe categorization of algorithms—what many consider the models in the narrow sense—leaves out semi-supervised learning, self-supervised learning, transfer learning, and other learning methods. Volumes of books and papers have been written about every item in the list of algorithms and many algorithms are represented by a simple description. Multilayer networks, for example, include artificial neural networks, deep networks such as convolutional and recurrent networks, and transformer architectures such as GPT.\n\n\n\n\n\n\nFigure 3.5: Structuring and categorizing input, algorithm, and output in data science models.\n\n\n\nGeorge E.P. Box is credited with coining the much-used phrase “all models are wrong, but some are useful”. The phrase appears partially (“all models are wrong”) twice in his 1976 paper on Science and Statistics (Box 1976):\n\nSince all models are wrong the scientist cannot obtain a “correct” one by excessive elaboration.\nSince all models are wrong the scientist must be alert to what is importantly wrong.\n\nThe full phrase appears on p. 424 of his book with Norman Draper (Box and Draper 1987).\nThe takeaway is that any model is an abstraction of a phenomenon and we strive to find a useful abstraction. The model does not attempt to reproduce the phenomenon. The tax algorithm converts the essence of the tax code into machine instructions, it is not an electronic copy of the entire law. The purpose is to accurately calculate an entity’s tax, anything else can be stripped away. An algorithm processing noisy data that reproduces the data is uninteresting. The goal is to abstract the data in such a way to allow separating the signal from the noise and to convert the signal into the desired output.\nThe first G.E.P. Box quote instructs us not to overdo it in building models; this translates to the problem of overfitting in data science, crafting a model that follows the training data too closely and as a result does not generalize well to new data points. If the goal is to predict, classify, or cluster the unseen; generalizability of the model is key. A model to forecast stock prices or trading volumes is judged by how well it can predict the future, not by how well it can predict the past. The adequate level of generalization for that model must be wrung from current and past stock prices. Finding the appropriate level of abstraction is resolved by striking the right balance in the bias-and-variance tradeoff.\nJohn von Neumann is said to have remarked\n\nWith four parameters I can fit an elephant and with five I can make him wiggle his trunk.\n\nIf the point of the model is to capture the essence of the elephant, then four parameters would be enough.\nThe second G.E.P. Box quote instructs us that models are abstracting away features of the phenomenon. If these are important features, the model is not useful. In the best case this model does not meet its goal and is revised or abandoned. In the worst case the model leads to bad decisions and harmful outcomes.\nNo matter how complex the model, we need to strive to understand how it works (interpret the model), not just what it does. If a model is not intrinsically interpretable then we need to strive to explain the forces that drive the model, keeping in mind that we are then making statements about the model and not about the underlying phenomenon we have abstracted.\nIt might seem like a daunting task to command the plethora of complexity displayed in Figure 3.5, understand all the pros and cons, grok the idiosyncrasies of software implementations, write code to train the model(s), communicate the results, and possibly implement a solution within a business context.\nThat is what we are here for.\n\n\nModel Planning\nWe have seen that model is a generic term for a system that takes inputs, processes them according to algorithmic logic and produces outputs (Figure 3.4). ChatGPT is a model (a large language model) that takes input in the form of text prompts, passes the prompts through a pre-trained transformer algorithm, and generates text responses. The ChatGPT model contains parameters, unknown quantities for which we need to find values before the model can be used. This is known as training the model. Once trained, the algorithmic logic of the model can be applied—this step is often referred to as model inference in machine learning or scoring in statistics.\n\nTypes of algorithms\nStatistical models and machine learning models are trained on data. The training process shapes the capabilities of the model based on the training data. As a rule, more complex models require larger training data sets. The following table shows the increasing complexity (as measured by the number of parameters) of Open AI’s GPT foundation models. GPT-3.5 has 175 billion parameters and essentially was trained on text data scraped from the internet.\n\n\n\nTable 3.1: Evolution of number of parameters in Open AI’s generative pre-trained transformer foundation models.\n\n\n\n\n\n\n\n\n\n\nVersion of GPT\nReleased\nNumber of Parameters\n\n\n\n\nGPT-1\nJune 2018\n117 million\n\n\nGPT-2\nFebruary 2019\n1.5 billion\n\n\nGPT-3.5\nNovember 2022\n175 billion\n\n\nGPT-4\nMarch 2023\nRumored to consist of 8 sub-models with 220 billion parameters each, 1.7 trillion total\n\n\n\n\n\n\nIn general, models do not have to rely on data to craft their algorithms. Expert systems, also known as hand-crafted knowledge systems, are translating through software the logic applied by experts to make decisions into computer instructions. Tax preparation software is an example of such an expert system. The input to the system is data about your financial situation, the algorithm represents the logic of the tax code translated into machine instructions, the output is your tax liability or refund. You would not want to use a model that is trained on data to predict your tax liability. The training data would be past tax returns, including fraudulent ones and returns that are not at all like yours. The algorithm would not attempt to reproduce the tax code, but to predict the average tax liability of returns similar to those used in training. That might be useful for government planning purposes to estimate budgets but not to ascertain an individual’s precise tax liability.\nThe algorithms in models can also derive from rules based on operating an organization; these are called business rules. They often take the form of “when this, do that” logic. For example,\n\nIf a job completes more than 5 business days after the request date, apply a 10% discount.\nA manager is defined as an employee to whom more than two people report directly.\nCustomers are grouped based on their payment behavior: those who pay monthly balances in full, those who pay in installments, and those who have late payments.\n\nA real-world problem can involve one or more model types, not all of which depend on data science. The solution to the problem can require incorporating, blending (an ensemble), or switching between models.\nDuring the Model Planning stage, we examine alternative approaches to capture the key decision in a data science project. What kind of model can produce a solution: a mathematical model, physical model, statistical model, machine learning model, expert system, etc.? What alternatives are available within the relevant classes? For example, if the goal is to classify a two-level target variable, we might consider logistic regression, classification trees, random forests, gradient boosted trees, support vector machines, regularized regression and so on. What are their pros and cons?\n\n\nPre-built and pre-trained models\nNot all models have to be built from scratch with an organization’s own data sources. A pre-built model is a model that was trained by someone else and is ready to use. Deployment options might be to invoke the model as a cloud service through an API (application programming interface) or to run the model in a container on premises. A sentiment model that classifies the writer’s opinion in a piece of text as positive, neutral, or negative, is an example of a pre-built model. Amazon Web Services (AWS) makes this capability available through the Amazon Comprehend API.\nThe terms pre-trained and pre-built models are often used interchangeably; AWS promotes pre-trained SageMaker models in a marketplace where vendors offer access to what we would call pre-built models. A pre-built model cannot be altered and is ready to use. You rely on someone else to train the model and to package it for use. A pre-built model is pre-trained in the sense that the model builder trained the model on some data. It is important that the training data are representative for your application. A pre-built model for sentiment analysis that was trained on data from a different language will not perform well.\nA pre-trained model is a statistical or machine learning model that was trained on data and the model is made available in that state with the ability to continue training. GPT (Generative Pre-trained Transformer) models are an example of pre-training large language models by Open AI. Each model in the GPT family has been pre-trained to a certain point. For example, text-davinci-003 in the GPT-3.5 family was trained on text data found on the internet through June 2021. These large models are also called foundation models, because they are used to build more specific applications with additional training. Chat-GPT, for example, is a question-answer application built on top of the GPT foundation models.\nYou can accept the pre-trained model as the final model and build applications with it—the pre-trained model has become the pre-built model. Or you can continue training the model with additional data. For example, you can continue to train text-davinci-003 by adding more recent data than June 2021. Or you can add data from sources more relevant to your domain than what can be scraped from the internet. OpenAI calls this process fine tuning of models. You will also see the term transfer learning in this context: transferring the capabilities of a model from one domain to another.\nPre-training is common in computer vision and natural language applications where many capable models already exist, building a model from scratch is difficult because of the massive amounts of data and computing resources required, and transferring capabilities by training on additional data has shown to be effective. Convolutional neural networks such as AlexNet, VGG, or GoogLeNet, ResNet trained on an existing corpus of images serve as the starting point\nDuring Model Planning you should decide which models are trained from scratch, pre-trained and fine-tuned, and pre-built. Beware of the Not Invented Here (NIH) attitude: the tendency to avoid using or buying other products and instead to build everything from scratch. There is strong bias against ideas or capabilities from the outside. If a model does not fall into the core specialty or competency of an organization, then it is best to look outside for pre-built or pre-trained models (if you have additional data to train for your context).\nDuring Model Planning we need to also consider the infrastructure available and/or necessary to train, test, and deploy models. Failure to consider deployment constraints at this stage will come back to haunt you during the Deployment stage. Imagine training a deep learning model with millions of parameters using PyTorch to find out later that it needs to be deployed on a small medical device with limiting processing capabilities.\n\n\n\nModel Building\nThe Model Building stage is where many data scientists and ML engineers prefer to spend their time. Here you train candidate models on data. You evaluate and validate them and determine the optimal values for tuning parameters (bandwidths, number of inputs, learning rate, depth of trees, regularization penalties, …). You use train:test data splits or cross-validation to choose between competing approaches and to balance the bias—variance tradeoff. You use bagging to improve the predictive accuracy of a weak learning technique and bootstrapping to estimate the variability of quantities. It is the fun part of data science; it is where we nerd out.\nMany programs teaching data science focus on the model building stage; it is a rich field that can fill semesters with material. New techniques increase the list of candidates all the time. Fifteen years ago, deep learning models were not a consideration, we lacked affordable compute power and sufficiently large data sets to train deep neural networks well. In the 2000s recurrent neural networks such as long short-term memory models became all the rage in natural language understanding and convolutional neural networks were the go-to architectures for computer vision applications. In the early 2020s, both network types have been mostly replaced by networks based on transformer architectures. How quickly things change.\nWith greater automation (AutoML, for example), tasks in the Model Building stage will be done more efficiently and at greater scale. Effort will shift from building and evaluating the models to applying the models, to building applications with them, and to helping others to reap the benefits of AI. There will always be room for algorithmic innovation in data science. But the goal post moves with advances in technology. Spending all your time building models at the expense of other stages of the project cycle does not reduce the failure rate of data science projects.\nDuring Model Planning and Model Building we consider the tradeoffs between model choices. The model with the highest accuracy or lowest prediction error is not necessarily the best solution. Interpretability of a model can tip the scale toward a less accurate model. A single decision tree, trained and pruned, is easy to visualize and to explain. A random forest of 500 trees cannot be visualized and is more difficult to explain, although it will likely perform better than the single tree. A neural network with thousands (or millions) of parameters is an inscrutable black box and is not intrinsically interpretable. Trying to explain how the network comes up with a decision, which factors drive it, is the best we can hope for. How do you convince stakeholders of the quality of your data-analytical work if they cannot understand it?\n\n\nExample: Concentration of a drug in patient’s body over time\n\n\nThe data in Figure 3.6 show the concentration of a drug in the body of patients over time. The drug is administered at time 0, followed by a period of absorption by the body, and a period of elimination from the body. Three candidate models are fit to the data, represented as different lines: A simple linear regression model (dashed line) A local 2nd degree polynomial smoothing model (LOESS, blue line) A nonlinear, first-order compartmental, model (black line) that expresses concentration as follows: \\[\n    C= \\frac{D k_a k_e}{C(k_a-k_e)}  (\\exp⁡(-tk_e)  -\\exp⁡( tk_a))\n\\] The quantities in the compartmental model represent \\(D\\): the dose of the drug\n\n\\(k_a\\): the rate of absorption by the body\n\\(k_e\\): the rate of elimination from the body\n\\(C\\): the clearance (volume of blood or plasma cleared of the drug per time unit)\n\\(t\\): time since administration of the drug\n\n\n\n\n\n\n\nFigure 3.6: Drug concentration due to absorption and elimination in the body over time and three possible approaches to modeling.\n\n\n\nThe simple linear regression (dashed) model is very simple, it consists of an intercept at time 0 and a slope. It is easy to explain but for data between 0 and 10 units of time the model is inadequate. The drug concentration over the entire observation period cannot be captured by a straight line. However, once the drug concentration is down to 50—60% of the maximum a straight line might be a reasonable model. The LOESS model (blue) is very flexible and follows the data more closely. It picks up some interesting cyclical behavior between time 12 and time 20. Is it a spurious effect of the model/data interaction or does it capture a real biological effect?\nThe compartmental model has a clear biological interpretation, its quantities relate to well-understood processes. That enables us to ask questions such as “at what time has 30% of the maximal absorbed amount cleared the body?”\nSuppose that the LOESS model predicts drug concentration in patients that did not participate in the study (unseen data) more accurately than the nonlinear compartmental model. Would we recommend the LOESS model over the nonlinear model? Does the answer change if testing hypothesis about the absorption/elimination rates is important?\nWhatever your decision you need to be able to communicate it and convince stakeholders of the value of your work.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Science Project Lifecycle</span>"
    ]
  },
  {
    "objectID": "proj/lifecycle.html#evaluation-and-communication",
    "href": "proj/lifecycle.html#evaluation-and-communication",
    "title": "3  The Data Science Project Lifecycle",
    "section": "3.5 Evaluation and Communication",
    "text": "3.5 Evaluation and Communication\nThe Communication stage is an essential part of the project life cycle. The solution we have derived so far does not speak for itself. We made choices and tradeoffs and settled on a particular approach. We learned some new things. Now that we have seen the data engineered to solve the problem through data science what are the data telling us? Were we able to solve the problem? What have we learned?\nSuppose that our data science solution checks all boxes and is the greatest things since sliced bread. It will not see the light of day unless we communicate it to the stakeholders of the problem and it meets the business requirements.\nCommunication can take many forms: memos, emails, reports, dashboards, presentations. The best scenario is to get a chance to present the results in person, connecting the data analysis to the original problem, letting data (not math or code) tell the story. During the Discovery stage we translated the business problem into a data science project. Now we need to translate the data science solution back to the business problem and communicate in the language of the business. The stakeholders need to understand the solution, why it matters, and why they should care. If the model output is not presented in the original problem context, why would anyone outside of the data science team trust it?\nThe communication needs to present a solution and why the team thinks it is viable. This does not need to be done as a live analytics demo, but the audience should be empowered to judge the solution. Decision makers need to be able to decide whether to move forward with implementation, whether to return to an earlier stage in the project cycle, or whether to stop. Model accuracy, test error, or specificity are metrics that matter to the data scientist, the business executive considers KPIs such as bookings, revenue, funnel size and conversion rates.\nDo not overestimate the persuasiveness of a black-box algorithm and do not underestimate how much information a simple bar chart or box plot conveys to a non-data scientist.\nA lot is at stake at this stage.\nBecause communications in data science is a critical step and communication skills are non-negotiable soft skills for today’s data scientists, a separate corpus of material is dedicated to communication in team-based data science.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Science Project Lifecycle</span>"
    ]
  },
  {
    "objectID": "proj/lifecycle.html#operationalization-deployment-monitoring",
    "href": "proj/lifecycle.html#operationalization-deployment-monitoring",
    "title": "3  The Data Science Project Lifecycle",
    "section": "3.6 Operationalization (Deployment & Monitoring)",
    "text": "3.6 Operationalization (Deployment & Monitoring)\nThe Operationalization (Deployment) and Monitoring stages are combined into Model Operations, or ModelOps, the discipline that implements algorithmic logic and monitors it over time. We call out deployment as a stage within ModelOps because so many projects fail here.\n\n\n\n\n\n\n\nModel operations combine deployment, management, and monitoring of analytic assets.\n\n\n\nDeployment\nDeployment can be simple, sometimes a report or a table in a spreadsheet is all that is needed. It can also be highly complex such as implementing a real-time prediction system across an organization in collaboration with the IT department.\n\n\nExample: Predicting Sepsis in Preemies\n\n\nOne of the major causes of death in premature babies (preemies) is sepsis. The fatality rate of early-onset sepsis in babies with low birth weight is 3—40%, up to 4 times higher than in full-term infants. Preemies have weaker immune system; their skin and gut are also more fragile. The presence of tubes for ventilation or IVs into veins increases the risk of infection.\nDoctors and nurses watch for signs of sepsis such as very high or low temperature, changes in skin color and changes in behavior. Those are non-specific signs, and it would be great if we can develop an early-warning system that alerts doctors when the probability that a preemie might develop system exceeds a threshold.\nA statistical model is developed based on data that integrates sensor information from incubators in neonatal intensive care units (temperature, breathing, vitals, …), computer vision (movement, skin color, …), and clinical records. Data from these three sources was captured over a period of six months and formatted by the data engineering team into a large data frame for the data science team.\nThe model developed by the data scientists based on the provided data is a success in that it can predict the probability of sepsis in preemies based on the integrated data with greater accuracy and more quickly than one would be able based on the clinical records alone.\nWhat difficulties might have to be overcome to deploy the model in the NICU of an hospital?\n\nThe hospital has incubators from different manufacturers. Not all incubators are equipped with sensors and cameras.\nThe model was developed based on 6-month historical data. It will be deployed in a real-time system where data from sensors and cameras must be prepared and merged with clinical records on the fly. The data engineering pipeline must be integrated with the operations.\nHow is the sensor and camera data be made available? Is it part of an existing data flow from the NICU? Is it collected over WiFi, Bluetooth, or some other network technology?\nHow does the data get to the model inference engine that calculates the predicted probabilities for sepsis?\nHow do the predicted probabilities of sepsis surface in the workflow of NICU nurses and doctors? A continuous real-time display added to the NICU machinery? Or is it accessed as a separate piece of information on a laptop or mobile device?\nHow do predicted probabilities map to sepsis risk and medical intervention?\n\n\n\nThis section is concerned with the more complex deployment alternatives and the shocking reality that roughly only half of all production-ready models are ever placed in production. Of those that are, many take three months or more to be deployed. If the model has a short shelf life due to seasonal variations it might already be obsolete when it finally sees the light of day.\nWhy is the success rate so low and why does it take so long to finally move into production?\nMany organizations have separate systems for development, testing, and production (dev-test-prod). Differences in architecture, operating systems, network support, language support, security, and access controls can cause delays. When a model is developed in Python or R and needs to run in production on a device that does not support their runtime or containers, then the model must be recoded. Translating a machine learning pipeline from Python or R into a language such as C/C++ or Rust is time-consuming and error prone, requiring testing and verification (confirmation that the model was correctly implemented). If, on the other hand, you are deploying on premises on standard hardware, then you can wrap the Python models in a web-service framework (Flask, Django) and deploy through a Docker container.\nMost algorithms are trained in batch on historical data. Many algorithms are deployed in a real-time system online where they process data points as they arise or periodically in small batches. Technical choices need to be made about how the online system accesses the algorithm:\n\nType of connection and communication\n\none-way stateless webhooks over HTTP\ntwo-way stateful WebSockets over WS\nan asynchronous publish/subscribe (pub/sub) architecture\n\nType of API\n\nRESTful\nSOAP\nRemote procedure calls (RPC, gRPC),\nEvent-driven\nFlask\n\n\nModels can be large, neural networks can have millions or even billions of parameters (see Table 3.1 for the GPT models). While such large models can be accurate, the process of applying the model to a new data point (a process known as scoring or inferencing the model) can be slow. Loading a large model into memory can require too many resources. Model compression techniques such as pruning, distillation, or quantization are then used to reduce the size of the model without sacrificing prediction or classification accuracy. A model modified in any form after its derivation needs to go through validation: confirming that the model still works as intended.\nDuring the operationalization stage the primary responsibility for the data science assets shifts from the data science team to the user-facing teams and the IT department. The model turns into a data product, a technological product that depends on data to function.\nMany organizations have internal processes that examine and approve products (built in-house or by a third party) for\n\nCompliance with regulatory requirements: Payment Card Industry Data Security Standard (PCI DSS), Federal Information Security Management Act (FISMA), General Data Protection Regulation (GDPR) of the EU, Health Insurance Portability and Accountability Act (HIPAA), etc.\nPrivacy\nSecurity\nTransparency\n\nThe output of a data science project is also a product—a data product and has to go through these processes as well; that takes time.\nWhen the data product replaces an existing solution, A/B testing is common to verify that the new solution is indeed superior. Just because a model passes unit tests does not mean it is moving the needle for the business. When data scientists build models, they measure model performance against historical data sets based on statistical metrics such as mean-squared prediction error, mis-classification rate, sensitivity, specificity, area-under-the-curve, etc. The business measures success in terms of OKRs (Objectives and Key Results) and/or KPIs (Key Performance Indicators) s uch as customer conversion rates, subscriptions, click-thru rates, bookings, revenue. In an A/B test users are randomly divided into two cohorts, experiencing the current and the new solution, respectively. The solution performance is evaluated through business metrics and technical criteria (speed, resource requirements, scalability).\n\n\nManagement and Monitoring\nOnce a model is deployed in production it enters the second phase of ModelOps, lifecycle model management and model governance. Governance includes the application of DevOps engineering principles such as continuous integration and continuous deployment (CI/CD), versioning of model, data, and code, and automation. The newly deployed model might be challenged by a new solution in a few months and becomes itself the target of A/B testing.\nNew data collected since the initial model deployment can be added to the training data and the model parameters can be re-estimated (continuous training); this leads to a situation where a more recent version of the model is A/B tested against an existing deployment of the same model.\nContinuous monitoring of the model is necessary to detect model drift. This is the condition where model performance deteriorates over time. Performance is tracked using statistical measures (accuracy, sensitivity, mean-squared error, …) and KPIs. There are two primary causes for drift in analytical models:\n\nConcept drift. The task the model was designed to perform changes over time. This happens when the nature of the target variable of the model changes or when fundamental assumptions of the model no longer hold. When the definition of the target changes, e.g., a new definition of spam email or changes in regulations that broaden the meaning of money laundering, a model derived under the previous definition might now be invalid. The distribution of the target can change over time or seasonally from symmetric to skewed, causing model drift. The COVID pandemic has fundamentally voided common assumptions that flow into many types of models: consumer behavior, liquidity of financial instruments, valuations of commercial real estate, gas and power consumption patterns, etc.\n\nLu et al. (2019) distinguish four types of concept drift depending on how the change over time takes place (Figure 3.7).\n\n\n\n\n\n\n\nFigure 3.7: The four types of concept drift according to Lu et al. (2019)\n\n\n\n\nData drift. The data has drifted when the distributions of the input variables into a model change or when the relationships between the inputs change (Figure 3.8). The data to which the model is applied in production is then not representative of the data on which it was trained. Data drift is a major source of bias in models. The model itself remains correct and is possibly unbiased based on the training data set. Bias is introduced when it is applied to data that is systematically different from the training data.\n\n\n\n\n\n\n\nFigure 3.8: An example of data drift. The training data for input variable X has a symmetric distribution with a mean of 2. The data seen by the model in production has a right-skewed distribution and a mean of 5. The production model will see values of X with different frequencies as during training and will see values of X it never encountered during training. We do not know whether the model applies for X &gt; 10, for example.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Science Project Lifecycle</span>"
    ]
  },
  {
    "objectID": "proj/lifecycle.html#case-study-airbnbs-party-pooper",
    "href": "proj/lifecycle.html#case-study-airbnbs-party-pooper",
    "title": "3  The Data Science Project Lifecycle",
    "section": "3.7 Case Study: Airbnb’s Party Pooper",
    "text": "3.7 Case Study: Airbnb’s Party Pooper\nThis case study is based on a Sept 2023 CNBC article.\nAirbnb operates an online marketplace for short- and long-term stays at private homes. The company acts as a middleman to connect homeowners with renters and charges a commission for the bookings. Some years ago, Airbnb became concerned about party bookings that cause “significant disruption to neighbors and the surrounding community.” The COVID-19 pandemic made things worse as folks were looking for creative ways to get together. The pandemic and Airbnb hosts’ fears of property damage were main drivers behind the company taking a stand against party bookings.\n\nDiscovery and Business Understanding\nNaba Banerjee joined Airbnb’s trust and safety team in 2020 and was charged with finding a solution. The business problem was clear: Prevent party bookings by blocking reservations but avoid blocking legitimate bookings.\nShe approached it in three phases: What can be done right now, within the next year, and in the future. This hints at an iterative process; one that starts with an immediate simpler intervention, followed by more deeply developed solutions. It also hints at the fact that whatever solution is being put into place will need to be monitored for performance and updated. Some Airbnb guests will try to circumvent the solution and get past the party-blocking methodology.\n\n\nData Engineering & Model Planning\nAll solution phases were driven by data. In the short term, for the right-now solution, they studied the patterns and signals of bookings that were reported as parties. The data for this effort was already available on their internal platform, it just needed to be analyzed. There was no modeling effort involved. It was about understanding the data they had on party bookings, profiling and describing the data, looking for signals that correlate highly with the booking label “party”. The data product of this phase was a simple business rule: ban high-risk reservations by users under age 25, in particular if they have received poor reviews from hosts or do not have much history on Airbnb. This measure, combined with removing the option for hosts to list their homes for gatherings of more than 16 people, was effective.\n\n\nModel Building\nBanerjee and her team had developed a classification model at this point. It was classifying booking attempts as high-risk party bookings or benign bookings based on a black-and-white categorization of bookings. The next step, the mid-term solution, was to build a more sophisticated analytic model that takes into account more factors and that produces a risk score, a probability that the booking is a party booking. Whether to block a booking based on the risk score can vary based on risk tolerance—regionally and temporally. For example, a risk score of x% might not lead to a reservation block during most periods, but around Halloween, New Year’s Eve and Fourth of July will deny the same reservation.\nThe model developed at Airbnb takes into account many factors such as the closeness of the reservation to the user’s birthday, the user’s age, length of stay, proximity to where the user lives, day of the week, location of the listing, etc. The type of model selected was a neural network trained using supervised learning on bookings labeled as parties or property damage. The performance of the model was tested against past incidences as well as hypothetical test cases and normal, “good” behavior. Airbnb developed multiple models to predict the likelihood of a party booking, to predict property damage, etc.\n\n\nOperationalization\nThis is a good example of the data—analytics—decision chain of intelligence. Building the predictive risk model was applying advanced analytics to data. The outcome of the data—analytics sequence was a machine learning model that can produce a risk score for any booking. But the machine learning model itself does not block a booking. It produces a risk score that—to complete the analytics—decision loop—must be translated into a decision: make the reservation or block the reservation. In Airbnb’s case the decision rule is nuanced temporally and geographically. The risk of parties is higher in the U.S. and Canada, followed by Australia, Europe, and Asia. It might also take other factors into account besides the likelihood of a party booking, e.g., the extent of property damage. Furthermore, in situations where the decision is not clear, the booking goes through human review to determine the party risk.\nTo implement the model the prediction logic and the decision logic (analytics—decision) had to be incorporated into the online booking system in real time. As soon as a eservation is attempted to be made, the system needs to calculate the risk score, make a decision and inform the user either by continuing the process or blocking it with the dialog (Figure 3.9).\n\n\n\n\n\n\nFigure 3.9: The Airbnb reservation system blocks a suspected party booking. Source.\n\n\n\nThe implementation was initially rolled out in Australia where Airbnb experienced an uptick in party bookings in early 2021 after travel had stopped because of the pandemic. The system was rolled out there as a pilot in October 2021 using A/B testing methods—some regions of Australia used the system while other regions did not. Party bookings dropped by 35% in the regions where the risk model was deployed.\nIn Fall of 2022 the system went live globally. Over Labor Day weekend alone, the system blocked or redirected 5,000 potential party bookings.\n\n\nModel Management and Monitoring\nAirbnb continuously monitors the performance of the model using statistics such as precision and recall for classification models. Recall, also called the sensitivity of the classification model, is the true positive rate of the model: the ratio of the actual party bookings that the model should have predicted as party bookings. Precision, on the other hand, is the ratio of the actual party bookings relative to any bookings that were predicted to be parties—which includes some false positives.\nThe model can drift as user’s behavior changes. Party-inclined users are trying to find ways to circumvent the booking block. The model has to be re-evaluated and cannot be static. A decline in the sensitivity of the model would suggest that the model is not as capable in detecting party bookings.\nAirbnb also points out that the system has been evaluated by the company’s anti-discrimination team for potential bias. You want to make sure that the system does not have unintended consequences such as unfairly disenfranchising bookings by one group over another.\nWhat is the next phase of solution development at Airbnb for this problem? You guessed it, something that has to do with large language models (LLMs) like ChatGPT.\n\n\n\nFigure 3.1: The concept of “T-shaped” competencies, combining deep expertise in one area with breadth in multiple disciplines and systems.\nFigure 3.2: The stages of the data science project lifecycle.\nFigure 3.3: The CRISP-DM model for the data science process.\nThe data science project cycle is iterative. You might have to move back to an earlier stage based on information discovered at a later stage.\nFigure 3.5: Structuring and categorizing input, algorithm, and output in data science models.\nFigure 3.6: Drug concentration due to absorption and elimination in the body over time and three possible approaches to modeling.\nModel operations combine deployment, management, and monitoring of analytic assets.\nFigure 3.7: The four types of concept drift according to Lu et al. (2019)\nFigure 3.8: An example of data drift. The training data for input variable X has a symmetric distribution with a mean of 2. The data seen by the model in production has a right-skewed distribution and a mean of 5. The production model will see values of X with different frequencies as during training and will see values of X it never encountered during training. We do not know whether the model applies for X &gt; 10, for example.\nFigure 3.9: The Airbnb reservation system blocks a suspected party booking. Source.\n\n\n\nBox, George E. P. 1976. “Science and Statistics.” Journal of the American Statistical Association 71 (356): 791–99.\n\n\nBox, George E. P., and Norman R. Draper. 1987. Empirical Model-Building and Response Surfaces. John Wiley & Sons, New York.\n\n\nLu, Jie, Liu Anjin, Dong Dan, Feng Gu, Gama Joao, and Guangquan Zhang. 2019. “Learning Under Concept Drift: A Review.” In IEEE Transactions on Knowledge and Dara Engineering, 31:2346–63. 12.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Science Project Lifecycle</span>"
    ]
  },
  {
    "objectID": "proj/thinkingds.html",
    "href": "proj/thinkingds.html",
    "title": "4  Thinking Like a Data Scientist",
    "section": "",
    "text": "4.1 Computational Thinking\nIn the broad sense, computational thinking (CT) is a problem-solving methodology that develops solutions for complex problems by breaking them down into individual steps. Well, are not most problems solved this way? Actually, yes. We all apply computational thinking methodology every day. As we will see in the example below, cooking a pot of soup involves computational thinking.\nIn the narrow sense, computational thinking is problem solving by expressing the solution in such a way that it can be implemented through a computer—using software and hardware. The term computational in CT derives from the fact that the methodology is based on core principles of computer science. It does not mean that all CT problems are solved through coding. It means to solve problems by thinking like a computer scientists.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Thinking Like a Data Scientist</span>"
    ]
  },
  {
    "objectID": "proj/thinkingds.html#sec-comp-thinking",
    "href": "proj/thinkingds.html#sec-comp-thinking",
    "title": "4  Thinking Like a Data Scientist",
    "section": "",
    "text": "Elements of CT\nWhat is that like? The five elements of computational thinking are\n\nProblem Definition\nThis should go without saying, before attempting to build a solution one should know what problem the solution solves. However, we often get things backwards, building solutions first and then looking for what problems to solve with them. The world of technology is littered with solutions looking for a problem.\n\n\nDecomposition (Factoring)\nThis element of computational thinking asks to break the complex problem into smaller, manageable parts and by doing so helps to focus the solution on the aspects that matter, eliminating extraneous stuff.\nSmaller problems are easier to solve and can be managed independently. A software developer decomposes a task into several functions, for example, one that takes user input, one that sorts data, one that displays results. These functions can be developed separately and are then combined to produce the solution. Sorting can further be decomposed into sub-problems, for example, the choice of data structure (list, tree, etc.), the sort algorithm (heap, quicksort, bubble, …) and so on.\nTo understand how something works we can factor it into its parts and study how the individual parts work by themselves. A better understanding of the whole results when we reassemble the components we now understand. For example, to figure out how a bicycle works, decompose it into the frame, seat, handle bars, chain, pedals, crank, derailleurs, brakes, etc.\n\n\nPattern recognition\nPattern recognition is the process of learning connections and relationships between the parts of the problem. In the bicycle example, once we understand the front and rear derailleurs, we understand how they work together in changing gears. Pattern recognition helps to simplify the problem beyond the decomposition by identifying details that are similar or different.\n\n\n\n\n\n\nCarl Friedrich Gauss\n\n\n\nCarl Friedrich Gauss (1777–1855) was one of the greatest thinkers of his time and widely considered one of the greatest mathematicians and scientists of all time. Many disciplines, from astronomy, geodesy, mathematics, statistics, and physics list Gauss as a foundational and major contributor.\nIn The Prince of Mathematics, Tent (2006) tells the story of an arithmetic assignment at the beginning of Gauss’ third school year in Brunswick, Germany. Carl was ten years old at the time. Herr Büttner, the teacher wanted to keep the kids quiet for a while and asked them to find the sum of the first 100 integers, \\[\n\\sum_{i=1}^{100}i\n\\] The students were to work the answer out on their slates and place them on Herr Büttner’s desk when done. Carl thought about the problem for a minute, wrote one number on his slate and placed it on the teacher’s desk. He was the first to turn in a solution and it took his classmates much longer. The slates were placed on top of the previous solutions as students finished. Many of them got the answer wrong, messing up an addition somewhere along the way. Herr Büttner, going through the slates one by one found one wrong answer after another and expected Gauss’ answer also to be wrong, since the boy had come up with it almost instantly. To his surprise Gauss’ slate showed no work, Carl had written on it just one number, 5,050, the correct answer.\nCarl explained\n\nWell, sir, I thought about it. I realized that those numbers were all in a row, that they were consecutive, so I figured there must be some pattern. So I added the first and the last number: 1 + 100 = 101. Then I added the second and the next to last numbers: 2 + 99 = 101. […] That meant I would find 50 pairs of numbers that always add up to 101, so the whole sum must be 50 x 101 = 5,050.\n\nCarl had recognized a pattern that helped him see the connected parts of the problem: a fixed number of partial sums of the same value.\n\n\n\n\nGeneralization (Abstraction)\nOnce the problem is decomposed and the patterns are recognized, we should be able to see the relevant details of the problem and how we go about solving the problem. This is where we derive the core logic of the solution, the rules. For example, to write a computer program to solve a jigsaw puzzle, you would not want to write code specific to one particular puzzle image. You want code that can solve jigsaw puzzles in general. The specific image someone will use for the jigsaw puzzle is an irrelevant detail.\nA rectangle can be decomposed into a series of squares (Figure 4.1). Calculating the area of a rectangle as width x height is a generalization of the rule to calculate the area of a square as width-squared.\n\n\n\n\n\n\nFigure 4.1: Decomposing a 12 x 8 rectangle into six 4 x 4 squares to generalize computation of the area.\n\n\n\n\n\nAlgorithm design\nThe final element of CT involves another form of thinking, algorithmic thinking. Here we define the solution as a series of steps to be executed. Algorithmic thinking does not mean the solution has to be implemented by a computer, although this is the case in the narrow sense of CT. The point of the algorithm is to arrive at a set of repeatable, step-by-step instructions, whether these are implemented by humans, machines, or a computer. Capturing the solution in an algorithm is a step toward automation.\n\nFigure 4.2 shows an algorithm to produce pumpkin soup, repeatable instructions that lay out the ingredients and how to process them in steps to transform them into soup.\n\n\n\n\n\n\nFigure 4.2: A recipe for pumpkin soup is an algorithm.\n\n\n\nIn data science solutions the steps to be executed are themselves algorithms. During the data preparation step, you might use one algorithm to de-duplicate the data and another algorithm to impute missing values. During the model training step you rely on multiple algorithms to train a model, cross-validate a model, and visualize the output of a model.\n\n\n\nMaking Pumpkin Soup\nLet’s apply the elements of computational thinking to the problem of making pumpkin soup.\n\nDecomposition\nDecomposition is the process of breaking down a complex problem into smaller, more manageable parts. In the case of making pumpkin soup, we can break it down into several steps:\n\nIngredients: Identify the key ingredients required for the soup.\n\nPumpkin\nOnion or Leek\nGarlic\nStock (vegetable or chicken)\nCream (optional)\nSalt, pepper, and spices (e.g., nutmeg, cinnamon)\nOlive oil or butter for sautéing\n\nPreparation: Break down the actions involved in preparing the ingredients.\n\nPeel and chop the pumpkin\nChop the onion and garlic\nPrepare spices and seasoning\n\nCooking: Identify the steps in cooking the soup.\n\nSauté the onion and garlic\nAdd the pumpkin and cook it\nAdd stock and bring to a simmer\nPuree the mixture\nAdd cream and season to taste\n\nFinal Steps: Focus on finishing touches.\n\nGarnish (optional)\nServe and taste for seasoning adjustments\n\n\n\n\nPattern recognition\nWhat are the similar elements or repeating steps in the problem?\n\nCommon cooking steps: Many soups follow a similar structure: sautéing vegetables, adding liquid, simmering, and then blending or pureeing.\nIngredient variations: While the exact ingredients for pumpkin soup may vary (e.g., adding coconut milk instead of cream), the basic framework of the recipe remains the same.\nTiming patterns: There’s a pattern to the cooking times: first sautéing for a few minutes, then simmering the soup for about 20-30 minutes, followed by blending.\n\n\n\nGeneralization\nWe can generalize (abstract) the process of making pumpkin soup into a more general recipe for making any pureed vegetable soup, regardless of the specific ingredients.\n\nEssential components:\n\nA base (onions, garlic, or other aromatics)\nA main vegetable (in this case, pumpkin)\nLiquid (stock, broth, or water)\nSeasoning and optional cream\n\nGeneral process:\n\nSauté aromatics.\nAdd the main vegetable and liquid.\nSimmer until the vegetable is tender.\nBlend until smooth.\nAdjust seasoning and add cream if desired.\n\n\n\n\nAlgorithm design\nHere is a simple algorithm for making pumpkin soup:\n\nPrepare ingredients:\n\nPeel and chop the pumpkin into cubes.\nChop the onion and garlic.\n\nSauté aromatics:\n\nIn a pot, heat oil or butter over medium heat.\nAdd chopped onion and garlic, sauté for 5 minutes until softened.\n\nCook pumpkin:\n\nAdd chopped pumpkin to the pot and sauté for 5 minutes.\nAdd stock to cover the pumpkin (about 4 cups) and bring to a boil.\n\nSimmer:\n\nLower the heat, cover, and let the soup simmer for 20-30 minutes until the pumpkin is tender.\n\nBlend the soup:\n\nUse an immersion blender or transfer the soup to a blender. Puree until smooth.\n\nAdd cream and seasoning:\n\nStir in cream (optional) and season with salt, pepper, and spices to taste (e.g., nutmeg or cinnamon).\n\nServe:\n\nPour into bowls and garnish with optional toppings (e.g., a swirl of cream, roasted seeds, or fresh herbs).\n\n\nFigure 4.2 is a specific implementation of the algorithm.\nBy applying computational thinking, we decomposed the task of making pumpkin soup into smaller steps, recognized patterns in the cooking process, abstracted the general process for making soups, and designed an algorithm to efficiently make pumpkin soup. This method helps streamline the cooking process, ensures nothing is overlooked and provides a clear, repeatable procedure.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Thinking Like a Data Scientist</span>"
    ]
  },
  {
    "objectID": "proj/thinkingds.html#sec-quant-thinking",
    "href": "proj/thinkingds.html#sec-quant-thinking",
    "title": "4  Thinking Like a Data Scientist",
    "section": "4.2 Quantitative Thinking",
    "text": "4.2 Quantitative Thinking\nQuantitative thinking (QT) is a problem-solving technique, like computational thinking. It views the world through measurable events, and approaches problems by analyzing quantitative evidence. At the heart of QT lies quantification, representing things in measurable quantities. The word quantification is rooted in the Latin word quantus, meaning “how much”. The purpose of quantification is to express things in numbers. The result is data in its many forms.\nWhen dealing with inherently measurable attributes such as height or weight, quantification is simple. We need an agreed-upon method of measuring and a system to express the measurement in. The former might be an electronic scale or an analog scale with counterweights for weight measurement, a ruler, yardstick, or laser device for height measurements. It seems obvious to report weights in metric units of milligrams, grams, pounds (500 grams), kilograms, and tons or in U.S. Customary units of ounces, pounds (16 ounces), and tons. As long as we know which measurement units apply, we can all agree on how heavy something is. And we need to keep in mind that the same word can represent different things: a metric pound is 500 grams, a U.S. pound is 453.6 grams. But wait, there is more: Apothecaries’ weights are slightly different, a pound a.p. is 12 ounces. And in some fields, weights are measured entirely differently, diamonds are measured in carats (0.2 grams). In the International System of Units (SI), weight is measured in Newtons, which is gravitational force on a mass, equivalent to kg * m /s2. As long as we know what units are used to report a weight, we can convert it into whatever system we want to use. So we could say that this attribute is easily quantifiable.\nOther attributes are difficult to quantify. How do you measure happiness? Finland has been declared the happiest country on earth for seven years running. This must involve some form of quantification otherwise we could not rank countries and declare one as “best”. How did they come up with that? The purpose of the World Happiness Report is to review the science of measuring well-being and to use survey measures of life satisfaction across 150 countries. Happiness according to the World Happiness Report is a combination of many other measurements. For example, a rating of one’s overall life satisfaction, the ability to own a home, the availability of public transportation, etc. Clearly, there is a subjective element in choosing the measurable attributes that are supposed to allow inference about the difficult to measure attribute happiness. Not all attributes weigh equally in the determination of happiness, the weighing scheme itself is part of the quantification. Norms and values also must play a role. The availability of public transportation affects quality of life differently in rural Arkansas and in downtown London. In short, the process of quantifying a difficult-to-measure attribute should be part of the conversation.\nThis is a common problem in the social sciences: what we are interested in measuring is difficult to quantify and the process of quantification is full of assumptions. Instead of getting at the phenomenon directly, we use other quantities to inform about all or parts of what we are really interested in. These surrogates are known by different names: we call them an indicator, an index (such as the consumer price index), a metric, a score, and so on.\n\n\nExample: Net Promoter Score (NPS)\n\n\nBuilding on the theme of “happiness”, a frequent question asked by companies that sell products or services is “are my customers satisfied and are they loyal?”\nRather than an extensive survey with many questions as in the World Happiness Report, the question of customer satisfaction and loyalty is often distilled into a single metric in the business world, the Net Promoter Score (NPS). NPS is considered by many the gold standard customer experience metric. It rests on a single question: “How likely are you to recommend the companies products or services?”\nThe calculation of NPS is as follows (Figure 4.3):\n\nCustomers answer the question on a scale of 0–10, with 0 being not at all likely to recommend and 10 being extremely likely to recommend.\nBased on their response, customers are categorized as promoters, passives, or detractors. A detractor is someone whose answer was between 0 and 6. A promoter is someone whose answer is 9 or 10. The others, which responded with a 7 or 8 are passives.\nThe NPS is calculated by subtracting the percentage of detractors from the percentage of promoters.\n\nThe NPS ranges from -100 to 100, higher scores imply more promoters. A NPS of 100 is achieved if everyone scores 9 or 10. A score of -100 is achieved if everyone scores 6 or below.\n\n\n\n\n\n\nFigure 4.3: Net promoter score\n\n\n\n\n\n\n\nExercise: Net Promoter Score\n\n\n\nWhat are the assumptions in the NPS calculation?\nCompany Foo improved its NPS from 30 to 40 over the last year. Explain how that can happen.\nWhat does NPS tell you about a company that has many products and/or services?\nWhat impact could cultural differences and societal norms and traditions have on NPS values around the world?\nWhat do you think is a great net promoter score? Does it depend on the industry?\nCompanies are applying NPS in other contexts, not just to measure customer satisfaction. For example, the employee NPS (eNPS) uses NPS methodology and the question “How likely are you to recommend company X as a place of work?” What do you think about that?\nIf you plot NPS by age, what would that look like? In other words, do you expect younger or older consumers to have higher/lower NPS?\nList reasons why NPS is (might be) a troubling indicator. \n\n\n\nThe NPS has many detractors, pun intended. Some describe it as “management snake oil”. Management touts it when the NPS goes up, nobody reports it when it goes down. It continues to be widely used. Forbes reported that in 50 earnings calls of S&P 500 companies NPS was mentioned 150 times in 2018.\n\nIndicators\nAn indicator is a quantitative or qualitative factor or variable that offers a direct, simple, unique and reliable signal or means to measure achievements.\nThe economy is a complex system for the distribution of goods and services. Such a complex system defies quantification by a single number. Instead, we use thousands of indicators to give us insight into a particular aspect of the economy: inflation rates, consumer price indices, unemployment numbers, gross domestic product, economic activity, etc.\n\n\nExercise: Quantifying the Economy\n\n\nFind at least two indicators in each of the following aspects:\n\nInternational trade\nHousing and construction\nConsumer spending\nManufacturing\nClimate\nLabor markets\n\nWhat are the indicators used for–that is, what do they indicate?\n\n\nAn indicator is called leading if it is predictive, informing us about what might happen in the future. The job satisfaction in an employee survey is a leading indicator of employee attrition in the future. Unhappy employees are more likely to quit and to move on to other jobs. A lagging indicator is descriptive, it looks at what has happened in the past. Last month’s resignation rate is a lagging indicator for the human resources department.\n\n\nTypes of Data\nData, the result of quantification, can be classified in a number of ways. The first distinction of quantified variables is by data type.\n\nContinuous: the number of possible values of the variable is not countable. Examples are physical measurements such as weight, height, length, pressure, temperature.\nDiscrete: the number of possible values is countable. Even if the number of possible values is infinite, the variable is still discrete. The number of fish caught per day does not have a theoretical upper limit, although it is highly unlikely that a weekend warrior will catch 1,000 fish. A commercial fishing vessel might.\n\nDiscrete variables are further divided into the following groups:\n\nCount Variables: the values are true counts, obtained by enumeration. There are two types of counts:\n\nCounts per unit: the count relates to a unit of measurement, e.g., the number of fish caught per day, the number of customer complaints per quarter, the number of chocolate chips per cookie, the number of cancer incidences per 100,000.\nProportions (Counts out of a total): the count can be converted to a proportion by dividing it with a maximum value. Examples are the number of heads out of 10 coin tosses, the number of larvae out of 20 succumbing to an insecticide,\n\nCategorical Variables: the values consist of labels, even if numbers are used for labeling.\n\nNominal variables: The labels are unordered, for example the variable “fruit” takes on the values “apple”, “peach”, “tomato” (yes, tomatoes are fruit but do not belong in fruit salad).\nOrdinal variables: the category labels can be arranged in a natural order in a lesser-greater sense. Examples are 1—5 star reviews or ratings of severity (“mild”, “modest”, “severe”).\nBinary variables: take on exactly two values (dead/alive, Yes/No, 1/0, fraud/not fraud, diseased/not diseased)\n\n\n\nCategorical variables are also called qualitative variables. They encode a quality, namely to belong to one of the categories. All other variables are also called quantitative variables. Note that quantifying something through numbers does not imply it is a quantitative variable. Highway exits might have numbers that are simple identifiers not related to distances. The number of stars on a 5-star rating scale indicates the category, not a quantified amount. The numeric values of quantitative variables, on the other hand, can be used to calculate meaningful differences and ratios. 40 kg is twice as much as 20 kg, but a 4-star review is not twice as much as a 2-star review—it is simply higher than a 2-star review.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Thinking Like a Data Scientist</span>"
    ]
  },
  {
    "objectID": "proj/thinkingds.html#dealing-with-uncertainty",
    "href": "proj/thinkingds.html#dealing-with-uncertainty",
    "title": "4  Thinking Like a Data Scientist",
    "section": "4.3 Dealing with Uncertainty",
    "text": "4.3 Dealing with Uncertainty\nIn “Think Like a Data Scientist”, Brian Godsey lists awareness in the face of uncertainty as one of the biggest strengths of a data scientist (Godsey 2017). Once source of uncertainty is the inherent complexity of data science projects that involve many parts of the organization. A data science project is full of problems, the real-world problem that needs to be solved and problems that arise during the projects. Deadlines will be missed. Data will not be as available as expected or of lower quality as expected. Budgets can change and goals are re-prioritized. Models that work well on paper fall apart when they get in contact with reality. The list goes on and on. The data scientist thinks through problems not from the perspective of the data but from the perspective what data can help us to accomplish. What matters is that we solve the business problem, not that we build a neural network.\nAnother source of uncertainty is the inherent variability of the raw material, data. Variability is contagious, it makes everything produced from data also variable. The point of data processing is to separate the signal from the noise and to find the systematic patterns and relationships in the data, the insights that help make decisions.\nData scientists are sometimes compared to software developers. They do share certain traits; both are using tools, languages, and frameworks to build complex systems with software. But analytic code is different from non-analytic code in that it is processing an uncertain input. A JSON parser also processes variability, each JSON document is different from the next. Does it not also deal with uncertain input? If the parser is free of bugs, the result of parsing is known with certainty. For example, we are convinced that the sentence “this book is certainly concerned with uncertainty” has been correctly extracted from the JSON file. Assessing the sentiment of the sentence, however, is a data science task: a sentiment model is applied to the text and returns a set of probabilities indicating how likely the model believes the sentiment of the text is negative, neutral, or positive. Subsequent steps taken in the software are based on interpreting what is probable.\nThere is uncertainty about which method to use. Whether a software developer uses a quicksort or merge sort algorithm to order an array has impact on the performance of the code but not on the result. Whether you choose a decision tree or a support vector machine to classify the data in the array impacts the performance and the result of the code. A chosen value for a tuning parameter, e.g., the learning rate, can produce stable results with one data set and highly volatile results with another.\nFurther uncertainty is introduced through analytic steps that are themselves random. Splitting data into training and test data sets, creating random folds for cross-validation, drawing bootstrap samples to estimate variability or to stabilize analytics through bagging, random starting values in clustering or neural networks, selecting the predictors in random forests, Monte Carlo estimation, are some examples where data analysis involves drawing random numbers. The data scientist needs to ensure that random number sequences that create different numerical results do not affect the quality of the answers. The results are frequently made repeatable by fixing the seed or starting value of the random number generator. While this makes the program flow repeatable, it is yet another quantity that affects the numerical results. It is also a potential source for misuse: “let me see if another seed value produces a smaller prediction error.”\nData are messy and possibly full of errors. It contains missing values. There is uncertainty about how disparate data sources represent a feature (a customer, a region, a temperature) that affects how you integrate the data sources. These sources of uncertainty can be managed through proper data quality and data integration. As a data scientist you need to be aware and respectful of these issues; they can doom a project if not properly addressed. In an organization without a dedicated data engineering team resolving data quality issues might fall on your shoulders. If you are lucky to work with a data engineering team you still need to be mindful of these challenges and able to confirm that they have been addressed or deal with some of them (missing values).\n\n\nExample: Imputation through Principal Component Analysis (PCA)\n\n\n\nA missing value pattern.\n\n\nID\nType\nX1\nX2\nX3\n\n\n\n\n1\nA\n3.71\n5.93\n55\n\n\n2\nB\n.\n4.29\n32\n\n\n3\nA\n0.98\n5.86\n55\n\n\n4\nB\n.\n4.28\n29\n\n\n5\nA\n6.04\n5.94\n48\n\n\n6\nB\n.\n5.25\n18\n\n\n7\nA\n1.52\n4.01\n61\n\n\n8\nB\n.\n5.55\n30\n\n\n\nThe table above shows eight observations on four features: Type, X1, X2, and X3.\nWhatever types A and B represent, we notice that values for X1 are missing whenever Type equals B. A complete-case analysis based on X1 through X3 would eliminate observations with missing values and leave us only with observations for Type A. This makes a comparison between the two types impossible. To facilitate such a comparison, we could limit any analysis to rely on only X1 and X2, or we could impute the missing values for X3.\nSuch an imputation could be based on a matrix completion algorithm that uses principal component analysis (PCA) to iteratively fill in the missing information for X1 based on the observed data for X1, X2, and X3. That sounds awesome. But what if there are systematic differences between the two types? Does it then make sense to fill in values for X1, Type B with information derived from Type A? Could this possibly bias the analysis and be worse than not using X1 in the analysis at all?",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Thinking Like a Data Scientist</span>"
    ]
  },
  {
    "objectID": "proj/thinkingds.html#perspective-about-data",
    "href": "proj/thinkingds.html#perspective-about-data",
    "title": "4  Thinking Like a Data Scientist",
    "section": "4.4 Perspective about Data",
    "text": "4.4 Perspective about Data\nMaintaining proper perspective about data means knowing where the important issues are. This can be a moving target.\n\nThe volume of data can be unproblematic for one analytic method and\na limiting factor if you want to derive prediction intervals by way of bootstrapping.\nA large but highly accurate model is being developed on historical training data in an Internet of Things (IoT) application. Due to the complexity of the model the scoring engine that applies the model in real time to data flowing off sensors cannot keep up.\nA stacked ensemble of classification models improves the ability to segment customers but is not interpretable.\n\nThere are many limiting factors in data-centric applications and many moving parts. The data scientist is in a unique position to have perspective and awareness of the factors related to data. It is unlikely that anyone else will have that 360-degree view about data.\nAs Brian Godsey put it:\n\nAs a data scientist, I have as my goal to make sure that no important aspect of a project goes awry unnoticed. When something goes wrong—and something will—I want to notice it so that I can fix it.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Thinking Like a Data Scientist</span>"
    ]
  },
  {
    "objectID": "proj/thinkingds.html#data-intuition",
    "href": "proj/thinkingds.html#data-intuition",
    "title": "4  Thinking Like a Data Scientist",
    "section": "4.5 Data Intuition",
    "text": "4.5 Data Intuition\nIntuition is the feeling of knowing something instinctively, without conscious reasoning. Developing intuition for data is one of the most valuable skills a data scientist can acquire. It will help to flag things that are surprising, curious, worth another look, or that do not pass the smell test. Developing intuition is helped by strong technical knowledge, it provides the foundation against which our conscious mind can judge surprise. Some intuition for data is innate, and it can be developed with experience and practice. Being curious, asking questions, requesting feedback, working with real data sets, and spending time exploring data go a long way in developing better intuition.\nLet’s test our intuition for data with a few examples.\n\nOutliers in Box and Whisker Plot\nThe following graph shows a box plot constructed from 200 observations on a variable. What does your intuition tell you about the five data points on the right side in of Figure 4.4?\n\n\n\n\n\n\nFigure 4.4: Box plot of 200 observations for a variable.\n\n\n\nAlso called the box-and-whisker plot, this plot places a box that covers the central 50% of the data—called the interquartile range—and extends from the edge of the box whiskers to the observations within \\(\\pm 1.5\\) times the interquartile range. Points that fall outside the whiskers are labeled as outliers. Does knowing the details of box plot construction change your intuition about the data points on the right?\nAfter having seen many box plots you will look at this specimen as an example of a continuous random variable with a right-skewed (long right tail) distribution. Five “outliers” out of 200 is not alarming when the distribution has a long tail.\n\n\nPrediction Error in Test and Training Data\nFigure 4.5 shows graphs of the mean-squared prediction error, a measure of model performance, as a function of model complexity. The complexity is expressed in terms of the model’s ability to capture more wiggly structures (flexibility). The training data is the set of observations used to determine the model parameters. The test data is a separate set of observations used to evaluate how well the fitted (=trained) model generalizes to new data points.\n\n\n\n\n\n\nFigure 4.5: The mean-squared prediction error for a set of training and test data as a function of model complexity.\n\n\n\nSomething is off here. The prediction error on the test data set should not decrease steadily as the model flexibility increases. The performance of the model on the test data will be poor if the model is too flexible or too inflexible. The prediction error on the training data on the other hand will decline with greater model flexibility. Intuition for data suggests that measures of performance are optimized on the training data set and should be lower for the test data set. It is worth checking whether the labels in the legend are reversed.\n\n\nStorks and Babies\nThe next example of data intuition is shown in the following scatterplot based on data in Neyman (1952). Data on the birth rate in 54 counties, calculated as the number of babies born relative to the number of women of child-bearing age, and the density of storks, calculated as the number of storks relative to the same number of women, suggests a trend between the density of storks and the birth rate.\n\n\n\n\n\n\nFigure 4.6: Scatterplot of stork density and birth rate along with a LOESS fit. The variables are calculated by dividing the number of babies and the number of storks in the county by the number of women of child-bearing age in the county (in 10,000).\n\n\n\nOur intuition tells us that something does not seem quite right. The myth of storks bringing babies has been debunked—conclusively. The data seem to tell a different story, however. What does your data intuition tell you?\nThere must be a different reason for the relationship that appears in the plot. Both variables plotted are divided by the same quantity, the number of women of child-bearing age. This number will be larger for larger counties, as will be the number of babies and, if the counties are comparable, the number of storks. In the absence of a relationship between the number of babies and the number of storks, a spurious relationship is introduced by dividing both with the same denominator.\n\n\nCluster Assignment\nClustering is an unsupervised learning technique where the goal is to find groups of observations (or groups of features) that are in some form alike. In statistical terms we try to assign data to groups such that the within-group variability is minimized, and the between-group variability is maximized. In this example of clustering, data on age, income, and a spending score were obtained for 200 shopping mall customers. The spending score is a value assigned by the mall, higher scores indicate a higher propensity of the customer to make purchases at the mall stores.\nA cluster analysis was performed to group similar customers into segments, so that a marketing campaign aimed at increasing mall revenue can target customers efficiently. A few observations are shown in the next table.\n\n\n\nCustomer ID\nAge\nIncome\nSpending Score\n\n\n1\n19\n15\n39\n\n\n2\n20\n16\n6\n\n\n3\n35\n120\n79\n\n\n4\n45\n126\n28\n\n\n\nFigure 4.7 shows a scatter plot of the standardized income and spending score attributes, overlaid with the customer assignment to one of five clusters.\n\n\n\n\n\n\nFigure 4.7: Results of hierarchical clustering with five clusters.\n\n\n\nThere are distinct groups of points with respect to (the standardized) spending and income. One group is near the center of the coordinate system, one group is in the upper-right quadrant, one group is in the lower-right quadrant. If the clustering algorithm worked properly, then these points should not be all assigned to the same cluster. This is an example where we need to take another look at the analysis. It turns out that the customer ID was erroneously included in the analysis. Unless that identifier is meaningful in distinguishing customers, it must be removed from the analysis. The results of the analysis without the customer ID are shown in Figure 4.8, confirming that the clustering algorithm indeed detected groups that have (nearly) distinct value ranges for spending score and income.\n\n\n\n\n\n\nFigure 4.8: Corrected cluster analysis after removing the non-informative customer ID variable from the analysis. The detected clusters map more clearly to groups of points that are distinct with respect to spending and income.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Thinking Like a Data Scientist</span>"
    ]
  },
  {
    "objectID": "proj/thinkingds.html#sec-thinkingds-quant-intuition",
    "href": "proj/thinkingds.html#sec-thinkingds-quant-intuition",
    "title": "4  Thinking Like a Data Scientist",
    "section": "4.6 Quantitative Intuition and Problem Solving",
    "text": "4.6 Quantitative Intuition and Problem Solving\nThe previous subsection presented some examples to test your intuition around data. What about quantitative intuition and general problem solving skills? This subsection presents a few problems and puzzles to flex your analytic and quantitative thinking muscles. Most of these are well-known problems that you might run into during a job interview for a technical/analytical role.\nSolutions are provided in the callout boxes. Try to solve the puzzles before taking a look at the solutions.\n\nThe Prisoner’s Dilemma\nSuppose that Andy and Brie are arrested as members of a criminal gang and held separately by the police. They cannot communicate. There is enough evidence to convict them on a lesser charge, but not on the principal charge. The police offers the following deal:\n\nIf they both remain silent, they will each serve one year in prison.\nIf one testifies against the other, but the other one does not, the one who testified will be set free while the other serves three years in prison.\nIf Andy and Brie both testify against each other, they will each serve two years.\n\nHow should Andy and Brie behave to optimize their positions, that is, look out after their own interest?\nThe result of such a game is typically displayed in a payoff matrix that shows in each cell the payoff for the two players.\n\n\n\nTable 4.1: Expected payoffs in prisoner dilemma\n\n\n\n\n\n\n\n\n\n\n\nBrie remains silent\nBrie testifies\n\n\n\n\nAndy remains silent\n\\((-1, -1)\\)\n\\((-3, 0)\\)\n\n\nAndy testifies\n\\((0, -3)\\)\n\\((-2, -2)\\)\n\n\n\n\n\n\nThe “payoffs” are shown in the matrix as negative numbers, as they represent a penalty, years of imprisonment. The goal is to maximize the payoff, a number as large as possible.\nThe best situation for Andy is to testify when Brie remains silent. He would go free in this case (and does not mind Brie spending three years behind bars). Similarly, the best situation for Brie is to testify when Andy remains silent. These are the two diagonal cells in Table 4.1.\nThe situation does not play out as well for them if one testifies and the other also testifies. What is the best strategy?\n\nNash equilibrium\nThe Nash equilibrium is a concept in game theory. It applies to non-cooperative games where players compete against each other. In the equilibrium state, no player can gain an advantage by changing their strategy. This assumes that the other player’s strategies do not change.\nSuppose players Andy and Brie have chosen strategies A and B, respectively. In the Nash equilibrium, there is no other strategy available to Andy that would increase his expected payoff if Brie stays with strategy B. Similarly, there is no other strategy available to Brie that would increase her expected payoff from the game if Andy stays with strategy A.\nThe Nash equilibrium tells us not to consider player’s action in isolation. Instead, we need to take into account what other players are expected to do in evaluating a player’s choices.\n\nThe best outcome for either Andy and Brie would be to go free. But they do not know how the other one will behave. So what is the best strategy to play this game? Let’s rephrase testifying and remaining silent in terms of defecting and collaborating players of a game.\n\n\n\nTable 4.2: Expected payoffs in prisoner dilemma\n\n\n\n\n\n\nBrie collaborates\nBrie defects\n\n\n\n\nAndy collaborates\n\\((-1, -1)\\)\n\\((-3, 0)\\)\n\n\nAndy defects\n\\((0, -3)\\)\n\\((-2, -2)\\)\n\n\n\n\n\n\nIf Andy defects, his penalty will be less, regardless of whether Brie is collaborative or not (0 or 2 years compared to 1 or 3 years). The same applies to Brie, if she defects her penalty will be less regardless of what Andy does. The Nash equilibrium is that both players defect although they suffer worse penalties than if they had both cooperated.\n\n\n\nGuarding Criminals\nSuppose you are guarding \\(n\\) criminals in an open field. You have one gun with a single bullet. You are a good shot and being fired at means death—the criminals know that. Their behavior is governed by the following rules:\n\nIf any of them has a non-zero probability of surviving, they will attempt to escape.\nIf a criminal is certain of death, they will not attempt to escape.\n\nHow do you guard the criminals and stop them from escaping?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nImagine there is only a single criminal, \\(n=1\\). Since he/she would definitely be shot at during an escape, they would face certain death and not escape.\nWhat happens if there are two criminals? If they both try to escape, there is a 50:50 chance to survive, hence they will both try to escape. To prevent that from happening you would tell one of the two (you do not need to tell both!) that you would shoot them, should they both attempt to escape. That criminal now faces certain death and will not escape. That brings you back to the situation with a single criminal.\nHow does this generalize to larger groups of criminals? Assign a number from 1 to \\(n\\) to the criminals and tell them that should any subgroup of them try to escape, the one with the highest number in the group will be shot. Or you can tell all of them that the first criminal who tries to escape will be shot. Nobody can afford to be first because it would mean certain death. Since noone will be first, noone can be second and have a non-zero probability of surviving.\n\n\n\n\n\nThree Jars\nThree opaque jars are sitting on a table. The jars are labeled “Apples”, “Oranges”, and “Apples & Oranges”. Unfortunately, all three are labeled incorrectly.\n\n\n\nThree opaque jars.\n\n\nYour task is to assign the labels correctly to the jars. What is the smallest number of fruit you have to choose in order to correctly label the three jars?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou choose one fruit from the jar that is labeled incorrectly as “Apples & Oranges”. If you pull an apple, you know this is the jar with the apples, otherwise it is the jar with the oranges. Now you have two jars left whose labels just need to be flipped since you were told that all three jars are labeled incorrectly.\n\n\n\n\n\nThe Two Door Problem\nThis puzzle has many variations:\n\nYou are walking in the desert and come to a fork in the road. Choose the wrong road and you will get lost in the desert. Choose the right road and you will reach the destination.\nYou have to choose between two doors, one leads to a good outcome, the other leads to a bad outcome you want to avoid at all cost.\nYou are trapped in a room with two doors, only one leads to freedom.\n\nThe trick is that each door or fork in the road is guarded by a separate guard. One of the guards is a liar who never tells the truth, the other guard always tells the truth. And, you get to ask only one question. What question do you ask to reveal the right door (assuming you want to get to a good outcome)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou should ask the following question of either of the guards:\n\nIf I would ask the other guard which road leads to my destination, what would they say?\n\nA variation for other setups would be “If I would ask the other guard which door leads to freedom, …”\nThen, when you hear the answer, choose the opposite road or door.\nWhy does this work? Regardless of which guard you ask you get the wrong answer, pointing out the fork in the road or the door you want to avoid.\nIf you ask the guard who never lies, he knows that the liar would point you to the wrong road. Since he speaks the truth, he will tell you that the other guard will lead you to the wrong road.\nIf you ask the guard who always lies, he knows the other guard would point you to the correct road. But since he always lies, he will tell you that the other guard will point you at the wrong road.\nEither way, the answer will be the road or door to avoid.\n\n\n\n\n\nPattern Recognition #1\nFigure 4.9 shows a logic reasoning puzzle. The first row makes sense if the strange operator is addition, but that does not work for the next rows. You have to find the meaning of that operator, then apply the pattern to solve the last equation.\n\n\n\n\n\n\nFigure 4.9: Can you solve this?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe need to find a pattern that expresses the operations in terms of familiar algebra. If the operator in Figure 4.9 is interpreted as multiplication then we get 4, 10, 18, all smaller than the values on the right hand side. How much smaller? Exactly by the left-most number. The pattern that seems to apply to the first three rows is\n\nmultiply the two numbers\nthen add the number on the left\n\nApplying this pattern to the last row yields 96 as the solution (Figure 4.10).\n\n\n\n\n\n\nFigure 4.10: A solution.\n\n\n\nThis, by the way, is not the only solution. There are other patterns that will lead to a different result for the last row. Those patterns are equally valid. Can you find another pattern that yields a solution?\n\n\n\n\n\nPattern Recognition #2\nHere is a sequence of numbers.\n\\[\n\\begin{array}{c}\n1 \\\\\n11 \\\\\n21 \\\\\n1211 \\\\\n111221 \\\\\n312211 \\\\\n??\n\\end{array}\n\\]\nWhat is the next number in the sequence?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWhat is the pattern in the sequence of numbers?\n\nThe first row is the number 1, it is also “one one”.\nThe second row is the number 11, it is also “two ones”.\nThe third row is the number 21, it is also “one two and one one”.\n\nThe pattern is that the numbers for the following row are obtained by spelling out the numbers in the current row, then replacing the words with the numbers they represent. For example, take 1211 in the fourth row. Spelling it out gives “one one one two two ones”. Now replace the words with numbers: “111221”.\nThe missing entry at the end of the sequence is thus \\[\n13112221\n\\]\n\n\n\n\n\nPattern Recognition #3\nThis could be called a math puzzle but it is just as much—maybe more—a pattern recognition puzzle. Consider the 4 statements:\n\\[\n\\begin{align*}\n5^a &= 6\\\\\n6^b &= 7\\\\\n7^c &= 8\\\\\n8^d &= 625\n\\end{align*}\n\\] Given these statements, can you find \\[\n\\sqrt{abcd} = ??\n\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTrying to solve this by brute force is going to be tricky,\n\\(a \\log(5) = \\log(6)\\), \\(b \\log(6) = \\log(7)\\), and so forth.\nBut by simply substituting the left hand side of the previous equation into the next equation we get: \\[\n6^b = (5^a)^b = 5^{a\\times b}=7\n\\] Continuing along those lines we arrive at \\[\n8^d = 5^{a \\times b \\times c \\times d} = 625\n\\] Now we know \\(abcd = 4\\) and \\(\\sqrt{abcd} = 2\\).\n\n\n\n\n\nPattern Recognitiion #4\nFigure 4.11 shows a matrix with 9 cells and eight of the cell values. Can you determine the relationship between the cells and their values to determine the value in the missing cell?\n\n\n\n\n\n\nFigure 4.11: Rectangle with numbers. What is the number in the missing cell?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStart by looking at the numbers in rows and columns, focusing on complete rows and columns. In the first colunm the numbers 7, 49, 46 do not stand in a discernible relationship. It is easier for the numbers in the first row, going left to right: \\[\n7 + 9 = 16 \\qquad 16 + 8 = 24\n\\] In the third column we see a similar pattern \\[\n24 + 7 = 31 \\qquad 31 + 6 = 37\n\\]\nIn the last row, going left to right, the pattern is \\[\n46 - 4 = 42 \\qquad 42 - 5 = 37\n\\] If the left-to-right pattern uses subtraction, the right-to-left pattern in the row uses addition: \\[\n37 + 5 = 42 \\qquad 42 + 4 = 46\n\\] Figure 4.12 shows the numbers added to the previous cell when you navigate the matrix in a clock-wise pattern. The missing value is thus 51.\n\n\n\n\n\n\nFigure 4.12: Walking the cells in clockwise direction.\n\n\n\n\n\n\n\n\nBirthday Problem\nThis is a classical problem in probability, and a popular one because it is relatable yet somewhat counterintuitive. The probability is higher than what most people expect. It goes like this:\n\nWhat is the probability that in a group of \\(n\\) randomly chosen people, at least two share the same birthday?\n\n“Birthday” is meant as one of 365 days of the year, not adjusting for leap years. Also, we are not taking the birth year into account. A birthday for the purpose of this problem is April 10, or August 15, etc.\nThe standard version of the problem uses \\(n=23\\), because you can imagine yourself in a group of that size—a classroom, for example—and the probability of at least two shared birthdays is also relatable.\nHow likely do you think at least two people share a birthday in a group of 23?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe probability of at least two shared birthdays in a group of 23 is about 0.5; it is 0.05073, to be more exact. How do you interpret that? If you were to assemble groups of 23 randomly chosen people, than half of those groups would have at least two shared birthdays. Pretty high, eh?\nWhat happens to the probability of a shared birthday when the groups get larger? How about in a group of 35 people? The probability of a shared birthday increases to 0.814. In a group of 50 people, the probability is 0.97. In a group of 100, it is virtually certain that there are at least two identical birthdays, \\(p=0.999999\\). With only 10 people in a group, it would be surprising to have identical birthdays, but it is not a rare event, \\(p=0.117\\).\n\nFor those interested, how do you calculate those probabilities? First, whenever you see the expression “at least” in a probability statement, it is probably easier to calculate the probability of the complement event and subtract that from 1. \\[\n\\Pr(\\text{at least two identical birthdays}) = 1 - \\Pr(\\text{no matching birthdays})\n\\]\nWhat is the probability that no birthdays match in a group of \\(n\\)? You can compute this by considering the possible choices as people enter the group. The birthday of the first person can be chosen from 365 days, but the birthday for the second person has only 364 choices, otherwise we would have a match. Since the members of the group are chosen at random, the birthdays are independent and the probability of no matches is the product \\[\n\\Pr(\\text{no matches}) = \\frac{365}{365} \\times \\frac{364}{365} \\times \\cdots \\times \\frac{365-n+1}{365}\n\\]\nYou can write this in terms of factorials as \\[\n\\Pr(\\text{no matches}) = \\frac{1}{365^n} \\frac{365!}{(365-n)!}\n\\] Finally, the probability of at least two shared birthdays is \\[\n\\Pr(\\text{at least two shared birthdays}) = 1 - \\frac{1}{365^n} \\frac{365!}{(365-n)!}\n\\]\nIf you were to compute this, you’d run into problems because the factorials are larger than what a finite precision computer can represent. The following R function uses two tricks to compute the birthday probability efficiently:\n\nCompute the probability on the logarithmic scale, then exponentiate at the end\nUse the fact that for an integer \\(k\\), \\(k!\\) is \\(\\Gamma(k+1)\\), where \\(\\Gamma()\\) is the Gamma function.\n\nThe lgamma function in R computes the log of the Gamma function, and that gives us access to an efficient way to compute the components of the probability on the log scale.\n\nbirthday_prob &lt;- function(n) {\n   log_p &lt;- lgamma(365+1) - lgamma(365-n+1) - n*log(365)\n   return (1-exp(log_p))\n}\n\nbirthday_prob(10)\n\n[1] 0.1169482\n\nbirthday_prob(23)\n\n[1] 0.5072972\n\nbirthday_prob(35)\n\n[1] 0.8143832\n\nbirthday_prob(50)\n\n[1] 0.9703736\n\nbirthday_prob(100)\n\n[1] 0.9999997\n\n\n\n\n\n\n\nMinimum Cuts\nImagine that you hire a consultant to work for you for five days. At the end of each day you need to pay them 1/5th of a gold bar. You have a single gold bar (worth 5 fifths) and need to cut it up so you can pay the consultant at the end of each day.\n\n\n\n\n\n\nFigure 4.13: A gold bar that needs to be cut up.\n\n\n\n\nWhat is the minimum number of cuts that allow you to pay the consultant every day?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou need only two cuts to cut the gold bar into three pieces of sizes 1/5, 1/5, and 3/5.\n\n\n\n\n\n\nFigure 4.14: No more than two cuts are needed.\n\n\n\nThen you pay the consultant as follows:\n\nDay 1: give them a 1/5 gold bar\nDay 2: give them the second 1/5 gold bar\nDay 3: take back the two 1/5 bars and hand them the 3/5 bar\nDay 4: give them a 1/5 gold bar\nDay 5: give them the second 1/5 gold bar\n\n\n\n\n\n\nWhen to Choose the Ticket\nAn airline has a single seat open on a flight, but \\(n=100\\) standby passengers hoping to get on the flight. You are one of the passengers on standby. To be fair to all standby passengers, the airline decides to drop 100 equal-sized pieces of paper into a bucket. 99 of them are blank, one says “Last Seat”. The papers are folded and shuffled in the bucket.\nThe standby passengers queue and each passenger gets to pick one piece of paper without replacement—that is, they keep the slip and do not return it to the bucket. Also they cannot unfold and look at the slip until all of them re drawn. After the last slip is drawn the standby passengers announce who is the lucky person that drew the “Last Seat” by checking their slip.\nHere is the question: if you have your choice to pick first, second, last, or at any particular position in the queue, which position would you choose?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt does not matter when you draw the paper if the pieces were properly shuffled. This is a completely random sample even if the sampling is done sequentially. Your chance of drawing the “Last Seat” slip is 1/100, whether you draw first, last, or at any other position in the queue.\nNote that this would be different if passengers would announce the result of their draws before the next draw. The conditional probability of choosing the “Last Seat” slip on the next draw increases with every bank slip that preceded.\n\n\n\n\n\nHow Many Squares on a Chessboard\nA chess board is made up of eight rows and columns of black and white positions (Figure 4.15). How many squares are on a board?\n\n\n\n\n\n\nFigure 4.15: Chess board.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe quick answer is \\(8 \\times 8 = 64\\) squares. However, that is only part of the story. The entire board is a single square as well, made up of the 64 individual squares. And we could place all kinds of \\(2 \\times 2\\) squares inside the larger frame.\nIf you think about it for a bit there are \\(8^2\\) squares of size \\(1 \\times 1\\), \\(7^2\\) squares of size \\(2 \\times 2\\), \\(6^2\\) squares of size \\(3 \\times 3\\) and so on. The total number of squares on a chess board is\n\\[\n8^2 + 7^2 + 6^2 + 5^2 + 4^2 + 3^2 + 2^2 + 1^2 = 204\n\\]\n\n\n\n\n\nBook Sorting\nSuppose you are working in a library and are sorting books from a box that contains 32 fiction (F) and 17 non-fiction (NF) books. A steady supply of new books is available to add to the box. Your sorting algorithm goes as follows:\n\nYou randomly choose 2 books from the box.\nBased on the types of books chosen you add another book from the supply to the box:\n\nif you choose two fiction books (F,F) you add a new fiction book to the box\nif you choose two non-fiction books (NF, NF) you also add a fiction book to the box\nif you choose one fiction and one non-fiction book (F,NF or NF,F) then you add a non-fiction book to the box.\n\n\nThe entire procedure is depicted in Figure 4.16.\n\n\n\n\n\n\nFigure 4.16: Book sorting routine. Source\n\n\n\nSince you add only one book to the box for every two books you remove, the box will eventually be empty. What is the type of the last book in the box? Is it a fiction or a non-fiction book?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe number of books in the bin goes down by one with each cycle: two books are removed from the bin, one book is added. How does this affect the number of fiction and non-fiction books that remain?\nLet’s see how the number of non-fiction books in the bin changes in cases 1.–3. In the first case, there is no change. In the second case, the number of non-fiction books goes down by 2. In the third case, the number of non-fiction books also does not change: one is removed, one is added.\nSince the number of NF books initially is an odd number, 17, we can conclude that after each cycle the number of NF books remains an odd number. It can never be an even number. Which leads to the conclusion that if there is only one book left in the bin it must be a non-fiction book.\n\n\n\n\n\nInverted Triangle\nFigure 4.17 shows a triangle made from 10 coins. Can you change this into an upside-down triangle by moving only 3 coins?\n\n\n\n\n\n\nFigure 4.17: Inverting the coin triangle.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe solution is shown in Figure 4.18. First, focus on the seven coins in the center of the triangle. The original and the inverted triangle share these; they do not need to move at all. We can focus on the three coins at the edges.\n\n\n\n\n\n\nFigure 4.18: Moving the three coins.\n\n\n\n\n\n\n\n\nThe Spare Tire\nYour car has four tires mounted to the wheels and a spare tire (S). That gives you five tires to work with. Each of the tires lasts at most 30,000 miles. If you can exchange tires among the five as many times as you wish, what is the furthest distance you can travel before you need to purchase a new tire?\nFigure 4.19 depicts the initial tire life prior to driving the first mile. All tires, including the spare (S) have the same life expectancy of 30,000 miles.\n\n\n\n\n\n\nFigure 4.19: Tire life before driving the first mile.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe maximum total distance the five tires could travel before they are all worn out is 30,000 x 5 = 150,000 miles. The minimum distance of travel before you have to buy at least one new tire is 30,000 miles; it is achieved if you do not use the spare tire and run down the four tires currently mounted.\nBy optimizing how you use the spare tire, there must be an achievable distance between 30,000 and 150,000 miles. The optimal strategy is to wear all tires equally and to use the spare tire as much as possible. But we cannot use the spare for more than 30,000 miles, same as with the other four tires.\nIf the four tires on the car are equally worn, we can go at most 150,000/4 = 37,500 miles. The strategy is to get 30,000 miles from each of the tires on the car and 4 times 7,500 = 30,000 miles from the spare tire. In other words, the spare will have to give each of the four tires a 7,500 mile break.\nFigure 4.20 shows how the spare tire is rotated for another tire after each leg of 7,500 miles. The right rear tire comes off after the fourth leg, it is worn out. The other tires still have 7,500 miles of life to go.\n\n\n\n\n\n\nFigure 4.20: Remaining tire life after 7,500, 15,000, 22,50, and 30,000 miles., miles\n\n\n\n\n\n\n\n\nRobot Triangle\nThere are many versions of this basic puzzle, using ants, camels, and other animals. We use robots here, the puzzle goes like this. Three robots are placed at the corners of a triangle. A robot can choose to move along either side of the triangle that meet at its corner (Figure 4.21). What is the probability that any two robots will collide?\n\n\n\n\n\n\nFigure 4.21: Robot triangle.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nEach robot has two possible movements, so there are a total of 2 x 2 x 2 = 8 possible moves on the triangle. There are two ways in which there won’t be any collisions, if all choose to go clockwise or counter-clockwise. In those cases they will follow each other around the triangle (Figure 4.22).\n\n\n\n\n\n\nFigure 4.22: Robots moving without running into each other.\n\n\n\nAny other choice of movements will result in at least one collision. So the probability of any collision if the robots choose their movements at random is 6/8 = 3/4. There is a 75% chance that any two robots will collide.\n\n\n\n\n\nTruth Telling\nThis puzzle is about logic reasoning and not about probability. Surprisingly, it is related to the previous robot movement puzzle.\nConsider the following three statements:\n\nGavin says that Brian is a liar.\nBrian says that Jenn is a liar.\nJenn says that both Gavin and Brian are liars.\n\nWho is telling the truth and who is lying?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis puzzle is related to the robot movement in that there are \\(2^3 = 8\\) possible choices, each of the three characters could either be truthful or lying. It is different from the robot movement in that it is not a question of probability. While robots choose one of the two directions at random, our characters are either lying or telling the truth. We have to reason which one it is.\nWith 8 possible choices you can go about it by finding combinations that are inconsistent, a process of elimination.\nSuppose that Jenn tells the truth. Then Gavin and Brian are liars. According to Gavin’s statement, that would mean Brian is telling the truth. But Brian’s statement contradicts the assumption that Jenn tells the truth. Jenn must be a liar.\nIf Jenn is not telling the truth, there are three possibilities:\n\nGavin is truthful and Brian is not\nGavin is a liar and Brian is truthful\nBoth are truthful.\n\nLet’s look at the first option. If Gavin tells the truth than Brian is lying, which means Jenn would be truthful. We already ruled out this possibility. But if Gavin is not truthful, then 3. cannot be the case either.\nWe are down to the second option: Brian speaks the truth and the other two are liars. Let’s see if everything makes sense in this scenario: If Gavin does not speak the truth, then Brian is not a liar. Brian’s statement that Jenn is a liar is consistent with what we already found.\nConclusion: Only Brian is truthful.\n\n\n\n\n\nClock Made With Matches\nYou have two wooden sticks and a box of matches. When a sticks is lit it will burn completely in exactly one hour. How do you use these ingredients to measure exactly 45 minutes?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLight the first stick on one end and light the second stick on both ends. Since an entire stick burns in one hour, the stick lit on both ends will burn down in 30 minutes (Figure 4.23).\n\n\n\n\n\n\nFigure 4.23: Initial lighting of sticks.\n\n\n\nAt that point light the first stick on the other end. This will double the speed with which that stick, now reduced to 30 minutes burn time, will burn.\nWhen the first stick is completely burned down, 45 minutes will have passed (Figure 4.24).\n\n\n\n\n\n\nFigure 4.24: After 30 minutes, light the other end of the first stick.\n\n\n\n\n\n\n\n\nTwo Stacks of Cards\nYou have two stacks of cards. The first is a regular 52-card deck. The second stack contains two regular 52-card decks, thus has 104 cards. Both stacks are shuffled well. You choose two cards in sequence and you win if they are both red. Would you prefer to choose from the 52-card stack or the 104-card stack?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou want to choose from the larger stack. The probability to draw two red cards in sequence from a stack of \\(n\\) cards (with \\(n/2\\) red ones) is \\[\n\\frac{n/2}{n} \\times \\frac{n/2-1}{n-1}\n\\] For the first draw the probabilities are identical: \\(26/52\\) and \\(52/104\\). But for the second draw the probabilities are \\[\n\\frac{51}{103}=0.495 &gt; \\frac{25}{51}=0.49\n\\]\nThere is a slightly higher chance to draw two red cards from the larger stack.\n\n\n\n\n\nRapid Fire\nCowboy Billy carries a Colt single action 6 shooter revolver. When he fires all 6 shots in a row, the time between the first bullet and the last is 60 seconds. How long would it take him to fire 3 shots?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt will take him 24 seconds to fire three shots. Wait, what?\nThe relevant pattern here is about the time elapsed between shots. If the shots are fired at regular intervals, then Billy will take 12 seconds between the six shots. 12 seconds after the first shot he fires the second bullet, 12 seconds after that he fires the third bullet.\nAnother way of thinking about this is the distance at which fence posts are placed. In a fence with six posts, the first one is at 0/5th total distance, the second post is located 1/5th of the total distance, and so on.\n\n\n\n\n\nCrossing the River\nA farmer is on his way back from the market, with him he has a fox, a chicken and some grain. To get home he needs to cross a river using a small boat that can accommodate only him and one of the other items. Unfortunately, if the fox is left alone with the chicken it will eat it. If the chicken is left alone with the grain, it will eat it. How can the farmer cross the river and bring home the fox, the chicken, and the grain?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis will take several trips across the river:\n\nHe takes the chicken across the river.\nHe returns in an empty boat and picks up the fox.\nHe takes the fox across the river and picks up the chicken.\nHe returns with the chicken in the boat and deposits it while picking up the grain.\nHe takes the grain across the river. Now he has the chicken on the near side of the river and the fox and the grain on the far side.\nHe returns in an empty boat and picks up the chicken.\nHe takes the chicken across the river, now all three items have crossed.\n\nThe trick is to take one item—here, the chicken—back and forth to make sure it is not alone with the item it would destroy.\n\n\n\n\n\nTen Coins\nYou have ten coins in front of you, but you cannot see whether heads or tails are up on any of them. You are being told that five of them show heads, five of them show tails (Figure 4.25).\n\n\n\n\n\n\nFigure 4.25: Setup of the coin puzzle\n\n\n\nWithout looking at the face of the coins, your task is to divide the coins into two groups, each contains the same number of heads. The number of heads in each group does not have to be five. For example, you can end up with two groups, each of which contains 2 heads. Or two groups, each of which contains 3 heads. You cannot look at the coins but you can flip any coin as many times as you like.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nMake two groups of five coins each. Whatever the number of heads or tails within the two groups, you know that originally five of the coins showed head. Now take the coins in one of the groups and turn them all over (Figure 4.26). Voilà.\n\n\n\n\n\n\nFigure 4.26: Solution of the coin puzzle\n\n\n\n\n\n\n\n\nRussian Roulette\nYou are engaged in a game of Russian Roulette with a six shooter. One bullet is placed in a chamber, the chamber is spun, and your opponent pulls the trigger. The gun does not fire. Now it is your turn and before you pull the trigger you are given the choice to spin the chamber one more time or leave the gun as is.\nHow do you decide?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou should spin the chamber again, because it will give a 1/6 chance to fire the bullet. If you leave the gun as is, because the gun did not previously fire, you have a 1/5 chance to fire the bullet.\nBy spinning the chamber you have a higher likelihood to live after the shot.\n\n\n\n\n\nRussian Roulette Again\nThis is the same setup as the previous problem. But now there are two bullets in consecutive chambers of the gun. The first shot does not go off and it is your turn to pull the trigger. Should you spin the chamber first?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf the chamber is spun, the probability to fire a bullet is 2/6 = 1/3. Since the previous shot did not go off, you know that the chamber is in one of four possible positions (Figure 4.27). One of those positions will be followed by a bullet in the chamber—the probability of a shot going off is 1/4.\nBecause 1/4 is less than 1/3 you do not want to spin the chamber again.\n\n\n\n\n\n\nFigure 4.27: Chamber configurations after the first trigger pull. Black dot marks the hammer and the arrow the chamber rotation.\n\n\n\n\n\n\n\n\nCat and Mouse\nIf five cats catch five mice in five minutes, how long does it take one cat to catch a mouse?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFive minutes.\n\n\n\n\n\nGlass (Barrel) Half Full or Half Empty?\nTwo merchants quarrel about how much wine is left in a barrel that has no lid. The first merchant peers into the barrel and says “It is more than half full.” The second merchant lookls into the barrel and says “Nonsense, it is more than half empty.”\nHow do you determine which merchant is correct?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTip the barrel on its edge until the liquid comes up to the edge of the barrel. If you can see the bottom of the barrel, it is more than half empty. If you cannot see the bottom of the barrel, it is more than half full.\n\n\n\n\n\nWhat is the Word?\nA teacher writes six words on a board:\n\ncat \\(\\quad\\) dog \\(\\quad\\) has \\(\\quad\\) max \\(\\quad\\) dim \\(\\quad\\) tag\n\nThe teacher chooses one of the words and writes each letter of the word on a separate piece of paper. Three students, Tobias, Julian, and Amelie receive one of the papers each. These students are known to do well on their logic exams. The teacher asks\n“Tobias, do you know the word?”\nTobias replies immediately, “Yes, I do.”\nThe teacher then asks\n“Julian, do you know the word?”\nHe thinks for a moment and replies, “Yes, I do.”\nThe teacher then asks\n“Amelie, do you know the word?”\nShe thinks for a moment and replies, “Yes, I do.”\nWhich of the six words did the teacher choose?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe word is dog.\nSince Tobias knew immediately what the word was he must have received one of the unique letters that identifies a word:\n\nc o h s x i\n\nThis eliminates tag as a possibility. Having heard Tobias’ answer, Julian now looks at the remaining unique letters that are left among the words\n\ncat \\(\\qquad\\) dog \\(\\qquad\\) has \\(\\qquad\\) max \\(\\qquad\\) dim\n\nThis list is\n\nt g h s\n\nbecause “a”, “d”, and “m” appear more than once and the other letters were eliminated because Tobias knew immediately what the word was. Notice that “h” and “s” are still in the list because if Tobias had received “s”, “h” would still be a possible letter and vice versa.\nThis eliminates max and dim from consideration. Based on the remaining unique letters and the piece of paper he received Julian can now figure out the word.\nAmelie narrows it down the same way. Among the remaining words,\n\nc a t \\(\\qquad\\) d o g \\(\\qquad\\) has\n\nthere are only two letters left to consider, “a”, and “d”. The letter “a” appears in multiple words so it cannot be it; the only unique letter left is “d”. The other letters are either not unique or were in the initial list based on which Tobias knew immediately what the word was and in the list eliminated by Julian.\nTobias had received the letter “o”, Julian had received the letter “g”, and Amelie had received the letter “d”.\n\n\n\n\n\nDucks in a Row\nThere are two ducks in front of a duck, two ducks behind a duck, and a duck in the middle. How many ducks are there?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere are three ducks (Figure 4.28).\n\n\n\n\n\n\nFigure 4.28: Three ducks in a row\n\n\n\nTwo ducks are in front of the last duck. Two ducks are behind the first duck. There is one duck in the middle.\n\n\n\n\n\n\nFigure 4.2: A recipe for pumpkin soup is an algorithm.\nFigure 4.4: Box plot of 200 observations for a variable.\nFigure 4.5: The mean-squared prediction error for a set of training and test data as a function of model complexity.\nFigure 4.6: Scatterplot of stork density and birth rate along with a LOESS fit. The variables are calculated by dividing the number of babies and the number of storks in the county by the number of women of child-bearing age in the county (in 10,000).\nFigure 4.7: Results of hierarchical clustering with five clusters.\nFigure 4.8: Corrected cluster analysis after removing the non-informative customer ID variable from the analysis. The detected clusters map more clearly to groups of points that are distinct with respect to spending and income.\nFigure 4.20: Remaining tire life after 7,500, 15,000, 22,50, and 30,000 miles., miles\n\n\n\nGodsey, Brian. 2017. Think Like a Data Scientist. Manning Publications. https://www.oreilly.com/library/view/think-like-a/9781633430273/.\n\n\nTent, M. B. W. 2006. The Prince of Mathematics: Carl Friedrich Gauss. CRC Press, Boca Raton, FL.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Thinking Like a Data Scientist</span>"
    ]
  },
  {
    "objectID": "proj/teams.html",
    "href": "proj/teams.html",
    "title": "5  Roles on Data Science Teams",
    "section": "",
    "text": "5.1 Upside Down, How Not to Build a Team\nBefore we discus roles on data science teams and whether centralized or decentralized teams are best for an organization, consider this story how a data science team comes to be, as told by Claire Sullivan in this Data Umbrella video.\nHere are the Cliffs notes:\nThe CEO hears about data as the new oil, the competitors all create data strategies, data science teams, and have a Chief Data Officer. The CEO develops a serious case of FOMO (fear of missing out). Worried about remaining competitive, the CEO directs his people to “hire me some data scientists”. The HR department has no experience in hiring data scientists since the company does not have any yet. They do some background work, maybe engage a recruiting company, and hire a bunch of data scientist.\nThe data scientists start but there is no data culture in the company yet. There are no projects for them to work on, no product managers to guide any data product development. Since these are creative people, the data scientists look around and figure out what projects to work on. However, IT does not give them access to the data because the projects they invented are not tied to a business need. IT hands the data science team some Excel spreadsheets to make them go away–sigh. Since these are creative people, the data scientists start pulling some data together, building on their own some sub-optimal data pipeline that should have been built under normal circumstances by a data engineer.\nFinally armed with some data, the data scientists engage in what they do best, building models. Once they have results they are anxious to show them to the mover and shakers in the company but have a hard time getting on people’s calendars; they are lacking a champion. Meanwhile, the CEO is getting impatient and is wondering where the return on investment (ROI) is in his data science initiative. Frustrated by lack of growth prospects at the company, lack of support, lack of mentoring, and lack of meaningful projects, the data scientists start leaving the company.\nThis is not an isolated story. It continues to play out in many companies and it contributes to the high turnover among data scientists. According to a study by online training company 365 Data Science Ltd., analyzing LinkedIn profiles, data scientists stay in their role a mere 1.7 years. Only 2% of the surveyed data scientists had been in their jobs for more than 5 years. Ugh.\nWhat went wrong in the fable told in Clair Sullivan’s video?\nStarting by focusing on hiring people without first figuring out what they will do once they are on the job, not having a clue about what data you have and need and which problems can be solved with it, is the upside-down model of running data initiatives. It will likely fail.\nThe better way to start is to identify a problem (business problem, research question, policy issue) that can be solved by data. A problem that can only be solved by data is the best candidate, but there may be existing solutions and processes for which a data-based approach or a data-assisted approach is viable. It is a common question in organizations: “can we improve on what we already do by using (more of) the data we already have?”\nA problem that can be solved by data does not necessarily require a data science team with data engineers, ML engineers, and architects. Maybe it just calls for replacing a bunch of Excel spreadsheets into a data pipeline that feeds a dashboard. You cannot properly staff a team if you do not know which tasks need to be accomplished. So let’s look at some common roles on data science teams.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Roles on Data Science Teams</span>"
    ]
  },
  {
    "objectID": "proj/teams.html#upside-down-how-not-to-build-a-team",
    "href": "proj/teams.html#upside-down-how-not-to-build-a-team",
    "title": "5  Roles on Data Science Teams",
    "section": "",
    "text": "There was no business problem that could only be solved with data.\nWithout a defined business problem there are no requirements for skill sets to be hired.\nHiring was not done in an effective sequence: you might need data engineers before bringing data scientists on board.\nNo data governance, the IT department did not know how to deal with a request for data from the data science team.\nSiloed data beyond the appropriate data governance structure reveals an absence of data culture.\nThe data scientists created a solution without interfacing it with the business.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Roles on Data Science Teams</span>"
    ]
  },
  {
    "objectID": "proj/teams.html#team-roles",
    "href": "proj/teams.html#team-roles",
    "title": "5  Roles on Data Science Teams",
    "section": "5.2 Team Roles",
    "text": "5.2 Team Roles\n\nIn the beginning there was the unicorn data scientist, the lone wolf who had to know and do everything, master of all tasks. Things have changed and the roles on data science teams today are more differentiated. In building, leading, and working on data science teams you should be thinking about the tasks first and then which role will perform the tasks.\nOn small teams, the tasks are not done by different people, in larger organizations you will have more differentiated roles with specific tasks assigned to specific roles. For those organizations, this also raises the question whether to choose a centralized, decentralized, or hybrid organizational structure for the data science team(s). More on that below.\nDrawing on this and this blog by the Data Science Process Alliance, the following roles and tasks are typically associated with data science teams.\n\nData Scientist\nThis is the key role on data science teams. Typical tasks and responsibilities of the data scientist include\n\nFinding and interpreting rich data sources.\nApplying the scientific method of asking a question, developing a hypothesis, and testing the hypothesis\nDesigning experiments to test hypotheses and measure the effectiveness of solutions.\nUsing statistical learning and machine learning to build models.\nUnderstanding the end-to-end process of data science project.\nInteracting with stakeholders throughout the data science project life cycle.\nPresenting and communicating data insight to technical and non-technical audiences.\nCollaborating with data engineers and data analysts to collect and preprocess data, and to build and maintain data pipelines.\n\n\n\nData Engineer\nThe data engineer is another key role in data science projects, making data accessible and available. Tasks and responsibilities of the data engineer include\n\nDesigning, building, and maintaining data pipelines to move and transform data.\nDeveloping and maintaining the infrastructure required to support data science projects (ETL or ELT tools, data lakes, data integration).\nEnsuring quality, accuracy, and consistency of data.\nWorking with data scientists to understand data requirements.\n\n\n\nData Analyst\nA data analyst is responsible for collecting, processing, and performing statistical analyses on data. Compared to the data scientist, analysts tend to be more focused on reporting the current state (what happened) than on modeling and predicting (what will happen).\nData analysts do not have (need) the mathematical and statistical knowledge of the data scientist and do not need training in the scientific method. A data scientist can more easily perform the tasks of a data analyst then the other way around. Data scientists are expected to have more expert-level statistical programming abilities, deeper knowledge in developing algorithms, mathematics and statistics, and more academic training.\nTypical responsibilities of the data analyst include\n\nCollecting and processing data to ensure accuracy, quality, and consistency (overlap with data engineer).\nAnalyzing data to find patterns and trends.\nSupporting business decisions through data insights.\nDeveloping and maintain dashboards and reports.\nCollaborating with data scientists and data engineers.\n\n\n\nData Science Architect\nThe data science architect is a data architect who supports and maintains the data infrastructure for data science projects. It is not a data scientist who architects data solutions. The tasks and responsibilities of the data science architect thus resemble more those of the general data architect, but with a specialization on data science projects:\n\nDesigning and overseeing the data infrastructure and architecture, including storage and access of data for data science projects.\nCreating data models and build data pipelines.\nChoosing appropriate tools and technologies for data management\nCollaborating with data scientists to translate their requirements into data architectures.\n\nThere is considerable overlap with the role of data engineer.\n\n\nData Science Developer\nLike the data science architect is a data architect who supports data science initiatives, the data science developer is more like a software developer who supports data science applications. The data science developer codes data applications and is a key role in deploying data science models into production. In some organizations, this role is also called the machine learning engineer.\nThe data science developer integrates data science solutions but is keenly aware of the differences between developing data applications and software development (see below).\nCompared to the data scientist, a data science developer is weaker in statistics and mathematics but stronger in writing production code and in building and maintaining enterprise-grade production systems. Tasks and responsibilities include\n\nDesigning, developing, and deploying scalable data science solutions.\nDeveloping and maintaining machine learning workflows.\nImplementing and monitoring solutions to track model performance.\nCollaborating with data scientists to build production applications from data science assets.\n\n\n\nProduct Manager\nThe product manager sets the vision for the product and defines the product requirements. Product managers collect requirements from the stakeholders (customers, business leaders) and is accountable for the deliverable to the organization. Tasks and responsibilities of the product manager include\n\nDeveloping and delivering data products that meet customer needs.\nCommunicating product requirements and progress to project stakeholders.\nDeveloping and maintaining product roadmaps.\nDeciding which features and functionality to build, and in what order.\nPrioritizing and signing off on work items.\n\n\n\nProject Manager\nWhile the product manager is responsible for the deliverable of a team the project manager is responsible for the project execution. The roles get often confused. The project manager is a coach, facilitator, obstacle remover, and cheerleader. Responsibilities and tasks include\n\nDeveloping and implementing data science project plans.\nEnsuring that projects are completed on time and within budget.\nCoordinating day-to-day tasks of the team (who does what when).\nManaging stakeholder expectations.\nScoping and defining tasks for the team.\n\n\n\nOther Roles\nThere are many other roles that are relevant to the data science team, although they might sit organizationally outside of the team.\n\nSubject matter expert (SME)\nSubject matter experts (domain experts) have deep knowledge about the project domain. Keeping SMEs close is critical to understand the problem context, the problem requirements, rules and regulations the project is subject to, and what is custom, expected, and acceptable in a specific domain. You would not want to develop a fraud model for debit transactions without an expert on banking and payments weighing in. Chances are the fraud model will not be actionable without their input.\nThe SME can tell whether a data science model makes sense from a practical viewpoint. They can explain data in context and ensure patterns found by the data scientist are interpreted correctly.\n\n\nTeam manager\nThe team manager supervises the members of the data science team and is responsible for the people management (human resources) and is accountable for the success of the team. Tasks and responsibilities include\n\nLeading the data science team.\nDirecting and mentoring the team members.\nManaging team resources (personnel, eqiupment, tools, vendor)\nReporting on the team’s performance",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Roles on Data Science Teams</span>"
    ]
  },
  {
    "objectID": "proj/teams.html#centralization-or-decentralization",
    "href": "proj/teams.html#centralization-or-decentralization",
    "title": "5  Roles on Data Science Teams",
    "section": "5.3 Centralization or Decentralization",
    "text": "5.3 Centralization or Decentralization\n\nThe question whether to operate data science team(s) as a centralized team—also known as a data science center of excellence (COE)—or as separate decentralized teams in the units of the organization, is irrelevant for small organizations with a single team or possibly a single data scientist.\nIn larger organizations the question is relevant. As data science grows in importance in those places and more data related roles are hired, it matters how to structure the team.\n\nCentralized structure (COE)\nIn the centralized structure all data roles are in one organization. Business units such as Sales, Marketing, Product, Human Resources, or Finance draw on the data professional in that COE (Figure 5.1).\n\n\n\n\n\n\nFigure 5.1: Centralized data team.\n\n\n\nThis structure has many advantages:\n\nBest practices. It is easier to establish and implement best practices in data governance, data management, model deployment, etc.\nScale. Centralized teams are larger. There are benefits of having multiple employees in each role working together. This also helps with recruiting new talent. A larger unit tends to have more influence in the organization and representation at a higher executive level. COEs report to a Chief Data Scientist, Chief Data Officer, or Chief Analytics Officer. Although these tend not (yet) to be C-suite roles, they are more influential than a director-level position in a business unit (where decentralized teams report into).\nSystems and tools can be managed from one place. Negotiating vendor contracts is more efficient and results in better deals when one organization negotiates for larger contracts.\nGrowth and advancement. COEs work on multiple projects across the entire organization. This gives exposure to different domains and enriches the data scientist experience. This opportunity for growth and learning can lead to promotions without having to change organizations.\n\nA centralized structure is not without disadvantages. The domain knowledge (SMEs) are in the business units, the data teams are removed from that. The centralized structure creates a wall between domain and data teams. The resources of a COE are not unlimited and every unit in the organization calls on them. It can be challenging to serve the needs of all units equitably. Units that have more data or that are closer to customers and revenue might get priority.\nSome business units are hesitant to provide access to sensitive data to employees outside of their control. Finance and Human Resources departments prefer to have their own employees work on finance or personnel data.\nFinally, the centralized structure, because of its size and central operation tends to be more bureaucratic and slower than a small agile team within a business unit.\n\n\nDecentralized Structure\nIn this organizational structure, Marketing, Sales, Finance, Human Resources, and so on, have their own data science teams. Advantages of this structure are data science teams that are aligned with the specific business unit and its domain (Figure 5.2).\n\n\n\n\n\n\nFigure 5.2: Decentralized data teams in the business units.\n\n\n\nThe Finance data science team knows how to work with finance data and what problems matter to the Finance org. The data roles sit right next to the domain experts. The teams are smaller and more agile, able to respond to to requests more quickly. Prioritization is not much of a problem if a request comes from within the hierarchy.\nThe downside of multiple data science teams in different parts of the organization are also significant. Smaller teams do not have experts in all data roles. They either rely on IT to provide data engineering and developers or try and roll their own pipelines and solutions. The data science teams in a decentralized structure are similar to teams in small organizations. There is no overarching data science strategy across the organization and each team figures out its own best practices, tool chains, and processes. This leads to very fractured approaches, inconsistent environments, and duplication. The teams report into domain experts such as a Director of Marketing or a VP of Finance who do not have a data science background.\n\n\nHybrid Structure\nThe advantages of the centralized structure are the disadvantages of the decentralized structure and vice versa. Is there a middle ground?\nIn a hybrid structure some aspects are centralized while the business units retain control over other elements. For example, data infrastructure, data strategy, hiring can be centralized, while the data professionals report into the individual business units, work on domain-specific problems by using the central data infrastructure (Figure 5.3).\n\n\n\n\n\n\nFigure 5.3: A hybrid organizational structure for data science teams. Central services are augmented by business unit-specific teams.\n\n\n\nKey for the success of the hybrid strategy is that business (decentral) and data science management (central) need to be closely aligned. Otherwise the data teams get pulled apart between the direction of the central organization and the needs of the business units.\n\n\n\nFigure 5.1: Centralized data team.\nFigure 5.2: Decentralized data teams in the business units.\nFigure 5.3: A hybrid organizational structure for data science teams. Central services are augmented by business unit-specific teams.",
    "crumbs": [
      "Module I. Data Science Projects",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Roles on Data Science Teams</span>"
    ]
  },
  {
    "objectID": "busi/bus_intro.html",
    "href": "busi/bus_intro.html",
    "title": "6  Introduction",
    "section": "",
    "text": "The CRISP-DM methodology remains one of the most widely adopted frameworks for data mining projects (Shearer 2000). While various other methodologies have emerged (Mariscal, Marban, and Fernandez 2010; Kurgan and Musilek 2006), CRISP-DM’s emphasis on business understanding has contributed to its longevity (Martinez-Plumed et al. 2021). Figure 6.1 shows an augmented flow of a data science project according to this methodology. You see the steps reflected in the module organization of our material.\nA majority of data science projects start with the Business Understanding phase, in which business stakeholders and the data science team collaboratively work to frame the business opportunity and explore possible data science solutions. While most of the work in this phase occurs early in the project, this phase will likely need to be revisited (often many times) as both stakeholders and data science teams better understand the problem and solution space. Most high-value data science projects are very complex with interconnected objectives and systems that require deep knowledge of processes, data, and (often competing) business objectives.\n\n\n\n\n\n\n\n\ngraph TD\n    B[\"Business Understanding\"]:::phase --&gt; D[\"Data Understanding\"]:::phase\n    D --&gt; B\n    D --&gt; P[\"Data Preparation\"]:::phase\n    P --&gt; D\n    P --&gt; M[\"Modeling\"]:::phase\n    M --&gt; P\n    M --&gt; E[\"Evaluation\"]:::phase\n    E --&gt; M\n    E --&gt; DEP[\"Deployment\"]:::phase\n    DEP --&gt; MON[\"Monitoring\"]:::phase\n\n    B -.- B1[\"• &lt;u&gt;Frame problem/solution&lt;/u&gt;&lt;br/&gt;• Define KPIs&lt;br/&gt;• Set success criteria\"]\n    D -.- D1[\"• Identify & explore data&lt;br/&gt;• Verify quality&lt;br/&gt;• Identify subsets\"]\n    P -.- P1[\"• Clean & transform&lt;br/&gt;• &lt;u&gt;Feature creation&lt;/u&gt;&lt;br/&gt;• &lt;u&gt;Feature selection I&lt;/u&gt;\"]\n    M -.- M1[\"• Build models&lt;br/&gt;• &lt;u&gt;Feature selection II&lt;/u&gt;&lt;br/&gt;• Evaluate models\"]\n    E -.- E1[\"• Evaluate vs KPIs\"]\n    DEP -.- DEP1[\"• Pilot & full deployment&lt;br/&gt;• KPI evaluation\"]\n    MON -.- MON1[\"• Monitor performance\"]\n\n    classDef phase fill:#f9f9f9,stroke:#333,stroke-width:2px\n    classDef note fill:#fff,stroke:#999,stroke-width:1px\n    \n    class B,D,P,M,E,DEP,MON phase\n    class B1,D1,P1,M1,E1,DEP1,MON1 note\n\n\n\n\n\n\n\n\nFigure 6.1: Augmented CRISP-DM\n\n\n\nThe primary role of this phase is to provide a two-way discovery process between stakeholders and the data science team. It’s a two-way process because stakeholders need to understand the art of the possible from a data science perspective, and the data science team needs to fully understand the business opportunity, including alignment with strategic company objectives, business benefits, measurement criteria, and potential roadblocks. The success of many data science projects is often determined by the effectiveness of the work done in this phase of the CRISP-DM process. Misalignment due to ineffective Business Understanding can result in project failure due to unrealistic expectations on outcomes, failure to identify “showstopper” risks, or lack of support due to poor strategic alignment.\n\n\n\n\n\n\nFigure 6.2: Business Understanding Venn Diagram\n\n\n\nWhen data scientists have a shallow understanding of business processes and opportunities, they often create trivial solutions that offer little value to the business. They may focus on building solutions where data is easily accessible or on leveraging cutting-edge algorithms. This can result in business users questioning the value of the entire data science approach.\nWhen business users don’t understand the capabilities of data science, they tend to fall back on old patterns of thinking (often business intelligence-based) that do not leverage the power of fully integrated predictive and prescriptive solutions (Debortoli, Müller, and Brocke 2014). Business intelligence applications masquerading as data science projects deliver less value, resulting in the business questioning the value of the entire data science investment (Provost and Fawcett 2013).\n\n\n\n\nDebortoli, Stefan, Oliver Müller, and Jan vom Brocke. 2014. “Comparing Business Intelligence and Big Data Skills.” Business & Information Systems Engineering 6 (5): 289–300.\n\n\nKurgan, Lukasz A, and Petr Musilek. 2006. “A Survey of Knowledge Discovery and Data Mining Process Models.” The Knowledge Engineering Review 21 (1): 1–24.\n\n\nMariscal, Gonzalo, Oscar Marban, and Covadonga Fernandez. 2010. “A Survey of Data Mining and Knowledge Discovery Process Models and Methodologies.” The Knowledge Engineering Review 25 (2): 137–66.\n\n\nMartinez-Plumed, Fernando, Lidia Contreras-Ochando, César Ferri, Peter Flach, José Hernández-Orallo, Meelis Kull, Nicolas Lachiche, and Marı́a José Ramı́rez-Quintana. 2021. “CRISP-DM Twenty Years Later: From Data Mining Processes to Data Science Trajectories.” IEEE Transactions on Knowledge and Data Engineering 33 (8): 3048–61.\n\n\nProvost, Foster, and Tom Fawcett. 2013. Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking. O’Reilly Media.\n\n\nShearer, Colin. 2000. “The CRISP-DM Model: The New Blueprint for Data Mining.” Journal of Data Warehousing 5 (4): 13–22.",
    "crumbs": [
      "Module II. Business Understanding: Discovery",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "busi/approach.html",
    "href": "busi/approach.html",
    "title": "7  Approach and Methodology",
    "section": "",
    "text": "7.1 General Approach\nBecause the Business Understanding phase is pivotal to a project’s ultimate success, it needs to be inclusive, iterative, and have specific deliverables that must be met to be effective.",
    "crumbs": [
      "Module II. Business Understanding: Discovery",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Approach and Methodology</span>"
    ]
  },
  {
    "objectID": "busi/approach.html#general-approach",
    "href": "busi/approach.html#general-approach",
    "title": "7  Approach and Methodology",
    "section": "",
    "text": "Inclusive Discovery\nHere is a list of typical participants in the Business Understanding phase for a large-scale project. These are the people who will be directly affected by the project or have insights that will be used to inform the requirements and/or project design. Usually, there will be a multi-day kickoff to perform the Business Understanding discovery with the entire group present. This is important to ensure that business problem framing and solution framing are aligned and that roadblocks are identified quickly and addressed inline. While this appears to be a large number of participants, some of the roles outlined here can be filled by a single person. Many projects have ultimately been derailed because key participants were not included in the Business Understanding meetings, leading to unidentified roadblocks or business objectives.\n\nStrategic stakeholders\nGenerally, executives at the SVP level or higher focus on the strategic objectives of the company. In some organizations, Directors also fill this role. Higher-level executives want to ensure that projects align with strategic objectives.\n\nC-level executives (including CEO, CFO, etc.)\nExecutive Vice Presidents (EVP-level) executives\nSenior Vice Presidents (SVP-level) executives\nDirectors (in some organizations)\n\n\n\nTactical stakeholders\n\nManagers\nTeam members\n\nTactical stakeholders are the process owners. They perform the “day-to-day” process work. Often, tactical stakeholders are the teams that will use the data science solutions to bring about change. They ensure continuity with current business processes and provide valuable feedback on how the data science-driven solution should work.\n\n\nData subject matter experts\n\nBusiness analysts\nData engineers\n\nData subject matter experts understand the structure, semantic meaning, and business importance of data in corporate data stores. They identify potential issues with data availability, quality, privacy, and other concerns.\n\n\nInformation technology (IT)\n\nEnterprise architects\nSoftware developers\n\nEnterprise architects understand the topology of hardware infrastructure and design of software systems and how they integrate to deliver business solutions. They ensure that the data science solution follows architectural and security standards. Software developers create the necessary middleware (software that contains business logic) and user interface elements of the data science solution.\n\n\nData science team\n\nData scientists\nBusiness analysts\n\nData scientists build predictive and prescriptive models that drive both tactical and strategic decisions. Business analysts deliver business intelligence reporting, dashboarding of Key Performance Indicators (KPIs), and visualizations.\n\n\n\nIterative Refinement\nThe CRISP-DM process was developed with iterative learning and continuous improvement in mind. The Business Understanding phase is iterative, meaning that it starts with an initial problem definition and evolves into a more refined problem and solution definition. It can be helpful to review the problem definition at each stage of the process to ensure business stakeholders remain aligned with your understanding of the opportunities and solutions. It is common for business stakeholders’ priorities to change as the project progresses. Multi-month (or even multi-year) projects can fail if priorities change, processes evolve, or new business problems are identified and the design is not updated accordingly.\n\n\nSpecific Deliverables: Business Understanding Document\nThe Business Understanding document serves as the foundational blueprint for any data science project, providing a comprehensive framework that connects business objectives with technical implementation. Its primary purpose is to ensure all stakeholders have a clear, shared understanding of what the project aims to achieve, how it will be executed, and how success will be measured. This document acts as both a strategic compass and an operational roadmap. Starting with strategic alignment and stakeholder analysis, it establishes the “why” and “who” of the project. The requirements documentation and feasibility assessment sections then build upon this foundation to define the “what” and “how possible.” The business process impact analysis ensures we understand the organizational changes needed, while the KPIs and success criteria establish how we’ll measure progress and success. Finally, the project plan transforms all these insights into actionable steps. Most importantly, this document serves as a single source of truth that teams can reference throughout the project lifecycle to maintain alignment, resolve disputes, and make informed decisions. It should be treated as a living document that can be updated as new information emerges or business conditions change while still maintaining its core purpose of keeping the project aligned with business objectives.\n\nOverview of strategic alignment\nStrategic alignment focuses on ensuring the data mining initiative directly supports business objectives and strategy. This includes documenting current business challenges, identifying key stakeholders and their needs, and understanding how the project’s outcomes will drive business value. It also involves mapping organizational capabilities, constraints, and assumptions that could impact project success.\n\n\nStakeholder analysis\nA detailed mapping of:\n\nKey stakeholders and their roles\nCommunication requirements and preferences\nDecision-making authority levels\nExpected involvement and responsibilities\nImpact assessment for each stakeholder group\n\n\n\nRequirements documentation\nA comprehensive documentation of both functional and technical requirements, including:\n\nData requirements and sources\nSystem integration needs\nSecurity and compliance requirements\nStakeholder access and reporting needs\nPerformance and scalability requirements\nData governance and privacy considerations\n\n\n\nFeasibility assessment\nA thorough analysis that evaluates:\n\nTechnical feasibility (available tools, data, expertise)\nEconomic feasibility (ROI analysis, cost-benefit breakdown)\nOperational feasibility (organizational readiness)\nLegal and ethical considerations\nData availability and quality assessment\nResource availability and constraints\n\n\n\nBusiness process impact analysis\nDocumentation of how the data mining project will affect:\n\nCurrent business processes\nWorkflow changes needed\nTraining requirements\nSystem integrations\nOrganizational change management needs\nDependencies on other systems or processes\n\n\n\nKPI and metrics\nThe KPI and metrics deliverable establishes quantifiable measures that will be used to track project progress and success. This includes identifying both business KPIs (like revenue growth, customer retention, or cost reduction) and technical KPIs (model accuracy, precision, recall). Each metric should have a clear baseline, target value, and measurement methodology. The document should also explain how these metrics connect to broader business objectives.\n\n\nSuccess criteria\nSuccess criteria define the specific thresholds and conditions that must be met for the project to be considered successful. This deliverable includes both business and technical success criteria:\n\nBusiness criteria might specify required improvements in operational efficiency, revenue targets, or cost savings\nTechnical criteria outline required model performance levels, data quality standards, and system performance requirements\n\nThe document should also detail how success will be measured and validated throughout the project lifecycle.\n\n\nProject plan\nThe project plan provides a comprehensive roadmap for executing the data science initiative. It includes:\n\nDetailed timeline with major milestones and dependencies\nResource requirements (technical, human, and infrastructure)\nRisk assessment and mitigation strategies\nBudget allocation and tracking mechanisms\nCommunication plan for stakeholders\nQuality assurance approach and checkpoints\nChange management strategy\nContingency plans\n\nThe plan should be realistic, accounting for available resources and organizational constraints while maintaining alignment with business objectives.",
    "crumbs": [
      "Module II. Business Understanding: Discovery",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Approach and Methodology</span>"
    ]
  },
  {
    "objectID": "busi/approach.html#sec-busi-methodology",
    "href": "busi/approach.html#sec-busi-methodology",
    "title": "7  Approach and Methodology",
    "section": "7.2 Methodology",
    "text": "7.2 Methodology\n\nPerforming Business Discovery and Framing the Problem\nHere are some requirements for effectively performing discovery with business stakeholders:\n\nStart with why\n\nBegin every discovery session by understanding the fundamental business drivers\nAsk “Why now?” to understand urgency and priority\nConnect the initiative to strategic business objectives\nDefine clear, measurable success metrics aligned with business KPIs\n\n\n\nStakeholder mapping\n\nCreate a comprehensive stakeholder map early in the process\nIdentify both direct and indirect stakeholders\nInclude domain experts who understand the business context\nDocument each stakeholder’s interests, influence, and concerns\nMap dependencies between stakeholder groups\n\n\n\nCurrent state analysis\n\nDocument existing business processes and workflows\nIdentify pain points and inefficiencies\nMap the current data landscape and systems\nUnderstand existing solutions and why they’re insufficient\nAnalyze current performance metrics\n\n\n\nProblem definition\n\nUse root cause analysis to identify underlying issues\nQuantify the business impact of the problem\nDefine clear boundaries and scope\nDistinguish between symptoms and core problems\nIdentify interconnected challenges\n\n\n\nValue proposition\n\nCalculate potential ROI and business benefits\nIdentify both tangible and intangible benefits\nEstimate resource requirements and costs\nSet realistic timeline expectations\nConsider long-term strategic value\n\n\n\nData science goals\n\nTranslate business objectives into specific data science goals\nDefine clear success criteria for the analytical solution\nIdentify technical objectives and requirements\nDocument constraints and limitations\nAlign with data governance policies\n\n\n\nRisk assessment\n\nIdentify potential technical and business risks\nAssess data quality and availability risks\nDevelop mitigation strategies\nConsider regulatory and compliance implications\nEvaluate security considerations\n\n\n\nSolution exploration\n\nExplore multiple potential approaches\nConduct feasibility analysis for each option\nIdentify quick wins and long-term solutions\nAnalyze trade-offs between different approaches\nConsider scalability requirements\n\n\n\nChange management\n\nDevelop a stakeholder communication plan\nIdentify training and support needs\nPlan for organizational adoption\nConsider cultural impacts and resistance\nCreate feedback mechanisms\n\n\n\nDocumentation\n\nMaintain clear documentation of requirements\nRecord assumptions and dependencies\nCreate a glossary of business terms\nDocument decisions and their rationale\nEstablish version control\n\nEach of these steps should be applied iteratively throughout the business understanding phase. Remember to:\n\nValidate findings with stakeholders regularly\nUpdate documentation as new information emerges\nKeep focus on business value rather than technical solutions\nMaintain clear communication channels with all stakeholders\nFoster collaborative problem-solving\n\n\n\n\nFraming/Presenting a Data Science Based Solution Back to the Business Stakeholders\nHere are some requirements for effectively presenting data science solutions to business stakeholders:\n\nExecutive summary focus\n\nBegin with clear business value proposition\nHighlight key findings and recommendations\nPresent ROI and business impact metrics upfront\nKeep technical details in appendices\nAddress strategic alignment\n\n\n\nStory structure\n\nStart with the business problem context\nNarrate the analytical journey\nShow the solution evolution\nEnd with concrete impact and results\nInclude real-world examples\n\n\n\nBusiness metrics first\n\nLead with improvements to key business KPIs\nQuantify cost savings and efficiency gains\nHighlight new revenue or growth opportunities\nCompare results to initial success criteria\nShow trends and projections\n\n\n\nVisual communication\n\nUse clear, business-focused visualizations\nCreate interactive demonstrations when possible\nDevelop intuitive business dashboards\nAvoid complex technical plots\nEnsure accessibility\n\n\n\nTechnical translation\n\nConvert technical concepts into business language\nUse relevant industry analogies\nProvide real-world examples\nLayer technical details based on audience\nCreate relatable metaphors\n\n\n\nImplementation plan\n\nOutline clear deployment steps\nDetail resource requirements\nPresent realistic timelines\nInclude key milestones and dependencies\nAddress operational impacts\n\n\n\nRisk management\n\nBe transparent about limitations\nClarify key assumptions\nPresent mitigation strategies\nAddress potential failure modes\nInclude contingency plans\n\n\n\nAdoption strategy\n\nDetail user training requirements\nPresent change management approach\nOutline ongoing support plan\nInclude feedback mechanisms\nConsider cultural factors\n\n\n\nFuture roadmap\n\nSuggest next steps and phases\nPresent scaling opportunities\nIdentify potential enhancements\nLink to long-term business strategy\nConsider emerging technologies\n\n\n\nStakeholder engagement\n\nPrepare for common questions\nIdentify key decision points\nDefine ongoing success metrics\nPlan for different audience levels\nBuild consensus\n\nKey Presentation Principles:\n\nTailor the depth based on audience\nFocus on outcomes over methods\nUse progressive disclosure for technical details\nEncourage interactive discussion\nPrepare multiple paths through the material\nHave technical details ready but not front-loaded\nMaintain engagement through storytelling\n\nRemember to:\n\nPrepare backup slides for detailed questions\nHave specific examples ready\nKnow your audience’s technical level\nBe ready to pivot based on stakeholder reactions\nDocument key decisions and next steps\nFollow up on action items promptly",
    "crumbs": [
      "Module II. Business Understanding: Discovery",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Approach and Methodology</span>"
    ]
  },
  {
    "objectID": "busi/examples.html",
    "href": "busi/examples.html",
    "title": "8  Example: Email Campaign Optimization",
    "section": "",
    "text": "8.1 Discovery Meeting Transcript\nParticipants:\nSarah: Thank you all for meeting today. I’d like to understand the business context and specific challenges around your email campaigns to ensure our data science project delivers meaningful value. Could you start by describing the current situation?\nMichael (CMO): Our email marketing costs are eating up too much of our budget. We’re paying per email sent through our vendor, ExactTarget, but our response rates are low. We need to be more strategic about who we’re targeting and when.\nSarah: Could you help me quantify the current situation? What are your current response rates and costs?\nDavid (Marketing Manager): Our average open rate is around 15%, and click-through rates hover around 2%. We’re paying $0.008 per email sent, and we’re sending about 500,000 emails per month across all campaigns. That’s $4,000 monthly just in email costs.\nSarah: What defines a successful email campaign for your team? What are your key performance indicators?\nJennifer (Director): Ultimate success is conversion to sale, but we also track open rates, click-through rates, and unsubscribe rates. We have different goals for different campaign types - promotional campaigns aim for immediate sales, while nurture campaigns focus on engagement metrics.\nSarah: Could you break down the different types of campaigns you run and their specific goals?\nJennifer: We have three main types: 1. Promotional campaigns for sales and special offers 2. Nurture campaigns for leads who’ve shown interest 3. Newsletter campaigns for general audience engagement\nSarah: What are your target improvement goals? What would success look like for this project?\nMichael: We’d like to reduce our monthly email spend by 30% while maintaining or improving our current revenue from email campaigns. That means we need to be much more precise about who we target.\nSarah: Do you have any constraints or requirements I should be aware of? For example, regulatory requirements or technical limitations?\nDavid: We need to comply with GDPR and CAN-SPAM Act. Also, our ESP (ExactTarget) has some technical limitations on segmentation and personalization. We can only use the fields available in their system.\nSarah: What data do you currently have available about your customers and their email interactions?\nDavid: We have:\nSarah: Are there any specific time constraints or deadlines for this project?\nMichael: We’re approaching our busy season in Q4. We’d like to have a solution implemented by the end of Q3 to maximize impact during the holiday season.\nSarah: What’s currently working well in your email campaigns that we should make sure to preserve?\nJennifer: Our segmented promotional campaigns perform better than broad campaigns. When we target based on previous purchase behavior, we see about double the conversion rate.\nSarah: Are there any past attempts at solving this problem that I should know about?\nDavid: We tried basic RFM (Recency, Frequency, Monetary) segmentation last year. It helped somewhat but wasn’t sophisticated enough to give us the precision we need.\nSarah: Who are the key stakeholders for this project, and how should we keep them informed?\nMichael: The marketing team here, plus our CFO will want to see the cost implications. Our technical team will need to be involved for implementation. Monthly updates to this group should work, with ad-hoc updates for significant findings.\nSarah: Let me summarize the key points to ensure I’ve understood correctly:\nIs there anything I’ve missed or misunderstood?\nJennifer: That’s accurate. One additional point is that we’d like to understand which customer segments are most valuable for different types of campaigns. This could help us with content strategy as well as targeting.\nSarah: Thank you, that’s helpful. Last question: What risks should we be aware of?\nMichael: Our biggest concern is maintaining revenue while reducing email volume. We can’t afford to cut off communication with valuable customers. Also, our email vendor contract renews in Q4, so any insights could help with negotiations.\nSarah: Thank you all for your time. I’ll document these requirements and come back with a project plan that addresses these goals and constraints. Would you like to schedule a follow-up meeting to review the project plan?\nMichael: Yes, please. Set something up for next week. And make sure to include any additional data requirements you identify — we want to make sure you have everything you need to succeed.",
    "crumbs": [
      "Module II. Business Understanding: Discovery",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Example: Email Campaign Optimization</span>"
    ]
  },
  {
    "objectID": "busi/examples.html#discovery-meeting-transcript",
    "href": "busi/examples.html#discovery-meeting-transcript",
    "title": "8  Example: Email Campaign Optimization",
    "section": "",
    "text": "Sarah Chen (Data Scientist)\nMichael Roberts (Chief Marketing Officer)\nJennifer Hayes (Director of Marketing)\nDavid Kim (Marketing Manager)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer demographics and purchase history\nEmail interaction data (opens, clicks, unsubscribes)\nWebsite behavior for logged-in users\nCampaign history and performance metrics\nCustomer service interaction records\n\n\n\n\n\n\n\n\n\n\n\nPrimary goal: 30% reduction in email costs while maintaining revenue\nCurrent spend: $4,000/month on 500,000 emails\nCurrent performance: 15% open rate, 2% CTR\nKey constraints: GDPR compliance, ESP technical limitations\nTimeline: Solution needed by end of Q3\nSuccess metrics: Cost reduction and maintained revenue\nAvailable data: Customer demographics, email interactions, purchase history, website behavior",
    "crumbs": [
      "Module II. Business Understanding: Discovery",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Example: Email Campaign Optimization</span>"
    ]
  },
  {
    "objectID": "busi/examples.html#business-understanding-document",
    "href": "busi/examples.html#business-understanding-document",
    "title": "8  Example: Email Campaign Optimization",
    "section": "8.2 Business Understanding Document",
    "text": "8.2 Business Understanding Document\n\nOverview of Strategic Alignment\nThe Email Campaign Optimization initiative directly addresses critical business challenges in marketing efficiency and cost management. The primary business challenge is the high cost of email marketing campaigns relative to their performance, with current spending at $4,000 monthly for 500,000 emails. The organization seeks to optimize email marketing expenditure while maintaining revenue generation, specifically targeting a 30% reduction in email costs.\nKey strategic elements include:\n\nCost optimization without sacrificing revenue generation\nEnhanced targeting precision for email campaigns\nImproved campaign effectiveness through data-driven segmentation\nStrategic timing for implementation before Q4 busy season\nVendor contract negotiation leverage through performance insights\n\nThe project aligns with organizational capabilities through existing data infrastructure and marketing automation tools (ExactTarget), though technical limitations in segmentation and personalization have been identified as constraints.\n\n\nStakeholder Analysis\n\nPrimary stakeholders\n\nChief Marketing Officer (Michael Roberts)\n\nRole: Executive sponsor and strategic direction\nDecision Authority: High - final approval on strategic decisions\nCommunication Needs: Monthly updates, critical milestone reviews\nImpact Level: High - accountable for marketing budget and performance\n\nDirector of Marketing (Jennifer Hayes)\n\nRole: Operational oversight and campaign strategy\nDecision Authority: Medium - campaign strategy and execution decisions\nCommunication Needs: Monthly updates, regular operational reviews\nImpact Level: High - responsible for campaign performance\n\nMarketing Manager (David Kim)\n\nRole: Technical implementation and campaign execution\nDecision Authority: Low-Medium - tactical decisions\nCommunication Needs: Regular operational updates\nImpact Level: Medium - responsible for day-to-day execution\n\nCFO\n\nRole: Financial oversight\nDecision Authority: High - budget approval\nCommunication Needs: Monthly cost-benefit updates\nImpact Level: Medium - monitors ROI and cost reduction\n\nTechnical Team\n\nRole: Implementation support\nDecision Authority: Low - technical recommendations\nCommunication Needs: Implementation requirements and technical updates\nImpact Level: Medium - responsible for technical execution\n\n\n\n\n\nRequirements Documentation\n\nData requirements\n\nCustomer Data\n\nDemographics\nPurchase history\nEmail interaction metrics\nWebsite behavior (logged-in users)\nCustomer service interaction records\n\n\nCampaign Data\n\nHistorical campaign performance metrics\nCampaign type classification\nResponse rates by segment\nCost per campaign\n\n\n\n\nTechnical requirements\n\nSystem Integration\n\nExactTarget ESP integration\nWebsite tracking system integration\nCRM system integration for customer data\n\n\nSecurity and Compliance\n\nGDPR compliance requirements\nCAN-SPAM Act compliance\nData privacy and protection measures\n\n\nPerformance Requirements\n\nCampaign response tracking\nReal-time segmentation capabilities\nAutomated reporting systems\n\n\n\n\n\nFeasibility Assessment\n\nTechnical feasibility\n\nAvailable Data: Comprehensive customer and campaign data available\nTools: ExactTarget ESP with some segmentation limitations\nExpertise: Marketing team with previous RFM segmentation experience\nAssessment: MEDIUM-HIGH feasibility with some technical constraints\n\n\n\nEconomic feasibility\n\nCurrent Costs: $4,000 monthly in email costs\nTarget Reduction: 30% ($1,200 monthly)\nRevenue Maintenance Required\nAssessment: HIGH feasibility with clear ROI potential\n\n\n\nOperational feasibility\n\nExisting Process Integration: Good alignment with current workflows\nTeam Capabilities: Experienced marketing team\nTechnical Support: Available implementation team\nAssessment: HIGH feasibility with strong operational foundation\n\n\n\n\nLegal and Ethical Considerations\n\nGDPR Compliance: Required\nCAN-SPAM Act Compliance: Required\nData Privacy: Essential consideration\nAssessment: MEDIUM feasibility with clear compliance requirements\n\n\n\nBusiness Process Impact Analysis\n\nWorkflow changes\n\nCampaign Planning Process\n\nImplementation of data-driven segmentation\nEnhanced targeting criteria\nModified campaign scheduling\n\n\nCustomer Segmentation Process\n\nNew segmentation models\nAutomated targeting rules\nRegular segment updates\n\n\nPerformance Monitoring\n\nEnhanced tracking systems\nNew reporting workflows\nRegular performance reviews\n\n\n\n\nTraining requirements\n\nMarketing Team\n\nNew segmentation model understanding\nEnhanced targeting criteria usage\nPerformance monitoring tools\n\n\nTechnical Team\n\nImplementation requirements\nSystem integration knowledge\nMaintenance procedures\n\n\n\n\n\nKPI and Metrics\n\nBusiness KPIs\n\nCost Metrics\n\nMonthly email spend (Baseline: $4,000)\nCost per email (Baseline: $0.008)\nTarget: 30% reduction\n\n\nCampaign Performance\n\nOpen rate (Baseline: 15%)\nClick-through rate (Baseline: 2%)\nConversion rate (Baseline: TBD by campaign type)\n\n\nRevenue Metrics\n\nRevenue per campaign\nOverall email marketing revenue\nRevenue per customer segment\n\n\n\n\nTechnical KPIs\n\nSegmentation Effectiveness\n\nSegment response rates\nSegment conversion rates\nSegment revenue contribution\n\n\nSystem Performance\n\nSegmentation accuracy\nData quality metrics\nSystem integration efficiency\n\n\n\n\n\nSuccess Criteria\n\nBusiness success criteria\n\nPrimary Criteria\n\n30% reduction in email marketing costs\nMaintained or improved revenue from email campaigns\nImproved campaign response rates\n\n\nSecondary Criteria\n\nEnhanced customer segmentation understanding\nImproved targeting efficiency\nBetter campaign type alignment with segments\n\n\n\n\nTechnical success criteria\n\nModel Performance\n\nAccurate customer segmentation\nReliable targeting recommendations\nConsistent system performance\n\n\nImplementation Criteria\n\nSuccessful ESP integration\nCompliant data handling\nEfficient workflow integration\n\n\n\n\n\nProject Plan\n\nTimeline\n\nProject Start: Immediate\nPhase 1: Data Analysis and Model Development (Q3)\nPhase 2: Implementation and Testing (Q3)\nTarget Completion: End of Q3\nGo-Live: Before Q4 busy season\n\n\n\nResource requirements\n\nHuman Resources\n\nData Science Team\nMarketing Team\nTechnical Implementation Team\nProject Management Support\n\n\nTechnical Resources\n\nData Analysis Tools\nESP System Access\nTesting Environment\nReporting Tools\n\n\n\n\nRisk assessment\n\nPrimary Risks\n\nRevenue maintenance during cost reduction\nTechnical integration challenges\nData quality issues\nCompliance violations\n\n\nMitigation Strategies\n\nPhased implementation approach\nRegular performance monitoring\nCompliance review checkpoints\nStakeholder feedback loops\n\n\n\n\nCommunication plan\n\nRegular Updates\n\nMonthly stakeholder meetings\nWeekly team updates\nAd-hoc critical updates\n\n\nReporting Structure\n\nExecutive dashboards\nOperational reports\nTechnical status updates\n\n\n\n\nQuality assurance\n\nTesting Phases\n\nModel validation\nIntegration testing\nUser acceptance testing\nPerformance monitoring\n\n\nSuccess Metrics Tracking\n\nRegular KPI reviews\nPerformance benchmarking\nCompliance audits",
    "crumbs": [
      "Module II. Business Understanding: Discovery",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Example: Email Campaign Optimization</span>"
    ]
  },
  {
    "objectID": "busi/examples.html#assignment",
    "href": "busi/examples.html#assignment",
    "title": "8  Example: Email Campaign Optimization",
    "section": "8.3 Assignment",
    "text": "8.3 Assignment\nReview the following business understanding transcript.\nCreate a business understanding document that summarizes your understanding of the business problem and provides a purely hypothetical solution. We aren’t looking for a realistic (or even workable solution). We want to understand the process of creating this document, what it contains and why it is important to the overall CRISP-DM process.\n\nHospital Readmissions CRISP-DM Discovery Interview\nDate: January 15, 2025 Participants:\n\nSarah Chen, Lead Data Scientist\nDr. James Morris, Chief Medical Officer\nLinda Thompson, Director of Quality Management\nRobert Garcia, Chief Financial Officer\nDr. Emily Wong, Director of Care Management\n\nSarah: Thank you all for meeting today. I understand reducing readmissions is a key priority. Could you help me understand how this fits into the hospital’s broader strategic goals?\nDr. Morris: Absolutely. Our hospital’s strategic plan has three main pillars:\n\nImproving patient outcomes while reducing total cost of care\nExpanding our integrated care network\nAchieving top-quartile quality metrics nationally\n\nReadmissions directly impact all three. We’re currently in the bottom 40th percentile for readmission rates compared to peer institutions.\nRobert: The financial impact is significant. Last year, we faced $3.8 million in Medicare penalties due to excess readmissions. Plus, with value-based care contracts now representing 35% of our revenue, readmission rates directly affect our reimbursements.\nSarah: Could you walk me through the current readmission landscape? What are the key metrics you’re tracking?\nLinda: Our overall 30-day readmission rate is 18.2%. For Medicare patients, it’s higher at 22.1%. We’re particularly concerned about:\n\nHeart failure: 24.8% readmission rate\nCOPD: 21.3%\nTotal joint replacement: 12.4%\nPneumonia: 17.9%\n\nWe’ve seen a troubling trend of increased readmissions in the past 18 months, particularly among elderly patients with multiple chronic conditions.\nSarah: What initiatives are already in place to address readmissions?\nDr. Wong: We implemented several programs:\n\nPost-discharge phone calls within 48 hours\nMedication reconciliation program\nCare transition nurses for high-risk patients\nPrimary care follow-up scheduling before discharge\n\nHowever, we’re struggling to identify which patients need what level of intervention. Our risk stratification process is largely manual and based on clinical judgment.\nSarah: What specific goals would you like to achieve through this analytics project?\nDr. Morris: Primary objectives are:\n\nReduce overall 30-day readmission rate to 15% or lower within 12 months\nAchieve top quartile performance in CMS Hospital Readmissions Reduction Program\nReduce readmission rates for heart failure patients to below 20%\n\nRobert: From a financial perspective, we need to:\n\nReduce Medicare penalties by at least 50%\nImprove performance-based payments in our value-based contracts\nOptimize resource allocation for post-discharge interventions\n\nSarah: What data do we currently have available?\nLinda: We have several relevant data sources:\n\nEHR clinical data going back 5 years\nClaims data from major payers\nRisk assessment scores from our case management system\nPatient satisfaction surveys\nSocial determinants of health screening data for about 60% of patients\nMedication adherence data from our pharmacy system\n\nSarah: Are there any data quality concerns I should be aware of?\nDr. Wong: Yes, several:\n\nSocial determinants data collection only started 18 months ago\nMedication adherence data is incomplete for patients using external pharmacies\nRisk scores are inconsistently documented\nSome clinical notes are unstructured and vary in detail level\n\nSarah: How will we measure the success of this analytics project?\nLinda: Key success metrics should include:\n\nReduction in overall readmission rate\nImprovement in risk prediction accuracy compared to current methods\nRate of appropriate intervention deployment based on risk levels\nCost savings from prevented readmissions\nStaff satisfaction with risk prediction tools\n\nDr. Morris: We also need to ensure:\n\nThe solution integrates with existing clinical workflows\nPredictions are available in real-time during admission\nRisk factors are explainable to clinical staff\nThe system accounts for social determinants of health\n\nSarah: What constraints should I be aware of?\nRobert: Several key considerations:\n\nIT resources are limited due to concurrent EHR upgrade\nCare management staff is already at capacity\nBudget for new intervention programs is capped at $500K for this fiscal year\nAny new processes need to work within existing staffing levels\n\nSarah: Based on our discussion, I’ll focus on:\n\nDeveloping a detailed project plan\nConducting initial data quality assessment\nCreating a baseline model using existing data\nDesigning a validation approach with clinical stakeholders\n\nDr. Morris: That sounds good. We should also plan regular check-ins with the clinical leadership team to ensure we’re staying aligned with operational needs.\nSarah: What potential risks should we plan for?\nLinda: Key risks include: - Data integration challenges across systems - Staff resistance to new predictive tools - Maintaining model accuracy as patient populations change - Ensuring interventions are cost-effective\nRobert: We need to show meaningful progress before the next Joint Commission review in 8 months. Can we set some interim milestones?\nSarah: Yes, I’ll propose a detailed timeline in the project plan, but preliminarily:\n\nMonth 1-2: Data assessment and preparation\nMonth 3-4: Initial model development and validation\nMonth 5-6: Pilot testing and refinement\nMonth 7-8: Full implementation and monitoring",
    "crumbs": [
      "Module II. Business Understanding: Discovery",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Example: Email Campaign Optimization</span>"
    ]
  },
  {
    "objectID": "data/data_intro.html",
    "href": "data/data_intro.html",
    "title": "9  Introduction",
    "section": "",
    "text": "Quotes\n\n\n\n\nData engineering is the unsung hero of data science,\nthe foundation upon which great data analysis is built.\nAndrew Brust\n\nData matures like wine, applications like fish.\nJames Governor\n\n\n\nThe goal of data engineering is to make raw data usable for consumption by data analysts, business analysts, data scientists, machine learning engineers, and so on. The CRISP-DM data science project cycle follows the business understanding (discovery) stage with two data-related phases:\n\nData understanding: identify, collect, and analyze data sets that help accomplish the project goals.\nData preparation: prepare the final data sets for the next stages of the project, in particular, the modeling stage.\n\nWe combine both efforts under the umbrella of data engineering, the tasks of selecting and cleaning data (under data preparation) cannot be separated from the tasks of collecting, describing (exploring) data, and working on data quality (under data understanding).\nThe discussion begins with a review of important file formats in data science, databases, and enterprise data architectures. Chapter 11 uses Python to ingest data in the major file formats and from a database. During data profiling, the first date with the data, we get a first impression of the content and quality of the data.\nSummarization (Chapter 13) and visualization {Chapter 14} distill the essence of the data and its patterns in numerical and graphical form.\nBecause the declarative programming language SQL is very important in the daily work of data scientists, we present some basic SQL knowledge in Chapter 15.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data/sources_and_files.html",
    "href": "data/sources_and_files.html",
    "title": "10  Data Sources and File Formats",
    "section": "",
    "text": "10.1 File Formats\nIf you are working with proprietary data analytics tools such as SAS, SPSS, or JMP, you probably do not worry much about the ways in which data are stored, read, and written by the software, except that you want the tool to support import to and export from your favorite non-proprietary format.\nMost data science today is performed on open-source or open standard file formats. Proprietary formats are not easily exchanged between tools and applications. Data science spans many technologies and storing data in a form accessible by only a subset of the tools is counterproductive. You end up reformatting the data into an exchangeable format at some point anyway.\nThe most important file formats in data science are CSV, JSON, Parquet, ORC, and Avro. CSV and JSON are text-based, human-readable formats, whereas Parquet, ORC, and Avro are binary formats. The last three were designed specifically to handle Big Data use cases, Parquet and ORC in particular are associated with the Hadoop ecosystem. Parquet, ORC, and Avro are Apache open-source projects.\nAlthough CSV and JSON are ubiquitous, they were never meant for large data volumes.\nYou will encounter many other data formats in data science work, but these five file formats cover a lot of ground, you should have a basic understanding of their advantages and disadvantages.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Sources and File Formats</span>"
    ]
  },
  {
    "objectID": "data/sources_and_files.html#sec-data-file-formats",
    "href": "data/sources_and_files.html#sec-data-file-formats",
    "title": "10  Data Sources and File Formats",
    "section": "",
    "text": "But what about Excel?\n\n\n\nSpreadsheet programs such as Microsoft Office Excel, Google Sheets, Apple Numbers, are everywhere. Excel is one of the most frequently encountered format to communicate data in the corporate world. Unfortunately so.\nWe discuss below why spreadsheets are not suitable for data science. Many analytic data sets ready for modeling started out as spreadsheets but data science rarely uses the format to store processed data that is ready for modeling. Maybe the best advice we can give about data in spreadsheets is that when you receive data in that format, make the files immutable (read-only) and convert the content as soon as possible into a format suitable for storing analytic data.\n\n\n\nCSV\nIn data science projects you will invariably work with comma-separated values (CSV) files. Not because that is a great file format, but because CSV files are ubiquitous for rectangular data sets made up of rows and columns. Each line of the file stores one record with values in plain text separated by commas.\nFigure 10.1 shows the contents of a CSV file with ten observations. The first line lists the column names. Strings are enclosed in quotes and values are separated by commas.\n\n\n\n\n\n\nFigure 10.1: A CSV file with ten records.\n\n\n\nAmong the advantages of CSV files are\n\nUbiquitous: every data tool can read and write CSV files. It is thus a common format to exchange (export/import) data between tools and applications.\nHuman readable: since the column names and values are stored in plain text, it is easy to look at the contents of a CSV file. When data are stored in binary form, you need to know exactly how the data are laid out in the file to access it.\nCompression: since the data are stored in plain text it is easy to compress CSV files.\nExcel: CSV files are easily exported from and imported to Microsoft Excel.\nSimple: the structure of the files is straightforward to understand and can represent tabular data well if the data types can be converted to text characters.\n\nThere are some considerable disadvantages of CSV files, however:\n\nHuman readable: To prevent exposing the contents of the file you need to use access controls and/or encryption. It is not a recommended file format for sensitive data.\nSimple structure: Complex data types such as documents with multiple fields and sub-fields cannot be stored in CSV files.\nPlain text: Some data types cannot be represented as plain text, for example, images, audio, and video. If you kluge binary data into a text representation the systems writing and reading the data need to know how to kluge and un-kluge the information—it is not a recommended practice.\nEfficiency: much more efficient formats for storing data exist, especially large data sets.\nBroken: CSV files can be easily broken by applications. Examples include inserting line breaks, limiting line width, not handling embedded quotes correctly, blank lines.\nMissing values (NaNs): The writer and reader of CSV files need to agree how to represent missing values and values representing not-a-number. Inconsistency between writing and reading these values can have disastrous results. For example, it is a bad but common practice to code missing values with special numbers such as 99999; how does the application reading the file know this is the code for a missing value?\nEncodings: When CSV files contain more than plain ASCII text, for example, emojis or Unicode characters, the file cannot be read without knowing the correct encoding (UTF-8, UTF-16, EBCDIC, US-ASCII, etc.). Storing encoding information in the header section of the CSV file throws off CSV reader software that does not anticipate the extra information.\nMetadata: The only metadata supported by the CSV format are the column names in the first row of the file. This information is optional and you will find CSV files without column names. Additional metadata common about columns in a table such as data types, format masks, number-to-string maps, cannot be stored in a CSV file.\nData Types: Data types need to be inferred by the CSV reader software when scanning the file. There is no metadata in the CSV header to identify data types, only column names.\nLoss of Precision: Floating point numbers are usually stored in CSV files with fewer decimal places than their internal representation in the computer. A double-precision floating point number occupies 64-bits (8 bytes) and has 15 digits of precision. Although it is not necessary, floating-point numbers are often rounded or truncated when they are converted to plain text.\n\nDespite these drawbacks, CSV is one of the most common file formats. It is the lowest common denominator format to exchange data between disparate systems.\n\n\nJSON\nJSON stands for JavaScript Object Notation, and although it was borne out of interoperability concerns for JavaScript applications and is based on a JavaScript standard, it is a language-agnostic data format. Initially used to pass information in human readable form between applications over APIs (Application Programmer Interfaces), JSON has grown into a general-purpose format for text-based, structured information. It is the standard for communicating on the web. The correct pronunciation of JSON is apparently akin to the name “Jason”, but “JAY-sawn” has become common.\nA binary, non-human readable form of JSON was created at MongoDB and is called BSON (binary JSON).\nIn contrast to CSV, JSON is not based on rows of data but three basic data elements:\n\nValue: a string, number, reserved word, or one of the following:\nObject: a collection of name—value pairs similar to a key-value store.\nArray: An ordered list of values\n\nAll modern programming languages support key—values and arrays, they might be calling it by different names (object, record, dictionary, struct, list, sequence, map, hash table, …). This makes JSON documents highly interchangeable between programming languages—JSON documents are easy to parse (read) and write by computers. Any modern data processing system can read and write JSON data, making it a frequent choice to share data between systems and applications.\nA value in JSON can be a string in double quotes, a number, true, false, or null, an object or an array. Objects are unordered collection of name—value pairs. An array is an ordered collection of values. Since values can contain objects and arrays, JSON allows highly nested data structures that do not fit the row structure of CSV files.\n\n\n\n\n\n\nFigure 10.2: Elements of a JSON document. Because values can contain objects and arrays, JSON documents can be highly structured and deeply nested.\n\n\n\nJSON documents are self-describing, the schema to make the data intelligible is built into the structures. It is also a highly flexible format that does not impose any structure on the data, except that it must comply with the JSON rules and data types—JSON is schema-free.\n\n\n\n\n\n\nFigure 10.3: A simple JSON document. The entire document is a name—value pair with name “menu”. The value is an object with names “id”, “value”, and “popup”. The value of “popup” is an object with name “menuitem” whose value is an array. The elements of the array are objects with names “value” and “onclick”.\n\n\n\nMany databases support JSON as a data type, allowing you to store hierarchical information in a cell of a row-column layout, limited only by the maximum size of a text data type in the database. Non-relational document databases often use JSON as the format for their documents.\nSince so much data is stored in JSON format, you need to get familiar and comfortable with working with JSON files. Data science projects are more likely consumers of JSON files rather than producer of files.\n\n\nApache Parquet\nThe Apache Parquet open-source file format is a binary format—data are not stored in plain text but in binary form. Originally conceived as a column-based file format in the Hadoop ecosystem, it has become popular as a general file format for analytical data inside and outside of Hadoop and its file system HDFS: for example, as an efficient analytic file format for data exported to data lakes or in data processing with Spark.\nWorking with Parquet files for large data is an order of magnitude faster than working with CSV files. The drawbacks of CSV files discussed previously all melt away with Parquet files.\nParquet was designed from the ground up with complex data structures and read-heavy analytics in mind. It uses principally columnar storage but does it cleverly by storing chunks of columns in row groups rather than entire columns.\n\n\n\n\n\n\nFigure 10.4: The Parquet file architecture. Chunks of columns are stored in row groups. The footer contains important metadata. Source: Parquet File Format: Everything You Need to Know, by Nikola Ilic.\n\n\n\nThis hybrid storage model is very efficient when queries select specific columns and filter rows at the same time; a common pattern in data science: compute the correlation between homeValue and NumberOfRooms for homes where ZipCode = 24060.\nParquet stores metadata about the row chunks to speed access to rows, the metadata tells the reader which row chunks to skip. Also, a single write to the Parquet format can generate multiple .parquet files. The total data is divided into multiple files collected within a folder. Like NoSQL and NewSQL databases, data are partitioned, but since Parquet is a file format and not a database engine, the partitioning results in multiple files. This is advantageous for parallel processing frameworks like Spark that can work on multiple partitions (files) concurrently.\nParquet uses several compression techniques to reduce the size of the files such as run-length encoding, dictionary encoding, Snappy, GZip, LZO, LZ4, ZSTD. Because of columnar storage compression methods can be specified on a per-column basis; Parquet files compress much more than text-oriented CSV files.\nBecause of its complex file structure, Parquet files are relatively slow to write. The file format is optimized for the WORM paradigm: write-once, read many times.\n\nComparison of popular file formats in data science.\n\n\n\n\n\n\n\n\n\nCSV\nJSON\nParquet\n\n\n\n\nColumnar\nNo\nNo\nYes\n\n\nCompression\nYes\nYes\nYes\n\n\nHuman Readable\nYes\nYes\nNo\n\n\nNestable\nNo\nYes\nYes\n\n\nComplex Data Structures\nNo\nYes\nYes\n\n\nNamed Columns\nYes, if in header\nBased on scan\nYes, metadata\n\n\nData Types\nBased on scan\nBased on scan\nYes, metadata\n\n\n\n\n\nApache ORC\nThe Apache ORC (Optimized Row Columnar) open-source file format, like the Parquet file format, was originally associated with the Hadoop Big Data ecosystem. ORC files have a purely columnar storage format unlike the row-group/column chunk hybrid storage in Parquet files.\nORC files are split into individual files that contain a collection of records; with columnar storage within the file. ORC files are organized into stripes, a stripe combines an index, the data in columnar form, and a footer with metadata. Stripes are essential to ORC files; they are treated independently of each other to support parallel data processing.\n\n\n\n\n\n\nFigure 10.5: Layout of an ORC file in stripes. Source.\n\n\n\nA stripe is by default 64 MB in size and ORC files can have multiple stripes. This gives you an idea that these file types were designed to store large amounts of data.\nThe ORC format was optimized for Hive, the data warehouse in the Hadoop ecosystem. For example, Hive and ORC together support ACID transactions on top of Hadoop. You will find that outside of Hadoop, support for ORC files is not as generous as for Parquet files. For example, you can read a multi-file Parquet file with Pandas in a single line of code. To read multi-file ORC files you need to append the results of reading the individual files. The Pandas functions read_orc() and to_orc() to read and write ORC files are not supported on Windows.\n\n\nApache Avro\nApache Avro is an open-source, row-based, schema-based, file format. It is often used in conjunction with the streaming platform Apache Kafka, but Avro files are useful for batch processing as well. The storage format in Avro files is mixed in that the schema information is stored in JSON format while the data is stored in binary form. The schema is thus human readable.\nLike the Parquet format, Avro files are self-describing, the information to access and to deserialize the records is stored in the file itself. Avro files can be compressed but they do not compress as well as column stores. Heterogeneous data types within a row do not compress as well as homogeneous data within a column. Furthermore, column-oriented storage can choose optimal compression algorithms based on the data type of a column whereas row-oriented storage uses the same compression algorithm across all data types.\nLike other row stores, Avro files are great for applications that write more than they read; the exact opposite of column stores.\n\n\nThe Apache Arrow Project\nIf you work with large data sets, you perform data analytics, you process batch and streaming data, you want to perform in-memory analytics and query processing, then you should pay attention to Apache Arrow. This is an open-source development platform for in-memory analytics that supports many programming environments, including R and Python, the primary languages for data science.\nMain contributors to the Python pandas project are also involved with Arrow, so there is a lot of portability between Arrow tables and pandas DataFrames. The pyarrow package is the backend for some of the read and write functions in pandas that handle Big Data formats such as Parquet and ORC.\nNewer releases of Pandas are leaning more on pyarrow, e.g., Pandas 2.0.\n\n\nSpreadsheets\nMicrosoft Excel is one of the most common formats to store data. Why are we discussing it here, after all the other file formats. Like CSV files, Excel files are ubiquitous and a horrific format to store data for analytics. What goes for Excel goes for all other spreadsheet formats. Do not use spreadsheets for analytic data! There be dragons!\nWe like rectangular (tabular) layouts in rows and columns: every row is an observation, every column is a variable. While spreadsheets appear to be organized this way, there are many ways in which the basic tabular layout can be messed with:\n\nCells can refer to other cells on the same or on another sheet.\nCells can contain data, figures, calculations, text, etc.\n\nCells can be merged\n\nOne of the most problematic features of spreadsheets is how easily the data can be modified—accidentally. How many times have you been in a spreadsheet, typed something, and wondered where the input went and whether contents of a cell were changed?\n\n\n\n\n\n\nTip\n\n\n\nThis is another great question to ask a potential employer when you interview: “What data formats and/or databases are you using for analytic data?” If the answer is “We email xlsx files around once a week” run; run like it is the plague. If the answer is “The primary data is stored in Parquet files in S3 buckets”, reply “Tell me more!”",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Sources and File Formats</span>"
    ]
  },
  {
    "objectID": "data/sources_and_files.html#sec-datasource-dbms",
    "href": "data/sources_and_files.html#sec-datasource-dbms",
    "title": "10  Data Sources and File Formats",
    "section": "10.2 Databases",
    "text": "10.2 Databases\nAccording to Wikipedia, a database is an “organized collection of data stored and accessed electronically through the use of a database management system” (DBMS). The three elements are the data, a form of data storage, and DBMS software that allows a user to interact with the system. The terms database and DBMS are often used interchangeably.\nDatabases play a central role in the IT stack of an organization and choosing the wrong database for an application can have dire consequences with respect to availability, scalability, security, data quality, data validity, etc. The site DB-engines collects information about databases and ranks them by popularity based on a somewhat transparent algorithm. If you think there are a lot of data science tools to choose from, wait until you wade into the world of databases—DB-engines lists over 400.\nUnless you build your own tech stack, write your own application, or start your own company, you will probably not select a database. In most situations the databases have been chosen and implemented as part of the backend tech stack. Often there will be more than one, for example, MongoDB for documents, MySQL for transactions, Redshift or Spark for analytics, Redis as a memory cache—the dreaded database sprawl in organizations. Databases are part of the backend infrastructure of organizations, the part that does not change frequently. The so-called Gartner spaghetti graph in Figure 10.6 shows market share of databases between 2011 and 2021. While there is some stability near the top of the market, several items are noteworthy:\n\nA change of guard at the top. Oracle has dominated the market for a long time and is being challenged and/or surpassed by Microsoft, Amazon Web Services, and Google Cloud Platform. The cloud providers are leading the way with compelling cloud database offerings.\nGartner depicts pure-play cloud database vendors in light blue. The spaghetti chart shows the revenue shift in databases to the cloud.\nThe vertical axis displays market share across all software markets. Database expenses make up about 35% of the overall software expenses. That share of the overall software market continues to increase (the chart is fanning out more toward the right).\nMarket share is calculated based on revenue, it does not account for free offerings, such as open-source databases.\nAfter a peak around 2017, Hadoop-based providers of big data resources are on the decline, for example, Cloudera.\nSome cloud database providers had an impressive rise since their inception, AWS since 2013, Alibaba Cloud since 2014, Google Cloud since 2015, Tencent since 2016, Snowflake since 2016.\nA surprising amount of movement in a market segment that is supposed to be relatively stable. Organizations are loath to change database management systems that are key to their operations. Migrating a database that is the backbone of an essential part of the business is the last thing a CIO wants to do. Databases are not as sticky as they used to be.\n\n\n\n\n\n\n\nFigure 10.6: Database market share 2011—2021, according to Gardner. Source.\n\n\n\nIt is important for anyone involved with data to understand some basics of database architectures and the strengths and weaknesses of the different database designs. A NoSQL key-value store can be a great way to work with unstructured data but can be a terrible choice for analytics compared to a relational column store. Many modern DBMS are multi-model, supporting multiple data models in a single backend, for example, key-value pairs, relational, and time-series data. There are so many databases because there are so many use cases for working with data; databases try to optimize for specific data access patterns and use cases.\nDatabases can be organized in several ways.\n\nRelational and Non-relational databases\nIn a relational DBMS (a RDBMS) the data are organized in 2-dimensional tables of rows and columns. A database consists of a collection of named tables, each with a named collection of attributes (columns). Typically, RDBMS use SQL (Structured Query Language) to manipulate the contents of the DBMS. Each row in a table has a unique key and can be linked to rows in other tables. Relationships are logical connections between tables through keys. The structure of a table (columns, primary keys) is known as the table schema, and it is defined before the table is populated with data—this is also called a schema-on-write design (Figure 10.7).\n\n\n\n\n\n\nFigure 10.7: Schema on write with PostgreSQL. The CREATE TABLE statement creates the table named titanic and defines the schema: names, order, and data types of columns and default values. The \\copy statement populates the database table with data.\n\n\n\nThe relational principle has dominated the world of databases since the 1980s. Seven relational databases are among the top ten databases according to DB-engines: Oracle, MySQL, Microsoft SQL Server, PostgreSQL, IBM DB2, Microsoft Access, and SQLite.\nWith the advent of Big Data more data was collected in ways that did not fit well with the relational paradigm: key-value pairs, documents, time series data. More fluid and dynamic relationships between data points were better captured with graph structures rather than rigid relational structures. Big Data analytics also asked for more flexibility in defining data structures on top of the raw data. The relational model was thought to be too restrictive. If the schema must be declared when the table is created and before rows are added (schema-on-write) it makes it difficult to bring new data in on the fly. Schema-on-write in relational databases requires to define the schema for the table and to structure the data based on the data. Changing the structure of the data, for example from text to numbers, requires changes to the schema and results in table copies.\nSchema-on-read, on the other hand, applies structure to the data when it is read (Figure 10.8). That gives users the ability to structure the same source data differently, depending on the application. Schema-on-read led to a new class of data frameworks and databases that broke with the highly structured relational logical data model. These non-relational databases store data in key-value pairs and documents rather than tables and were also called NoSQL databases because they eschewed the structured nature of SQL-based schema-on-write. However, there is still a structuring step in non-relational databases. It just happens on demand when the data is read instead of at the beginning before the first row of data is inserted.\n\n\n\n\n\n\nFigure 10.8: An example of schema on read with Hadoop and its file system (HDFS). The data are loaded into Hadoop with the hdfs dfs command. No schema is defined at this stage. The data is then processed with the hadoop command. The customer-mapper.py Python script determines how the data is interpreted for this job; the Python script applies the schema on read.\n\n\n\nNon-relational databases are sometimes called schemaless databases; that goes a bit too far as some structure is needed to read the data from a key-value or document store into an application. The best translation of “NoSQL” is “not-only-SQL”, these databases still need a structured way to interact with the database.\nIt is easy to see how the push for more flexibility through schema-on-read goes hand in hand with the greater flexibility achieved by moving from the ETL to the ELT processing paradigm. Non-relational databases also gained popularity because they support horizontal scaling more easily than their relational counterparts. When a system scales horizontally—also called scaling out—additional machines are added to support increasing workload. Vertical scaling—also called scaling up—adds more computing resources (CPU, RAM, GPU, …) to existing machines.\nYou can explain the difference between the scaling models by thinking of a building with a fixed number of rooms. To increase the total number of rooms you can either add more floors (scaling up) or add more buildings (scaling out). Both types of scaling reach limits, a single building cannot be made arbitrarily tall and adding buildings consumes land. The limits for horizontal scaling are much higher than for vertical scaling.\n\n\n\n\n\n\nFigure 10.9: Scaling up (vertically) and scaling out (horizontally). Scaling up adds more rooms to an existing building, scaling out adds more buildings.\n\n\n\nBecause non-relational systems partition data on just a primary key and do not need to maintain relationships between tables, scaling them horizontally is relatively simple. As the database grows (contains more keys) add new machines to hold the additional keys. The data can be re-partitioned by re-hashing keys across the larger cluster.\nWith relational systems it is necessary to maintain the relationships between the tables. When data are distributed over multiple machines, this becomes more difficult. The response of relational databases to growing data sizes has historically been to scale up. Relational databases designed with horizontal scaling in mind are referred to as NewSQL databases.\nNon-relational (NoSQL) databases are categorized by their underlying data models into pure document stores, key-value stores, and graph databases—more on these designs below. NoSQL is primarily used in transactional databases for OLTP (online transactional processing) and applications where interactions with the database are limited to CRUD operations (Create, Read, Update, Delete). They are not performant for analytical jobs.\n\n\nTransactional (OLTP) and Analytical (OLAP) Databases\nA transactional database is a repository to record the transactions of the business, the ways in which an organization interacts with others and exchanges goods and services. Business interactions that are recorded in transactional systems are, for example, purchases, returns, debits, credits, signups, subscriptions, dividends, interest, trades, payroll, donations.\nDatabases that support business transactions are optimized for a high volume of concurrent write operations: adding new rows, updating rows, inserting rows. These transactions are often executed in real time, hence the name online transaction processing (OLTP).\nThe concept of a database transaction is different from a business transaction. A database transaction is a single logical unit of work that, if successful, changes the state of the database. It can comprise multiple database operations such as reading, writing, indexing, updating logs. A business transaction, a customer orders an item online, can be associated with one or more database transactions.\nDatabase transactions are not just reserved for OLTP systems, although supporting transactional integrity with so-called ACID properties is a must for them. Analytic databases, graph databases, NoSQL databases typically support transactions. In relational SQL systems, look for BEGIN statements to flag the start of a transaction, the COMMIT statement to commit the results of a transaction and the ROLLBACK statement to reset the state of the database prior to the transaction.\nThe ACID properties refer to atomicity, consistency, isolation, and durability to ensure the integrity of the transaction.\n\nAtomicity: all changes are performed as if they were a single operation. Either all of them are performed or none of them.\nConsistency: When the transaction starts and when it finishes, the data are in a consistent state.\nIsolation: intermediate states of the transaction are invisible to the rest of the system. When multiple transactions happen at the same time, it appears as if they execute serially.\nDurability: After a transaction completes, changes to the data are not undone.\n\nTake as an example the transfer of funds from one account to another. Atomicity requires that if a debit is made from one account a credit in the same amount is made to the other account. Consistency requires that the sum of the balances in the two accounts is the same at the start of the transaction and at the end. Isolation implies that a concurrent transaction sees the amount to be transferred either in the debited account or in the credited account. If a concurrent transaction would see the amount in neither account or in both accounts the database fails the isolation test. Durability means that if the database fails for some reason after the transaction completes, the changes made to the accounts will persist.\nExamples of transactional (OLTP) relational databases are MySQL, PostgreSQL, Microsoft SQL Server, Oracle Database. These systems are also called Systems of Record (SoR) because they store the official records of an organization—the ground truth. While NoSQL databases can support transactions, they relax some of the criteria to support greater scalability. The According to the CAP theorem, a distributed database that partitions data across different machines cannot be fully ACID compliant and guarantee availability. When a network partition fails one must choose between availability of the system and consistency. A relaxed condition is eventual consistency, where the data achieves a consistent state sometime in the future. Eventually, all reads will return the most recently written data value. For systems of records in financial services eventual consistency is not acceptable; SoRs must be ACID compliant.\nWhy are these arcane details of database architecture important for a data scientist?\n\nDatabase designers make tradeoffs when optimizing for a use case. You should be aware how these tradeoffs can affect data integrity if that matters for your project—it might not be an issue at all.\nUsing an OLTP system for analytical work is usually a bad idea. Transactional systems need to respond to frequent data updates and tend to return small record sets when queried (look up a customer, look up an account balance). The best way of storing information for this pattern is as a row-store: the data for a record is stored in a chunk of memory which makes retrieval of a record or set of record very efficient.\n\n\n\nRow and Column Storage\nThe following table shows seven of the 150 observations from the famous Iris data set that is used in statistics and machine learning courses to apply regression, classification, and clustering methods. The full data set contains 50 observations for each of three Iris species: Iris setosa, Iris versicolor, and Iris virginica. Four measurements of the flowers, the length and width of the petals (the large leaves on the flowers) and the sepals (the small leaves) were taken on each plant.\n\nFlower measurements for five Iris setosa and two Iris versicolor from the famous iris data set.\n\n\n\n\n\n\n\n\n\nSepalLength\nSepalWidth\nPetalLength\nPetalWidth\nSpecies\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n6.4\n3.2\n4.5\n1.5\nversicolor\n\n\n\nSuppose we store this data in a row-store with a fixed record length. The numerical measurements can be stored in 4-byte floats. The longest string in the Species column is 10 characters long. To align the records on 4-byte boundaries for faster access, we set the record length to 4 x 4 + 12 = 28. Figure 10.10 shows the first row of data set is laid out across 28 bytes.\n\n\n\n\n\n\nFigure 10.10: Layout in memory of row 1 of Iris data set with four-byte storage of floating-point numbers and 12-byte storage for the species string. (It seems that the number 5.1 uses 3 bytes for storage. It uses all 4 x 8 = 32 bits of the 4 bytes to store the number. The most significant bit stores the sign, the next 8 bits store the exponent, the following 23 bits store the mantissa of the float.)\n\n\n\nSuppose that the data is stored in one big chunk of memory, each row starts 28 bytes after the previous row.\n\n\n\n\n\n\nFigure 10.11: The first three rows of the Iris data set in row-store form with contiguous memory.\n\n\n\nLooking up rows is extremely fast. To read the data for the third observation jump to byte 56 and read the next 28 bytes. Adding new records in row-oriented storage is also fast: allocate a new chunk of 28 bytes and fill in the fields. However, if we want to perform an analytic operation such as computing the average petal length, we must jump through the memory and grab values at position 8, 8 + 28, 8 + 56, …., 8 + 150 x 28. That is very inefficient, especially when tables have many columns and records are long.\nTo support fast analytic processing, analytical databases are optimized for an access pattern where data is mostly read and infrequently written (changed). The ease of appending or looking up entire rows is then not as important as the efficiency of accessing memory for a column. Analytical queries are deep in that they process information from many records in the table—rather than looking up records—and they can return large result sets. Also, analytical queries tend to operate on a small subset of the columns of a table rather than all the columns.\nThe best-performing format for storing information for this workload is a column-store: each column in the table is stored in one or more chunks of memory.\n\n\n\n\n\n\nFigure 10.12: The seven observations of the Iris data set in column-store format.\n\n\n\nFigure 10.12 shows the five columns in the Iris data set in column store format. Computing the average of, say, sepal length, can now be done efficiently by scanning the memory for that column. The data for the other columns does not have to be loaded into memory for this operation.\nColumnar storage has other advantages for analytic databases: the data types are homogeneous within a block of memory (all characters, all integers, all floats, etc.) and compress well. Except for sepal length, all columns have repeated values. These can be stored along with an integer that counts the number of times a value appears. This simple technique—called run-length encoding—can greatly reduce the storage requirements when the data are ordered and have repeat values.\nThe following is a non-exhaustive list of analytic databases that use columnar storage:\n\nAmazon Redshift\nGoogle BigQuery\nAzure Cosmos DB\nDuckDB\nVertica\nScyllaDB\nDataStax\nClickHouse\nApache Druid\nApache Cassandra\nApache HBase\n\n\n\nHybrid (HTAP) Databases\nSome databases combine row-oriented storage with column-oriented storage in the same database engine; this paradigm is called hybrid transactional-analytical processing (HTAP). The idea behind HTAP databases is that you do not need two separate databases, one for transactional workloads and one for analytical workloads. The ability to perform both workloads efficiently in the same database reduces data movement and data duplication between databases. This reduces the dreaded database sprawl in organizations. The term HTAP was coined by industry analyst firm Gartner. Forrester, Gartner’s competitor, refers to this processing paradigm as translytical.\nAnother motivation of HTAP databases is that transactional systems, where information about the business is recorded in real time, also need support for real-time analytics to produce insights “in the moment”. Transactional systems are becoming more analytical, the results and side effects of algorithms triggered by a transaction can be the most important piece of information of the transaction.\n\n\nRide Share Booking\n\n\nSuppose you land at the San Francisco airport and use your favorite ride share app to book a ride into the city. This starts a transaction with the ride share company. The data recorded as part of the transaction includes customer information, origin and destination of the trip, time of day, etc. In real-time, as the transaction occurs, the backend of the app calculates the price for the ride based on location, traffic density, ride demand, available drivers, vehicle type, etc.\nThis is an analytical operation that invokes a price-optimization algorithm. Based on the real-time prices offered to you in the app you choose the preferred ride. The optimized price is recorded as part of the transaction.\n\n\nThe following are examples of HTAP databases:\n\nPingCAP\nAerospike\nSingleStoreDB (formerly MemSQL)\nInterSystems\nOracle Exadata\nSplice Machine\nGridGrain\nRedis Lab\nSAP/HANA\nVoltDB\nSnowflake (since the addition of Unistore in 2022)\nGreenplum\n\n\n\nNewSQL Databases\nAs a broad generalization we can state that non-relational NoSQL databases sacrifice consistency for horizontal scalability and that traditional relational SQL databases are ACID compliant but do not scale well horizontally, they are designed for scale-up.\nNewSQL databases try to bridge this gap, providing horizontal scalability and transactional integrity (ACID compliance). There are variations in the types of transactional guarantees they provide, so it is a good idea to read the fine print. For example, a transaction might be defined by a single read or write to the database rather than the operations wrapped in a BEGIN—COMMIT block.\nNewSQL databases tend to have in common:\n\nData partitioning: the data are divided into partitions (also called shards). Partitions can reside on different machines.\nHorizontal scaling: because of sharding, the database can accommodate an increase in size by adding more machines.\nReplication: The data appears in the database more than once. Replicates can be stored on the same or different machines, availability zones or regions. For example, one machine can serve as the primary node for some partitions and as a backup node for replicates of other partitions. This provides failure resilience and the ability to recover the database from disasters.\nConcurrency control: to maintain data integrity under concurrent transactions.\nFailure resilience: the databases recover to a previous consistent state when machines fail or the database crashes.\n\nThe following are examples of NewSQL databases:\n\nCockroachDB\nVoltDB\nSingleStoreDB\nClustrixDB\nPivotal Gemfire\nNuoDB\n\n\n\nNon-relational Database Designs\nBecause non-relational (NoSQL) databases have become popular and play an important role in data analytics, we want to spend a few moments on their principal architecture. The primary designs for NoSQL databases are\n\nKey-value stores\nDocument stores\nGraph databases\n\nNon-relational databases can be multi-model and support more than one design. Many graph databases on the DB-Engines list support document store, key-value, and graph models.\n\nKey-value stores\nIn a key-value store, every item in the database has only two fields, a key and a value. For example, the key could be a product number and the value could be a product name. This seems restrictive, but the value does not have to be a scalar, it can be, for example, a JSON documents with subfields.\n\nExamples of keys and values in key-value stores. The schema-less nature of the store is apparent in the last two table rows. Two keys can be associated with values of different types. Database items do not have to conform to a specific structure except that an item consists of a key and a value.\n\n\n\n\n\n\nKey\nValue2\n\n\n\n\n“username”\n“oschabenberger”\n\n\n“account type”\n“personal”\n\n\n345625\n{\nname: “MacBook Pro 16”,\nprocessor: “M2”,\nmemory: 32GB”\n}\n\n\n165437\n{\nname: “Paper towel”,\navailable: true,\ndiscount: 10%”\n}\n\n\n\nTo achieve horizontal scaling (scaling out) of key-value stores, the data are partitioned across multiple machines based on a hash table. For a database with \\(n\\) machines the hash is essentially a mapping from the key to the integers from 0 to \\(n-1\\) that determines on which machine a particular key is kept. As the database grows (more keys added) more machines can be easily added. Such scaling out is more difficult in relational systems where the relationships between the tables must be maintained as the data is distributed over more machines. That is why not building tables on relations supports horizontal scaling. While documents are grouped into collections (the NoSQL version of a table), there is no association or relationship between one document or any other. The relative ease to scale horizontally is one reason why non-relational databases became so popular for handling large and growing databases.\nDisadvantages of key-value stores are also apparent: items can only be looked up by their (primary) key. Queries that filter on other attributes are less efficient than in relational systems: what is the average order amount of customers who ordered fewer than 5 items in the last 12 month? It is difficult to join data based on keys. They are not as performant for analytical work as relational systems, especially those with columnar storage layers. A good application for key-value stores are CRUD applications where items are merely created, read, updated, and deleted.\n\n\nDocument stores\nA document store extends the simple design of the key-value store. The value is now stored as a document-oriented set of fields in JSON, XML, YAML, BSON, or similar format. While the data in a key-value store is transparent to the application but deliberately opaque to the database, in a document store the values are transparent to the database. This enables more complex queries than just using the primary key, the fields in the document can be queried.\n\n\n\n\n\n\nFigure 10.13: Examples of documents in MongoDB. A simple document on the left and a more complex document on the right. The first field, _id, is a unique identifier of the document.\n\n\n\nThe fields of the documents do not have to be identical across records in the database. For example, documents containing customer names have a field for a middle initial only if the individual has a middle initial. Documents do not contain empty fields. In contrast, a relational system that stores a middle initial will have this field for all records and NULL values are used to signal an empty field when a middle initial is missing.\nLike key-value stores, document databases are great for CRUD applications, but do not perform well for analytical queries and cannot represent relationships and associations. Each document exists as an independent unit unrelated to other documents.\n\n\nGraph databases\nIn graph databases relationships are a first-class citizen. The relationships are not expressed through keys in tables, but through edges that connect nodes (vertices). Nodes are the entities of interest in the database such as people, products, cities. Nodes have attributes (called properties) stored as key-value pairs or documents (Figure 10.14).\n\n\n\n\n\n\nFigure 10.14: A simple graph with 6 nodes and 7 edges.\n\n\n\nBecause of this storage model graph databases are considered NoSQL databases. Labeling them as non-relational databases is only correct with respect to the traditional table-based relational systems. Relationships are a central property of graph databases, and they are more flexible and dynamic compared to RDBMS. Relationships emerge as patterns through the connections of the nodes rather than predefined elements of the database.\nThis list shows the DB-Engines ranking of graph database systems. Note that many of them also support key-value or document stores.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Sources and File Formats</span>"
    ]
  },
  {
    "objectID": "data/sources_and_files.html#cloud-databases",
    "href": "data/sources_and_files.html#cloud-databases",
    "title": "10  Data Sources and File Formats",
    "section": "10.3 Cloud Databases",
    "text": "10.3 Cloud Databases\nAccording to statista, the share of corporate data stored in the cloud has increased from 30% in 2015 to 60% in 2022. The total addressable market (TAM) in cloud computing is estimated to exceed $200 billion per year and is growing by double digits.\nThe market for storing data in the cloud as files, blocks, or generic objects is a staggering $78 billions of that TAM (2022) and the market for cloud databases is $21 billions in 2023. The cumulative aggregate growth rate (CAGR) for cloud storage and cloud databases is 18% and 22%, respectively. What do we take away from these numbers?\n\nCloud computing is one of the most fundamental revolutions in computing in the last 20 years.\nHalf of the addressable market is in storing data, either in databases or in some other form of cloud storage (files, blocks, objects). Getting your data into their cloud is a key element in the business model of the cloud service providers (CSP). Data has gravity and its “gravitational constant” seems to increase as it comes to rest in a cloud data center.\nThe economics of inexpensive cloud storage will continue to erode Big Data storage systems like Hadoop (HDFS).\nTransactional and operational systems are unlikely to be supported by object storage, while it is least expensive it is also least performant. That is where databases optimized for transactions and/or analytic of cloud data come in.\n\nA database is said to be cloud-ready if it can be run on cloud infrastructure. That is true for most databases. It is said to be cloud-native if the database was designed for the cloud and takes advantage of the full functionality of the cloud, for example, horizontal scaling (scale-out) with increased workload, separation of storage and compute, container deployment and Kubernetes orchestration, multi-tenancy, disaster recovery, bottomless storage.\nAmong cloud databases we can broadly distinguish three service models according to who takes on the responsibility for maintaining the infrastructure, the database instance, and the access to the database: self-managed databases, managed services (DBaaS), and serverless databases.\n\nSelf-managed\nA self-managed cloud database is not much different from a database installed on premises, except that it uses cloud infrastructure. As the name suggests, you are responsible for administering and maintaining all aspects of the deployment. The CSP will provide the infrastructure and make sure that it is operational, but you must make sure that the database running on the platform is operational, updated. You can install any database of your choice, including not cloud-native databases.\nSome cloud-native databases offer a self-managed option, but a managed service or serverless offering is more typical when databases were designed specifically for the cloud.\n\n\nManaged Service\nAlso known as database-as-a-service (DBaaS), this deployment methodology is a special case of software as a service, where the software managed on behalf of the user is a database system. In exchange for a subscription, the service provider handles the management of the database, including provisioning, support, and maintenance. The service provider chooses the hardware instances on which the database runs, frequently using shared resources for multiple databases and customers.\nHere are examples of relational and NoSQL databases offered as a managed service:\n\nRelational\n\nSingleStoreDB\nCockroachDB\nAmazon RDS (Relational Database Service)\nAzure SQL\nMotherDuck (SaaS for DuckDB)\nAzure DB for MySQL, PostgreSQL, …\nGoogle BigQuery\nGoogle Spanner\nOracle Database\n\nNoSQL\n\nMongoDB Atlas\nAzure Cosmos DB\nCouchbase\nGoogle Datastore\nGoogle BigTable\nGoogle FireStore\nRedis\nAmazon DynamoDB\n\n\nCloud service providers have been pushing non-relational systems because horizontal scaling fits well with their business model: adding cloud infrastructure drives revenue for the cloud provider.\n\n\nServerless Systems\nWe need to explain what we mean by “serverless computing” because all code executes on a computer (a server) somewhere. Serverless computing does not do away with servers. It eliminates for software developers the particulars of worrying about which servers their code runs on. This sounds a bit like the SaaS model, but there are important distinctions between a serverless and a serverful (e.g., SaaS) system:\n\nIn serverless computing you execute code without managing resource allocation. You provide a piece of code to the CSP, and the cloud automatically provisions the resources necessary to execute the code.\nn serverless computing you pay for the time the code is executing, not for the resources reserved to (eventually) execute the code. The provider of a serverless service is then encouraged to scale back computing resources as much as possible when not in use (known as scale-to-zero).\n\nServerless computing can be seen as the latest form of virtualization in computing. The user writes a cloud function and ties it to a trigger that runs the function, for example, when a customer opens an online shopping cart. The serverless system then takes care of everything else such as instance selection, logging, scaling, security, etc.\nNot all applications are suitable for serverless computing and for some time databases where thought to be among the backend services that do not fit with the serverless paradigm (see, for example, this view from Berkeley):\n\nServerless computing is essentially stateless computing: the state necessary to execute code is either sent over an API along with the request or is stored somewhere server-side between function calls.\nDatabases have a lot of state, such as connection protocols, metadata, access controls, schemas, etc.\nDatabases often use connection-based protocols which assume a stable connection over a port between a host and a client. That conflicts with the design of serverless systems.\nDatabases have a cold-start problem. A cold start is the time required to instantiate an environment and to get things up and running when a function is called for the first time. In a serverful environment you encounter a cold start only once at the beginning. In a serverless environment cold starts happen the first time you invoke a service and every time the system has scaled back. For example, if the serverless database quiesces after 5 minutes of inactivity and you submit queries every 10 minutes then every query must go through an instantiation of the database environment, including loading the data.\n\nDespite these challenges there has been a lot of progress in serverless database systems in recent years. Here is an incomplete list of relational and NoSQL serverless cloud databases. Notice that the list contains databases listed earlier; some providers make DBaaS and serverless options available:\n\nRelational\n\nMotherDuck (collaborative serverless platform built on DuckDB)\nNeon Serverless Postgres\nPlanetScale DB (Serverless MySQL)\nCockroachDB Serverless\nAmazon Aurora Serverless\n\nNoSQL\n\nFauna DB\nGoogle FireStore\nAmazon DynamoDB\nMongoDB Serverless\n\n\nA special shout-out to DuckDB, a lightweight, embedded, analytical RDBMS that runs inside a host process, for example, inside a Python or R session. DuckDB integrates very well with other systems; it makes working with a relational system from Python or R very easy. MotherDuck is a serverless platform and cloud database built on DuckDB. The integration between the two makes working with databases locally and in the cloud particularly easy. And since DuckDB is optimized for analytic workloads, DuckDB & MotherDuck are great choices for data scientists. We will cover more about DuckDB and MotherDuck in the chapters that follow.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Sources and File Formats</span>"
    ]
  },
  {
    "objectID": "data/sources_and_files.html#enterprise-data-systems",
    "href": "data/sources_and_files.html#enterprise-data-systems",
    "title": "10  Data Sources and File Formats",
    "section": "10.4 Enterprise Data Systems",
    "text": "10.4 Enterprise Data Systems\nWe used the term data warehouse frequently throughout this document and made several references to data lake. What are they and how do data marts fit into the picture?\nData warehouses and data lakes represent two distinct philosophies to store data and to make it available in enterprises. Many organizations have both a data warehouse and a data lake, some enterprises have several of each. The Data lakehouse is a recent development aimed at combining the pros of warehouses and lakes into a single architecture.\n\nData Warehouse\nThe main differences between a data warehouse, also called an enterprise data warehouse (EDW), and a data lake are the level of curation of the data, the storage technology, and the level of access. The data in data warehouses is highly organized and curated and access to it tends to be more tightly controlled than in a data lake.\nA data warehouse contains structured data for business intelligence, reporting, and visualization. Data warehouses are updated with data from transactional source systems such as CRM, ERP, Salesforce on a regular schedule. Data are extracted to a staging area where they are transformed, normalized, and enriched before being loaded into the data warehouse. This process of extracting—transformation—loading is abbreviated as the ETL approach.\n\n\n\n\n\n\nFigure 10.15: An enterprise data warehouse (EDW) is loaded with data transformed and curated in a staging area.\n\n\n\nData warehouses consist of schema-on-write tables—dimension and fact tables—and indexes that support consistency (ACID) and are optimized for analytical queries. Fact tables hold numerical data and primary keys; the dimension tables hold the descriptive information for all fields included in a fact table. A typical example is to store orders in a fact table and customers and products information in dimension tables. When relations between the fact and dimension tables are represented by a single join, the arrangement is called a star schema due to the central role of the fact table.\n\n\n\n\n\n\nFigure 10.16: One fact table (Orders) and four dimension tables in a star schema. PK denotes primary keys, FK denotes foreign keys. The fact table has foreign keys that support relations with the dimension table. Source.\n\n\n\nThe star schema is one of the simplest ways of organizing data in a data warehouse. The snowflake schema has a more complex structure with normalized dimension tables (without duplicates) and possibly multiple levels of joins.\n\n\n\n\n\n\nFigure 10.17: Snowflake schema with up to two levels of relational joins. Source.\n\n\n\nThe snowflake schema in the preceding figure can require two levels of table joins to query data. For example, to calculate revenue by country requires a join of the fact table with the join of the `Dealer` and Country tables. In a star schema the location and country information would be incorporated into the `Dealer` table; that would increase the size of that dimension table due to multiple locations within countries and multiple dealers at locations, but it would simplify the relationship among the tables.\nData warehouses uses SQL as the primary interface, are highly relational, ACID compliant, and schema-dependent—these are attributes of relational database management systems. But you should not equate EDWs with any specific RDBMS. Data warehouses are built on relational database technology but not every RDMBS can serve as a data warehouse. Examples of data warehouses include:\n\nTeradata (on-premises) and Teradata Vantage (cloud-based)\nGoogle BigQuery\nAmazon Redshift\nMicrosoft SQL Server\nOracle Exadata\nIBM Db2 and IBM Infosphere\nIBM Netezza\nSAP/HANA and SAP Datasphere\nSnowflake\nYellowbrick\n\n\n\nData Mart\nA data mart is a section of a data warehouse where data is curated for a specific line of business, team, or organizational unit. For example, a data warehouse might contain a data mart for the Marketing organization, a data mart for the R&D organization, and a data mart for the Finance team. These teams have very different needs and different levels of access privileges.\n\n\n\n\n\n\nFigure 10.18: A data warehouses supports different data marts. The teams access their data marts rather than the data warehouse directly.\n\n\n\n\n\nData Lake\nData warehouses have dominated enterprise data storage and analysis for decades. They are not without drawbacks and their disadvantages were amplified during the rise of Big Data with new data types, new workloads, and the need for more flexibility. Data warehouses are often expensive, custom-built appliances that do not scale out easily. They use read-on-write schemas with proprietary storage formats. First Hadoop with the Hadoop Distributed File System (HDFS) and then cloud object storage (Amazon S3, Azure Blob Storage, Google Cloud Storage) presented a much cheaper storage option to reimagine data storage, data curation, and data access. Open-source data formats such as Parquet, ORC, and Avro presented an alternative to storing data in a proprietary format and promised multi-use of the data.\nData lakes were born as centralized repositories where data is stored in raw form. The name suggests that data are like water in a lake, free flowing.\nA common data lake architecture is the medallion system named after bronze, silver, and gold medals awarded in competition. The structure and quality improve as one moves from the bronze to the silver to the gold tier. The bronze layer contains the raw data in formats such as CSV, JSON, XML, XLS and is not accessible by the end user. From here the data are cleansed and enriched and formatted into open-source formats such as parquet or Avro. The data in the silver layer is validated and standardized, schemas are defined but can evolve as needed. Users of the silver layer are data scientists and data analysts who perform self-service analysis, data science, and machine learning. Data engineers also use the silver layer to structure and curate data even more for project-specific databases that you find in the gold layer.\n\n\n\n\n\n\nFigure 10.19: Medallion data lake design.\n\n\n\nThe data can be structured, unstructured, or semi-structured. Data lakes support many storage and file formats—CSV, JSON, Parquet, ORC, and Avro are common file formats for data sets. The data is kept in the lake in raw form until it is needed. RDBMS use databases and tables to organize the data. NoSQL databases use document collections and key-value pairs to organize the data. File systems use folders and files to organize files. A data lake uses a flat structure where elements have unique identifiers and are tagged with metadata. Like a NoSQL database, data lakes are schema-on-read systems.\nData lakes are another result of the Big Data era. Increasingly heavy analytical workloads that burned CPU cycles and consumed memory were not welcome in databases and data warehouses that were meant to serve existing business reporting needs. A new data architecture was needed where data scientists and machine learning engineers can go to work, unencumbered by the rigidity of existing data infrastructure and not encumbering the existing data infrastructure with additional number crunching.\nThe data lake is manifestation of the belief that great insights will (magically) result when you throw together all your data. When data are stored in a data lake without a pre-defined reason and in arbitrary formats, and poorly organized, they can quickly turn into data swamps.\nHere are some vendors of data lake solutions:\n\nMicrosoft Azure Data Lake\nGoogle Cloud’s Data Lake\nAWS Lake Formation\nCloudera Data Lake Service\nIBM data lake solution\nDelta Lake (an open-source metadata layer that sits on top of open file formats like Delta and Parquet)\n\nData lakes are flexible, scale horizontally (scale-out), and are quite cost effective. They are often built on cheap but scalable object/block/file storage systems that helps reduce costs at the expense of performance. A data warehouse on the other hand is a highly optimized, highly performant system for business intelligence. EDWs often come in the form of packaged appliances (Teradata, Oracle Exadata, IBM Netezza) that makes scaling more difficult. These systems are easier to scale up (adding more memory, more CPUs, etc.) rather than scale horizontally (scale out by adding more machines). EDWs support ACID transactions and allow updates, inserts, and deletes of data. Altering data in a data lake is very difficult, it supports mostly append operations.\nData science and machine learning is more directed toward the data lake whereas business intelligence is directed at the warehouse. Can the two worlds come together, enabling cost-effective and performant business intelligence and data science on all data in a central place? Maybe. That is the premise and promise of the data lakehouse!\n\n\nData Lakehouse\nThe term data lakehouse was coined by Databricks:\n\na new, open data management architecture that combines the flexibility, cost-efficiency, and scale of data lakes with the data management and ACID transactions of data warehouses, enabling business intelligence (BI) and machine learning (ML) on all data.\n\nThe lakehouse adds a layer on top of the low-cost storage of a data lake that provides data structures and data management akin to a data warehouse.\n\n\n\n\n\n\nFigure 10.20: Data warehouse, data lake, and data lakehouse as seen by Databricks. Source.\n\n\n\nThis is a compelling vision that, if it delivers what it promises, would be a major step into the right direction: to reduce the number of data architectures while enabling more teams to work with data.\nIt is early days for the data lakehouse but there is considerable momentum. Some vendors are quick to point out that their cloud data warehouses also operate as a lakehouse. Other vendors position SQL query engines on top of S3 object storage as lakehouse solutions. Then there are solutions designed as a data lakehouse, e.g.,\n\nDatabricks Lakehouse\nOracle Cloud Infrastructure Data Lakehouse\nBlossom Sky\nDataLakeHouse.io\n\n\n\n\nFigure 10.1: A CSV file with ten records.\nFigure 10.2: Elements of a JSON document. Because values can contain objects and arrays, JSON documents can be highly structured and deeply nested.\nFigure 10.3: A simple JSON document. The entire document is a name—value pair with name “menu”. The value is an object with names “id”, “value”, and “popup”. The value of “popup” is an object with name “menuitem” whose value is an array. The elements of the array are objects with names “value” and “onclick”.\nFigure 10.4: The Parquet file architecture. Chunks of columns are stored in row groups. The footer contains important metadata. Source: Parquet File Format: Everything You Need to Know, by Nikola Ilic.\nFigure 10.5: Layout of an ORC file in stripes. Source.\nFigure 10.6: Database market share 2011—2021, according to Gardner. Source.\nFigure 10.7: Schema on write with PostgreSQL. The CREATE TABLE statement creates the table named titanic and defines the schema: names, order, and data types of columns and default values. The \\copy statement populates the database table with data.\nFigure 10.8: An example of schema on read with Hadoop and its file system (HDFS). The data are loaded into Hadoop with the hdfs dfs command. No schema is defined at this stage. The data is then processed with the hadoop command. The customer-mapper.py Python script determines how the data is interpreted for this job; the Python script applies the schema on read.\nFigure 10.9: Scaling up (vertically) and scaling out (horizontally). Scaling up adds more rooms to an existing building, scaling out adds more buildings.\nFigure 10.10: Layout in memory of row 1 of Iris data set with four-byte storage of floating-point numbers and 12-byte storage for the species string. (It seems that the number 5.1 uses 3 bytes for storage. It uses all 4 x 8 = 32 bits of the 4 bytes to store the number. The most significant bit stores the sign, the next 8 bits store the exponent, the following 23 bits store the mantissa of the float.)\nFigure 10.11: The first three rows of the Iris data set in row-store form with contiguous memory.\nFigure 10.12: The seven observations of the Iris data set in column-store format.\nFigure 10.13: Examples of documents in MongoDB. A simple document on the left and a more complex document on the right. The first field, _id, is a unique identifier of the document.\nFigure 10.14: A simple graph with 6 nodes and 7 edges.\nFigure 10.15: An enterprise data warehouse (EDW) is loaded with data transformed and curated in a staging area.\nFigure 10.16: One fact table (Orders) and four dimension tables in a star schema. PK denotes primary keys, FK denotes foreign keys. The fact table has foreign keys that support relations with the dimension table. Source.\nFigure 10.17: Snowflake schema with up to two levels of relational joins. Source.\nFigure 10.18: A data warehouses supports different data marts. The teams access their data marts rather than the data warehouse directly.\nFigure 10.19: Medallion data lake design.\nFigure 10.20: Data warehouse, data lake, and data lakehouse as seen by Databricks. Source.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Sources and File Formats</span>"
    ]
  },
  {
    "objectID": "data/data_access.html",
    "href": "data/data_access.html",
    "title": "11  Data Access",
    "section": "",
    "text": "11.1 Accessing Local Files",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Access</span>"
    ]
  },
  {
    "objectID": "data/data_access.html#accessing-local-files",
    "href": "data/data_access.html#accessing-local-files",
    "title": "11  Data Access",
    "section": "",
    "text": "CSV files\nTo convert CSV files to a pandas DataFrame use the pandas read_csv() method. The default behavior is to look for a header line with column names, to interpret commas as field delimiters, to skip blank lines in the file, and to apply utf-8 encoding.\nConsider the following file, fitness.csv:\ndata&gt; head fitness.csv\nAge,Weight,Oxygen,RunTime,RestPulse,RunPulse,MaxPulse\n44,89.47,44.609,11.37,62,178,182\n40,75.07,45.313,10.07,62,185,185\n44,85.84,54.297, 8.65,45,156,168\n42,68.15,59.571, 8.17,40,166,172\n38,89.02,49.874, 9.22,55,178,180\nThe first line contains comma-separated names for the columns. The following lines contain the data for the attributes, one record per line. To convert the CSV file to a pandas DataFrame use the read_csv() method:\n\nimport numpy as np\n\ndf = pd.read_csv(\"../datasets/fitness.csv\")\nprint(\"Default result from read_csv\")\nprint(df.head())\ndf.describe(include=[np.float64])\n\nDefault result from read_csv\n   Age  Weight  Oxygen  RunTime  RestPulse  RunPulse  MaxPulse\n0   44   89.47  44.609    11.37         62       178       182\n1   40   75.07  45.313    10.07         62       185       185\n2   44   85.84  54.297     8.65         45       156       168\n3   42   68.15  59.571     8.17         40       166       172\n4   38   89.02  49.874     9.22         55       178       180\n\n\n\n\n\n\n\n\n\n\nWeight\nOxygen\nRunTime\n\n\n\n\ncount\n31.000000\n31.000000\n31.000000\n\n\nmean\n77.444516\n47.375806\n10.586129\n\n\nstd\n8.328568\n5.327231\n1.387414\n\n\nmin\n59.080000\n37.388000\n8.170000\n\n\n25%\n73.200000\n44.964500\n9.780000\n\n\n50%\n77.450000\n46.774000\n10.470000\n\n\n75%\n82.325000\n50.131000\n11.270000\n\n\nmax\n91.630000\n60.055000\n14.030000\n\n\n\n\n\n\n\n\nYou can assign your own variable names with the names= option:\n\ndf2 = pd.read_csv(\"../datasets/fitness.csv\", \n                  header=0,\n                  names=[\"Age\", \"Wgt\",\"Oxy\",\"RT\",\"ReP\",\"RuP\",\"MP\" ])\ndf2.head()\n\n\n\n\n\n\n\n\n\nAge\nWgt\nOxy\nRT\nReP\nRuP\nMP\n\n\n\n\n0\n44\n89.47\n44.609\n11.37\n62\n178\n182\n\n\n1\n40\n75.07\n45.313\n10.07\n62\n185\n185\n\n\n2\n44\n85.84\n54.297\n8.65\n45\n156\n168\n\n\n3\n42\n68.15\n59.571\n8.17\n40\n166\n172\n\n\n4\n38\n89.02\n49.874\n9.22\n55\n178\n180\n\n\n\n\n\n\n\n\nString variables are indicated in the CSV file with quotes:\ndata&gt;  head herding-cats.csv\n\"address_full\",\"street\",\"coat\",\"sex\",\"age\",\"weight\",\"fixed\",\"wander_dist\",\"roamer\",\"cat_id\"\n\"15 Fillmer Ave Los Gatos 95030\",\"15 Fillmer Ave\",\"brown\",\"male\",5.594,5.016,1,0.115,\"yes\",1\n\"244 Harding Ave Los Gatos 95030\",\"244 Harding Ave\",\"tabby\",\"male\",6.852,6.314,1,0.129,\"yes\",2\n\"16570 Marchmont Dr Los Gatos 95032\",\"16570 Marchmont Dr\",\"maltese\",\"female\",4.081, 2.652,0, 0.129,\"yes\",3\n\"100 Stonybrook Rd Los Gatos 95032\",\"100 Stonybrook Rd\",\"tabby\",\"female\",3.806,3.413,1, 0.107,\"yes\",4\n\ncats = pd.read_csv(\"../datasets/herding-cats.csv\",\n                   index_col=\"cat_id\")\nprint(cats.head())\n\n                              address_full              street     coat  \\\ncat_id                                                                    \n1           15 Fillmer Ave Los Gatos 95030      15 Fillmer Ave    brown   \n2          244 Harding Ave Los Gatos 95030     244 Harding Ave    tabby   \n3       16570 Marchmont Dr Los Gatos 95032  16570 Marchmont Dr  maltese   \n4        100 Stonybrook Rd Los Gatos 95032   100 Stonybrook Rd    tabby   \n5           266 Kennedy Rd Los Gatos 95032      266 Kennedy Rd   calico   \n\n           sex    age  weight  fixed  wander_dist roamer  \ncat_id                                                    \n1         male  5.594   5.016      1        0.115    yes  \n2         male  6.852   6.314      1        0.129    yes  \n3       female  4.081   2.652      0        0.129    yes  \n4       female  3.806   3.413      1        0.107    yes  \n5         male  5.164   6.820      1        0.039     no  \n\n\nPandas is smart about assigning data types to columns, note that string columns are stored as data type object.\n\ncats.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 400 entries, 1 to 400\nData columns (total 9 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   address_full  400 non-null    object \n 1   street        394 non-null    object \n 2   coat          400 non-null    object \n 3   sex           400 non-null    object \n 4   age           400 non-null    float64\n 5   weight        400 non-null    float64\n 6   fixed         400 non-null    int64  \n 7   wander_dist   400 non-null    float64\n 8   roamer        400 non-null    object \ndtypes: float64(3), int64(1), object(5)\nmemory usage: 31.2+ KB\n\n\nYou can overwrite the data types by supplying either a default type or a dictionary of types. To change the data type for the fixed column, for example:\n\ncats = pd.read_csv(\"../datasets/herding-cats.csv\",\n                   dtype={'fixed':np.float16})\n\nMissing values (see below) can be indicated in CSV files in several ways. The common technique is to leave the entry for the unobserved value empty (no spaces). In the following file the value for appraisal is missing in the third row. The fifth row contains values for land and appraisal only.\ndata&gt; head landsales2.csv\nland, improve, total, sale, appraisal\n30000,64831,94831,118500,1.25\n30000,50765,80765,93900,1.16\n46651,18573,65224,,\n45990,91402,137392,184000,1.34\n42394,,,168000,\n,133351,,169000,\n63596,2182,65778,,\n56658,153806,210464,255000,1.21\nread_csv() converts this file correctly to missing values using the NaN (not-a-number) sentinel for the unobserved data:\n\nland = pd.read_csv(\"../datasets/landsales2.csv\")\nland\n\n\n\n\n\n\n\n\n\nland\nimprove\ntotal\nsale\nappraisal\n\n\n\n\n0\n30000.0\n64831.0\n94831.0\n118500.0\n1.25\n\n\n1\n30000.0\n50765.0\n80765.0\n93900.0\n1.16\n\n\n2\n46651.0\n18573.0\n65224.0\nNaN\nNaN\n\n\n3\n45990.0\n91402.0\n137392.0\n184000.0\n1.34\n\n\n4\n42394.0\nNaN\nNaN\n168000.0\nNaN\n\n\n5\nNaN\n133351.0\nNaN\n169000.0\nNaN\n\n\n6\n63596.0\n2182.0\n65778.0\nNaN\nNaN\n\n\n7\n56658.0\n153806.0\n210464.0\n255000.0\n1.21\n\n\n8\n51428.0\n72451.0\n123879.0\nNaN\nNaN\n\n\n9\n93200.0\nNaN\nNaN\n422000.0\nNaN\n\n\n10\n76125.0\n78172.0\n275297.0\n290000.0\n1.14\n\n\n11\n154360.0\n61934.0\n216294.0\n237000.0\n1.10\n\n\n12\n65376.0\nNaN\nNaN\n286500.0\nNaN\n\n\n13\n42400.0\nNaN\nNaN\nNaN\nNaN\n\n\n14\n40800.0\n92606.0\n133406.0\n168000.0\n1.26\n\n\n\n\n\n\n\n\nIn addition to empty entries, read.csv() interprets a number of entries as missing (unobserved) values: “#N/A”, “#N/A N/A”, “#NA”, “-1.#IND”, “-1.#QNAN”, “-NaN”, “-nan”, “1.#IND”, “1.#QNAN”, “&lt;NA&gt;”, “N/A”, “NA”, “NULL”, “NaN”, “None”, “n/a”, “nan”, “null “. Despite those, CSV files might have special values to indicate missing values, for example, when they are exported from other software packages. Suppose that the missing values are indicated by a dot (“.”). Also, the columns in the CSV file are formatted with white space:\ndata&gt; head landsales.csv\nland, improve, total, sale, appraisal\n30000,     64831,     94831,    118500,   1.25\n30000,     50765,     80765,     93900,   1.16\n46651,     18573,     65224,         .,    .\n45990,     91402,    137392,    184000,   1.34\n42394,         .,         .,    168000,    .\n    .,    133351,         .,    169000,    .\n63596,      2182,     65778,         .,    .\n56658,    153806,    210464,    255000,   1.21\n51428,     72451,    123879,         .,    .\nThe file is much more human readable, but requires additional options to convert to the intended DataFrame:\n\nland2 = pd.read_csv(\"../datasets/landsales.csv\", \n                   na_values=\".\",\n                   skipinitialspace=True)\nland2.head()\n\n\n\n\n\n\n\n\n\nland\nimprove\ntotal\nsale\nappraisal\n\n\n\n\n0\n30000.0\n64831.0\n94831.0\n118500.0\n1.25\n\n\n1\n30000.0\n50765.0\n80765.0\n93900.0\n1.16\n\n\n2\n46651.0\n18573.0\n65224.0\nNaN\nNaN\n\n\n3\n45990.0\n91402.0\n137392.0\n184000.0\n1.34\n\n\n4\n42394.0\nNaN\nNaN\n168000.0\nNaN\n\n\n\n\n\n\n\n\nThe na.values= option specifies additional values that should be recognized as missing values. The skipinitalspace= option tellsread_csv() to ignore spaces following the delimiters. The result is as intended.\n\n\nJSON files\nPandas has great support for reading and writing JSON files. A general issue is that JSON is a hierarchical data format that allows nested data structures whereas pandas DataFrames have a row—column layout. Fortunately, the pandasread_json() method has many options to shape nested JSON structures into DataFrames.\nYou can convert any JSON document into a Python dictionary with the load() function in the json library. Suppose we want to read this document, stored in ../datasets/JSON/simple1.json into Python:\n&gt; cat ../datasets/JSON/simple1.json\n{\n    \"firstName\": \"Jane\",\n    \"lastName\": \"Doe\",\n    \"hobbies\": [\"running\", \"sky diving\", \"singing\"],\n    \"age\": 35,\n    \"children\": [\n        {\n            \"firstName\": \"Alice\",\n            \"age\": 6\n        },\n        {\n            \"firstName\": \"Bob\",\n            \"age\": 8\n        }\n    ]\n}\n\nimport numpy as np\nimport pandas as pd\nimport json\nwith open(\"../datasets/JSON/simple1.json\",\"r\") as f:\n    data = json.load(f)\ndata\n\n{'firstName': 'Jane',\n 'lastName': 'Doe',\n 'hobbies': ['running', 'sky diving', 'singing'],\n 'age': 35,\n 'children': [{'firstName': 'Alice', 'age': 6},\n  {'firstName': 'Bob', 'age': 8}]}\n\n\nThe next example reads a record-oriented JSON file (../datasets/JSON/simple.json) directly into a DataFrame. Each record consists of fields id, name, math, statistics, and weight.\ncat simple.json \n[\n  {\n    \"id\": \"A00123\",\n    \"name\": \"David\",\n    \"math\": 70,\n    \"statistics\": 86,\n    \"weight\": 156.3\n  },\n  {\n    \"id\": \"B00422\",\n    \"name\": \"Andrew\",\n    \"math\": 89,\n    \"statistics\": 80,\n    \"weight\": 210.6\n  },\n  {\n    \"id\": \"C004543\",\n    \"name\": \"Tobias\",\n    \"math\": 79,\n    \"statistics\": 90,\n    \"weight\": 167.0\n  }\n]\n\ndf = pd.read_json(\"../datasets/JSON/simple.json\")\ndf.info()\ndf\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   id          3 non-null      object \n 1   name        3 non-null      object \n 2   math        3 non-null      int64  \n 3   statistics  3 non-null      int64  \n 4   weight      3 non-null      float64\ndtypes: float64(1), int64(2), object(2)\nmemory usage: 252.0+ bytes\n\n\n\n\n\n\n\n\n\n\nid\nname\nmath\nstatistics\nweight\n\n\n\n\n0\nA00123\nDavid\n70\n86\n156.3\n\n\n1\nB00422\nAndrew\n89\n80\n210.6\n\n\n2\nC004543\nTobias\n79\n90\n167.0\n\n\n\n\n\n\n\n\nIf you’d rather assign data type string to the id and name fields than data type object, you can set the data type with\n\ndf2 = pd.read_json(\"../datasets/JSON/simple.json\", \n                   dtype= {\"id\": \"string\", \"name\" : \"string\"})\ndf2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   id          3 non-null      string \n 1   name        3 non-null      string \n 2   math        3 non-null      int64  \n 3   statistics  3 non-null      int64  \n 4   weight      3 non-null      float64\ndtypes: float64(1), int64(2), string(2)\nmemory usage: 252.0 bytes\n\n\nHow can we handle a nested JSON structure? The data in ../datasets/JSON/nested_array.json is\ndata&gt; cat nested_array.json\n{\n    \"school_name\": \"Virginia Tech\",\n    \"class\": \"Class of 2026\",\n    \"students\": [\n      {\n        \"id\": \"A00123\",\n        \"name\": \"David\",\n        \"math\": 70,\n        \"statistics\": 86,\n        \"weight\": 156.3\n      },\n      {\n        \"id\": \"B00422\",\n        \"name\": \"Andrew\",\n        \"math\": 89,\n        \"statistics\": 80,\n        \"weight\": 210.6\n      },\n      {\n        \"id\": \"C004543\",\n        \"name\": \"Tobias\",\n        \"math\": 79,\n        \"statistics\": 90,\n        \"weight\": 167.0\n      }]\n}\nThe students fields have a record structure but there are additional fields school_name and class that do not fit that structure. Converting the JSON file directly into a DataFrame does not produce the desired result:\n\ndf3 = pd.read_json('../datasets/JSON/nested_array.json')\ndf3\n\n\n\n\n\n\n\n\n\nschool_name\nclass\nstudents\n\n\n\n\n0\nVirginia Tech\nClass of 2026\n{'id': 'A00123', 'name': 'David', 'math': 70, ...\n\n\n1\nVirginia Tech\nClass of 2026\n{'id': 'B00422', 'name': 'Andrew', 'math': 89,...\n\n\n2\nVirginia Tech\nClass of 2026\n{'id': 'C004543', 'name': 'Tobias', 'math': 79...\n\n\n\n\n\n\n\n\nTo create the appropriate DataFrame we take two steps: convert the JSON file into a dictionary, then shape the dictionary into a DataFrame with json_normalize():\n\nwith open('../datasets/JSON/nested_array.json','r') as f:\n    data = json.load(f)\n\ndf4 = pd.json_normalize(data, \n                        record_path =[\"students\"],\n                        meta=[\"school_name\", \"class\"])\ndf4\n\n\n\n\n\n\n\n\n\nid\nname\nmath\nstatistics\nweight\nschool_name\nclass\n\n\n\n\n0\nA00123\nDavid\n70\n86\n156.3\nVirginia Tech\nClass of 2026\n\n\n1\nB00422\nAndrew\n89\n80\n210.6\nVirginia Tech\nClass of 2026\n\n\n2\nC004543\nTobias\n79\n90\n167.0\nVirginia Tech\nClass of 2026\n\n\n\n\n\n\n\n\nThe meta= option lists the JSON fields that are used as data that applies to each record in the result table. The record_path= option points at the field that contains the list of records.\nNow suppose that in addition to the list of records the JSON metadata contains a more complex structure:\ncat nested_obj_array.json\n{\n   \"school_name\": \"Virginia Tech\",\n   \"class\": \"Class of 2026\",\n    \"info\": {\n      \"president\": \"Timothy D. Sands\",\n      \"address\": \"Office of the President\",\n      \"social\": {\n        \"LinkedIn\": \"tim-sands-49b8b95\",\n        \"Twitter\": \"@VTSandsman\"\n      }\n    },\n    \"students\": [\n      {\n        \"id\": \"A00123\",\n        \"name\": \"David\",\n        \"math\": 70,\n        \"statistics\": 86,\n        \"weight\": 156.3\n      },\n. . .\n      }]\n}\nYou can specify which fields to extract from the metadata in the meta= option of json_normalize():\n\nwith open('../datasets/JSON/nested_obj_array.json','r') as f:\n    data = json.load(f)\n\ndf5 = pd.json_normalize(data,\n                       record_path =[\"students\"], \n                       meta=[\"school_name\",\"class\",\n                             [\"info\", \"president\"], \n                             [\"info\", \"social\", \"Twitter\"]])\ndf5\n\n\n\n\n\n\n\n\n\nid\nname\nmath\nstatistics\nweight\nschool_name\nclass\ninfo.president\ninfo.social.Twitter\n\n\n\n\n0\nA00123\nDavid\n70\n86\n156.3\nVirginia Tech\nClass of 2026\nTimothy D. Sands\n@VTSandsman\n\n\n1\nB00422\nAndrew\n89\n80\n210.6\nVirginia Tech\nClass of 2026\nTimothy D. Sands\n@VTSandsman\n\n\n2\nC004543\nTobias\n79\n90\n167.0\nVirginia Tech\nClass of 2026\nTimothy D. Sands\n@VTSandsman\n\n\n\n\n\n\n\n\nHow are missing values handled? In the next example, the first has a complete record. The second student misses a statistics score, the third student has no weight data. pd.json_normalize() fills in NaN for the unobserved data.\n\ndata = [\n    {\n        \"students\": [\n        {\n            \"id\": \"A00123\",\n            \"name\": \"David\",\n            \"math\": 70,\n            \"statistics\": 86,\n            \"weight\": 156.3\n        },\n        {\n            \"id\": \"B00422\",\n            \"name\": \"Andrew\",\n            \"math\": 89,\n            \"weight\": 210.6\n          },\n        {\n            \"id\": \"C004543\",\n            \"name\": \"Tobias\",\n            \"math\": 79,\n            \"statistics\": 90,\n        }\n        ]\n}\n]\n\ndf6 = pd.json_normalize(data, record_path =[\"students\"])\ndf6\n\n\n\n\n\n\n\n\n\nid\nname\nmath\nstatistics\nweight\n\n\n\n\n0\nA00123\nDavid\n70\n86.0\n156.3\n\n\n1\nB00422\nAndrew\n89\nNaN\n210.6\n\n\n2\nC004543\nTobias\n79\n90.0\nNaN\n\n\n\n\n\n\n\n\njson_normalize() is less forgiving if fields in the metadata are unobserved; it will throw an error. To prevent the error and assign NaN values, use the errors=’ignore’ option. In the following example the teachers meta data field does not always have a l_name entry.\n\ndata = [\n    { \n        'class': 'STAT 5525', \n        'student count': 60, \n        'room': 'Yellow',\n        'teachers': { \n                'f_name': 'Elon', \n                'l_name': 'Musk',\n            },\n        'students': [\n            { 'name': 'Tom', 'sex': 'M' },\n            { 'name': 'James', 'sex': 'M' },\n        ]\n    },\n    { \n        'class': 'STAT 5526', \n        'student count': 45, \n        'room': 'Blue',\n         'teachers': { \n                'f_name': 'Albert'\n            },\n        'students': [\n            { 'name': 'Tony', 'sex': 'M' },\n            { 'name': 'Jacqueline', 'sex': 'F' },\n        ]\n    },\n]\n\ndf7 = pd.json_normalize(data, \n                        record_path =[\"students\"], \n                        meta=[\"class\", \"room\", [\"teachers\", \"l_name\"]],\n                        errors='ignore')\n\ndf7\n\n\n\n\n\n\n\n\n\nname\nsex\nclass\nroom\nteachers.l_name\n\n\n\n\n0\nTom\nM\nSTAT 5525\nYellow\nMusk\n\n\n1\nJames\nM\nSTAT 5525\nYellow\nMusk\n\n\n2\nTony\nM\nSTAT 5526\nBlue\nNaN\n\n\n3\nJacqueline\nF\nSTAT 5526\nBlue\nNaN\n\n\n\n\n\n\n\n\n\n\nParquet files\nTo read parquet files into pandas DataFrames we have two choices: the read_parquet() pandas method or the read_table() method from pyarrow.parquet.\nThe data we work with in this section is stored in a parquet multi-file structure; that means the data are split into multiple files stored in a local directory:\nParquet ll userdata\ntotal 1120\n-rw-r--r--@ 1 olivers  staff   111K Aug 31 14:36 userdata1.parquet\n-rw-r--r--@ 1 olivers  staff   110K Aug 31 14:36 userdata2.parquet\n-rw-r--r--@ 1 olivers  staff   111K Aug 31 14:37 userdata3.parquet\n-rw-r--r--@ 1 olivers  staff   110K Aug 31 14:37 userdata4.parquet\n-rw-r--r--@ 1 olivers  staff   111K Aug 31 14:37 userdata5.parquet\nEach of the *.parquet files in the directory contains 1,000 observations.\nPandas’ read_parquet() reads the entire directory and combines the records from the multiple files into a single DataFrame.\n\ndf = pd.read_parquet(\"../datasets/Parquet/userdata\", engine=\"pyarrow\")\ndf.info()\ndf.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5000 entries, 0 to 4999\nData columns (total 13 columns):\n #   Column             Non-Null Count  Dtype         \n---  ------             --------------  -----         \n 0   registration_dttm  5000 non-null   datetime64[ns]\n 1   id                 4999 non-null   float64       \n 2   first_name         5000 non-null   object        \n 3   last_name          5000 non-null   object        \n 4   email              5000 non-null   object        \n 5   gender             5000 non-null   object        \n 6   ip_address         5000 non-null   object        \n 7   cc                 5000 non-null   object        \n 8   country            5000 non-null   object        \n 9   birthdate          5000 non-null   object        \n 10  salary             4689 non-null   float64       \n 11  title              5000 non-null   object        \n 12  comments           4966 non-null   object        \ndtypes: datetime64[ns](1), float64(2), object(10)\nmemory usage: 507.9+ KB\n\n\n\n\n\n\n\n\n\n\nregistration_dttm\nid\nsalary\n\n\n\n\ncount\n5000\n4999.000000\n4689.000000\n\n\nmean\n2016-02-03 21:04:13.699399936\n500.598720\n150772.222890\n\n\nmin\n2016-02-03 00:00:07\n1.000000\n12068.960000\n\n\n25%\n2016-02-03 08:51:55.750000128\n251.000000\n83480.380000\n\n\n50%\n2016-02-03 20:01:17\n501.000000\n152877.190000\n\n\n75%\n2016-02-04 07:42:54.500000\n750.500000\n215405.220000\n\n\nmax\n2016-02-04 23:59:55\n1000.000000\n286735.820000\n\n\nstd\nNaN\n288.648331\n78171.513062\n\n\n\n\n\n\n\n\nThe size of df confirms that all 5,000 observations were retrieved from the five component files.\nWith pyarrow we have more control over the files, for example, we can inquire about the schema and metadata of the component files.\n\nimport pyarrow.parquet as pq\n\npq_file = pq.ParquetFile(\"../datasets/Parquet/userdata/userdata1.parquet\")\npq_file.metadata\npq_file.schema\n\n&lt;pyarrow._parquet.ParquetSchema object at 0x147814b00&gt;\nrequired group field_id=-1 hive_schema {\n  optional int96 field_id=-1 registration_dttm;\n  optional int32 field_id=-1 id;\n  optional binary field_id=-1 first_name (String);\n  optional binary field_id=-1 last_name (String);\n  optional binary field_id=-1 email (String);\n  optional binary field_id=-1 gender (String);\n  optional binary field_id=-1 ip_address (String);\n  optional binary field_id=-1 cc (String);\n  optional binary field_id=-1 country (String);\n  optional binary field_id=-1 birthdate (String);\n  optional double field_id=-1 salary;\n  optional binary field_id=-1 title (String);\n  optional binary field_id=-1 comments (String);\n}\n\n\nTo convert all component files into a pandas DataFrame with pyarrow, use the read_table() method and send the result to_pandas():\n\ndf2 = pq.read_table(source=\"../datasets/Parquet/userdata/\").to_pandas()\ndf2.describe()\n\n\n\n\n\n\n\n\n\nregistration_dttm\nid\nsalary\n\n\n\n\ncount\n5000\n4999.000000\n4689.000000\n\n\nmean\n2016-02-03 21:04:13.699399936\n500.598720\n150772.222890\n\n\nmin\n2016-02-03 00:00:07\n1.000000\n12068.960000\n\n\n25%\n2016-02-03 08:51:55.750000128\n251.000000\n83480.380000\n\n\n50%\n2016-02-03 20:01:17\n501.000000\n152877.190000\n\n\n75%\n2016-02-04 07:42:54.500000\n750.500000\n215405.220000\n\n\nmax\n2016-02-04 23:59:55\n1000.000000\n286735.820000\n\n\nstd\nNaN\n288.648331\n78171.513062\n\n\n\n\n\n\n\n\n\n\nORC files\nPandas (and pyarrow) supports ORC files but working with ORC files is not quite as convenient as with parquet files. The read_orc() method reads one ORC file rather than an entire directory and it is not supported on Windows.\n\ndf = pd.read_orc(\"../datasets/ORC/userdata/userdata1.orc\")\n\nWhen data is stored in multi-file format, as in the parquet example, you need to concatenate the contents into a pandas DataFrame. The glob module is helpful to retrieve the list of file names that match a pattern.\n\nimport glob\nfolder_path = \"../datasets/ORC/userdata\"\nfile_list = glob.glob(folder_path + \"/*.orc\")\ndf_from_orc = pd.DataFrame(pd.read_orc(file_list[0]))\n\nfor i in range(1,len(file_list)):\n    data = pd.read_orc(file_list[i])\n    df = pd.DataFrame(data)\n    df_from_orc = pd.concat([df_from_orc,df],axis=0)\n\nYou can also use the read_table() method for ORC files in pyarrow to read ORC an file into a pyarrow table and to_pandas() to convert the table to a pandas DataFrame:\n\nfrom pyarrow import orc\n\ndf = orc.read_table(source=\"../datasets/ORC/userdata/userdata1.orc\").to_pandas()",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Access</span>"
    ]
  },
  {
    "objectID": "data/data_access.html#working-with-a-database",
    "href": "data/data_access.html#working-with-a-database",
    "title": "11  Data Access",
    "section": "11.2 Working with a Database",
    "text": "11.2 Working with a Database\nAs a data scientist you need to be comfortable working with databases. In analytical work, you will encounter schema-centric relational OLAP databases that use SQL (structured query language) to interact with the database and the data. SQL interfaces are standard for data warehouses and common for many data platforms and tools.\nData scientists generally prefer writing Python or R code over SQL. SQL is extremely powerful to express relational concepts and can make some data processing tasks much easier than rolling your own code. Also, databases are optimized for joins, merges, deep queries, aggregations, indexes, handling JSON, etc. It would be a shame not to take advantage of that power or to lose it behind the syntactic sugar of a tool’s API. Many employers have voiced concerns that SQL ranks high among the technical skills data scientists often lack. As the founder and chairman of a data science research and consultancy company put it:\n\nThere are a lot of things folks don’t agree on. But everyone agrees that as a data scientist you will need to retrieve data from a database, and you will need version control. Learn SQL and learn git.\n\nWe are covering SQL basics in Chapter 15 after sprinkling some SQL statements along the way.\nIf you want to work with SQL directly from Python, there are several alternatives.\n\nSQLAlchemy\nSQLAlchemy is a SQL toolkit for Python that gives Python developers the power of SQL. While some tools try to hide SQL and relational details behind syntactic sugar, SQLAlchemy’s philosophy is to fully expose the relational details of SQL within transparent tools. Specific databases, beyond those for which support is built into SQLAlchemy, are supported as dialects.\n\n\nIbis\nIbis is a portable Python dataframe library that supports multiple database backends. After making a connection to a backend database, you interact with the data using Python syntax rather than SQL statements. The Ibis function calls are translated into SQL for the specific backend under the covers. For example, the first statement in the next code block creates a Python object named penguins through which you can interact with the database table of the same name. The second statement filters the data for a specific species.\n\npenguins = con.table(\"penguins\")\npenguins.filter(penguins.species == \"Adelie\")\n\nThe SQL equivalent would be a SELECT statement with a WHERE clause:\nSELECT * FROM penguins WHERE species = “Adelie”\nHaving stable Python code for different database backends makes it easy to move from one database to another. During development of analytic code you might be working with SQLite and the production database is Oracle. The same Python code will work in both environments.\nThe default backend of Ibis is the embedded database DuckDB. If you are working with DuckDB you can issue SQL statements directly from Python or you can rely on its dedicated Python API.\n\n\nDuckDB\nDuckDB has all the ingredients we are looking for in a database for data science work:\n\nDesigned to support analytical queries (OLAP workloads) characterized by complex queries that process large amounts of data.\nColumnar, vectorized storage engine. Columnar storage is more efficient for analytical queries, vectorized queries execute much faster than systems that process rows sequentially.\nProvides transactional guarantees (ACID properties)\nDeeply integrated with Python or R for efficient data analysis.\nEasy to install and operate, without any external dependencies.\nOpen-source database that is free to use.\nCan query CSV, JSON, Parquet files and DataFrames directly.\nSupports a streaming model for analyzing large data sets that exceed memory capacity, e.g., streaming from Parquet files.\nIn-memory database with graceful on-disk operation.\nEasy transition to the cloud through partnership with MotherDuck.\nSQL code can be submitted from the CLI or through the Python API, so you can take advantage of the power of SQL.\nRuns as an embedded database within a host process.\n\nBecause DuckDB is an embedded database, there is no database server to install and to maintain. Because the database runs within the host process, data can be transferred very quickly. And if you want to move the data to the cloud, you can do that easily with the MotherDuck serverless platform.\nThe Python package for DuckDB can run queries directly from Pandas DataFrame without copying any data.\nWhy is it called DuckDB? We let the folks who created it explain it:\n\nDucks are amazing animals. They can fly, walk and swim. They can also live off pretty much everything. They are quite resilient to environmental challenges. A duck’s song will bring people back from the dead and inspires database research. They are thus the perfect mascot for a versatile and resilient data management system. Also the logo designs itself.\n\nFor the Python user, installation is as simple as with any other Python package:\npip install duckdb\nIf you install with conda, the command is\nconda install python-duckdb -c conda-forge\nDuckDB is an in-memory database. If you want to persist the database across Python sessions, then you need to create a connection that stores the data in a single file on disk. Otherwise, the database will go away when the Python session ends. You can work without a connection to a database by using methods on the duckdb module. For example, the following statements directly query a CSV file using DuckDB:\n\nimport duckdb\nduckdb.sql('select * from \"../datasets/fitness.csv\" where Age &gt; 50')\n\n┌───────┬────────┬────────┬─────────┬───────────┬──────────┬──────────┐\n│  Age  │ Weight │ Oxygen │ RunTime │ RestPulse │ RunPulse │ MaxPulse │\n│ int64 │ double │ double │ double  │   int64   │  int64   │  int64   │\n├───────┼────────┼────────┼─────────┼───────────┼──────────┼──────────┤\n│    54 │  83.12 │ 51.855 │   10.33 │        50 │      166 │      170 │\n│    51 │  69.63 │ 40.836 │   10.95 │        57 │      168 │      172 │\n│    51 │  77.91 │ 46.672 │    10.0 │        48 │      162 │      168 │\n│    57 │  73.37 │ 39.407 │   12.63 │        58 │      174 │      176 │\n│    54 │  79.38 │  46.08 │   11.17 │        62 │      156 │      165 │\n│    52 │  76.32 │ 45.441 │    9.63 │        48 │      164 │      166 │\n│    51 │  67.25 │ 45.118 │   11.08 │        48 │      172 │      172 │\n│    54 │  91.63 │ 39.203 │   12.88 │        44 │      168 │      172 │\n│    51 │  73.71 │  45.79 │   10.47 │        59 │      186 │      188 │\n│    57 │  59.08 │ 50.545 │    9.93 │        49 │      148 │      155 │\n│    52 │  82.78 │ 47.467 │    10.5 │        53 │      170 │      172 │\n├───────┴────────┴────────┴─────────┴───────────┴──────────┴──────────┤\n│ 11 rows                                                   7 columns │\n└─────────────────────────────────────────────────────────────────────┘\n\n\nThe object returned by duckdb is a relation, a symbolic representation of the query that can be referenced in other queries. The next statements save the result of the previous query and reference the relation in the second query:\n\nrel = duckdb.sql('select * from \"../datasets/fitness.csv\" where Age &gt; 50')\nduckdb.sql(\"select Oxygen, RunTime, RestPulse from rel where RestPulse &lt; 49\")\n\n┌────────┬─────────┬───────────┐\n│ Oxygen │ RunTime │ RestPulse │\n│ double │ double  │   int64   │\n├────────┼─────────┼───────────┤\n│ 46.672 │    10.0 │        48 │\n│ 45.441 │    9.63 │        48 │\n│ 45.118 │   11.08 │        48 │\n│ 39.203 │   12.88 │        44 │\n└────────┴─────────┴───────────┘\n\n\nThe results of a DuckDB query can be easily converted into other formats. fetchnumpy() fetches data as a dictionary of NumPy arrays, df() fetches a Pandas DataFrame, pl() fetches a Polars DataFrame, and arrow() fetches an Arrow table.\n\nduckdb.sql(\"select Oxygen, RunTime, RestPulse from rel where RestPulse &lt; 49\").df()\n\n\n\n\n\n\n\n\n\nOxygen\nRunTime\nRestPulse\n\n\n\n\n0\n46.672\n10.00\n48\n\n\n1\n45.441\n9.63\n48\n\n\n2\n45.118\n11.08\n48\n\n\n3\n39.203\n12.88\n44\n\n\n\n\n\n\n\n\n\nReading files\nTo read the major data science file formats into DuckDB, use the read_csv(), read_parquet(), read_json(), and read_arrow() methods. For example, to read the simple JSON file from the previous section:\n\nduckdb.read_json('../datasets/JSON/simple.json')\n\n┌─────────┬─────────┬───────┬────────────┬────────┐\n│   id    │  name   │ math  │ statistics │ weight │\n│ varchar │ varchar │ int64 │   int64    │ double │\n├─────────┼─────────┼───────┼────────────┼────────┤\n│ A00123  │ David   │    70 │         86 │  156.3 │\n│ B00422  │ Andrew  │    89 │         80 │  210.6 │\n│ C004543 │ Tobias  │    79 │         90 │  167.0 │\n└─────────┴─────────┴───────┴────────────┴────────┘\n\n\nTo read a directory of Parquet files, you can use wildcard syntax in the path of the read_parquet() function:\n\nduckdb.read_parquet('../datasets/Parquet/userdata/user*.parquet')\n\n┌─────────────────────┬───────┬────────────┬───┬────────────┬───────────┬──────────────────────┬──────────────────────┐\n│  registration_dttm  │  id   │ first_name │ … │ birthdate  │  salary   │        title         │       comments       │\n│      timestamp      │ int32 │  varchar   │   │  varchar   │  double   │       varchar        │       varchar        │\n├─────────────────────┼───────┼────────────┼───┼────────────┼───────────┼──────────────────────┼──────────────────────┤\n│ 2016-02-03 16:07:46 │     1 │ Ernest     │ … │            │ 140639.36 │                      │                      │\n│ 2016-02-03 21:52:07 │     2 │ Anthony    │ … │ 1/16/1998  │ 172843.61 │ Developer II         │ 👾 🙇 💁 🙅 🙆 🙋 …  │\n│ 2016-02-03 02:22:19 │     3 │ Ryan       │ … │ 11/21/1978 │ 204620.66 │ Developer I          │ ␢                    │\n│ 2016-02-03 04:20:04 │     4 │ Brenda     │ … │ 10/29/1998 │ 260474.12 │ GIS Technical Arch…  │                      │\n│ 2016-02-03 00:15:16 │     5 │ Jacqueline │ … │ 7/12/1959  │ 286038.78 │ Marketing Assistant  │                      │\n│ 2016-02-03 19:48:14 │     6 │ Paul       │ … │            │ 241518.24 │                      │                      │\n│ 2016-02-03 08:59:05 │     7 │ Linda      │ … │ 3/30/1988  │ 192756.38 │ Professor            │                      │\n│ 2016-02-03 08:04:51 │     8 │ Frances    │ … │            │ 188511.28 │                      │ &lt;svg&gt;&lt;script&gt;0&lt;1&gt;a…  │\n│ 2016-02-03 08:12:33 │     9 │ Jason      │ … │ 7/29/1982  │ 238068.56 │ Web Designer III     │                      │\n│ 2016-02-03 17:08:02 │    10 │ Carolyn    │ … │ 4/28/1977  │ 132718.26 │ Research Nurse       │                      │\n│          ·          │     · │  ·         │ · │     ·      │     ·     │       ·              │          ·           │\n│          ·          │     · │  ·         │ · │     ·      │     ·     │       ·              │          ·           │\n│          ·          │     · │  ·         │ · │     ·      │     ·     │       ·              │          ·           │\n│ 2016-02-04 04:48:13 │   991 │ Fred       │ … │ 1/25/1975  │ 280835.07 │ Cost Accountant      │                      │\n│ 2016-02-04 00:03:33 │   992 │ Anna       │ … │ 5/29/1962  │ 286181.88 │ Automation Special…  │                      │\n│ 2016-02-04 07:05:48 │   993 │ Donna      │ … │ 6/3/1962   │  245704.0 │ Senior Financial A…  │                      │\n│ 2016-02-04 21:15:41 │   994 │ Annie      │ … │            │ 194931.47 │                      │                      │\n│ 2016-02-04 05:05:12 │   995 │ Carlos     │ … │ 8/11/1986  │  39424.88 │ Teacher              │                      │\n│ 2016-02-04 21:45:05 │   996 │ Stephanie  │ … │ 5/9/1962   │  273504.1 │ Chief Design Engin…  │                      │\n│ 2016-02-04 09:20:21 │   997 │ Kathy      │ … │ 12/4/1986  │ 109525.48 │ Director of Sales    │                      │\n│ 2016-02-04 00:26:05 │   998 │ Louis      │ … │ 11/20/1982 │  13134.47 │ Office Assistant IV  │                      │\n│ 2016-02-04 04:50:33 │   999 │ Elizabeth  │ … │ 3/14/1976  │ 207194.65 │ Research Assistant I │                      │\n│ 2016-02-04 16:14:42 │  1000 │ Susan      │ … │ 12/19/1999 │ 229961.89 │ Human Resources Ma…  │                      │\n├─────────────────────┴───────┴────────────┴───┴────────────┴───────────┴──────────────────────┴──────────────────────┤\n│ 5000 rows (20 shown)                                                                           13 columns (7 shown) │\n└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n\n\n\n\nConnecting to the database\nIf you make an explicit connection to the database with the connect() method, the arguments determine whether the database resides completely in memory—and is destroyed when the Python process exits—or whether the database persists on disk. The following statements are equivalent and create a non-persistent database:\nconn = duckdb.connect(database=\":default:\")\nconn = duckdb.connect(database=\":memory:\")\nconn = duckdb.connect(\":memory:\")\nconn = duckdb.connect()\nOnce the connection is established you use the same methods on the connection object as on the duckdb object:\n\nconn = duckdb.connect()\nconn.sql(\"select Oxygen, RunTime, RestPulse from rel where RestPulse &lt; 49\").df()\n\n\n\n\n\n\n\n\n\nOxygen\nRunTime\nRestPulse\n\n\n\n\n0\n46.672\n10.00\n48\n\n\n1\n45.441\n9.63\n48\n\n\n2\n45.118\n11.08\n48\n\n\n3\n39.203\n12.88\n44\n\n\n\n\n\n\n\n\nTo create or access a persisted database, specify the name of the database using a .db, .ddb, or .duckdb extension. The following statements create or open the database test.db, read a CSV file into a Pandas DataFrame and create the table fitness from the DataFrame in the database.\n\nconn = duckdb.connect(database=\"test.db\")\ndf = conn.read_csv('../datasets/fitness.csv').df()\nconn.execute('CREATE OR REPLACE TABLE fitness AS SELECT * FROM df')\n\n&lt;duckdb.duckdb.DuckDBPyConnection at 0x168f82270&gt;\n\n\nYou can see the tables in a database by querying the information schema:\n\nconn.sql('select * from information_schema.tables')\n\n┌───────────────┬──────────────┬────────────┬────────────┬───┬────────────────────┬──────────┬───────────────┐\n│ table_catalog │ table_schema │ table_name │ table_type │ … │ is_insertable_into │ is_typed │ commit_action │\n│    varchar    │   varchar    │  varchar   │  varchar   │   │      varchar       │ varchar  │    varchar    │\n├───────────────┼──────────────┼────────────┼────────────┼───┼────────────────────┼──────────┼───────────────┤\n│ test          │ main         │ fitness    │ BASE TABLE │ … │ YES                │ NO       │ NULL          │\n├───────────────┴──────────────┴────────────┴────────────┴───┴────────────────────┴──────────┴───────────────┤\n│ 1 rows                                                                                12 columns (7 shown) │\n└────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n\n\nor with\n\nconn.sql(\"show tables\")\n\n┌─────────┐\n│  name   │\n│ varchar │\n├─────────┤\n│ fitness │\n└─────────┘\n\n\nAnd you can see the available databases with\n\nconn.sql(\"show databases\")\n\n┌───────────────┐\n│ database_name │\n│    varchar    │\n├───────────────┤\n│ test          │\n└───────────────┘\n\n\nYou could have created the table in the database from the CSV file in a single step:\n\nconn = duckdb.connect(database=\"ads5064.db\")\nconn.execute('CREATE TABLE fitness AS SELECT * FROM “../data/fitness.csv” ')\n\nNotice the use of single and double quotes to distinguish the string for the file name inside the SQL statement.\nTo close the connection from the Python session to the database, use\n\nconn.close()",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Access</span>"
    ]
  },
  {
    "objectID": "data/data_access.html#cloud-data-resources",
    "href": "data/data_access.html#cloud-data-resources",
    "title": "11  Data Access",
    "section": "11.3 Cloud Data Resources",
    "text": "11.3 Cloud Data Resources\nCloud resources are increasingly important for data storage and analytics. The growth trends in the cloud are staggering. Storing data in cloud file, block, or object storage is a $78.6 billion market with a compound annual growth rate (CAGR) of 18.5% (2022 numbers). Cloud databases in 2023 are a $21.3 billion market that grows at 22%.\nYou will encounter cloud-based data in your data science work, and you will take advantage of cloud resources, especially for handling and analyzing large data sets.\nThe most important cloud storage solutions for data are\n\nObject storage\nCloud databases\nCloud data warehouses (Snowflake, BigQuery)\nCloud lakehouse (Databricks)\n\n\nObject storage\nObject, file, and block storage work differently. In object storage, data is managed as generic objects. These can be files or parts of files and combine the data to be stored with metadata about the object being stored. Metadata could be, for example, a unique identifier, size, creation date, time, and location for an image. Objects are stored in a flat structure; the object’s identifier is used to locate it in the storage system. Amazon Web Services (AWS) calls the container objects are stored in a bucket (rather than a directory).\nFile storage is what you are used to from a directory-based file system on your computer’s operating system. Data is stored in files, organized in hierarchical folders. You look up data by traversing the directory until you have located the file with the correct name.\nBlock storage breaks data into fixed-sized blocks and stores the blocks as units. The Hadoop Distributed File System (HDFS) is a block-oriented storage system, where data is split into 16 MB blocks and the blocks are replicated on multiple machines in a distributed system for durability and high availability.\nCloud object storage has nearly replaced other Big Data storage solutions such as Hadoop and HDFS. You can still find Hadoop environments in organizations—lots of money was invested in building those and organizations are slow to migrate off Hadoop. But the writing is on the wall: HDFS is replaced with cloud storage because it is cheaper and more durable, in particular cloud object storage. Since object storage supports storing files as objects, is the most economical storing alternative, and can scale out without limits, it is the primary storage form for fie-based data in the cloud. Even cloud data warehouses have been built with cloud object storage as the primary storage layer.\nThe object storage systems of the three major cloud providers are\nAmazon S3 (Simple Storage Service) from AWS, Azure Blob Storage, and Google Cloud Storage. Common file formats for you find in these storage systems are CSV, JSON, and Parquet.\n\n\nManaged cloud database\nTODO\n\n\nMotherDuck serverless cloud database\nThe convenience of a serverless database is not having to worry about the cloud instances the database is running on. With a local, on-premises database, you manage the storage and compute infrastructure—the machines—the database is running on. With a managed service (DBaaS), this aspect is taken care of by the managed service provider. However, you still must manage your database instances. For example, during periods of low usage, you might want to scale down the database or even shut it down to reduce cloud expenses.\n\n\nExample: Please Turn Off Your Instances\n\n\nA common mistake by cloud newbies is to assume that just because you are not actively using the cloud, you are not incurring expenses. When you start an instance in the cloud you need to stop it when it is not in use. Otherwise, it keeps running. Even if there is no workload on the instance, it will cost you money.\nA class at a major university—which shall not be named—used cloud resources and the instructors forgot to tell students to shut down their cloud instances. Instead, the students started new instances every time they needed to do cloud computing. The academic unit conducting the class had allocated a $40,000 cloud computing budget over the next three years. The class spent $140,000 in one month.\n\n\nA serverless system automatically scales to zero when not in use for a while. With serverless systems you tend to pay only for what you use, when you use it. An exception are charges for storing data, you cannot turn the storage off. The MotherDuck serverless database is a great complement to the DuckDB analytic database. In fact, MotherDuck is built on DuckDB and you can think of it as the cloud version of DuckDB. That makes MotherDuck a good choice for analytical workloads since DuckDB is a relational column-store, an analytic database.\nAnother nice feature of MotherDuck is the ability to connect to local databases and to cloud databases at the same time. This hybrid mode is useful if you have large data sets in cloud storage and small-to-large data sets stored locally. Since we already know how to work with DuckDB from Python, working with MotherDuck is a snap. The following statements create a connection to a MotherDuck serverless database:\n\nimport duckdb\ncon = duckdb.connect('md:')\n\nNotice that we use the same duckdb library as previously. The special string ‘md:’ or ‘motherduck:’ inside the connect() function triggers a connection attempt to MotherDuck. Whether the connection attempt succeeds depends on whether you can authenticate to MotherDuck. By default, the connect() function will open a browser through which you can authenticate to the MotherDuck service using a previously established account.\nAlternatively, you can click on Settings in the UI of the MotherDuck console at https://app.motherduck.com/ and download your authentication token. Store this token in the environment variable motherduck_token on your system and you can authenticate and connect directly to MotherDuck with the previous duckdb.connect() statement. It is also possible to list the authentication token directly in the duckdb.connect() function,\n\ncon = duckdb.connect('md:?motherduck_token=&lt;token&gt;')\n\nbut this is discouraged. Access credentials should not be shown in clear text in a source file that can easily be read and that might be shared with others. That is a security nightmare waiting to happen.\nDuckDB and MotherDuck are relational database management systems, they manage more than one database. You can see the list of databases with the SHOW DATABASES command:\n\ncon.sql(\"SHOW DATABASES\")\n\nBy default, MotherDuck creates a database called my_db and a shared database with sample data. One of the cool features of MotherDuck is the ability to attach local databases to the instance. The following statement attaches a local version of the database for this course to MotherDuck.\n\ncon.sql(\"ATTACH 'ads5064.ddb'\")\ncon.sql(\"SHOW DATABASES\")\n\nSince MotherDuck manages multiple databases you have to tell it which database to work with. This is called the current database, by default this is my_db when you first connect. You can see which database is current with\n\ncon.sql(\"SELECT current_database()\").show()\n\nYou can set any database as the current one with the USE statement:\n\ncon.sql(\"USE ads5064\")\ncon.sql(\"SHOW TABLES\")\n\nYou can access a table in a database that is not the current database by using a two-level name (database.table). This even works if one table is local and the other is in a cloud database. Suppose that ads5064 is a local version of the database for the course and that ads5064_md is stored in the cloud.\n\ncon.sql(\"SELECT count(*) FROM ads5064.apples\")\ncon.sql(\"SELECT count(*) FROM ads5064_md.glaucoma\")\n\nThis comes in very handy if you want to process local and cloud data in the same query, for example when joining tables.\nMotherDuck also supports shared read-only databases, to which you connect through a URL. The URL is created by an administrator of the database with the CREATE SHARE command. You can then attach the database by using the URL:\n\ncon.sql(\"ATTACH 'md:_share/ads5064_md/e00063c9-568c-45ec-a372-dca85f6915fd' as course_db\")\n\nThe URL is a link to a point-in-time snapshot of the database when it was shared. The administrator can update the share of the database, the link will remain valid.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Access</span>"
    ]
  },
  {
    "objectID": "data/quality.html",
    "href": "data/quality.html",
    "title": "12  Data Quality",
    "section": "",
    "text": "12.1 Data Profiling",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data Quality</span>"
    ]
  },
  {
    "objectID": "data/quality.html#sec-data-profiling",
    "href": "data/quality.html#sec-data-profiling",
    "title": "12  Data Quality",
    "section": "",
    "text": "The First Date with your Data\nData profiling is one of the first steps to take when you encounter a data set for the first time. It is how you kick-start the exploratory data analysis (EDA). (Borne 2021) refers to it as “having that first date with your data.” We are not looking to derive new insights from the data or to build amazing machine learning models at this stage; we want to create a high-level report of the data’s content and condition. We want to know what we are dealing with. Common questions and issues addressed during profiling are\n\nWhich variables (attributes) are in the data?\nHow many rows and columns are there?\nWhich variables are quantitative (represent numbers), and which variables are qualitative (represent class memberships)\nAre qualitative variables coded as strings, objects, numbers?\nAre there complex data types such as JSON documents, images, audio, video encoded in the data?\nWhat are the ranges (min, max) of the variables. Are these reasonable or do they suggest outliers or measurement errors?\nWhat is the distribution of quantitative variables?\nWhat is the mean, median, and standard deviation of quantitative variables?\nWhat are the unique values of qualitative variables?\nDo coded fields such as ZIP codes, account numbers, email addresses, state codes have the correct format?\nAre there attributes that have only a single value?\nAre there duplicate entries?\nAre there missing values in one or more variables?\nWhat are the strengths and direction of pairwise associations between the variables?\nAre some attributes perfectly correlated, for example, birthdate and age or temperatures in degree Celsius and degree Fahrenheit.\n\n\n\nCalifornia Housing Prices\nIn this subsection we consider a data set on housing prices in California, based on the 1990 census and available on Kaggle. The data contain information about geographic location, housing, and population within blocks. California has over 8,000 census tracts and a tract can have multiple block groups. There are over 20,000 census block groups and over 700,000 census blocks in California.\nThe variables in the data are:\n\nVariables in the California Housing Prices data set.\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nlongitude\nA measure of how far west a house is; a higher value is farther west\n\n\nlatitude\nA measure of how far north a house is; a higher value is farther north\n\n\nhousing_median_age\nMedian age of a house within a block; a lower number is a newer building\n\n\ntotal_rooms\nTotal number of rooms within a block\n\n\ntotal_bedrooms\nTotal number of bedrooms within a block\n\n\npopulation\nTotal number of people residing within a block\n\n\nhouseholds\nTotal number of households, a group of people residing within a home unit, for a block\n\n\nmedian_income\nMedian income for households within a block of houses (measured in tens of thousands of US Dollars)\n\n\nmedian_house_value\nMedian house value for households within a block (measured in US Dollars)\n\n\nocean_proximity\nLocation of house with respect to ocean/sea\n\n\n\nThe variable description is important metadata to understand the data. A variable such as totalRooms could be understood as the number of rooms in a building and the medianHouseValue could then mean the median of the houses that have that number of rooms. However, since the data are not for individual houses but blocks, totalRooms represents the sum of all the rooms in all the houses in that block.\nWe start data profiling a pandas DataFrame of the data with getting basic info and a listing of the first few observations.\n\nimport numpy as np\nimport pandas as pd\nCA_houses = pd.read_csv(\"../datasets/CaliforniaHousing_1990.csv\")\n\nCA_houses.info()\nCA_houses.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\n\nWe can immediately answer several questions about the data:\n\nThere are 20,640 rows and 10 columns\nExcept for ocean_proximity, which is of object type (string), all other variables are stored as 64-bit floats. Variables total_rooms, total_bedrooms, population, and households are naturally integers; since they appeared in the CSV file with a decimal point, they were assigned a floating point data type.\nOnly the total_bedrooms variable has missing values, 20,640 – 20,433 = 207 values for this variable are unobserved. This shows a high level of completeness of the data set. (More on missing values in the next section).\nThe listing of the first five observations confirms that variables are counts or sums at the block-level, rather than data for individual houses.\nThe latitude and longitude values differ in the second decimal place, suggesting that blocks (=rows of the data set) have unique geographic location, but we cannot be sure.\nThe ocean_proximity entries are in all caps. We want to see the other values in that column to make sure the entries are consistent. Knowing the format of strings is important for filtering (selecting) or grouping observations.\n\nThe next step is to use pandas describe() function to compute basic summaries of the variables.\n\nCA_houses.describe()\n\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20433.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n-119.569704\n35.631861\n28.639486\n2635.763081\n537.870553\n1425.476744\n499.539680\n3.870671\n206855.816909\n\n\nstd\n2.003532\n2.135952\n12.585558\n2181.615252\n421.385070\n1132.462122\n382.329753\n1.899822\n115395.615874\n\n\nmin\n-124.350000\n32.540000\n1.000000\n2.000000\n1.000000\n3.000000\n1.000000\n0.499900\n14999.000000\n\n\n25%\n-121.800000\n33.930000\n18.000000\n1447.750000\n296.000000\n787.000000\n280.000000\n2.563400\n119600.000000\n\n\n50%\n-118.490000\n34.260000\n29.000000\n2127.000000\n435.000000\n1166.000000\n409.000000\n3.534800\n179700.000000\n\n\n75%\n-118.010000\n37.710000\n37.000000\n3148.000000\n647.000000\n1725.000000\n605.000000\n4.743250\n264725.000000\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6445.000000\n35682.000000\n6082.000000\n15.000100\n500001.000000\n\n\n\n\n\n\n\n\nFor each of the numeric variables, describe() computes the number of non-missing values (count), the sample mean (mean), the sample standard deviation (std), the minimum (min), maximum (max) and three percentiles (25%, 50%, 75%).\nThe results confirm that all variables have complete data (no missing values) except for total_bedrooms. The min and max values are useful to see the range (range = max – min) for the variables and to spot outliers and unusual values. It is suspicious that there is one or more blocks with a single household. This is not necessarily the same record that has 2 total_rooms and a population of 3.\n\nCA_houses[CA_houses[\"households\"] == 1]\n\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n16171\n-122.5\n37.79\n52.0\n8.0\n1.0\n13.0\n1.0\n15.0001\n500001.0\nNEAR BAY\n\n\n\n\n\n\n\n\nThis is indeed a suspicious record. There is a single household in the block, but thirteen people are living in the block in a house with eight rooms and one bedroom. An unusual configuration that should be examined for possible data entry errors.\n\n\nProfiling Tools\nYou can accelerate the data profiling task by using packages such as ydata_profiling (fka pandas_profiling), lux, or sweetviz. Sweetviz, for example, generates detailed interactive visualizations in a web browser or a notebook that help to address some of the profiling questions we raised at the beginning of the section.\nTo create a profile report for the housing prices data with sweetviz, use the following:\n\nimport sweetviz as sv\nmy_report = sv.analyze(CA_houses)\nmy_report.show_html(filepath='Profiling_CA_1.html', open_browser=False)\n\n\n\n\nReport Profiling_CA_1.html was generated.\n\n\nFigure 12.1 displays the main screen of the visualization. You can access the entire interactive html report here. For each numeric variable sweetviz reports the number of observed and missing values, the number of distinct values, and a series of summary statistics similar to the output from describe().  A histogram is also produced that gives an idea of the distribution of the variable in the data set. For example, housing_median_age has a fairly symmetric distribution, whereas total_rooms and total_bedrooms are highly concentrated despite a wide range.\n\n\n\n\n\n\nFigure 12.1: Sweetviz visualization for California Housing Prices. Main screen, some numeric variables.\n\n\n\nFigure 12.2 shows the bottom of the main screen that displays information on the ocean_proximity variable. We now see that there are five unique values for the variable with the majority in the category &lt;1H OCEAN.\n\n\n\n\n\n\nFigure 12.2: Profiling information for qualitative variable ocean_proximity.\n\n\n\nClicking on any variable brings up more details. For housing_median_age that detail is shown in Figure 12.3. It includes a detailed histogram of the distribution, largest, smallest, and most frequent values.\n\n\n\n\n\n\nFigure 12.3: Sweetviz detail for the variable housing_median_age.\n\n\n\nThe graphics are interactive, the number of histogram columns can be changed to the desired resolution.\nSweetviz displays pairwise associations between variables. You can see those for housing_median_age in Figure 12.3 or for all pairs of variables by clicking on Associations (Figure 12.4).\n\n\n\n\n\n\nFigure 12.4: Sweetviz visualization of pairwise associations in California Housing Prices data.\n\n\n\nAssociations are calculated and displayed differently depending on whether the variables in a pair are quantitative or not. For pairs of quantitative variables, sweetviz computes the Pearson correlation coefficient. It ranges from –1 to +1; a coefficient of 0 indicates no (linear) relationship between the two variables, they are uncorrelated. A coefficient of +1 indicates a perfect positive correlation, knowing one variable allows you to perfectly predict the other variable. Similarly, a Pearson coefficient of –1 means that the variables are perfectly correlated and one variable decreases as the other increases.\nStrong positive correlations are present between households and the variables total_rooms, total_bedrooms, and population. That is expected as these variables are accumulated across all households in a block. There is a moderate positive association between median income and median house value. More expensive houses are associated with higher incomes—not surprising. A strong negative correlation exists between longitude and latitude, a consequence of the geography of California: as you move further west (east) the state reaches further south (north). \nAssociations between quantitative and qualitative variables are calculated as the correlation ratio that ranges from 0 to 1 and displayed as squares in the Associations matrix. The correlation ratio is based on means within the categories of the qualitative variables. A ratio of 0 means that the means of the quantitative variable are identical for all categories. Since the data contains only one qualitative variable, ocean_proximity, squares appear only in the last row and column of the Associations matrix.\nIf the data contains an obvious target variable for modeling, you can indicate that when creating the profiling report. Sweetviz then adds information on that variable to the visualizations. Suppose that we are interested in modeling the median house value as a function of other attributes. The following statement requests a report with median_house_value as the target.\n\nhousevalue_report = sv.analyze(CA_houses, target_feat=\"median_house_value\")\nhousevalue_report.show_html(filepath='Profiling_CA_2.html', open_browser=False)\n\n\n\n\nReport Profiling_CA_2.html was generated.\n\n\nFigure 12.5 shows the detail on ocean_proximity from this analysis; the complete report is here. The average of the block’s median housing values in the five groups of ocean proximity are shown on top of the histogram. The highest average median house value is found on the island, the lowest average in the inland category.\n\n\n\n\n\n\nFigure 12.5: Profiling details for ocean_proximity with target median_house_value.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data Quality</span>"
    ]
  },
  {
    "objectID": "data/quality.html#sec-missing-values",
    "href": "data/quality.html#sec-missing-values",
    "title": "12  Data Quality",
    "section": "12.2 Missing Values",
    "text": "12.2 Missing Values\nWhen observations do not have values assigned to them, we say that the value is missing. This is a fact of life in data analytics; whenever you work with a set of data you should expect values to be missing.\n\n\nDefinition: Missing Value\n\n\nA missing value is an observation that has no value assigned to it.\n\n\nMissingness is obvious when you see incomplete columns in the data. The problem can be inconspicuous when entire records are missing from the data. A survey that fails to include a key demographic misses the records of those who should have been sampled in the survey.\nYou should check the software packages used for data analysis on how they handle missing values—by default and how the behavior can be affected through options. In many cases the default behavior is casewise deletion, also known as complete-case analysis: any record that have a missing value in one or more of the analysis variables is excluded from the analysis. Pairwise deletion removes only those records that have missing values for a specific analysis. To see the difference, consider the data in Table 12.1 and suppose you want to compute the matrix of correlations among the variables.\n\n\n\nTable 12.1: Three variables with different missing value patterns.\n\n\n\n\n\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\n\n\n\n1.0\n3.0\n.\n\n\n2.9\n.\n3.4\n\n\n3.8\n.\n8.2\n\n\n0.5\n3.7\n.\n\n\n\n\n\n\nA complete-case analysis of \\(X_1\\), \\(X_2\\) , and \\(X_3\\) would result in a data frame without observations since each row of the table has a missing value in one column. Pairwise deletion computes the correlation between \\(X_1\\) and \\(X_2\\) based on the first and last observation, the correlation between \\(X_1\\) and \\(X_3\\) based on the second and third observation and fail to compute a correlation between \\(X_2\\) and \\(X_3\\).\nWhat are some possible causes for missing values:\n\nMembers of the target population not included (missing records)\nData entry errors\nVariable transformations that lead to invalid values: division by zero, logarithm of zeros or negative values\nMeasurement equipment malfunction\nMeasurement equipment limits exceeded\nAttrition (drop-outs) of subject in longitudinal studies (death, moving, refusal, changes in medical condition, …)\nNonresponse of subjects in surveys\nVariables not measured\nNot all combinations of factors are observable. For example, the data set of your Netflix movie ratings is extremely sparse, unless you “finished Netflix” and rated all movies.\nRegulation requires removal of sensitive information\n\nData transformations can introduce missing values into data sets when mathematical operations are not valid. To accommodate nonlinear relationships between target and input variables, transformations of inputs such as ratios, square roots, and logarithms are common. These transformations are sometimes applied to change the distribution of data, for example, to create more symmetry by taking logarithms of right-skewed data (Figure 12.6).\n\n\n\n\n\n\nFigure 12.6: Distribution of home values and logarithm of home values in Albemarle County, VA. The log-transformed data is more symmetric distributed. Since all home values are positive, the transformation does not lead to missing values.\n\n\n\nThe log-transformation is meaningful in the home values example, it is not unreasonable to proceed with an analysis that assumes log(value) is normally distributed. Suppose you are log-transforming another highly skewed variable, the amount of individual’s annual medical out-of-pocket expenses. Most people have a moderate amount of out-of-pocket expenses, a smaller percentage have very high annual expenses. However, many will have no out-of-pocket expenses at all. Taking the logarithm will invalidate the records of those individuals. To get around the numerical issue of taking logarithms of zeros, transformations are sometimes changed to log(expenses + 1). This avoids missing values but fudges the data by pretending that everyone has at least some medical expenses.\nRemoving missing values from the analysis is appropriate only when the reason for the missingness is unrelated to any other information in the study. The relationship between absence of information and the study is known as the missing value process.\n\nMissing Value Process\n\n\n\n\n\n\nImportant\n\n\n\nMaking the wrong assumption about the missing value process can bias the results. A complete case analysis is not necessarily unbiased only if the data are missing completely at random. But the bias can at least be corrected in that case.\n\n\nYou need to be aware of three types of missing data based on that process:\n\nMCAR: Data is said to be missing completely at random when the missingness is unrelated to any study variable, including the target variable. There are no systematic differences between the records with missing data and the records with complete data. MCAR is a very strong assumption, and it is the best you can hope for. If the data are MCAR, you can safely delete records with missing values because the complete cases are a representative sample of the whole. Case deletion reduces the size of the available data but does not introduce bias into the analysis.\nMAR: Data is said to be missing at random when the pattern of missingness is related to the observed data but not to the unobserved data. Suppose you are conducting a survey regarding depression and mental health. If one group is less likely to provide information in the survey for reasons unrelated to their level of depression, then the group’s data is missing at random. Complete-case analysis of a data set that contains MAR data can result in bias.\nMNAR: Data is said to be missing not at random if the absence of information is systematically related to the unobserved data. For example, employees do not report salaries in a workspace survey or a group that is less likely to report in a depression survey because of their level of depression.\n\nA complete-case analysis if the data are MAR or MNAR does not necessarily bias the results. If the missingness is related to the primary target variable, then the results are biased. In the MAR case that bias can be corrected. As noted by the NIH in the context of patient studies,\n\nThe import of the MAR vs. MNAR distinction is therefore not to indicate that there definitively will or will not be bias in a complete case analysis, but instead to indicate – if the complete case analysis is biased – whether that bias can be fully removed in analysis.\n\nMissing values are represented in data sets in different ways. The two basic methods are to use masks or extra bits to indicate whether a value is available and to use sentinel value, special entries that indicate that a value is not available (missing).\n\n\nDefinition: Sentinel Value\n\n\nIn programming, a sentinel value is a special placeholder that indicates a special condition in the data or the program. Applications of sentinel values are to indicate when to break out of loops or to indicate unobserved values.\n\n\nSentinel values such as –9999 to indicate a missing value are dangerous, they can be mistaken too easily for a valid numerical entry. The only sentinel value one should use is NaN (not-a-number), a specially defined IEEE floating-point value. Software implements special logic for handling NaNs. Unfortunately, NaN is available only for floating point data types, so software uses different techniques to implement missing values across all data types. For example, in databases you find masking based on the concept of NULL values to indicate absence (=nullity) of a value.\n\n\n\n\n\n\nCaution\n\n\n\nDo not use sentinel values that could be confused with real data values to indicate that a value is missing.\n\n\n\n\nMissing Values in Pandas\nPython has the singleton object None which can be used to indicate missingness and it supports the IEEE NaN (not a number) to indicate missing values for floating-point types. Pandas uses the sentinel value approach based on NaN for floating-point and None for all other data types. This choice has some side effects, None and NaN do not behave the same way.\n\nimport numpy as np\nimport pandas as pd\n\nx1 = np.array([1, None, 3, 4])\nx2 = np.array([1, 2, 3, 4])\ndisplay(x1)\ndisplay(x2)\n\narray([1, None, 3, 4], dtype=object)\n\n\narray([1, 2, 3, 4])\n\n\nThe array with None value is represented internally as an array of Python objects. Operating on objects is slower than on basic data types such as integers. Having missing values in non-floating-point columns thus incurs some drag on performance.\nFor floating point data use np.nan to indicate missingness.\n\nf1 = np.array([1, np.nan, 3, 4])\nf1\n\narray([ 1., nan,  3.,  4.])\n\n\nThe behavior of None and NaN in operations is different. For example, arithmetic operations on NaNs result in NaNs, whereas arithmetic on None values results in errors.\n\nf1.sum()\n\nnan\n\n\n\nx1.sum()\n\nTypeError: unsupported operand type(s) for +: 'int' and 'NoneType'\n\n\nWhile None values result in errors and stop program execution, NaNs are contagious; they turn everything they come in touch with into NaNs—but the program keeps executing. Pandas mixes None and NaN values and follows casting rules when np.nan is stored.\n\nx2 = pd.Series([1,2,3,4], dtype=int)\ndisplay(x2)\nx2[2] = np.nan\ndisplay(x2)\n\n0    1\n1    2\n2    3\n3    4\ndtype: int64\n\n\n0    1.0\n1    2.0\n2    NaN\n3    4.0\ndtype: float64\n\n\nThe integer series is converted to a float series when a NaN was inserted. The same happens when you use None instead of NaN:\n\nx3 = pd.Series([1,2,3,4], dtype=int)\nx3[1] = None\nx3\n\n0    1.0\n1    NaN\n2    3.0\n3    4.0\ndtype: float64\n\n\n\n\nWorking with Missing Values in Data Sets\nThe following statements create a Pandas DataFrame from a CSV file that contains information about 7,303 traffic collisions in New York City. You can use the info() attribute of the DataFrame for information about the columns, including missing value counts.\n\ncollisions = pd.read_csv(\"../datasets/nyc_collision_factors.csv\")\ncollisions.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7303 entries, 0 to 7302\nData columns (total 26 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   DATE                           7303 non-null   object \n 1   TIME                           7303 non-null   object \n 2   BOROUGH                        6920 non-null   object \n 3   ZIP CODE                       6919 non-null   float64\n 4   LATITUDE                       7303 non-null   float64\n 5   LONGITUDE                      7303 non-null   float64\n 6   LOCATION                       7303 non-null   object \n 7   ON STREET NAME                 6238 non-null   object \n 8   CROSS STREET NAME              6166 non-null   object \n 9   OFF STREET NAME                761 non-null    object \n 10  NUMBER OF PERSONS INJURED      7303 non-null   int64  \n 11  NUMBER OF PERSONS KILLED       7303 non-null   int64  \n 12  NUMBER OF PEDESTRIANS INJURED  7303 non-null   int64  \n 13  NUMBER OF PEDESTRIANS KILLED   7303 non-null   int64  \n 14  NUMBER OF CYCLISTS INJURED     0 non-null      float64\n 15  NUMBER OF CYCLISTS KILLED      0 non-null      float64\n 16  CONTRIBUTING FACTOR VEHICLE 1  7303 non-null   object \n 17  CONTRIBUTING FACTOR VEHICLE 2  6218 non-null   object \n 18  CONTRIBUTING FACTOR VEHICLE 3  303 non-null    object \n 19  CONTRIBUTING FACTOR VEHICLE 4  59 non-null     object \n 20  CONTRIBUTING FACTOR VEHICLE 5  14 non-null     object \n 21  VEHICLE TYPE CODE 1            7245 non-null   object \n 22  VEHICLE TYPE CODE 2            5783 non-null   object \n 23  VEHICLE TYPE CODE 3            284 non-null    object \n 24  VEHICLE TYPE CODE 4            54 non-null     object \n 25  VEHICLE TYPE CODE 5            12 non-null     object \ndtypes: float64(5), int64(4), object(17)\nmemory usage: 1.4+ MB\n\n\nColumns DATE and TIME contain no null (missing) values, their count equals the number of observation (7,303). The number of cyclists injured or killed in the accidents in columns 14 and 15 contain only missing values. Is this a situation where the data was not entered, or should the entries be zeros? We should get back with the domain experts who put the data together to find out how to handle these columns.\nTable 12.2 displays Pandas DataFrame methods to operate on missing values.\n\n\n\nTable 12.2: Methods to operate on missing values in pandas DataFrames.\n\n\n\n\n\n\n\n\n\n\nMethod\nDescription\nNotes\n\n\n\n\nisnull()\nReturns a boolean same-sized object indicating if the missing values are missing\nisna() is an alias\n\n\nnotnull()\nOpposite of isnull(), indicating if values are not missing\nnotna() is an alias\n\n\ndropna()\nRemove missing values, dropping either rows (axis=0) or columns (axis=1)\nhow={'any','all'} to determine when to drop a row or column\n\n\nfillna()\nReplace missing values with designated values\nmethod= to propagate last valid value or backfill with next valid value\n\n\ninterpolate()\nFill in missing values using an interpolation method\n\n\n\n\n\n\n\nThe isnull() method can be used to return a data frame of Boolean (True/False) values that indicate missingness. You can sum across rows or columns of the data frame to count the missing values:\n\ncollisions.isnull().sum()\n\nDATE                                0\nTIME                                0\nBOROUGH                           383\nZIP CODE                          384\nLATITUDE                            0\nLONGITUDE                           0\nLOCATION                            0\nON STREET NAME                   1065\nCROSS STREET NAME                1137\nOFF STREET NAME                  6542\nNUMBER OF PERSONS INJURED           0\nNUMBER OF PERSONS KILLED            0\nNUMBER OF PEDESTRIANS INJURED       0\nNUMBER OF PEDESTRIANS KILLED        0\nNUMBER OF CYCLISTS INJURED       7303\nNUMBER OF CYCLISTS KILLED        7303\nCONTRIBUTING FACTOR VEHICLE 1       0\nCONTRIBUTING FACTOR VEHICLE 2    1085\nCONTRIBUTING FACTOR VEHICLE 3    7000\nCONTRIBUTING FACTOR VEHICLE 4    7244\nCONTRIBUTING FACTOR VEHICLE 5    7289\nVEHICLE TYPE CODE 1                58\nVEHICLE TYPE CODE 2              1520\nVEHICLE TYPE CODE 3              7019\nVEHICLE TYPE CODE 4              7249\nVEHICLE TYPE CODE 5              7291\ndtype: int64\n\n\nIf you choose to remove records with missing values, you can use the dropna() method. The how=’any’|’all’ option specifies whether to remove records if any variable is missing (complete-case analysis) or if all variables is missing. Because the columns referring to cyclists contain only missing values, a complete-case analysis will result in an empty DataFrame.\n\ncoll_no_miss = collisions.dropna(how='any')\ndisplay(len(collisions))\ndisplay(len(coll_no_miss))\n\n7303\n\n\n0\n\n\nSuppose we verified that the missing values in columns 14 & 15 were meant to indicate that no cyclists were injured or killed. Then we can replace the missing values with zeros using the fillna() method.\n\ncollisions[\"NUMBER OF CYCLISTS INJURED\"].fillna(0,inplace=True)\ncollisions[\"NUMBER OF CYCLISTS KILLED\"].fillna(0,inplace=True)\n\n/var/folders/ff/6m670wks1776ck_gbfxynxrc0000gq/T/ipykernel_76737/612697346.py:1: FutureWarning:\n\nA value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n\n/var/folders/ff/6m670wks1776ck_gbfxynxrc0000gq/T/ipykernel_76737/612697346.py:2: FutureWarning:\n\nA value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n\n\n\nTechniques for imputing missing values are discussed in more detail below.\nThe notnull() method is useful to select records without missing values. Since it returns a boolean same-sized object, you can use it to filter:\n\nbool_series = pd.notnull(collisions[\"CONTRIBUTING FACTOR VEHICLE 5\"]) \ncollisions.loc[bool_series, [\"DATE\", \"TIME\", \"BOROUGH\", \"ON STREET NAME\"]] \n\n\n\n\n\n\n\n\n\nDATE\nTIME\nBOROUGH\nON STREET NAME\n\n\n\n\n217\n04/11/2016\n13:22:00\nBROOKLYN\nSTUYVESANT AVENUE\n\n\n896\n10/08/2016\n14:10:00\nQUEENS\nNaN\n\n\n1112\n10/18/2016\n07:10:00\nQUEENS\n124 STREET\n\n\n1348\n11/23/2016\n14:11:00\nBROOKLYN\nSUTTER AVENUE\n\n\n1551\n11/16/2016\n11:11:00\nQUEENS\n112 AVENUE\n\n\n1554\n11/29/2016\n04:11:00\nBROOKLYN\nSNYDER AVENUE\n\n\n2794\n11/15/2016\n03:11:00\nMANHATTAN\nNaN\n\n\n2899\n03/07/2016\n08:45:00\nQUEENS\n105 AVENUE\n\n\n3292\n10/25/2016\n18:10:00\nBROOKLYN\nHIGHLAND BOULEVARD\n\n\n3813\n10/03/2016\n17:10:00\nSTATEN ISLAND\nMANOR ROAD\n\n\n4346\n02/08/2016\n12:08:00\nBROOKLYN\nEAST 35 STREET\n\n\n5286\n01/14/2016\n20:00:00\nNaN\nNaN\n\n\n5556\n01/07/2016\n18:50:00\nQUEENS\n31 AVENUE\n\n\n6737\n01/11/2016\n09:03:00\nBROOKLYN\nBROADWAY\n\n\n\n\n\n\n\n\n\n\nVisualizing Missing Value Patterns\nThe Missingno Python package has some nice methods to inspect the missing value patterns in data. This is helpful to see the missing value distribution across multiple columns. The matrix() method displays the missing value pattern for the DataFrame.\n\nimport missingno as msno\nmsno.matrix(collisions)\n\n\n\n\n\n\n\n\nColumns without missing values (DATE, TIME) are shown as a solid gray bar. Missing values are displayed in white. The following graph shows the result of matrix() prior to filling in zeros in the cyclist columns. The sparkline at right summarizes the general shape of the data completeness and points out the rows with the maximum and minimum number of missing values in the dataset. At best 11 of the columns have missing values, at worst 23 of the 26 values are missing.\nTable 12.3 contains data on property sales. The total value of the property is the sum of the first two columns, the last column is the ratio between sales price and total value. A missing value in one of the first two columns triggers a missing value in the Total column. If either Total or Sales are not present, the appraisal ratio in the last column must be missing.\n\n\n\nTable 12.3: Data with column dependencies that propagate missing values.\n\n\n\n\n\nLand\nImprovements\nTotal\nSale\nAppraisal Ratio\n\n\n\n\n30000\n64831\n94831\n118500\n1.25\n\n\n30000\n50765\n80765\n93900\n1.16\n\n\n46651\n18573\n65224\n.\n.\n\n\n45990\n91402\n137392\n184000\n1.34\n\n\n42394\n.\n.\n168000\n.\n\n\n.\n133351\n.\n169000\n.\n\n\n63596\n2182\n65778\n.\n.\n\n\n56658\n153806\n210464\n255000\n1.21\n\n\n51428\n72451\n123879\n.\n.\n\n\n93200\n.\n.\n422000\n.\n\n\n76125\n78172\n275297\n290000\n1.14\n\n\n154360\n61934\n216294\n237000\n1.10\n\n\n65376\n.\n.\n286500\n.\n\n\n42400\n.\n.\n.\n.\n\n\n40800\n92606\n133406\n168000\n1.26\n\n\n\n\n\n\nThe heatmap() method shows a matrix of nullity correlations between the columns of the data. Note that the CSV file contains dots (“.”) for the missing values. To make sure the data are correctly converted to numerical types and the dots are interpreted as missing values, the na_values= and skipinitalspace= options are added to pd.read_csv().\n\nland = pd.read_csv(\"../datasets/landsales.csv\", \n                   na_values=\".\",\n                   skipinitialspace=True)\n\nmsno.heatmap(land)\n\n\n\n\n\n\n\n\nThe nullity correlations are Pearson correlation coefficients computed from the isnull() boolean object for the data, excluding columns that are completely observed or completely unobserved. A correlation of –1 means that presence/absence of two variables is perfectly correlated: if one variable appears the other variable does not appear. A correlation of +1 similarly means that the presence of one variable goes together with the presence of another variable. The total is not perfectly correlated with land or improve columns because a null value in either or both of these can cause a null value for the total. Similarly, the large correlations between appraisal & total and appraisal & sale are indicative that their missing values are likely to occur together.\n\n\nData Imputation\nIn a previous example we used the fillna() method to replace missing values with actual values: the unobserved values for the number of cyclists in the collisions data set were interpreted as no cyclists were injured, replacing NaNs with zeros. This is an example of data imputation.\n\n\nDefinition: Data Imputation\n\n\nData imputation is the process of replacing unobserved (missing) values with usable values.\n\n\n\nImputation must be carried out with care. It is tempting to replace absent values with numbers and to complete the data: records are not removed from the analysis, the sample size is maintained, and calculations no longer fail. Imputing values that are not representative introduces bias into the data.\nCompleting missing values based on information in other columns often seems simple on the surface, but it is fraught with difficulties—there be dragons! Suppose using address information to fill in missing zip codes. It is not sufficient to know that the city is Blacksburg. If we are talking about Blacksburg, SC, then we know the ZIP code is 29702. If it is Blacksburg, VA, however, then there are four possible ZIP codes; we need the street address to resolve to a unique value. Inferring a missing attribute such as gender should never be done. You cannot safely do it using names. Individuals might have chosen to not report gender information. You cannot afford to get it wrong.\nIf string-type data is missing, and you want to include them into the analysis, you can replace the missing values with an identifying string such as “Unobserved” or “Unknown”. That allows you to break out results for these observations in group-by observations, for example.\nIf you decide to proceed with imputation of missing values based on algorithms, here are some options:\n\nRandom replacement. Also called hot-deck imputation, the missing value is replaced with a randomly selected similar record in the same data set that has complete information.\nLOCF. The missing value is replaced with the last complete observation preceding it: the last observation is carried forward. It is also called a forward fill. This method requires that the order of the observations in the data is somehow meaningful. If observations are grouped by city, it is probable that a missing value for the city column represents the same city as the previous record, unless the missing value falls on the record boundary between two cities.\nBackfill. This is the opposite of LOCF; the next complete value following one or more missing values is propagated backwards.\nMean/Median imputation. This technique applies to numeric data; the missing value is replaced based on the sample mean, the sample median, or other statistical measures of location calculated from the non-missing values in a column. If the data consists of groups or classes, then group-specific means can be used. For example, if the data comprises age groups or genders, then missing values for a numeric variable can be replaced with averages for the variabler by age groups or genders.\nInterpolation methods. For numerical data, missing values can be interpolated from nearby values. Interpolation calculations are based on linear, polynomial, or spline methods. Using the interpolate() method in pandas, you can choose between interpolating across rows or columns of the DataFrame.\nRegression imputation. The column with missing values is treated as the target variable of a regression model, using one or more other columns as input variables of the regression. The missing values are then treated as unobserved observations for which the target is predicted.\nMatrix completion. Based on principal component analysis, missing values in a \\(r \\times c\\) numerical array are replaced with a low-rank approximation of the missing values based on the observed values.\nGenerative imputation. If the data consists of images or text and portions are unobserved, for example, parts of the image are obscured, then generative methods can be used to fill in missing pixels in the image or missing words in text.\n\nTo demonstrate some of these techniques, consider this simple 6 x 3 DataFrame.\n\nnp.random.seed(42)\ndf = pd.DataFrame(np.random.standard_normal((6, 3)))\ndf.iloc[:4,1] = np.nan\ndf.iloc[1:3,2] = np.nan\ndf\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.496714\nNaN\n0.647689\n\n\n1\n1.523030\nNaN\nNaN\n\n\n2\n1.579213\nNaN\nNaN\n\n\n3\n0.542560\nNaN\n-0.465730\n\n\n4\n0.241962\n-1.913280\n-1.724918\n\n\n5\n-0.562288\n-1.012831\n0.314247\n\n\n\n\n\n\n\n\nYou can choose a common fill value for all columns or vary the value by column.\n\ndf.fillna({1:0.3, 2:0})\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.496714\n0.300000\n0.647689\n\n\n1\n1.523030\n0.300000\n0.000000\n\n\n2\n1.579213\n0.300000\n0.000000\n\n\n3\n0.542560\n0.300000\n-0.465730\n\n\n4\n0.241962\n-1.913280\n-1.724918\n\n\n5\n-0.562288\n-1.012831\n0.314247\n\n\n\n\n\n\n\n\nMissing values in the second column are replaced with 0.3, those in the last column with zero. Forward (LOCF) and backward imputation are available by setting the method= parameter of fillna():\n\ndf.fillna(method=\"ffill\")\n\n/var/folders/ff/6m670wks1776ck_gbfxynxrc0000gq/T/ipykernel_76737/3944122520.py:1: FutureWarning:\n\nDataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.496714\nNaN\n0.647689\n\n\n1\n1.523030\nNaN\n0.647689\n\n\n2\n1.579213\nNaN\n0.647689\n\n\n3\n0.542560\nNaN\n-0.465730\n\n\n4\n0.241962\n-1.913280\n-1.724918\n\n\n5\n-0.562288\n-1.012831\n0.314247\n\n\n\n\n\n\n\n\nNotice that the forward fill does not replace the NaNs in the second column as there is no non-missing last value to be carried forward. The second and third row in the third column are imputed by carrying forward 0.647689 from the first row. With a backfill imputation the DataFrame is fully completed, since both columns have an observed value after the last missing value.\n\ndf.fillna(method=\"bfill\")\n\n/var/folders/ff/6m670wks1776ck_gbfxynxrc0000gq/T/ipykernel_76737/3896554658.py:1: FutureWarning:\n\nDataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.496714\n-1.913280\n0.647689\n\n\n1\n1.523030\n-1.913280\n-0.465730\n\n\n2\n1.579213\n-1.913280\n-0.465730\n\n\n3\n0.542560\n-1.913280\n-0.465730\n\n\n4\n0.241962\n-1.913280\n-1.724918\n\n\n5\n-0.562288\n-1.012831\n0.314247\n\n\n\n\n\n\n\n\nTo replace the missing values with the column-specific sample means, simply use\n\ndf.fillna(df.mean())\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.496714\n-1.463056\n0.647689\n\n\n1\n1.523030\n-1.463056\n-0.307178\n\n\n2\n1.579213\n-1.463056\n-0.307178\n\n\n3\n0.542560\n-1.463056\n-0.465730\n\n\n4\n0.241962\n-1.913280\n-1.724918\n\n\n5\n-0.562288\n-1.012831\n0.314247\n\n\n\n\n\n\n\n\nThe sample means of the non-missing observations in the second and third columns are -1.463056 and -0.307178, respectively. Imputing with the sample mean has the nice property to preserve the sample mean of the completed data:]\n\ndf.fillna(df.mean()).mean()\n\n0    0.636865\n1   -1.463056\n2   -0.307178\ndtype: float64\n\n\nYou can use other location statistics than the mean. The following statement imputes with the column-specific sample median:\n\ndf.fillna(df.median())\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.496714\n-1.463056\n0.647689\n\n\n1\n1.523030\n-1.463056\n-0.075741\n\n\n2\n1.579213\n-1.463056\n-0.075741\n\n\n3\n0.542560\n-1.463056\n-0.465730\n\n\n4\n0.241962\n-1.913280\n-1.724918\n\n\n5\n-0.562288\n-1.012831\n0.314247\n\n\n\n\n\n\n\n\nTo demonstrate imputation by interpolation, consider this simple series:\n\ns = pd.Series([0, 2, np.nan, 8])\ns\n\n0    0.0\n1    2.0\n2    NaN\n3    8.0\ndtype: float64\n\n\nThe default interpolation method is linear interpolation. You can choose other method, for example, spline or polynomial interpolation, with the method= option.\n\ns.interpolate()\ns.interpolate(method='polynomial', order=2)\n\n0    0.000000\n1    2.000000\n2    4.666667\n3    8.000000\ndtype: float64\n\n\nTo see interpolation operate on a DataFrame, consider this example:\n\ndf = pd.DataFrame([(0.0, np.nan, -1.0, 1.0),\n                    (np.nan, 2.0, np.nan, np.nan),\n                    (2.0, 3.0, np.nan, 9.0),\n                    (np.nan, 4.0, -4.0, 16.0)],\n                   columns=list('abcd'))\ndf\n\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n0.0\nNaN\n-1.0\n1.0\n\n\n1\nNaN\n2.0\nNaN\nNaN\n\n\n2\n2.0\n3.0\nNaN\n9.0\n\n\n3\nNaN\n4.0\n-4.0\n16.0\n\n\n\n\n\n\n\n\nThe default interpolation method is linear with a forward direction across rows.\n\ndf.interpolate()\n\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n0.0\nNaN\n-1.0\n1.0\n\n\n1\n1.0\n2.0\n-2.0\n5.0\n\n\n2\n2.0\n3.0\n-3.0\n9.0\n\n\n3\n2.0\n4.0\n-4.0\n16.0\n\n\n\n\n\n\n\n\nBecause there was no observed value preceding the missing value in column b, the NaN cannot be interpolated. Because there was no value past the last missing value in column a, the NaN is replaced with the last value carried forward.\nTo interpolate in the forward and backward direction, use limit_direction=’both’:\n\ndf.interpolate(method='linear', limit_direction='both')\n\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n0.0\n2.0\n-1.0\n1.0\n\n\n1\n1.0\n2.0\n-2.0\n5.0\n\n\n2\n2.0\n3.0\n-3.0\n9.0\n\n\n3\n2.0\n4.0\n-4.0\n16.0\n\n\n\n\n\n\n\n\nTo interpolate across the columns, set the axis= option to axis=1:\n\ndf.interpolate(method='linear', limit_direction='forward', axis=1)\n\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n0.0\n-0.5\n-1.0\n1.0\n\n\n1\nNaN\n2.0\n2.0\n2.0\n\n\n2\n2.0\n3.0\n6.0\n9.0\n\n\n3\nNaN\n4.0\n-4.0\n16.0\n\n\n\n\n\n\n\n\nImputation seems convenient and relatively simple, and there are many methods to choose from. As you can see from this discussion, there are also many issues and pitfalls. Besides introducing potential bias by imputing with bad values, an important issue is the level of uncertainty associated with imputed values. Unless the data were measured with error or entered incorrectly, you have confidence in the observed values. You cannot have the same level of confidence in the imputed values. The imputed values are estimates based on processes that involve randomness. One source of randomness is that our data represents a random sample—a different set of data gives a different estimate for the missing value. Another source of randomness is the imputation method itself. For example, hot-deck imputation chooses the imputed value based on randomly selected observations with complete data.\nAccounting for these multiple levels of uncertainty is non-trivial. If you pass imputed data to any statistical algorithm, it will assume that all values are known with the same level of confidence. There are ways to take imputation uncertainty into account. You can assign weights to the observations where the weight is proportional to your level of confidence. Assigning smaller weights to imputed values reduces their impact on the analysis. A better approach is to use bootstrap techniques to capture the true uncertainty and bias in the quantities derived from imputed data. This is more computer intensive than a weighted analysis but very doable with today’s computing power.\n\n\nDefinition: Bootstrap\n\n\nTo bootstrap a data set is to repeatedly sample from the data with replacement. Suppose you have a data set of size \\(n\\) rows. \\(B\\) = 1,000 bootstrap samples are 1,000 data sets of size \\(n\\), each drawn independently from each other, and the observations in each bootstrap sample are drawn with replacement.\nBootstrapping is a statistical technique to derive the sample distribution of a random quantity by simulation. You apply the technique to each bootstrap sample and average the results.\n\n\nI hope you take away from this discussion that imputation is possible, imputation is not always necessary, and imputation must be done with care.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data Quality</span>"
    ]
  },
  {
    "objectID": "data/quality.html#outlier-and-anomalies",
    "href": "data/quality.html#outlier-and-anomalies",
    "title": "12  Data Quality",
    "section": "12.3 Outlier and Anomalies",
    "text": "12.3 Outlier and Anomalies",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data Quality</span>"
    ]
  },
  {
    "objectID": "data/quality.html#unbalanced-data",
    "href": "data/quality.html#unbalanced-data",
    "title": "12  Data Quality",
    "section": "12.4 Unbalanced Data",
    "text": "12.4 Unbalanced Data\n\n\n\nFigure 12.1: Sweetviz visualization for California Housing Prices. Main screen, some numeric variables.\nFigure 12.2: Profiling information for qualitative variable ocean_proximity.\nFigure 12.3: Sweetviz detail for the variable housing_median_age.\nFigure 12.4: Sweetviz visualization of pairwise associations in California Housing Prices data.\nFigure 12.5: Profiling details for ocean_proximity with target median_house_value.\nFigure 12.6: Distribution of home values and logarithm of home values in Albemarle County, VA. The log-transformed data is more symmetric distributed. Since all home values are positive, the transformation does not lead to missing values.\n\n\n\nBorne, Kirk. 2021. “Data Profiling–Having That First Data with Your Data.” Medium. https://medium.com/codex/data-profiling-having-that-first-date-with-your-data-2e05de50fca7.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data Quality</span>"
    ]
  },
  {
    "objectID": "data/summarization.html",
    "href": "data/summarization.html",
    "title": "13  Data Summarization",
    "section": "",
    "text": "13.1 Location and Dispersion Statistics\nTable 33.1 shows important location statistics and Table 33.2 important dispersion statistics.\nThe most important location measures are the sample mean \\(\\overline{Y}\\) and the median. The sample mean is the arithmetic average of the sample values. Because the sample mean can be affected by outliers—large values are pulling the sample mean up—the median is preferred in those situations. For example, in reporting central tendency of annual income, the median income is chosen over the arithmetic average.\nIn the parlance of databases, the descriptive statistics in the previous tables are called aggregates.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Summarization</span>"
    ]
  },
  {
    "objectID": "data/summarization.html#location-and-dispersion-statistics",
    "href": "data/summarization.html#location-and-dispersion-statistics",
    "title": "13  Data Summarization",
    "section": "",
    "text": "Table 13.1: Important statistics measuring location attributes of a distribution. Sample mean, sample median, and sample mode are measures of the central tendency of a variable. \\(Y_{(k)}\\) denotes the value at the \\(k\\)th position when the values are arranged in ascending order; this is called the \\(k\\)th order statistic. The min is defined as the smallest non-missing values because NaNs often sort as smallest values in software packages.\n\n\n\n\n\n\n\n\n\n\n\nSample Statistic\nSymbol\nComputation\nNotes\n\n\n\n\nMin\n\n\\(Y_{(1)}\\)\nThe smallest non-missing value\n\n\nMax\n\n\\(Y_{(n)}\\)\nThe largest value\n\n\nMean\n\\(\\overline{Y}\\)\n\\(\\frac{1}{n}\\sum_{i=1}^n Y_i\\)\nMost important location measure, but can be affected by outliers\n\n\nMedian\nMed\n\\(\\left \\{ \\begin{array}{cc} Y_{\\left(\\frac{n+1}{2}\\right)} & n \\text{ is even} \\\\\n\\frac{1}{2} \\left( Y_{\\left(\\frac{n}{2} \\right)} + Y_{\\left(\\frac{n}{2}+1\\right)} \\right) & n\\text{ is odd} \\end{array}\\right .\\)\nHalf of the observations are smaller than the median; robust against outliers\n\n\nMode\nMode\n\nThe most frequent value; not useful when real numbers are unique\n\n\n1st Quartile\n\\(Q_1\\)\n\\(Y_{\\left(\\frac{1}{4}(n+1) \\right)}\\)\n25% of the observations are smaller than \\(Q_1\\)\n\n\n2nd Quartile\n\\(Q_2\\) = Med\nSee Median\n50% of the observations are smaller than \\(Q_2\\). This is the median\n\n\n3rd Quartile\n\\(Q_3\\)\n\\(Y_{\\left(\\frac{3}{4}(n+1) \\right)}\\)\n75% of the observations are smaller than \\(Q_3\\)\n\n\nX% Percentile\n\n\\(Y_{\\left(\\frac{X}{100}(n+1) \\right)}\\)\nFor example, 5% of the observations are larger than \\(P_{95}\\), the 95% percentile\n\n\n\n\n\n\n\n\n\n\nDefinition: Aggregation\n\n\nAggregation in a database management system (DBMS) is the process of combining records into meaningful quantities called aggregates.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Summarization</span>"
    ]
  },
  {
    "objectID": "data/summarization.html#iris-data",
    "href": "data/summarization.html#iris-data",
    "title": "13  Data Summarization",
    "section": "13.2 Iris Data",
    "text": "13.2 Iris Data\nWe are now calculating these summary statistics with Pandas, Polars, and with SQL for the famous Iris data set; a staple of data science and machine learning instruction. The data was used by R.A. Fisher, a pioneer and founder of modern statistics, in a 1936 paper “The use of multiple measurements in taxonomic problems.” The data set contains fifty measurements of four flower characteristics, the length and width of sepals and petals for each of three iris species: Iris setosa, Iris versicolor, and Iris virginica. The petals are the large leaves on the flowers, sepals are the smaller leaves at the bottom of the flower. You will likely see the iris data again, it is used to teach visualization, regression, classification, clustering, etc.\n\n\n\n\n\n\nFigure 13.1: Iris versicolor flowers\n\n\n\nWe can load the data from our DuckDB database into Pandas and Polars DataFrames with the following statements:\n\nimport pandas as pd\nimport polars as pl\nimport duckdb\ncon = duckdb.connect(database=\"../ads5064.ddb\")\n\npd_df = con.sql('select * from iris').df()\npd_dl = con.sql('select * from iris').pl()\n\nA basic set of statistical measures is computed with the describe() methods of these libraries:\n\npd_df.describe()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.054000\n3.758667\n1.198667\n\n\nstd\n0.828066\n0.433594\n1.764420\n0.763161\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\n\npd_dl.describe()\n\n\n\nshape: (9, 6)\n\n\n\nstatistic\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\nstr\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n\"count\"\n150.0\n150.0\n150.0\n150.0\n\"150\"\n\n\n\"null_count\"\n0.0\n0.0\n0.0\n0.0\n\"0\"\n\n\n\"mean\"\n5.843333\n3.054\n3.758667\n1.198667\nnull\n\n\n\"std\"\n0.828066\n0.433594\n1.76442\n0.763161\nnull\n\n\n\"min\"\n4.3\n2.0\n1.0\n0.1\n\"setosa\"\n\n\n\"25%\"\n5.1\n2.8\n1.6\n0.3\nnull\n\n\n\"50%\"\n5.8\n3.0\n4.4\n1.3\nnull\n\n\n\"75%\"\n6.4\n3.3\n5.1\n1.8\nnull\n\n\n\"max\"\n7.9\n4.4\n6.9\n2.5\n\"virginica\"\n\n\n\n\n\n\n\nPandas and Polars produce very similar output statistics for the numeric variables. Polars adds a row for the number of missing observations (null_count) and some basic summaries for the qualitative species variable. Apart from formatting, the results agree. The output column std represents the sample standard deviation, 25%, 50%, and 75% are the first, second, and third quartile (the 25th, 50th, and 75th percentiles), respectively.\nTo produce this table of summary statistics with SQL requires a bit more work. To compute the statistics for a particular column, say, sepal_length,\n\ncon.sql(\"select count(sepal_length) as count, \\\n        mean(sepal_length) as mean,  \\\n        stddev(sepal_length) as std,\\\n        min(sepal_length) as min, \\\n        quantile(sepal_length,.25) as q1, \\\n        quantile(sepal_length,.50) as q2, \\\n        quantile(sepal_length,.75) as q3, \\\n        max(sepal_length) as max from iris\")\n\n┌───────┬───────────────────┬────────────────────┬────────┬────────┬────────┬────────┬────────┐\n│ count │       mean        │        std         │  min   │   q1   │   q2   │   q3   │  max   │\n│ int64 │      double       │       double       │ double │ double │ double │ double │ double │\n├───────┼───────────────────┼────────────────────┼────────┼────────┼────────┼────────┼────────┤\n│   150 │ 5.843333333333335 │ 0.8280661279778637 │    4.3 │    5.1 │    5.8 │    6.4 │    7.9 │\n└───────┴───────────────────┴────────────────────┴────────┴────────┴────────┴────────┴────────┘\n\n\nAnd now we have to repeat this for other columns. In this case, SQL is much more verbose and clunky compared to the simple describe() call. However, the power of SQL becomes clear when you further refine the analysis. Suppose you want to compute the previous result separately for each species in the data set. This is easily done by adding a GROUP BY clause to the SQL statement (group by species) and listing species in the SELECT:\n\ncon.sql(\"select species, count(sepal_length) as count, \\\n        mean(sepal_length) as mean,  \\\n        stddev(sepal_length) as std,\\\n        min(sepal_length) as min, \\\n        quantile(sepal_length,.25) as q1, \\\n        quantile(sepal_length,.50) as q2, \\\n        quantile(sepal_length,.75) as q3, \\\n        max(sepal_length) as max from iris group by species\")\n\n┌────────────┬───────┬───────────────────┬────────────────────┬────────┬────────┬────────┬────────┬────────┐\n│  species   │ count │       mean        │        std         │  min   │   q1   │   q2   │   q3   │  max   │\n│  varchar   │ int64 │      double       │       double       │ double │ double │ double │ double │ double │\n├────────────┼───────┼───────────────────┼────────────────────┼────────┼────────┼────────┼────────┼────────┤\n│ virginica  │    50 │ 6.587999999999998 │  0.635879593274432 │    4.9 │    6.2 │    6.5 │    6.9 │    7.9 │\n│ setosa     │    50 │ 5.005999999999999 │ 0.3524896872134513 │    4.3 │    4.8 │    5.0 │    5.2 │    5.8 │\n│ versicolor │    50 │             5.936 │ 0.5161711470638635 │    4.9 │    5.6 │    5.9 │    6.3 │    7.0 │\n└────────────┴───────┴───────────────────┴────────────────────┴────────┴────────┴────────┴────────┴────────┘\n\n\nInterpretation: The average sepal length increases from I. setosa (5.006) to I. versicolor (5.936) to I. virginica (6.588). The variability of the sepal length measurements also increased in that order. You can find I. versicolor plants with large sepal length for that species that exceed the sepal length of the I. virginica specimens at the lower end of the spectrum. That can be gleaned from the proximity of the sample means and the size of the standard deviation. It is confirmed by comparing max and min of the species.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Summarization</span>"
    ]
  },
  {
    "objectID": "data/summarization.html#group-by-aggregation",
    "href": "data/summarization.html#group-by-aggregation",
    "title": "13  Data Summarization",
    "section": "13.3 Group-by Aggregation",
    "text": "13.3 Group-by Aggregation\nA group-by analysis computes analytic results separately for each group of observations. It is a powerful tool to gain insight on how data varies within a group and between groups. Groups are often defined by the unique values of qualitative variables, but they can also be constructed by, for example, binning real-valued variables. In the Iris example, the obvious grouping variable is species.\nGoing from an ungrouped to a grouped analysis is easy with SQL—just add a GROUP BY clause. With Pandas and Polars we need to do a bit more work. Suppose we want to compute the min, max, mean, and standard deviation for the petal_width of I. setosa and I. virginica. This requires filtering records (excluding I. versicolor) and calculating summary statistics separately for the two remaining species.\nThe syntax for aggregations with group-by is different in Pandas and Polars. With Pandas, you can use a statement such as this:\n\npd_df[pd_df[\"species\"] != \"versicolor\"][[\"species\",\"sepal_width\"]] \\\n    .groupby(\"species\").agg(['min','max','mean','std'])\n\n\n\n\n\n\n\n\n\nsepal_width\n\n\n\nmin\nmax\nmean\nstd\n\n\nspecies\n\n\n\n\n\n\n\n\nsetosa\n2.3\n4.4\n3.418\n0.381024\n\n\nvirginica\n2.2\n3.8\n2.974\n0.322497\n\n\n\n\n\n\n\n\nThe first set of brackets applies a filter (selects rows), the second set of brackets selects the species and sepal_width columns. The resulting DataFrame is then grouped by species and the groups are aggregated (summarized); four statistics are requested in the aggregation.\n\nEager and lazy evaluation\nThe execution model in Pandas is called an eager evaluation. Polars can support eager and lazy evaluation.\n\n\nDefinition: Eager and Lazy Evaluation\n\n\nIn an eager evaluation model, operations are executed as soon as they are encountered. In a lazy evaluation model, the execution of operations is delayed until their results are needed. For example, lazy evaluation of a file read delays the retrieval of data until records need to be processed.\n\n\nLazy evaluation has many advantages:\n\nThe overall query can be optimized.\nFilters (called predicates in data processing parlance) can be pushed down as soon as possible, eliminating loading data into memory for subsequent steps.\nColumn selections (called projections) can also be done early to reduce the amount of data passed to the following step.\nYou can write a lazily evaluated query and analyze it prior to execution; restructuring the query can lead to further optimization.\n\nYou can see the full list of lazy query optimization in Polars here.\nThe Polars syntax to execute the group-by aggregation eagerly is:\n\npd_dl.filter(pl.col(\"species\") != \"versicolor\").group_by(\"species\").agg(\n    pl.col(\"sepal_width\").min().alias('min'),\n    pl.col(\"sepal_width\").max().alias('max'),\n    pl.col(\"sepal_width\").mean().alias('mean'),\n    pl.col(\"sepal_width\").std().alias('std')\n    )\n\n\n\nshape: (2, 5)\n\n\n\nspecies\nmin\nmax\nmean\nstd\n\n\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n\"virginica\"\n2.2\n3.8\n2.974\n0.322497\n\n\n\"setosa\"\n2.3\n4.4\n3.418\n0.381024\n\n\n\n\n\n\n\nThe Polars syntax should be familiar to users of the dplyr package in R. Operations on the DataFrame are piped from step to step. .filter() selects rows, .group_by() groups the filtered data, .agg() aggregates the resulting rows. Since variable sepal_width is used in multiple aggregates, we are adding an .alias() to give the calculated statistic a unique name in the output.\nThe following statements perform lazy evaluation for the same query. The result of the operation is what Polars calls a LazyFrame, rather than a DataFrame. A LazyFrame is a promise on computation. The promise is fulfilled—the query is executed—with q.collect(). Notice that we call the .lazy() method on the DataFrame pd_dl. Because of this, the object q is a LazyFrame. If we had not specified .lazy(), q would be a DataFrame.\n\nq = (\n    pd_dl.lazy()\n    .filter(pl.col(\"species\") != \"versicolor\")\n    .group_by(\"species\")\n    .agg(\n        pl.col(\"sepal_width\").min().alias('min'),\n        pl.col(\"sepal_width\").min().alias('max'),\n        pl.col(\"sepal_width\").mean().alias('mean'),\n        pl.col(\"sepal_width\").min().alias('std'),\n \n        )\n)\n\nq.collect()\n\n\n\nshape: (2, 5)\n\n\n\nspecies\nmin\nmax\nmean\nstd\n\n\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n\"virginica\"\n2.2\n2.2\n2.974\n2.2\n\n\n\"setosa\"\n2.3\n2.3\n3.418\n2.3\n\n\n\n\n\n\n\nThe results of eager and lazy evaluation are identical. The performance and memory requirements can be different, especially for large data sets. Fortunately, the eager API in Polars calls the lazy API under the covers in many cases and collects the results immediately.\nYou can see the optimized query with\n\nq.explain(optimized=True)\n\n'AGGREGATE\\n\\t[col(\"sepal_width\").min().alias(\"min\"), col(\"sepal_width\").min().alias(\"max\"), col(\"sepal_width\").mean().alias(\"mean\"), col(\"sepal_width\").min().alias(\"std\")] BY [col(\"species\")] FROM\\n  DF [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]; PROJECT 2/5 COLUMNS; SELECTION: \"[(col(\\\\\"species\\\\\")) != (String(versicolor))]\"'\n\n\n\n\nStreaming execution\nAnother advantage of lazy evaluation in Polars is the support for streaming execution—where possible. Rather than loading the selected rows and columns into memory, Polars processes the data in batches allowing you to process large data volumes that exceed the memory capacity. To invoke streaming on a LazyFrame, simply add streaming=True to the collection:\n\nq.collect(streaming=True)\n\n\n\nshape: (2, 5)\n\n\n\nspecies\nmin\nmax\nmean\nstd\n\n\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n\"setosa\"\n2.3\n2.3\n3.418\n2.3\n\n\n\"virginica\"\n2.2\n2.2\n2.974\n2.2\n\n\n\n\n\n\n\nStreaming capabilities in Polars are still under development and not supported for all operations. However, the operations for which streaming is supported include many of the important data manipulations, including group-by aggregation:\n\nfilter, select,\nslice, head, tail\nwith_columns\ngroup_by\njoin\nsort\nscan_csv, scan_parquet\n\nIn the following example we are calculating the sample mean and standard deviation for three variables (num_7, num_8, num_9) grouped by variable cat_1 for a data set with 30 million rows and 18 columns, stored in a Parquet file.\n\nnums = ['num_7','num_8', 'num_9']\n\nq3 = (\n    pl.scan_parquet('../datasets/Parquet/train.parquet')\n    .group_by('cat_1')\n    .agg(\n        pl.col(nums).mean().suffix('_mean'),\n        pl.col(nums).std().suffix('_std'),\n    )\n)\nq3.collect(streaming=True)\n\n/var/folders/ff/6m670wks1776ck_gbfxynxrc0000gq/T/ipykernel_76787/2211535918.py:7: DeprecationWarning:\n\n`suffix` is deprecated. It has been moved to `name.suffix`.\n\n/var/folders/ff/6m670wks1776ck_gbfxynxrc0000gq/T/ipykernel_76787/2211535918.py:8: DeprecationWarning:\n\n`suffix` is deprecated. It has been moved to `name.suffix`.\n\n\n\n\n\nshape: (4, 7)\n\n\n\ncat_1\nnum_7_mean\nnum_8_mean\nnum_9_mean\nnum_7_std\nnum_8_std\nnum_9_std\n\n\ni64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n3\n0.499942\n0.499939\n0.500096\n0.288645\n0.288696\n0.288626\n\n\n4\n0.499922\n0.500028\n0.500063\n0.288682\n0.288736\n0.288723\n\n\n1\n0.499961\n0.499809\n0.499931\n0.288738\n0.288651\n0.2887\n\n\n2\n0.500114\n0.500171\n0.499994\n0.288637\n0.288664\n0.288766\n\n\n\n\n\n\n\nThe scan_parquet() function reads lazily from the file train.parquet, .group_by() and .agg() functions request the statistics for a list of columns. The .suffix() method adds a string to the variable names to distinguish the results in the output.\nThe streaming Polars code executes on my machine in 0.3 seconds. The Pandas equivalent produces identical results in about 4 seconds—an order of magnitude slower with much higher memory consumption.\nYou can use lazy execution with data stored in other formats. Suppose we want to read data from a DuckDB database and analyze it lazily with minibatch streaming. The following statements do exactly that for the Iris data stored in DuckDB\n\nqduck = (\n    con.sql('select * from iris').pl().lazy()\n    .select([\"species\", \"sepal_width\"])\n    .filter(pl.col(\"species\") != \"versicolor\")\n    .group_by(\"species\")\n    .agg(\n        pl.col(\"sepal_width\").min().alias('min'),\n        pl.col(\"sepal_width\").min().alias('max'),\n        pl.col(\"sepal_width\").mean().alias('mean'),\n        pl.col(\"sepal_width\").min().alias('std'),\n \n        )\n)\nqduck.collect(streaming=True)\n\n\n\nshape: (2, 5)\n\n\n\nspecies\nmin\nmax\nmean\nstd\n\n\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n\"virginica\"\n2.2\n2.2\n2.974\n2.2\n\n\n\"setosa\"\n2.3\n2.3\n3.418\n2.3\n\n\n\n\n\n\n\n\n\n\nFigure 13.1: Iris versicolor flowers",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Summarization</span>"
    ]
  },
  {
    "objectID": "data/visualization.html",
    "href": "data/visualization.html",
    "title": "14  Data Visualization",
    "section": "",
    "text": "14.1 Visual Learning\nIt is often said that “a picture is worth more than a thousand words”. That does not mean that you should always choose a data visualization over a tabular data summary or over a descriptive statement. To choose the best medium for communicating information, you need to understand what medium works best for the audience. Tabular and graphical displays complement each other, or in the words of Gelman and Unwin (2013):\nAccording to the VARK theory, there are four predominant learning styles, Visual, Auditory (aural), Read/Write, and Kinesthetic (tactile). Kinesthetic learners learn best through experience and interactions—they are hands-on learners. Read/Write learners prefer text as input and output; they interact with material through reading and writing. Aural learners prefer to retain information via listening—lectures, podcasts, discussions, talking out loud. Visual learners, finally, retain information best when it is presented in the forms of graphs, figures, images, charts, photos, videos.\nMost humans are visual learners, one estimate claims 65% of us learn best through visual means. Visuals add speed to communication. If the mode of communication matches the preferred learning style, information is retained more easily, the learning process is more engaging and fun, and the memory created is stronger. As a visual learner you are more likely to recall a graphic than you are a paragraph of text.\nWhy is that?\nThe answer lies in the way in which we process information in our sub-conscious and conscious minds. The sub-conscious is uncontrolled, always on, and effortless—it is your brain on autopilot. The conscious mind is where the hard work happens, it requires effort to engage.\nWe can take in much more information through sight than through any other sense. The amount of information processed in the conscious mind is much lower compared to the sub-conscious mind for any of the senses, as shown in the following figure. That means the sub-conscious acts as a filter for information, passing to the conscious mind that which needs more in-depth processing and engagement. The combination of processing power and bandwidth is why sight is most suited for understanding data.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data/visualization.html#visual-learning",
    "href": "data/visualization.html#visual-learning",
    "title": "14  Data Visualization",
    "section": "",
    "text": "a picture may be worth a thousand words, but a picture plus 1000 words is more valuable than two pictures or 2000 words.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.1: Sensory bandwidth of the sub-conscious and the conscious mind. From Creative Bloq.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data/visualization.html#twenty-apple-trees",
    "href": "data/visualization.html#twenty-apple-trees",
    "title": "14  Data Visualization",
    "section": "14.2 Twenty Apple Trees",
    "text": "14.2 Twenty Apple Trees\nTo describe data, visualization is key in data analytics. Tabular data is how computers process information. It is not how we can best process data. However, do not discount showing numbers in tables. Whether raw data or summarized data, tabular displays have their place. Let’s look at an example.\nTable 14.1 displays the diameters in inches of 20 apples from 2 trees over six measurements periods. The measurements are spaced 2 weeks apart and were collected at the Winchester Agricultural Experiment Station of Virginia Tech. The total data set contains 80 apples from 10 trees.\n\n\n\nTable 14.1: Diameters in inches of twenty apples from two trees over six two-week measurement periods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTree\nApple\nPeriod 1\nPeriod 2\nPeriod 3\nPeriod 4\nPeriod 5\nPeriod 6\n\n\n\n\n1\n1\n2.90\n2.90\n2.90\n2.93\n2.94\n2.94\n\n\n\n2\n2.86\n2.90\n2.93\n2.96\n2.99\n3.01\n\n\n\n3\n2.75\n2.78\n2.80\n2.82\n2.82\n2.84\n\n\n\n4\n2.81\n2.84\n2.88\n2.92\n2.92\n2.95\n\n\n\n5\n2.75\n2.78\n2.80\n2.82\n2.83\n2.90\n\n\n\n6\n2.92\n2.96\n2.96\n3.02\n3.02\n3.04\n\n\n\n7\n3.08\n\n\n\n\n\n\n\n\n8\n3.04\n3.10\n3.11\n3.15\n3.18\n3.21\n\n\n\n9\n2.78\n2.82\n2.83\n2.86\n2.87\n\n\n\n\n10\n2.76\n2.78\n2.82\n2.85\n2.86\n2.87\n\n\n\n11\n2.79\n2.86\n2.88\n2.93\n2.95\n3.98\n\n\n\n12\n2.76\n2.81\n2.82\n2.86\n2.90\n2.90\n\n\n\n\n\n\n\n\n\n\n\n\n2\n1\n2.84\n2.89\n2.92\n2.93\n2.95\n\n\n\n\n2\n2.75\n2.80\n2.82\n2.84\n2.86\n2.86\n\n\n\n3\n2.78\n2.81\n2.84\n2.85\n2.87\n2.90\n\n\n\n4\n2.84\n2.86\n2.86\n\n\n\n\n\n\n5\n2.83\n2.88\n2.89\n2.92\n2.93\n2.93\n\n\n\n6\n2.80\n2.86\n2.89\n2.92\n2.93\n2.95\n\n\n\n7\n2.86\n2.89\n2.92\n2.96\n2.96\n2.99\n\n\n\n8\n2.75\n2.80\n2.83\n2.85\n2.86\n2.88\n\n\n\n\n\n\nIf we were to display the entire data in a table, it would use four times as much space and it would be difficult to comprehend the data—to see what is going on. However, the tabular display is useful in some respects:\n\nThe exact values are shown.\nWe see that there are missing values for some apples. For example, apple #14 on tree 1 has only one diameter measurement at the first occasion. Apple #15 on tree 2 has thee measurements.\nOnce measurements are missing, they remain missing, at least for the apples displayed in the table. That suggests that apples dropped out of the study, maybe they fell to the ground, were eaten or harvested.\nApple identifiers are not unique across the trees. Apples with id 11, 15, and 17 appear on tree #1 and on tree #2. That is important information; if we want to calculate summary statistics for apples, we must also take tree numbers into account. The technical term for this arrangement is that apple ids are nested within the tree ids.\n\nOther aspects of the data that are difficult to ascertain from the table:\n\nApple diameters should not decrease over time. You need to scan every row to check for possible violations; they would suggest measurement errors.\nWhat do the trends over time look like? Are there significant changes in apple diameter over the 12-week period? If so, what is the shape of the trend?\nDo apples grow at similar rates on the different trees?\nWhat does the data from the other trees look like?\nHow many apples were measured on each tree?\n\nWe can see trends much better than we can read trends.\nFigure 14.2 shows a trellis plot of the diameters of all eighty apples over the twelve-week study period. This type of graph is also called a lattice plot or a conditional plot. The display is arranged by one or more variables, and a separate plot is generated for the data associated with values of the conditioning variables. Here, the trellis plot is conditioned on the tree number, producing ten scatter plots of apple diameters for a given tree. The plots are not unrelated, however. They share the same \\(x\\)-axis and \\(y\\)-axis to facilitate comparisons across the plots.\n\n\n\n\n\n\nFigure 14.2: Trellis (lattice) graphic showing the diameters over time for apples on ten trees.\n\n\n\nFrom the trellis plot we can easily see information that is difficult to read from the tabular display:\n\nThe varying number of apples per tree\nThere is a definite trend over time in apple growth and it appears to be linear over time. Every two weeks each apple seems to grow by a steady amount; the amount differs between apples and trees.\nThere is variability between the trees and variability among the apples from the same tree. The apple-to-apple variability is small on tree #2, and it is larger on trees #9 and #10, for example.\nAll eighty apples are shown in a compact display.\nThe smallest diameter seems to be around 2.8 inches. In fact, these apples are a subset of a larger sample of apples, limited to fruit with at least 2.75 inches of diameter at the initial measurement.\nThe measurements are evenly spaced.\n\nThe graph is helpful to see patterns: trends, groupings, variability.\nIt does make consuming some information more difficult. For example, we are losing track of the actual diameter measurements. We see which measurements are larger and smaller (the pattern), but not their actual value. Showing the actual values to two decimal places is not the purpose of the graph. If we want to know the diameter of apple id 4 on tree #7 at time 5, we can query the data to retrieve the exact value.\n\nimport duckdb\nimport polars as pl\ncon = duckdb.connect(database=\"../ads5064.ddb\")\napples = con.sql(\"SELECT * FROM apples\").pl()\napples.filter((pl.col(\"Tree\")==7) & (pl.col(\"appleid\")==4) & (pl.col(\"measurement\")==5))\n\n\n\nshape: (1, 4)\n\n\n\nTree\nappleid\nmeasurement\ndiameter\n\n\ni64\ni64\ni64\nf64\n\n\n\n\n7\n4\n5\n2.92\n\n\n\n\n\n\n\nWe could add labels to the data symbols in the graph, or even replace the circles with labels that show the actual value, but this would make the graph really busy and messy to read.\nWe also have lost information about which data point belongs to which apple. Since diameters grow over time, our brain naturally interpolates the sequence of dots. For the largest measurements on say, tree #7 and tree #10, we are easily convinced that the dots belong to the same apple. We cannot be as confident when data points group together more. And we cannot be absolutely certain that the largest observations on tree #7 and tree #10 belong to the same apple. And without further identifying information, we do not know which apple that is.\nThere are other ways in which the graphic can be enhanced or improved:\n\nAdd trends over time for individual apples and/or trees. That can help identify a model and show the variability between and within trees.\nAdding the word “Tree” to the trellis labels if it is not clear what the numbers in the grey bars refer to. A downside would be that repeating the word “Tree” ten times does not add new information beyond the first cell of the plot.\nVarying the plotting symbols or colors within a cell of a lattice to show which data points belong to the same apple. (Is it better to vary colors or symbols or both?)\nAdd an apple id, maybe in the right margin of each cell, aligned with the last measurement. (Would that work if an apple is not observed at the last measurement?)\n\nThe tableau and the trellis graph display the raw data. We can also choose to work with summaries. Suppose we are interested in understanding the distribution of apple diameters by measurement time. The following statements compute several descriptive statistics from diameters at each measurement occasion.\n\nq = (\n    apples.lazy()\n    .filter(pl.col(\"diameter\") != None)\n    .group_by(\"measurement\")\n    .agg(\n        pl.col(\"diameter\").count().alias('count'),\n        pl.col(\"diameter\").mean().alias('mean'),\n        pl.col(\"diameter\").min().alias('min'),\n        pl.col(\"diameter\").quantile(0.25).alias('q1'),\n        pl.col(\"diameter\").quantile(0.5).alias('median'),\n        pl.col(\"diameter\").quantile(0.75).alias('q3'),\n        pl.col(\"diameter\").max().alias('max')\n        )\n    .sort(\"measurement\")\n)\n\nq.collect()\n\n\n\nshape: (0, 8)\n\n\n\nmeasurement\ncount\nmean\nmin\nq1\nmedian\nq3\nmax\n\n\ni64\nu32\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\n\n\n\n\nThe output shows that the number of observations contributing at each measurement time decreases; this is expected as apples are lost during the study. The location statistics (mean, min, max, median, Q1, and Q3) all increase over time, showing that the average apple grows.\nA visual display that produces the same information and conveys the distributions as well as the trend over time more clearly arranges box plots for the measurement times.\n\n\n\n\n\n\nFigure 14.3: Box plots for apple diameters by measurement time.\n\n\n\nThe grey box in the center of the box plot extends from the first to the third quartile; its width is called the inter-quartile range (IQR = \\(Q_3 - Q_1\\)). The box thus covers the central 50% of the distribution. The median is shown as the mid line of the box. The upper and lower extensions of the box are called the whiskers of the box plot. They extend to the largest and smallest observations, respectively, that are within 1.5 times the IQR from the edge of the box. If there are no dots plotted beyond the whiskers, they fall on the max and min, respectively. “Outliers” are shown as dots above and below the whiskers. In the apple data, they appear only at the upper end of the measurements. These outliers are not incorrect observations, they are simply unusual given the probability distribution. If you collect enough samples from any distribution, you will expect to draw a certain number of values from the tails of the distribution.\nThe box plot does not reveal the exact values of the points plotted, but it allows us to see the pattern in the data more easily than the tabular display of descriptive statistics. Except for outlier information, the tabular display contains the same information as the box plots.\nMany decisions go into creating a good data visualization. What should we pay attention to? How do we choose a good plot type? How much embellishment of the graph is too little or too much.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data/visualization.html#process-and-best-practices",
    "href": "data/visualization.html#process-and-best-practices",
    "title": "14  Data Visualization",
    "section": "14.3 Process and Best Practices",
    "text": "14.3 Process and Best Practices\nThe data visualization process starts with defining the intention of the visualization:\n\nWhat is the context for the graphic? Are you in exploratory mode to learn about the data or are you in presentation mode to tell the story of the data.\nWhat is the statistical purpose of the visualization? Are you in a classification, regression, or clustering application? Are you describing the distributions and relationships in the data or the result of training a model?\nWhat is the data to be visualized? Is it raw data or summarized data or data derived from a model?\n\nThe next step in the process is to examine candidate visualizations and to select a concept. Libraries of graphics examples come in handy at this step. Here are some sites that show worked graphics examples:\n\nR Graph Gallery: an extensive collection of graphics in R using base R graphing functions and ggplot2.\nPython Graph Gallery: an extensive collection of graphics in Python organized like the R gallery. Also offers tutorials for Matplotlib, Seaborn, Plotly, and Pandas.\nMakeoverMonday: this is a social data visualization project in the UK that posts a new data set every Monday and invites users to visualize the data. Several visualizations are chosen and displayed in the gallery and the blog each week. The site is also a great resource for data sets.\nData Viz Project: a collection of data visualizations to get inspired and find the right visualization for your data. The project is run by Ferdio, an infographic and data viz company in Copenhagen.\nDataviz Inspiration: Hundreds of impactful data visualization projects from the person behind the R Graph Gallery. With code where available.\nInformation is Beautiful: great visualizations of good news, positive trends, and uplifting statistics.\nDatylon’s Inspiration: Datylon is a data visualization platform, this page shows visualizations by category created with Datylon.\n\nThe next step is to implement the chosen design and to perform the Trifecta checkup discussed in more detail below. Finally, it is a good idea to test the visualization with others and to get constructive feedback on what works and what does not. Does the visualization achieve the goals set out in the first step of the process? What attracts the audience, what distracts the audience?\n\nPre-attentive attributes\nPre-attentive attributes are attributes such as length, width, size, shape, etc., that our brain processes almost instantaneously and with little effort.\n\n\n\n\n\n\n\nVisual processing from light entering to comprehension. Source.\n\n\nWhen information (light) enters the retina, it is processed in iconic memory, a short-term buffer and processor that maintains an image of the world and enriches the information. This is where pre-attentive attributes are processed, rapid and massively parallel in the sub-conscious mind. The visual information is then passed on to visual working memory, short term storage that is combined with information from long-term memory for conscious processing.\nEffective data visualizations encode as much possible information through pre-attentive attributes.\n\n\n\n\n\n\nFigure 14.4: Pre-attentive attributes of data visualization. Source.\n\n\n\nTo encode quantitative values, length and position (in 2-dimensional space) are best as they are naturally interpreted as quantitative. Lines of different lengths are more easily interpreted as smaller and larger values than lines of different widths. Shapes are not useful as a pre-attentive attribute to encode quantitative values. Is a circle larger than a square? That takes more conscious processing to answer and slows down comprehension of the visualization.\nOn the other hand, if we want to identify groups of data points that belong together, then shape or color are very effective pre-attentive attributes, as well as proximity and connecting points.\nYou cannot infer actual values from pre-attentive attributes such as length or size, we only get a greater—lesser impression. Most of the times that is sufficient. Information about actual values has to be added through text, labels, grid lines. These are not pre-attentive attributes but learned symbols that require conscious processing. The effort to comprehend a visualization increases with the addition of non-pre-attentive attributes. You should weigh whether the additional processing effort is justified relative to the information gained. A labeled axis requires fewer annotations and mental processing than labeling every data point with its actual value.\nIf your visualization is used in a context where comparisons are required, the choice of attributes and features determines whether comparisons are more accurate or more generic. The most accurate comparisons are possible using 2-dimensional position and length attributes. Color intensity, color hue, area, and volume allow more generic comparisons but are not useful when accuracy is required.\nA good example of the importance of pre-attentive attributes is displaying quantitative information in pie charts versus bar charts.\nThe pie chart in Figure 14.5 displays five values and uses area to compare and color to differentiate. Can you tell from the chart whether foo is larger than bar? How does ipsum compare to bar and lorem?\n\n\n\n\n\n\nFigure 14.5: A pie chart displaying five values.\n\n\n\nSince it is difficult to make accurate comparisons based on area, why not add labels to the chart. While we are at it, we can also dress up the display by using more color and 3-dimensional plotting.\n\n\n\nA 3-dimensional pie chart. Yikes.\n\n\nAdding percentages to the labels allows us to compare the values and conclude which slices of the pie are larger and which slices are smaller. But if we show the percentages, then why use a graphic in the first place? By using labels to show values the chart requires as much cognitive engagement as a table of values:\n\nThe information from the pie chart as a tabular display.\n\n\nCategory\nfoo\nbar\nbaz\nlorem\nipsum\n\n\n\n\nPercentage\n20\n24\n8\n32\n16\n\n\n\nThe more vibrant colors do not add to the comprehension of the data and the 3-dimensional display makes things worse: comparing values based on volume is more difficult than comparing values based on area which in turn is more difficult than comparing values based on length.\nQuantitative values can be encoded as pre-attentive attributes and comparisons are most accurate for lines and 2-dimensional positions. Figure 14.6 visualizes the data using pre-attentive attributes. The values are easily distinguished based on the length (height) of the bars. The categories have been ordered by magnitude. No color is necessary to distinguish the categories, the labels are sufficient.\n\n\n\n\n\n\n\n\nFigure 14.6: A nice bar chart using the pre-attentive attribute length to convey differences.\n\n\n\n\n\n\n\nChartjunk\nThe term chartjunk was coined by Edward Tufte in his influential (cult) 1983 book “The Display of Quantitative Information” (Tufte 1983, 2001). Chartjunk are the elements of a data visualization that are not necessary to comprehend the information. Chartjunk distracts from the information the graph wants to convey. Tufte, who taught at Princeton together with John W. Tukey, subscribed to minimalist design: if it does not add anything to the interpretation of the data, do not add it to the chart.\nThe excessive use of colors, heavy grid lines, ornamental shadings, unnecessary color gradients, excessive text, background images, excessive annotations and decorations are examples of chartjunk. Not all graphical elements are chartjunk—you should ask yourself before adding an element to a graphic: is it helpful? Text annotations can be extremely helpful, but overdoing it can lead to busy charts that are not intelligible. By adding too many text labels, the label avoidance algorithm of graphing software might place labels in areas of the graph where they are misleading. If grid lines are not necessary for the interpretation of the graphic, leave them off. If grid lines are helpful, add them in a color or with transparency that does not distract from the data in the graph.\nUnfortunately, it is all too easy to add colors, styles, annotations, grid lines, inserts, legends, etc. to graphics. Software makes it easy to overdo it.\nTufte’s war path on chartjunk needs to be moderated for our purposes. The minimalist view that anything that is unnecessary to comprehend the information is junk goes too far. We must keep the purpose of the data visualization in mind. In exploratory mode, you generate lots of graphics and different views of the data to learn about the data, find patterns, and stimulate thought—the audience is you. Naturally, we eschew adding too many elements to graphics, everything is about speed and flexibility—polish takes time. In presentation mode, the data visualization needs to grab attention and open the door for the audience to engage with the data. Annotations, colors, labels, headlines, titles, which would be chartjunk in exploratory mode, have a different role and place in presentation mode. They might not be necessary to comprehend the information but can reduce the cognitive burden the audience members have to expend.\nTufte also introduced the concept of the data-ink ratio: a visualization should maximize the amount of ink it uses on displaying data and minimize the amount of ink used on embellishments and annotations.\nFigure 14.7 is a junkified version of the bar chart in Figure 14.6—it is full of chartjunk and devotes too much ink to things other than data:\n\nThe legend is not necessary; categories are identified through the labels.\nThe axis label “Category” is not necessary, it is clear what is being displayed.\nThe vertical orientation of the axis label and the horizontal orientation of the categories is visually distracting.\nVarying the colors of the bars is distracting and does not add new information. The relevant information for comparison is the length of the bars.\nThe grid lines are intrusive and add too much ink to the plot.\n\n\n\n\n\n\n\n\n\nFigure 14.7: A junkified bar chart.\n\n\n\n\n\n\n\nThe Trifecta checkup\nThe data visualization expert Kaiser Fung created a framework to criticize data visualizations. It is recommended that you run your visualizations through this checkup. It rests on three simple investigations:\n\nWhat question are we trying to answer (Q)?\nWhat do the data say (D)?\nWhat does the visualization say (V)?\n\nHopefully, the answers to the tree lines of inquiry are the same. The framework is arranged in a Question—Data—Visualization triangle, in what Fung calls the junk charts trifecta:\n\n\n\n\n\n\nFigure 14.8: The Junk Charts Trifecta Checkup according to Kaiser Fung. Source.\n\n\n\nA good visualization scores on all three dimensions, Q—D—V. It poses an interesting question, uses quality data that are relevant to the question, and visualization techniques that follow best practices and bring out the relevant information (without adding chartjunk). Tufte’s concerns about chartjunk and maximizing the data-ink ratio fall mostly in the V corner of the trifecta. But even the best visualization techniques are useless when applied to bad or irrelevant data or attempting to answer an irrelevant or ill-formed question.\nAn example of a graphic that fails on all three dimensions is discussed by Fung here and shown below. This graphic appeared in the Wall Street Journal.\n\n\n\n\n\n\nFigure 14.9: Citi Bike riders.\n\n\n\n\nQ—What question are we trying to answer? How many riders use Citi Bike during a weekday? That is not a very interesting question, unless you are a city planner. Even then, you are more interested in when and where the bikes are used, rather than some overall number. The chart breaks the daily usage down over time. We see that most riders are in the morning and early evening—going to work and leaving work. That too is not very interesting and not at all surprising.\nD—What do the data say? The data were collected on two days in the fall. What does this represent? Certainly not the average usage over the year. How were those two days selected? Where was the data collected? Randomly throughout the city, only downtown, in certain districts? What was sampled? The days of the months? The riders? The city districts?\nV—What does the visualization say? Does the graphic answer the question using best practices for data visualization? There is much going wrong here.\n\nThe city background is chartjunk, an unnecessary embellishment that does not add any information.\nSimilarly, the bicycle icon is unnecessary. It might be moderately helpful in clarifying that “Bike” refers to bicycle and not motorbike, but that could be made clear without adding a graphics element.\nThe blue dots and the connecting lines are a real problem. How are the connections between the dots drawn? Are the segments (some curved, some jagged) based on observations taken at those times? If so, the data should be displayed. If not, what justifies connecting the dots in an irregular way?\nThe scale of the data is misleading. If there was a vertical axis with labels, one could clearly see that the dots are not plotted along an even scale. The vertical distance between the points labeled 65 and 166.5 is about 100 units and is greater than the distance between the points labeled 166.5 and 366.5, about 200 units apart. Once we discover this, we can no longer trust the vertical placement of the dots. Instead, we must make mental arithmetic to interpret the data by value. Displaying the data in a table would have had the same effect.\n\n\n\nFigure 14.10 is a screenshot from a local TV newscast in Virginia; it shows a histogram for the five most active years in terms of number of tornadoes. Does this graphic pass the trifecta checkup?\n\n\n\n\n\n\nFigure 14.10: Tornado frequency reported by local news in Virginia, June 2024.\n\n\n\n\nQ—What question are we trying to answer? Is 2024 an unusual year in terms of tornado activity?\nD—What do the data say? 2024 is among the top-5 years of tornado activity. But we do not know whether this is for the entire U.S., the entire world, or for the state of Virginia? It is probably not the latter, 1,000 reported tornadoes per year in Virginia is a bit much. How far along are we in 2024 when this was reported? 2024 might not be an unusual year compared to the top-5 if the report was issued at the end of tornado season; 2024 might be an outlier if the report was issued early on in the tornado season.\nV—What does the visualization say? There is a lot going on in the visualization. The bars are ordered from high to low, which disrupts the ordering by time. The eye is naturally drawn to the year labels at the bottom of the bars and has to work overtime to make sense of the chronology. When data are presented in a temporal context our brain wants the data arranged by time (Figure 14.11). The asterisk near the number 1,109 for 2024 leads nowhere. We associate it with the text in the bar for that year anyway, so the asterisk is not necessary. The visualization does not tell us what geographic region the numbers belong to. Is this global data or U.S. data?\n\n\n\n\n\n\n\n\n\nFigure 14.11: Tornado frequencies arranged chronologically.\n\n\n\n\n\nThe problem with using a bar chart for select years only is not to give an accurate impression of how far the data points are separated in time. The first two bars are 3 years apart, the next two bars are 6 years apart. Figure 14.12 fixes that issue by spacing the bars approrpriately.\n\n\n\n\n\n\n\n\nFigure 14.12: Tornado frequencies arranged chronologically and spaced correctly.\n\n\n\n\n\n\n\nInfographics\nInfographics are often guilty of adding extraneous information or displaying data in sub-optimal (nonsensical) ways. Below is another example from Junk Charts. The graphic visualizes the 2022 oil production measured in 1,000 barrels per day by country. The data are projected onto a barrel. Countries are grouped by geographic region and by an industry-specific classification into OPEC, non-OPEC, and OPEC+ countries. The geographic regions are delineated on the barrel surface with thick white lines. Thin white lines mark polygons associated with each country. Aggregations by geographic region are shown below the barrel. The industry-specific categorization is displayed as colored rings around the country flag.\nShapes are not a good pre-attentive attribute to display values, and polygons are particularly difficult to comprehend. Presumably, the size of the country polygons is proportional to their oil production—but who knows, there is no way of validating this. The use of polygons increases the cognitive burden to comprehend the visualization.\nThe visualization contains duplicated information:\n\nEach polygon is labeled with the countries’ oil production; this information duplication is required to make sense of the data because the polygon area is difficult to interpret.\nGreater/lesser oil production by country is also displayed through the size of the map inserts and the font (boldness and font size) of the country names.\nThe country information is duplicated unnecessarily. Countries are shown by name and with their flags. Some country names are abbreviated, and this adds extra mental processing to identify the country. If you are not familiar with working with country codes, identifying the oil production for Angola is tricky (AGO).\nThe geographic summaries display totals as labels and graphically by stacking barrel symbols; each barrel corresponds to 1,000 barrels of oil produced per day.\n\n\n\n\n\n\n\nFigure 14.13: A display of 2022 oil production by country; full of chartjunk. Source.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data/visualization.html#choosing-the-data-visualization",
    "href": "data/visualization.html#choosing-the-data-visualization",
    "title": "14  Data Visualization",
    "section": "14.4 Choosing the Data Visualization",
    "text": "14.4 Choosing the Data Visualization\nAndrew Abela published the Chart Chooser, a great visualization to help select the appropriate data visualization based on the data type and the goal of the visualization (Figure 14.14, Abela (2020)). An online version with templates for PowerPoint and Excel is available here.\n\n\n\n\n\n\nFigure 14.14: Choosing a good chart type for data visualization. Source.\n\n\n\nTo select a chart type, start in the center of the chooser with the purpose of the visualization. Do you want to compare items or variables? Do you want to see the distribution of one or more variables? Do you want to show the relationship between variables or how totals disaggregate?\nThe next two figures show adaptations of the Chart Chooser for continuous and discrete target variables by Prof. Van Mullekom (Virginia Tech).\n\n\n\n\n\n\nFigure 14.15: Chart chooser for continuous target variable.\n\n\n\n\n\n\n\n\n\nFigure 14.16: Chart chooser for discrete target variable.\n\n\n\nSuppose you wish to display the monthly temperatures in four cities, Chicago, Houston, San Diego, and Death Valley. The goal is to compare the temperature throughout the year between the cities. According to the chart chooser for continuous target variables, a paneled scatterplot or overlaid time series plot could be useful. Since the data are cyclical, we could also consider a cyclical chart type.\nA sub-optimal chart type, possibly inspired by plotting temperature, would be a “heat” map. Heat maps are used to display the values of a target variable across the values of two other variables, using color-type attributes (color gradient, transparency, …) to distinguish values. Here, temperature is the target variable, displayed across city and month.\n\n\n\n\n\n\nFigure 14.17: Heat map of temperatures in four cities.\n\n\n\nYou can think of a heat map as the visualization of a matrix; the row-column grid defines the cells, and the color depends on the values in the cells. When properly executed, the heat map reveals patterns between the variables, such as hot spots. Correlation matrices are good examples for the use of heat maps. Also, when you are plotting large data sets, binning the data and using a heat map can reveal patterns while limiting the amount of memory needed to generate the graph. An example is a residual plot for a model with millions of data points.\nThe problems with using the heat map in this example are:\n\nThere is no specific ordering between Chicago, San Diego, Houston, and Death Valley. The cities on the vertical axis are not arranged from North to South either. Death Valley Junction is further north than San Diego and Houston; Houston is the southern-most city of the four. Heat maps are best used when the vertical and horizontal axis can be interpreted in a greater/lesser sense or when both axis refer to the same categories.\nThe cyclical nature of the year is somewhat lost in the display. January connects to December in the same way that it connects to February.\nColors are not a good pre-attention attribute for value comparisons. It is clear that the summer months are hotter in Death Valley than in the other cities, but how much hotter?\nA lot of ink is spent on coloring the squares.\n\nA simpler—and more informative—display of the same data is shown below. A line chart of temperature by month, separate for each location. The differences between the cities are easier to see. The cyclical nature of the data is hinted at through the \\(x\\)-axis label—it begins and ends with January. The grid lines help to identify the actual temperature values.\n\n\n\n\n\n\nFigure 14.18: Line chart for temperatures in four cities.\n\n\n\nData visualization is only one method of information visualization. The interactive periodic table of visualization methods, presents visualization of data, methods, information, strategy, metaphors, and concepts, in the form of a periodic table. Hover over an element to see an example of the visualization method.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data/visualization.html#data-visualization-with-python",
    "href": "data/visualization.html#data-visualization-with-python",
    "title": "14  Data Visualization",
    "section": "14.5 Data Visualization with Python",
    "text": "14.5 Data Visualization with Python\nA large number of Python tools are available for data visualization. You can find the open-source software tools at PyViz.org. From this page of all OSS Python visualization tools on PyViz you see that many of them are built on the same backends, primarily matplotlib, bokeh, and plotly.\n\nMatplotlib\nThe matplotlib library was one of the first Python visualization libraries and is built on NumPy arrays and the SciPy stack. It pre-dates Pandas and was originally conceived as a Python alternative for MATLAB users; that explains the name and why it has a MATLAB-style API. However, it also has an object-oriented API that is used for complex visualizations.\nWhile you can do anything in matplotlib, it does require a lot of boilerplate code for complex graphic; other libraries are providing higher-level APIs to speed up the creation of good data visualizations. Packages such as seaborn are built on matplotlib, so the general vernacular and layout of a seaborn chart is the same as for matplotlib. You can find the extensive matplotlib documentation here.\n\n\nSeaborn\nThe seaborn library has a higher-level API built on top of matplotlib and is deeply integrated with Pandas, remedying two of the frequent complaints about matplotlib. Seaborn alone will get you far, but code often calls matplotlib functions. A typical preamble in Python modules using seaborn is thus something like this:\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nimport seaborn.objects as so\n\nSeaborn categorizes plotting functions into figure-level and axes-level functions. In matplotlib vernacular, axes-level functions plot data onto a matplotlib.pyplot.Axes object. Figure-level functions such as relplot(), displot(), and catplot(), interact with matplotlib through the seaborn FacetGrid object.\n\n\n\n\n\n\nFigure 14.19: Seaborn plotting function overview. Source.\n\n\n\nThe figure-level functions, e.g, displot(), provide an interface to its axes-level functions, e.g., histplot(), and each figure-level module has a default axes-level function (histplot() in the displot() module).\n\nimport seaborn as sns\nimport seaborn.objects as so\n\npenguins = sns.load_dataset(\"penguins\")\nsns.histplot(data=penguins, x=\"flipper_length_mm\", hue=\"species\", multiple=\"stack\")\n\n\n\n\n\n\n\n\nThe figure-level histogram is created by calling the displot() function. You can explicitly ask for histograms with kind=”hist”, or let the function default to producing histograms.\n\nsns.displot(data=penguins, x=\"flipper_length_mm\", hue=\"species\", multiple=\"stack\",kind=\"hist\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA side-effect of using a figure-level function is that the figure owns the canvas. The legend is placed outside of the chart of the figure-level function and inside the chart of the axes-level function.\nOne advantage of figure-level functions is that they can create subplots easily. Removing multiple=\"stack\" produces:\n\nsns.displot(data=penguins, x=\"flipper_length_mm\", hue=\"species\", col=\"species\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemoving multiple=\"stack\" from the axes-level chart produces three overlaid histograms that are difficult to interpret:\n\nsns.histplot(data=penguins, x=\"flipper_length_mm\", hue=\"species\")\n\n\n\n\n\n\n\n\n\n\nPlotly\nPlotly is a Python library for interactive graphics, based on the d3.js JavaScript library. The makers of plotly also have a commercial analytic platform, Dash, but plotly is open source and free to use.\nTo install plotly, assuming you are using pip to manage Python packages, simply run\npip install plotly\nOn my system I also had to\npip install --upgrade nbformat\nand restart VSCode after the upgrade. You will know that this step is necessary when fig.show() throws an error about requiring a more recent version of nbformat than is installed.\nThe plotly library has two APIs, plotly graph objects and plotly express. The express API allows you to generate interactive graphics quickly with minimal code.\nThe southern_oscillation table contains monthly measurements of the southern oscillation index (SOI) from 1951 until today. The SOI is a standardized index based on sea-level pressures between Tahiti and Darwin, Australia. Although the two locations are nearly 5,000 miles apart, that pressure difference corresponds well to changes in ocean temperatures and coincides with El Niño and La Niña weather patterns. Prolonged periods of negative SOI values coincide with abnormally warm ocean waters typical of El Niño. Prolonged positive SOI values correspond to La Niña periods.\nThe following statements load the SOI data from the DuckDB database into a Pandas DataFrame and use the express API of plotly to produce a box plot of SOI values for each year.\n\nimport pandas as pd\nimport plotly.express as px\n\nso = con.sql(\"SELECT * FROM southern_oscillation\").df()\nfig = px.box(x=so[\"year\"],y=so[\"soi\"])\nfig.show() \n\n                        \n                                            \n\n\nThere is a lot happening with just one line of code. The graphic produced by fig.show() is interactive. Hovering over a box reveals the statistics from which the box was constructed. The buttons near the top of the graphic enable you to zoom in and out of the graphic, pan the view, and export it as a .png file.\nThe graph object API of plotly is more detailed, requiring a bit more code, but giving more control. With this API you initiate a visualization with go.Figure(), and update it with update_layout(). The following statements recreate the series of box plots above using plotly graph objects.\n\nimport plotly.graph_objects as go\n\nfig = go.Figure(data=[go.Box(x=so[\"year\"],y=so[\"soi\"])])\nfig.update_layout(title=\"Box Plots of SOI by Year\",\n                  xaxis_title=\"Year\",\n                  yaxis_title=\"SOI\")\n\n                        \n                                            \n\nfig.show()\n\n                        \n                                            \n\n\nNext is a neat visualization that you do not see every day. A parallel coordinate plot represents each row of a data frame as a line that connects the values of the observation across multiple variables. The following statements produce this plot across the sepal and petal measurements of the Iris data. The three species are identified in the plot through colors, which requires a numeric value. Species 1 corresponds to I. setosa, species 2 corresponds to I. versicolor, and species 3 to I. virginica.\n\nfrom functools import reduce\niris = con.sql(\"SELECT * FROM iris\").df()\n\n# recode species as numeric so it can be used as a value for color\nunique_list = reduce(lambda l, x: l + [x] if x not in l else l, iris[\"species\"], [])\nres = [unique_list.index(i) for i in iris[\"species\"]] \ncolors = [x + 1 for x in res]\n\nfig = px.parallel_coordinates(\n    iris, \n    color=colors, \n    labels={\"color\"       : \"Species\",\n            \"sepal_width\" : \"Sepal Width\", \n            \"sepal_length\": \"Sepal Length\", \n            \"petal_width\" : \"Petal Width\", \n            \"petal_length\": \"Petal Length\", },\n    color_continuous_scale=px.colors.diverging.Tealrose,\n    color_continuous_midpoint=2)\n\nfig.update_layout(coloraxis_showscale=False)\n\n                        \n                                            \n\nfig.show()\n\n                        \n                                            \n\n\nThe parallel coordinates plot shows that the petal measurements for I. setosa are smaller than for the other species and that I. setosa has fairly wide sepals compared to the other species. If you wish to classify iris species based on flower measurements, petal length and petal width seem like excellent candidates. Sepal measurements, on the other hand, are not as differentiating between the species.\n\n\nVega-Altair\nVega-Altair is a declarative visualization package for Python and is built on the Vega-Lite grammar. The key concept is to declare links between data columns and visual encoding channels such as the axes and colors. The library attempts to handle a lot of things automatically, for example, deciding chart types based on column types in data frames. The following image is from the Vega-Altair documentation:\n\nimport altair as alt\n\nfrom vega_datasets import data\ncars = data.cars()\n\nalt.Chart(cars).mark_point().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    color='Origin',\n).interactive()\n\n\n\n\n  \n  \n  \n  \n\n\n  \n  \n\n\n\n\nalt.Chart(cars) starts the visualization. mark_point() instructs to display points encoded as follows: Horsepower on the \\(x\\)-axis, Miles_per_Gallon on the \\(y\\)-axis and the color of the points associated with the Origin variable.\n\n\n\nPandas vs Polars\nMatplotlib, Seaborn, Plotly, ggplot, and Altair (since v5+) can work with Polars DataFrames out-of-the-box. If you are running into problems passing a Polars DataFrame to a visualization routine that works fine with a Pandas DataFrame you can always convert using the .to_pandas() function. For example, the parallel coordinates plot in plotly express works with a Pandas DataFrame but generates an AttributeError with a Polars DataFrame. Using the .to_pandas() method took care of the problem.\n\niris = con.sql(\"SELECT * FROM iris\").pl()\n\nfig = px.parallel_coordinates(\n    iris.to_pandas(), \n    color_continuous_scale=px.colors.diverging.Tealrose,\n    color_continuous_midpoint=2)\n\nfig.update_layout(coloraxis_showscale=False)\n\n                        \n                                            \n\nfig.show()\n\n                        \n                                            \n\n\n\n\nGrammar of Graphics (ggplot)\nThe grammar of graphics was described by statistician Leland Wilkinson and conceptualizes data visualization as a series of layers, in an analogy with linguistic grammar (Wilkinson 2005). Just like a sentence has subject and predicates, a scientific graph has parts.\nThe grammar of graphics is helpful because we associate data visualization not by the name of this plot or that chart type, but by a series of elements, depicted as layers.\nEach graphic consists of at least the following layers:\n\nThe data itself\nThe mappings from data attributes to perceptible qualities\nThe geometrical objects that represent the data\n\nIn addition, we might apply statistical transformations of the data, must place all objects in a 2-dimensional space and decide on presentation elements such as fonts and colors. And if the data consist of multiple groups, we need a faceting specification to organize the graphic or the page in multiple groups.\nFigure 14.20 shows the layered representation of the grammar of graphics.\n\n\n\n\n\n\nFigure 14.20: The grammar of graphics; a layered approach to building data visualizations.\n\n\n\nThinking about data visualization in these terms is helpful because we get away from thinking about pie charts and box plots and line charts, and instead about how to organize the basic elements of a visualization.\nR programmers are familiar with the grammar of graphics from the ggplot2() package. The grammar of graphics paradigm is implemented in Python in the plotnine library.\nThe following statements generate a data visualization from data frame df (layer 1). This data frame contains data from 196 observations of the optic nerve head from patients with and without glaucoma. The aes() function defines the aesthetics layer, associating variable eag with the \\(x\\)-axis and specifying variable Glaucoma as a grouping variable (eag is the global effective area of the optic nerve head measured on a confocal laser image). The geometries and statistics layers are added to the previous layers with the geom_density() function, requesting a kernel density plot. The scale_x_log10() function modifies the data-to-aesthetics mapping by applying a log10 scale to the \\(x\\)-axis. The result is a grouped density plot on the log10 scale.\n\nfrom plotnine import ggplot, aes, geom_density, scale_x_log10   \n\ndf = con.sql(\"SELECT eag, Glaucoma from glaucoma\").df()\n\n(ggplot(df, aes(x=\"eag\", group=\"factor(Glaucoma)\", fill=\"factor(Glaucoma)\"))\n    + geom_density() \n    + scale_x_log10()\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\nggplot works out of the box with Polars DataFrames:\n\nglauc_pl = con.sql(\"SELECT eag, Glaucoma from glaucoma\").pl()\n\nggplot(glauc_pl) + aes(x=\"eag\", group=\"factor(Glaucoma)\", fill=\"factor(Glaucoma)\") \\\n    + geom_density() + scale_x_log10()\n\n&lt;Figure Size: (640 x 480)&gt;",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data/visualization.html#snafu-or-misinformation",
    "href": "data/visualization.html#snafu-or-misinformation",
    "title": "14  Data Visualization",
    "section": "14.6 Snafu or Misinformation?",
    "text": "14.6 Snafu or Misinformation?\nData visualizations are powerful tools, they allow the author to focus our attention on information. In choosing a visualization type, filling it with data, and annotating it, you exercise control over what is displayed and how it is interpreted. In 1954, Darrell Huff published one of the most widely sold statistics text, “How to Lie with Statistics” (Huff 1954). Huff covered such topics as how to introduce bias through sampling, how to cherry-pick statistics (“the well-chosen average”), how to misinterpret correlation for causation, and how to distort with statistical graphics. He said\n\nMany a statistic is false on its face. It gets by only because the magic of numbers brings about a suspension of common sense.\n\nExamples of poorly designed graphics and misleading graphics abound. The reasons might be malfeasance, chicanery, disingenuity, or incompetence.\n\n\nExample: Covid-19 Statistics\n\n\nThe following graphic aired on a local TV channel in North Carolina on April 5, 2020. It was the beginning of the Covid-19 pandemic and audiences were keen to hear about the local case counts. Is there anything wrong with the visualization?\n\n\n\n\n\n\n\nCovid-19 cases per day as reported on April 5, 2020 by a local TV station.\n\n\nThe number of daily cases has been more or less steadily increasing since March 18 (33 cases) and two weeks later stands at 376 cases per day.\nThe placement of the bubbles seems odd. The first bubble, 33 cases, seem further away from the horizontal grid line than, say, the bubble for 112 cases on March 21 is distant from the grid line at 100 cases. Maybe it is the angle at which the graph is viewed, but 112 (March 21) and 116 (March 22) should be about 1/2 way between 100 and 130, they are drawn closer to the line at 100.\nSpending a bit more time with the \\(y\\)-axis you notice that the grid lines are evenly spaced, but that the reference labels are not equi-distant. The differences between the grid labels are 30, 30, 10, 30, 30, 30, 50, 10, 50, 50, and 50 units. Why would someone do this?\n\n\nIn a blog entitled “How to Spot Visualization Lies”, Nathan Yau gives numerous examples how chart construction can mislead. For example, a common device to exaggerate differences between groups is to draw bar charts with a baseline different than zero. The height of the bar is the information conveyed by the bar chart so bars should always start at zero (Figure 14.21, data from NOAA).\n\n\n\n\n\n\n\n\nFigure 14.21: Three-month (February–April) tornado occurrence in the U.S. from 1950–2023.\n\n\n\n\n\nWhen the baseline of the bar chart is changed, the length of the bars represent the difference from the baseline and the data need to be presented as such. The coloring of the bars in Figure 14.22 draws extra attention to the fact that we are looking at deviations from a baseline.\n\n\n\n\n\n\n\n\nFigure 14.22: Three-month (February–April) tornado occurrence in the U.S. from 1950–2023 compared to 1980.\n\n\n\n\n\nStatistics that are false on their face are comparisons that are based on absolute numbers but should be done on a relative scale. The absolute tornado numbers are comparable in Figure 14.21 because they relate to time intervals of the same length, February–April in each year. Someone could raise an objection here, because leap years have an extra day in February compared to non-leap years, so 1/4 of the bars have a basis of 90 days and 3/4 of the bars have a basis of 89 days. OK, sue me. The comparisons between the bars is still pretty darn fair.\nHowever, if you compare, say, crime statistics, cancer incidences, service subscribers, etc., between regions or states, then large absolute numbers in large regions are probably not a surprise. There being fewer crimes in Blacksburg, VA than in Chicago does not make Blacksburg a safer place to live. It is a safer place because there are fewer crimes per resident or per 10,000 residents in Blacksburg, VA compared to Chicago.\n\nGallery of Terrible Graphics\n\nAnd now, without much commentary or warning, here are some truly terrible examples of data visualization.\n\n\n\nOlder people are taller? I reckon you never stop growing.\n\n\n\n\n\n\nIs this misleading or what?\n\n\n\n\n\n\nThe bar chart that wasn’t.\n\n\n\n\n\n\n3 out of 4 people are bad with fractions.\n\n\n\n\n\n\nMore graphics elements, please.\n\n\n\n\n\n\nThere are five numbers here! Just five!\n\n\n\n\n\n\nOuch. This is called gaslighting.\n\n\n\n\n\n\n\n\n\n\nThis is known as a fuzzball.\n\n\n\n\n\n\nWait, what?\n\n\n\n\n\n\nThis was in the New York Times, apparently.\n\n\n\n\n\n\nWhat is going on here?\n\n\n\n\n\n\nThose are supposed to be error bars. Not the letter “T”!\n\n\n\n\n\n\nAnother one messing with axes.\n\n\n\n\n\n\nA pie chart gone mad. And what is the “World”?\n\n\n\n\n\n\nA classic: the bicylce of education.\n\n\n\n\n\n\nNever gonna give you up. Thank you Rick.\n\n\n\n\n\n\nFigure 14.1: Sensory bandwidth of the sub-conscious and the conscious mind. From Creative Bloq.\nFigure 14.2: Trellis (lattice) graphic showing the diameters over time for apples on ten trees.\nFigure 14.3: Box plots for apple diameters by measurement time.\nVisual processing from light entering to comprehension. Source.\nFigure 14.4: Pre-attentive attributes of data visualization. Source.\nFigure 14.6: A nice bar chart using the pre-attentive attribute length to convey differences.\nFigure 14.7: A junkified bar chart.\nFigure 14.10: Tornado frequency reported by local news in Virginia, June 2024.\nFigure 14.13: A display of 2022 oil production by country; full of chartjunk. Source.\nFigure 14.14: Choosing a good chart type for data visualization. Source.\nFigure 14.15: Chart chooser for continuous target variable.\nFigure 14.16: Chart chooser for discrete target variable.\nFigure 14.19: Seaborn plotting function overview. Source.\nFigure 14.20: The grammar of graphics; a layered approach to building data visualizations.\nCovid-19 cases per day as reported on April 5, 2020 by a local TV station.\nFigure 14.21: Three-month (February–April) tornado occurrence in the U.S. from 1950–2023.\nFigure 14.22: Three-month (February–April) tornado occurrence in the U.S. from 1950–2023 compared to 1980.\nThis is known as a fuzzball.\n\n\n\nAbela, Andrew. 2020. “Choosing a Good Chart.” https://extremepresentation.typepad.com/blog/2006/09/choosing_a_good.html.\n\n\nGelman, A., and A. Unwin. 2013. “Infovis and Statistical Graphics: Different Goals, Different Looks.” Journal of Computational and Graphical Statistics 22: 2–28. https://www.tandfonline.com/doi/full/10.1080/10618600.2012.761137.\n\n\nHuff, Darrell. 1954. How to Lie with Statistics. W.W. Norton & Company, New York.\n\n\nTufte, E. 1983. The Visual Display of Quantitative Information. Graphics Press.\n\n\n———. 2001. The Visual Display of Quantitative Information, 2nd Ed. Graphics Press.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. Springer Verlag.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data/sqlbasics.html",
    "href": "data/sqlbasics.html",
    "title": "15  SQL Basics",
    "section": "",
    "text": "15.1 Introduction\nThe question is often asked: “Why should I learn SQL as a data scientist?”. Part of the motivation for the question is that SQL has a reputation as difficult to master, is thought of as dated compared to newer languages like Python, Julia, or Go, and is believed to be the domain of the data base administrator and data engineer.\nFigure 15.1 and Figure 15.2 show 2023 results of the annual survey of Master graduates in data science, statistics, and related disciplines, published in the November 2024 issue of Amstat News, the membership magazine of the American Statistical Association.\nAccording to Figure 15.1, 44% of the respondents use SQL at least weekly or even daily to perform their work. A third (32%) of the respondents use SQL daily, presumably to interact with data stored in a database. On the flip side, 42% of the respondents sue SQL rarely or never. You either find yourself working for an organization where data scientists encounter no SQL at all or encounter SQL at least once a week.\nTable 10 of the survey in Figure 15.2 breaks the data down further by disciplines. A stunning 93% of the respondents in Statistics and ML and over 50% of the respondents in the Data Science discipline use SQL every day.\nIn fact, after the primary programming language (R and Python) and Excel—presumably for receiving data that needs to be processed into a different format—SQL is the most frequently used tool for Master graduates.\nThis should settle it. Like it or not, SQL is an important tool for (almost) anyone working with data.\nSQL is a declarative programming language, you express through programming statements what you want to happen, not how to control the flow of the program. Other examples of declarative languages are CSS, HTML, XML, and Prolog. To support writing scripts, functions, and procedures, database vendors have added flow-control statements (if-then, loops, switches, etc.), but these are extensions of the database and not of the SQL language.\nThe concept of a query in SQL is very general: any interaction with a (relational) database. A query in the narrow sense asks questions about the data in the database or retrieves data. Commands that create tables, alter databases, update or delete records, and so on, are also considered queries.\nSQL has been standardized and revised since 1986. SQL-92, for example, refers to the revision of the standard in 1992. However, SQL implementations are vendor-specific and do not necessarily comply with any standard. The SQL syntax between databases is similar enough that you can move your SQL code from one dialect to another without too much pain. Moving between databases you will find that the devil is not in the details of the SQL language, but in the details of how NULLs are represented, how date-time values are handled, how indexes work, how data are partitioned, whether foreign keys are supported, and so on. Since we use DuckDB in this chapter, we are following the DuckDB SQL syntax.\nSQL has a reputation to be difficult to master. The reputation is not entirely unjustified.\nAn interesting aspect (quirk) of domain-specific logic in SQL is three-valued logic. In classical (boolean) logic, there are two logical values, TRUE and FALSE, expressing the relation of a proposition to the truth. In three-valued logic there is a third value, NULL, expressing an unknown state. NULL values are used in databases to represent absent entries and are akin to the concept of the missing value in analytics. NULL values are not zero or empty values. An empty string (“”) in a text column is a known value, a zero-byte string. A NULL value on the other hand states that a string is absent.\nThe examples that follow in this section use DuckDB with the ads5064.ddb database from the command line as follows:\nBecause NULL is a logical state, SQL defines operations on NULL values. Suppose A is TRUE, B is FALSE, and C is NULL, then A or B and A or C both evaluate to TRUE, but A and B is FALSE whereas A and C is also NULL (unknown). To achieve “expected” results when operating on NULL values, special syntax is needed. For example,\nreturns no rows since the comparison x = NULL is never TRUE, its result is always unknown (NULL). If you use this syntax to check for missing values in your data you will conclude that there are none.\nThe correct query uses syntax designed to operate with NULL values:",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data/sqlbasics.html#introduction",
    "href": "data/sqlbasics.html#introduction",
    "title": "15  SQL Basics",
    "section": "",
    "text": "Figure 15.1: Table 9 from the 2023 survey of Master graduates.\n\n\n\n\n\n\n\n\n\nFigure 15.2: Table 10 from the 2023 survey of Master graduates.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nS-Q-L or sequel?\n\n\n\nIs it pronounced “S-Q-L” or “sequel”? Both are acceptable but if you want to give a nod to its origin, then you would pronounce it ”sequel” like SEQUEL, the name given to the project by its inventors at IBM in the 1970s. The description as structured query language and the acronym SQL came later.\n\n\n\n\n\n\nMany programmers are more familiar and comfortable with imperative languages that specify the program flow step by step.\nDeclarative languages are domain specific, the domain for SQL is the manipulation of data in relational systems. This is associated with a lot of special terminology and domain-specific logic. The data scientist thinks in terms of rows and columns, the database engineer thinks in terms of predicates, projections, clauses, constraints, and relations.\nWhile the number of relevant SQL commands is rather small, SQL queries can become astonishingly complex and unwieldy. A single SQL query that accesses multiple tables with correlated (dependent) subqueries, joins, and common table expressions can be hundreds of lines long. If you are not used to reading SQL, a simple subquery such as this one might give you a headache:\n\nSELECT employee_number, name,\n        (SELECT AVG(salary) \n           FROM employees\n          WHERE department = emp.department) AS department_average\n   FROM employees emp\n\nThe point of a declarative language is that the programmer specifies the desired result and leaves it to the particular implementation to achieve it. SQL novices often get frustrated because they know exactly what they want but not how to ask for it using SQL syntax.\nDebugging SQL code is more difficult than debugging code in an imperative language. Error messages can be cryptic. A query that runs successfully is never wrong from the databases point of view, but it is wrong from the coder’s point of view if it does not produce the desired result.\nA query that executes quickly for one database can run slowly in another database. It depends on database architecture, implementation, optimizations, etc. You must learn to ask for the right thing in the right way.\n\n\n\n&gt; duckdb ads5064.ddb\nD\n\nSELECT * FROM landsales WHERE improve = NULL;\n\n┌───────┬─────────┬───────┬───────┬───────────┐\n│ land  │ improve │ total │ sale  │ appraisal │\n│ int64 │  int64  │ int64 │ int64 │  double   │\n├─────────────────────────────────────────────┤\n│                   0 rows                    │\n└─────────────────────────────────────────────┘\n\n\nSELECT * FROM landsales WHERE improve IS NULL;\n┌───────┬─────────┬───────┬────────┬───────────┐\n│ land  │ improve │ total │  sale  │ appraisal │\n│ int64 │  int64  │ int64 │ int64  │  double   │\n├───────┼─────────┼───────┼────────┼───────────┤\n│ 42394 │         │       │ 168000 │           │\n│ 93200 │         │       │ 422000 │           │\n│ 65376 │         │       │ 286500 │           │\n│ 42400 │         │       │        │           │\n└───────┴─────────┴───────┴────────┴───────────┘\n\nSELECT count(*) FROM landsales WHERE improve IS NULL;\n┌──────────────┐\n│ count_star() │\n│    int64     │\n├──────────────┤\n│            4 │\n└──────────────┘",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data/sqlbasics.html#types-of-sql-commands",
    "href": "data/sqlbasics.html#types-of-sql-commands",
    "title": "15  SQL Basics",
    "section": "15.2 Types of SQL Commands",
    "text": "15.2 Types of SQL Commands\nSQL commands are arranged in logical groups. The most important group for the data scientist is the Data Manipulation Language (DML) group, it consists of statements that change the data stored within a database table and traditionally also includes the most important SQL statement, SELECT. The second most important group of SQL commands is the Data Definition Language (DDL), it provides syntax to create and modify database objects.\n\nData Definition Language (DDL): syntax to create and modify database objects. The commands are about the objects themselves, not the data they contain. For example, the CREATE TABLE statement defines the schema of a table and creates it, but it does not populate the table with data. Commands in this group include CREATE, DROP, ALTER.\nData Manipulation Language (DML): syntax to query and modify the contents of tables. The most important SQL command in this group is SELECT. Other commands in this group are INSERT, UPDATE, DELETE. The SELECT statement is sometimes pulled into a separate group, the Data Querying Language (DQL). Since it would be the only statement in that group and because SELECT INTO also modifies rows in a table, the SELECT statement is often included as part of the DML.\nData Control Language (DCL): to control access and manage permissions for users of the database. Commands such as GRANT, DENY, and REVOKE are part of the DCL. As a data scientist you are unlikely to work with DCL commands, they are the domain of the database administrator (DBA). However, you might be charged with setting up and maintaining a database and DCL will help you to change what users (or roles) are allowed to do in the database. The following statements allow user1 to run a SELECT statement on table_foo but disallows updates to the table.\n\nGRANT SELECT ON table_foo TO user1;\nDENY UPDATE ON table_foo TO user1;\nNot all databases support DCL. SQLite and DuckDB do not.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data/sqlbasics.html#most-common-sql-commands",
    "href": "data/sqlbasics.html#most-common-sql-commands",
    "title": "15  SQL Basics",
    "section": "15.3 Most Common SQL Commands",
    "text": "15.3 Most Common SQL Commands\nThe most important SQL commands for a data scientist are the following:\n\nCREATE TABLE\nALTER TABLE\nDROP TABLE\nSELECT\nINSERT INTO\nUPDATE\nDELETE FROM\n\n\nCreating and Modifying a Table\nWe demonstrate these with a small example using DuckDB. You can find the complete documentation for SQL in DuckDB here. A separate section then dives into the SELECT statement, the workhorse for data analysis in relational DBMS.\nFirst, let’s create a small table with information on some cities around the world.\nCREATE TABLE Cities (Country VARCHAR, Name VARCHAR, Year INT, Population INT);\nThe table has four columns named Country, Name, Year, and Population, respectively. The first two are of type VARCHAR (allowing character strings of variable length), the last two are of type INT.\nIf we were to issue the CREATE TABLE statement again, the database would throw an error because the Cities table now exists. To replace an existing table, use CREATE OR REPLACE TABLE:\nCREATE OR REPLACE TABLE Cities (Country    VARCHAR, \n                                Name       VARCHAR, \n                                Year       INT, \n                                Population INT);\nA very useful feature is the addition of check constraints. Suppose you want to make sure that only data after the year 2000 are entered for the Year column. You can add a check constraint like this:\nCREATE OR REPLACE TABLE Cities (Country    VARCHAR, \n                                Name       VARCHAR, \n                                Year       INT CHECK (Year &gt;= 2000), \n                                Population INT);\nTrying to insert a record that fails the check will result in an error:\nINSERT INTO Cities VALUES ('NL', 'Amsterdam', 1999, 1005);\n\nError: Constraint Error: CHECK constraint failed: Cities\nOnce the table exists in the catalog, we can use the ALTER TABLE statement to change its structure, for example, by renaming, adding, or removing columns, changing data types or defaults.\nALTER TABLE Cities RENAME Name to CityName;\nALTER TABLE Cities ADD Column k INTEGER;\nALTER TABLE Cities RENAME to WorldCities;\nThe first statement renames the Name column to CityName, the second statement adds an integer column named k, and the third statement renames the table.\nTo populate the table with data, we insert rows. The following statements add one row at a time and then retrieves the entire contents of the table:\nINSERT INTO WorldCities VALUES ('NL', 'Amsterdam', 2000, 1005, 1);\nINSERT INTO WorldCities VALUES ('NL', 'Amsterdam', 2010, 1065, 3);\nINSERT INTO WorldCities VALUES ('NL', 'Amsterdam', 2020, 1158, 4);\nINSERT INTO WorldCities VALUES ('US', 'Seattle', 2000, 564, 5432);\nINSERT INTO WorldCities VALUES ('US', 'Seattle', 2010, 608, 46);\nINSERT INTO WorldCities VALUES ('US', 'Seattle', 2020, 738, 986);\nINSERT INTO WorldCities VALUES ('US', 'New York City', 2000, 8015, 0);\nINSERT INTO WorldCities VALUES ('US', 'New York City', 2010, 8175, 987);\nINSERT INTO WorldCities VALUES ('US', 'New York City', 2020, 8772, 23);\n\nSELECT * FROM WorldCities;\n┌─────────┬───────────────┬───────┬────────────┬───────┐\n│ Country │   CityName    │ Year  │ Population │   k   │\n│ varchar │    varchar    │ int32 │   int32    │ int32 │\n├─────────┼───────────────┼───────┼────────────┼───────┤\n│ NL      │ Amsterdam     │  2000 │       1005 │     1 │\n│ NL      │ Amsterdam     │  2010 │       1065 │     3 │\n│ NL      │ Amsterdam     │  2020 │       1158 │     4 │\n│ US      │ Seattle       │  2000 │        564 │  5432 │\n│ US      │ Seattle       │  2010 │        608 │    46 │\n│ US      │ Seattle       │  2020 │        738 │   986 │\n│ US      │ New York City │  2000 │       8015 │     0 │\n│ US      │ New York City │  2010 │       8175 │   987 │\n│ US      │ New York City │  2020 │       8772 │    23 │\n└─────────┴───────────────┴───────┴────────────┴───────┘ \nWe won’t need the column k anymore, so we can drop it with\nALTER TABLE WorldCities DROP COLUMN k;\n\n\nPivoting a Table\nA pivot table is a summary for data that contains categorical variables. If the aggregation is a simple count, the pivot table is also called a contingency table. These tables are useful to summarize data by categories and to see trends. DuckDB implements the SQL PIVOT statement; the basic syntax is\nPIVOT ⟨dataset⟩ \n  ON ⟨columns⟩\n  USING ⟨values⟩ \n  GROUP BY ⟨rows⟩  \n  ORDER BY ⟨columns_with_order_directions⟩\n  LIMIT ⟨number_of_rows⟩;\nThe ON clause lists the columns on which to pivot the table. In the WorldCities example we might want to pivot on Year to retrieve the average population for each year:\n PIVOT WorldCities on YEAR USING mean(population);\n┌─────────┬───────────────┬────────┬────────┬────────┐\n│ Country │   CityName    │  2000  │  2010  │  2020  │\n│ varchar │    varchar    │ double │ double │ double │\n├─────────┼───────────────┼────────┼────────┼────────┤\n│ US      │ New York City │ 8015.0 │ 8175.0 │ 8772.0 │\n│ NL      │ Amsterdam     │ 1005.0 │ 1065.0 │ 1158.0 │\n│ US      │ Seattle       │  564.0 │  608.0 │  738.0 │\n└─────────┴───────────────┴────────┴────────┴────────┘ \nThe USING clause determines how to aggregate the values that are split into columns. If it is not provided, the PIVOT statement defaults to count(*) aggregation.\nThe GROUP BY clause allows you to further aggregate the results and the ORDER BY statement affects how the output is displayed:\nPIVOT WorldCities on YEAR\n      USING mean(population) \n      GROUP BY Country \n      ORDER BY Country DESC;\n┌─────────┬────────┬────────┬────────┐\n│ Country │  2000  │  2010  │  2020  │\n│ varchar │ double │ double │ double │\n├─────────┼────────┼────────┼────────┤\n│ US      │ 4289.5 │ 4391.5 │ 4755.0 │\n│ NL      │ 1005.0 │ 1065.0 │ 1158.0 │\n└─────────┴────────┴────────┴────────┘ \nTo obtain multiple aggregations, provide a list in the USING clause:\nPIVOT WorldCities on YEAR\n      USING mean(population) as mn,\n            max(population) as mx\n      GROUP BY Country \n      ORDER BY Country DESC;\n┌─────────┬─────────┬─────────┬─────────┬─────────┬─────────┬─────────┐\n│ Country │ 2000_mn │ 2000_mx │ 2010_mn │ 2010_mx │ 2020_mn │ 2020_mx │\n│ varchar │ double  │  int32  │ double  │  int32  │ double  │  int32  │\n├─────────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n│ US      │  4289.5 │    8015 │  4391.5 │    8175 │  4755.0 │    8772 │\n│ NL      │  1005.0 │    1005 │  1065.0 │    1065 │  1158.0 │    1158 │\n└─────────┴─────────┴─────────┴─────────┴─────────┴─────────┴─────────┘ \nYou might want to combine creation of a table and populating it with data into a single step. For example, you might want to create a database table from the contents of a CSV file. That is accomplished by adding an AS SELECT clause to the CREATE TABLE statement:\nCREATE OR REPLACE TABLE iris AS SELECT * FROM \"../datasets/iris.csv”;\nYou can use just FROM as a shorthand for SELECT * FROM:\nCREATE OR REPLACE TABLE iris AS FROM \"../dataswts/iris.csv”;\nThis shorthand works in general in DuckDB:\nFROM WorldCities WHERE Country='NL';\n┌─────────┬───────────┬───────┬────────────┐\n│ Country │ CityName  │ Year  │ Population │\n│ varchar │  varchar  │ int32 │   int32    │\n├─────────┼───────────┼───────┼────────────┤\n│ NL      │ Amsterdam │  2000 │       1005 │\n│ NL      │ Amsterdam │  2010 │       1065 │\n│ NL      │ Amsterdam │  2020 │       1158 │\n└─────────┴───────────┴───────┴────────────┘ \nIf you want to create a temporary table that resides in memory and is automatically dropped when the connection to DuckDB is closed, use CREATE TEMP TABLE:\nCREATE TEMP TABLE foo AS FROM \"../datasets/iris.csv\";\n\n\nDropping a Table\nFinally, when the table is no longer needed, we can remove it from the catalog with DROP TABLE:\nDROP TABLE WorldCities;",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data/sqlbasics.html#select-statement",
    "href": "data/sqlbasics.html#select-statement",
    "title": "15  SQL Basics",
    "section": "15.4 SELECT Statement",
    "text": "15.4 SELECT Statement\nThe SELECT statement is the workhorse for querying, analyzing, combining, and ordering data in database tables. The more important basic syntax elements of SELECT are\nSELECT select_list\n   FROM tables\n   WHERE condition\n   GROUP BY groups\n   HAVING group_filter\n   ORDER BY order_expr\n   LIMIT n;\nThe logical start of the SELECT query is the FROM clause where you specify the data source for the query. The select_list prior to the FROM clause specifies the columns in the result set of the query. This can be columns from results in the FROM clause, aggregations, and combinations. If you want to specifically name element of the select_list, you can assign an alias with the AS clause, as in this example:\nSELECT sepal_length AS SL, sepal_width AS SW from iris LIMIT 5;\n┌────────┬────────┐\n│   SL   │   SW   │\n│ double │ double │\n├────────┼────────┤\n│    5.1 │    3.5 │\n│    4.9 │    3.0 │\n│    4.7 │    3.2 │\n│    4.6 │    3.1 │\n│    5.0 │    3.6 │\n├────────┴────────┤\n│     5 rows      │\n└─────────────────┘ \nTo include everything in the FROM clause into the select_list, use the asterisk wildcard (star notation):\nSELECT * FROM iris LIMIT 4;\n┌──────────────┬─────────────┬──────────────┬─────────────┬─────────┐\n│ sepal_length │ sepal_width │ petal_length │ petal_width │ species │\n│    double    │   double    │    double    │   double    │ varchar │\n├──────────────┼─────────────┼──────────────┼─────────────┼─────────┤\n│          5.1 │         3.5 │          1.4 │         0.2 │ setosa  │\n│          4.9 │         3.0 │          1.4 │         0.2 │ setosa  │\n│          4.7 │         3.2 │          1.3 │         0.2 │ setosa  │\n│          4.6 │         3.1 │          1.5 │         0.2 │ setosa  │\n└──────────────┴─────────────┴──────────────┴─────────────┴─────────┘ \nSELECT queries can quickly get complicated because the FROM clause can contain a single table, multiple tables that are joined together, or another SELECT query (this is called a subquery):\nSELECT land, improve, \n       (SELECT avg(sale) FROM landsales as average) \nFROM (SELECT * from landsales where total &gt; 100000);\n┌────────┬─────────┬──────────────────────────────────────────────┐\n│  land  │ improve │ (SELECT avg(sale) FROM landsales AS average) │\n│ int64  │  int64  │                   double                     │\n├────────┼─────────┼──────────────────────────────────────────────┤\n│  45990 │   91402 │                           217445.45454545456 │\n│  56658 │  153806 │                           217445.45454545456 │\n│  51428 │   72451 │                           217445.45454545456 │\n│  76125 │   78172 │                           217445.45454545456 │\n│ 154360 │   61934 │                           217445.45454545456 │\n│  40800 │   92606 │                           217445.45454545456 │\n└────────┴─────────┴──────────────────────────────────────────────┘ \nIn the previous SQL code, the select_list contains a SELECT statement that returns the average sale price for all land sales. The FROM clause contains another SELECT statement with a WHERE clause. The result set consists of the land and improvement value for sales where the total value exceeded 100,000 and the average sale for all properties (including the ones with total value below 100,000).\nThe WHERE clause in the SELECT applies filters to the data. Only rows that match the WHERE clause are processed.\nSELECT * FROM iris WHERE species LIKE '%osa' LIMIT 10;\n┌──────────────┬─────────────┬──────────────┬─────────────┬─────────┐\n│ sepal_length │ sepal_width │ petal_length │ petal_width │ species │\n│    double    │   double    │    double    │   double    │ varchar │\n├──────────────┼─────────────┼──────────────┼─────────────┼─────────┤\n│          5.1 │         3.5 │          1.4 │         0.2 │ setosa  │\n│          4.9 │         3.0 │          1.4 │         0.2 │ setosa  │\n│          4.7 │         3.2 │          1.3 │         0.2 │ setosa  │\n│          4.6 │         3.1 │          1.5 │         0.2 │ setosa  │\n│          5.0 │         3.6 │          1.4 │         0.2 │ setosa  │\n│          5.4 │         3.9 │          1.7 │         0.4 │ setosa  │\n│          4.6 │         3.4 │          1.4 │         0.3 │ setosa  │\n│          5.0 │         3.4 │          1.5 │         0.2 │ setosa  │\n│          4.4 │         2.9 │          1.4 │         0.2 │ setosa  │\n│          4.9 │         3.1 │          1.5 │         0.1 │ setosa  │\n├──────────────┴─────────────┴──────────────┴─────────────┴─────────┤\n│ 10 rows                                                 5 columns │\n└───────────────────────────────────────────────────────────────────┘ \nThe GROUP BY clause specifies the groupings for aggregations.\nSELECT count(*), max(sepal_length) FROM iris GROUP BY species;\n┌──────────────┬───────────────────┐\n│ count_star() │ max(sepal_length) │\n│    int64     │      double       │\n├──────────────┼───────────────────┤\n│           50 │               7.9 │\n│           50 │               5.8 │\n│           50 │               7.0 │\n└──────────────┴───────────────────┘ \nYou can specify multiple columns in the grouping or use the GROUP BY ALL shorthand. The following are equivalent:\nSELECT League, Division, mean(RBI) FROM Hitters GROUP BY League, Division;\nSELECT League, Division, mean(RBI) FROM Hitters GROUP BY ALL;\n┌─────────┬──────────┬───────────────────┐\n│ League  │ Division │     mean(RBI)     │\n│ varchar │ varchar  │      double       │\n├─────────┼──────────┼───────────────────┤\n│ A       │ E        │ 54.37647058823529 │\n│ A       │ W        │ 48.81111111111111 │\n│ N       │ E        │ 44.94444444444444 │\n│ N       │ W        │ 42.85333333333333 │\n└─────────┴──────────┴───────────────────┘\nThe HAVING clause in the SELECT statement causes confusion sometimes, but it is fairly easy to understand as a filter that is applied after the GROUP BY clause. The WHERE clause on the other hand is a filter applied before the GROUP BY clause. In other words, the WHERE clause selects which rows participate in the GROUP BY operation. The HAVING clause determines which result of the GROUP BY operation are filtered.\nSELECT League, Division, \n       count(*) as count, \n       avg(RBI) AS average \nFROM Hitters \nGROUP BY ALL\nHAVING count &gt; 80;\n\n┌─────────┬──────────┬───────┬───────────────────┐\n│ League  │ Division │ count │      average      │\n│ varchar │ varchar  │ int64 │      double       │\n├─────────┼──────────┼───────┼───────────────────┤\n│ A       │ E        │    85 │ 54.37647058823529 │\n│ A       │ W        │    90 │ 48.81111111111111 │\n└─────────┴──────────┴───────┴───────────────────┘ \nWithout the HAVING clause, the SELECT statement would return results on two additional groupings:\nSELECT League, Division, \n       count(*) as count, \n       avg(RBI) AS average \nFROM Hitters \nGROUP BY ALL;\n┌─────────┬──────────┬───────┬───────────────────┐\n│ League  │ Division │ count │      average      │\n│ varchar │ varchar  │ int64 │      double       │\n├─────────┼──────────┼───────┼───────────────────┤\n│ A       │ E        │    85 │ 54.37647058823529 │\n│ A       │ W        │    90 │ 48.81111111111111 │\n│ N       │ E        │    72 │ 44.94444444444444 │\n│ N       │ W        │    75 │ 42.85333333333333 │\n└─────────┴──────────┴───────┴───────────────────┘  \nThe ORDER BY and LIMIT clause also operate on the result set of the query. LIMIT is applied at the end of the query and returns only a specified number of rows in the result set. The ORDER BY clause affects how the rows of the result set are arranged. LIMIT without an ORDER BY can be non-deterministic, multiple runs can produce different results depending on the parallel processing of the data. However, combining ORDER BY and LIMIT generates reproducible results and is a common technique to fetch the top or bottom rows. The following SELECT statement retrieves statistics for the baseball players with the five highest RBI values:\nSELECT AtBat, Hits, HmRun, Runs, RBI, Walks, Errors \n   FROM Hitters ORDER BY RBI DESC LIMIT 5;\n\n┌───────┬───────┬───────┬───────┬───────┬───────┬────────┐\n│ AtBat │ Hits  │ HmRun │ Runs  │  RBI  │ Walks │ Errors │\n│ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64  │\n├───────┼───────┼───────┼───────┼───────┼───────┼────────┤\n│   663 │   200 │    29 │   108 │   121 │    32 │      6 │\n│   600 │   144 │    33 │    85 │   117 │    65 │     14 │\n│   637 │   174 │    31 │    89 │   116 │    56 │      9 │\n│   677 │   238 │    31 │   117 │   113 │    53 │      6 │\n│   618 │   200 │    20 │    98 │   110 │    62 │      8 │\n└───────┴───────┴───────┴───────┴───────┴───────┴────────┘",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data/sqlbasics.html#subqueries",
    "href": "data/sqlbasics.html#subqueries",
    "title": "15  SQL Basics",
    "section": "15.5 Subqueries",
    "text": "15.5 Subqueries\nA subquery is a query inside another query. The subquery is also called the inner query and the query containing it is referred to as the outer query. Subqueries are said to be uncorrelated if they are self-contained and can be run without the outer query. For example, the following SQL statement contains an uncorrelated subquery, select min(weight) from auto, which you can run as a stand-alone query\nSELECT name FROM auto WHERE weight = (SELECT MIN(weight) FROM auto);\n┌─────────────┐\n│    name     │\n│   varchar   │\n├─────────────┤\n│ datsun 1200 │\n└─────────────┘ \nThe outer query in this example is SELECT name FROM auto WHERE weight =.\nYou can use subqueries together with SELECT EXISTS to test for the existence of rows in the subquery:\nSELECT EXISTS(SELECT * FROM auto WHERE horsepower &gt; 300);\n┌─────────────────────────────────────────────────────┐\n│ EXISTS(SELECT * FROM auto WHERE (horsepower &gt; 300)) │\n│                       boolean                       │\n├─────────────────────────────────────────────────────┤\n│ false                                               │\n└─────────────────────────────────────────────────────┘\nA subquery is said to be correlated if the subquery uses values from the outer query. You can think of the subquery as a function that is run for every row in the data. Suppose we want to find the minimum reaction time for each subject in the sleep table. The data represent reaction times for 18 subjects measured on ten days of a sleep deprivation study.\nSELECT *\n  FROM sleep AS s\n  WHERE Reaction =\n      (SELECT MIN(Reaction)\n       FROM sleep\n       WHERE sleep.Subject=s.Subject);\n┌──────────┬───────┬─────────┐\n│ Reaction │ Days  │ Subject │\n│  double  │ int64 │  int64  │\n├──────────┼───────┼─────────┤\n│   249.56 │     0 │     308 │\n│ 202.9778 │     2 │     309 │\n│ 194.3322 │     1 │     310 │\n│ 280.2396 │     6 │     330 │\n│    285.0 │     1 │     331 │\n│ 234.8606 │     0 │     332 │\n│ 276.7693 │     2 │     333 │\n│ 243.3647 │     2 │     334 │\n│  235.311 │     7 │     335 │\n│ 291.6112 │     2 │     337 │\n│ 230.3167 │     1 │     349 │\n│ 243.4543 │     1 │     350 │\n│ 250.5265 │     0 │     351 │\n│ 221.6771 │     0 │     352 │\n│ 257.2424 │     2 │     369 │\n│  225.264 │     0 │     370 │\n│ 259.2658 │     6 │     371 │\n│ 269.4117 │     0 │     372 │\n├──────────┴───────┴─────────┤\n│ 18 rows          3 columns │\n└────────────────────────────┘ \nSubject 308 has the smallest reaction time on day 0 of the study, subject 309 has the smallest reaction time at day 2, and so forth. The subquery is correlated, because it uses the column s.Subject from the outer query.\nWe could have achieved the same aggregation of smallest reaction time by subject with a simple query with GROUP BY clause:\nSELECT min(Reaction), Subject FROM sleep GROUP BY subject LIMIT 5;\n┌───────────────┬─────────┐\n│ min(Reaction) │ Subject │\n│    double     │  int64  │\n├───────────────┼─────────┤\n│        249.56 │     308 │\n│      202.9778 │     309 │\n│      194.3322 │     310 │\n│      280.2396 │     330 │\n│         285.0 │     331 │\n└─────────────────────────┘ \nThe difference is that the correlated subquery returns actual records from the table whereas the SELECT with GROUP BY clause returns the result of aggregation. Put it another way: do you want to see the min reaction time for each subject or do you want to see the records that match the min reaction time for each subject?\n\n\n\nFigure 15.1: Table 9 from the 2023 survey of Master graduates.\nFigure 15.2: Table 10 from the 2023 survey of Master graduates.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>SQL Basics</span>"
    ]
  },
  {
    "objectID": "data/integration.html",
    "href": "data/integration.html",
    "title": "16  Data Integration",
    "section": "",
    "text": "16.1 Combining Tables with SQL\nThe process of combining tables is based on set operations or joins. A join uses the values in specific columns of the tables to match records. A set operation is a merging of columns without considering the values in the columns. Appending the rows of one table to another table is a set operation. What happens to columns that exist in one table but not in the other during the append depends on the implementation. Similarly, what happens to columns that share the same name when tables are merged horizontally depends on the implementation.",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Integration</span>"
    ]
  },
  {
    "objectID": "data/integration.html#combining-tables-with-sql",
    "href": "data/integration.html#combining-tables-with-sql",
    "title": "16  Data Integration",
    "section": "",
    "text": "Set Operations\nWe draw on an example in the DuckDB documentation to explain how tables can be merged by rows or columns. First, let’s create two small tables with information on cities:\nCREATE TABLE capitals(city VARCHAR, country VARCHAR);\nINSERT INTO capitals VALUES ('Amsterdam', 'NL'), ('Berlin', 'Germany');\n\nCREATE TABLE weather(city VARCHAR, degrees INTEGER, date DATE);\nINSERT INTO weather VALUES ('Amsterdam', 10, '2022-10-14'), \n                           ('Seattle', 8, '2022-10-12');\n\nFROM capitals;\n\n┌───────────┬─────────┐\n│   city    │ country │\n│  varchar  │ varchar │\n├───────────┼─────────┤\n│ Amsterdam │ NL      │\n│ Berlin    │ Germany │\n└───────────┴─────────┘\n\nFROM weather;\n\n┌───────────┬─────────┬────────────┐\n│   city    │ degrees │    date    │\n│  varchar  │  int32  │    date    │\n├───────────┼─────────┼────────────┤\n│ Amsterdam │      10 │ 2022-10-14 │\n│ Seattle   │       8 │ 2022-10-12 │\n└───────────┴─────────┴────────────┘  \n\nSet operations involve two SELECT queries. The queries are connected with clauses that control how the rows are combined:\n\nUNION: combines rows from queries that have the same columns (number and types) and eliminates duplicates.\nUNION ALL: combines rows from queries that have the same columns (number and types) and preserves duplicates.\nINTERSECT: selects rows that occur in both queries and removes duplicates.\nEXCEPT: selects rows that occur in the left query and removes duplicates.\nUNION BY NAME: works like UNION but does not require the queries to have the same number and types of columns. Eliminates duplicates, like UNION.\nUNION ALL BY NAME: works like UNION BY NAME but does not eliminate duplicate rows.\n\nHere are the result of UNION and UNION ALL clauses:\nSELECT city FROM capitals UNION SELECT city FROM weather;\n\n┌───────────┐\n│   city    │\n│  varchar  │\n├───────────┤\n│ Amsterdam │\n│ Seattle   │\n│ Berlin    │\n└───────────┘\n\nSELECT city FROM capitals UNION ALL SELECT city FROM weather;\n\n┌───────────┐\n│   city    │\n│  varchar  │\n├───────────┤\n│ Amsterdam │\n│ Berlin    │\n│ Amsterdam │\n│ Seattle   │\n└───────────┘ \n\nHere are the results of INTERSECT and EXCEPT clauses:\nSELECT city FROM capitals INTERSECT SELECT city FROM weather;\n\n┌───────────┐\n│   city    │\n│  varchar  │\n├───────────┤\n│ Amsterdam │\n└───────────┘\n\nSELECT city FROM capitals EXCEPT SELECT city FROM weather;\n\n┌─────────┐\n│  city   │\n│ varchar │\n├─────────┤\n│ Berlin  │\n└─────────┘ \n\nNotice that UNION, UNION ALL, INTERSECT, and EXCEPT require the two queries to return the same columns. The horizontal column matching is done by position. To merge rows across tables with different columns, use UNION (ALL) BY NAME:\nSELECT * FROM capitals UNION BY NAME SELECT * FROM weather;\n\n┌───────────┬─────────┬─────────┬────────────┐\n│   city    │ country │ degrees │    date    │\n│  varchar  │ varchar │  int32  │    date    │\n├───────────┼─────────┼─────────┼────────────┤\n│ Amsterdam │ NL      │         │            │\n│ Seattle   │         │       8 │ 2022-10-12 │\n│ Berlin    │ Germany │         │            │\n│ Amsterdam │         │      10 │ 2022-10-14 │\n└───────────┴─────────┴─────────┴────────────┘ \n\n\n\nJoins\nThe previous set operations combine rows of data (vertically). To combine tables horizontally we use join operations. Joins typically are based on the values in columns of the tables to find matches. There are two exceptions, positional and cross joins.\nFor data scientists working with rectangular data frames in which observations have a natural order, merging data horizontally is a standard operation. In relational databases this is a somewhat unnatural operation because relational tables do not work from a natural ordering of the data, they are based on keys and indices. The positional join matches row-by-row such that rows from both tables appear at least once:\nselect capitals.*, weather.* from capitals positional join weather;\n\n┌───────────┬─────────┬───────────┬─────────┬────────────┐\n│   city    │ country │   city    │ degrees │    date    │\n│  varchar  │ varchar │  varchar  │  int32  │    date    │\n├───────────┼─────────┼───────────┼─────────┼────────────┤\n│ Amsterdam │ NL      │ Amsterdam │      10 │ 2022-10-14 │\n│ Berlin    │ Germany │ Seattle   │       8 │ 2022-10-12 │\n└───────────┴─────────┴───────────┴─────────┴────────────┘ \n\nThe cross join is actually the simplest join, it returns all possible pairs of rows:\nselect capitals.*, weather.* from capitals cross join weather;\n\n┌───────────┬─────────┬───────────┬─────────┬────────────┐\n│   city    │ country │   city    │ degrees │    date    │\n│  varchar  │ varchar │  varchar  │  int32  │    date    │\n├───────────┼─────────┼───────────┼─────────┼────────────┤\n│ Amsterdam │ NL      │ Amsterdam │      10 │ 2022-10-14 │\n│ Amsterdam │ NL      │ Seattle   │       8 │ 2022-10-12 │\n│ Berlin    │ Germany │ Amsterdam │      10 │ 2022-10-14 │\n│ Berlin    │ Germany │ Seattle   │       8 │ 2022-10-12 │\n└───────────┴─────────┴───────────┴─────────┴────────────┘ \n\nJoins are categorized as outer or inner joins depending on whether rows with matches are returned. An outer join returns rows that do not have any matches whereas the inner join returns only rows that get paired. The two tables in a join are called the left and right sides of the relation and outer joins are further classified as\n\nLeft outer join: all rows from the left side of the relation appear at least once.\nRight outer join: all rows from the right side of the relation appear at least once.\nFull outer join: all rows from both sides of the relation appear at least once.\n\nTo demonstrate the joins in DuckDB, let’s set up some simple tables:\nCREATE TABLE weather (\n      city           VARCHAR,\n      temp_lo        INTEGER, -- minimum temperature on a day\n      temp_hi        INTEGER, -- maximum temperature on a day\n      prcp           REAL,\n      date           DATE\n  );\nCREATE TABLE cities (\n      name            VARCHAR,\n      lat             DECIMAL,\n      lon             DECIMAL\n  );\nINSERT INTO weather VALUES ('San Francisco', 46, 50, 0.25, '1994-11-27');\nINSERT INTO weather (city, temp_lo, temp_hi, prcp, date)\n      VALUES ('San Francisco', 43, 57, 0.0, '1994-11-29');\nINSERT INTO weather (date, city, temp_hi, temp_lo)\n      VALUES ('1994-11-29', 'Hayward', 54, 37);\n\nFROM weather;\n\n┌───────────────┬─────────┬─────────┬───────┬────────────┐\n│     city      │ temp_lo │ temp_hi │ prcp  │    date    │\n│    varchar    │  int32  │  int32  │ float │    date    │\n├───────────────┼─────────┼─────────┼───────┼────────────┤\n│ San Francisco │      46 │      50 │  0.25 │ 1994-11-27 │\n│ San Francisco │      43 │      57 │   0.0 │ 1994-11-29 │\n│ Hayward       │      37 │      54 │       │ 1994-11-29 │\n└───────────────┴─────────┴─────────┴───────┴────────────┘ \n\nINSERT INTO cities VALUES ('San Francisco', -194.0, 53.0);\nFROM cities;\n\n┌───────────────┬───────────────┬───────────────┐\n│     name      │      lat      │      lon      │\n│    varchar    │ decimal(18,3) │ decimal(18,3) │\n├───────────────┼───────────────┼───────────────┤\n│ San Francisco │      -194.000 │        53.000 │\n└───────────────┴───────────────┴───────────────┘ \n\nAn inner join between the tables on the columns that contain the city names will match the records for San Francisco:\nSELECT * FROM weather INNER JOIN cities ON (weather.city = cities.name);\n\n┌───────────────┬─────────┬─────────┬───────┬────────────┬───────────────┬───────────────┬───────────────┐\n│     city      │ temp_lo │ temp_hi │ prcp  │    date    │     name      │      lat      │      lon      │\n│    varchar    │  int32  │  int32  │ float │    date    │    varchar    │ decimal(18,3) │ decimal(18,3) │\n├───────────────┼─────────┼─────────┼───────┼────────────┼───────────────┼───────────────┼───────────────┤\n│ San Francisco │      46 │      50 │  0.25 │ 1994-11-27 │ San Francisco │      -194.000 │        53.000 │\n│ San Francisco │      43 │      57 │   0.0 │ 1994-11-29 │ San Francisco │      -194.000 │        53.000 │\n└───────────────┴─────────┴─────────┴───────┴────────────┴───────────────┴───────────────┴───────────────┘ \n\nNote that the values for lat and lon are repeated for every row in the weather table that matches the join in the relation. Because this is an inner join (the DuckDB default), and the weather table had no matching row for city Hayward, this city does not appear in the join result. We can change that by modifying the type of join to a left outer join:\nSELECT * FROM weather LEFT OUTER JOIN cities ON (weather.city = cities.name);\n\n┌───────────────┬─────────┬─────────┬───────┬────────────┬───────────────┬───────────────┬───────────────┐\n│     city      │ temp_lo │ temp_hi │ prcp  │    date    │     name      │      lat      │      lon      │\n│    varchar    │  int32  │  int32  │ float │    date    │    varchar    │ decimal(18,3) │ decimal(18,3) │\n├───────────────┼─────────┼─────────┼───────┼────────────┼───────────────┼───────────────┼───────────────┤\n│ San Francisco │      46 │      50 │  0.25 │ 1994-11-27 │ San Francisco │      -194.000 │        53.000 │\n│ San Francisco │      43 │      57 │   0.0 │ 1994-11-29 │ San Francisco │      -194.000 │        53.000 │\n│ Hayward       │      37 │      54 │       │ 1994-11-29 │               │               │               │\n└───────────────┴─────────┴─────────┴───────┴────────────┴───────────────┴───────────────┴───────────────┘\n\nBecause the join is an outer join, rows that do not have matches in the relation are returned. Because the outer join is a left join, every row on the left side of the relation is returned (at least once). If you change the left- and right-hand side of the relation you can achieve the same result by using a right outer join:\nSELECT * FROM cities  RIGHT OUTER JOIN weather ON (weather.city = cities.name);\n\n┌───────────────┬───────────────┬───────────────┬───────────────┬─────────┬─────────┬───────┬────────────┐\n│     name      │      lat      │      lon      │     city      │ temp_lo │ temp_hi │ prcp  │    date    │\n│    varchar    │ decimal(18,3) │ decimal(18,3) │    varchar    │  int32  │  int32  │ float │    date    │\n├───────────────┼───────────────┼───────────────┼───────────────┼─────────┼─────────┼───────┼────────────┤\n│ San Francisco │      -194.000 │        53.000 │ San Francisco │      46 │      50 │  0.25 │ 1994-11-27 │\n│ San Francisco │      -194.000 │        53.000 │ San Francisco │      43 │      57 │   0.0 │ 1994-11-29 │\n│               │               │               │ Hayward       │      37 │      54 │       │ 1994-11-29 │\n└───────────────┴───────────────┴───────────────┴───────────────┴─────────┴─────────┴───────┴────────────┘ \n\nNow let’s add another record to the cities table without a matching record in the weather table:\nINSERT INTO cities VALUES ('New York',40.7, -73.9);\nFROM cities;\n\n┌───────────────┬───────────────┬───────────────┐\n│     name      │      lat      │      lon      │\n│    varchar    │ decimal(18,3) │ decimal(18,3) │\n├───────────────┼───────────────┼───────────────┤\n│ San Francisco │      -194.000 │        53.000 │\n│ New York      │        40.700 │       -73.900 │\n└───────────────┴───────────────┴───────────────┘ \n\nA full outer join between the two tables ensures that rows from both sides of the relation appear at least once:\nSELECT * FROM cities FULL OUTER JOIN weather ON (weather.city = cities.name);\n\n┌───────────────┬───────────────┬───────────────┬───────────────┬─────────┬─────────┬───────┬────────────┐\n│     name      │      lat      │      lon      │     city      │ temp_lo │ temp_hi │ prcp  │    date    │\n│    varchar    │ decimal(18,3) │ decimal(18,3) │    varchar    │  int32  │  int32  │ float │    date    │\n├───────────────┼───────────────┼───────────────┼───────────────┼─────────┼─────────┼───────┼────────────┤\n│ San Francisco │      -194.000 │        53.000 │ San Francisco │      46 │      50 │  0.25 │ 1994-11-27 │\n│ San Francisco │      -194.000 │        53.000 │ San Francisco │      43 │      57 │   0.0 │ 1994-11-29 │\n│               │               │               │ Hayward       │      37 │      54 │       │ 1994-11-29 │\n│ New York      │        40.700 │       -73.900 │               │         │         │       │            │\n└───────────────┴───────────────┴───────────────┴───────────────┴─────────┴─────────┴───────┴────────────┘",
    "crumbs": [
      "Module III. Data Engineering",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Integration</span>"
    ]
  },
  {
    "objectID": "models/model_intro.html",
    "href": "models/model_intro.html",
    "title": "17  Introduction",
    "section": "",
    "text": "17.1 Statistical Models\nThe models you train in data science applications are statistical models, they are trained on data that is inherently uncertain. Sources of uncertainty might be the random sampling of observations from a population, the random assignment of treatments to experimental units, measurement variability, natural variability of an attribute, incompleteness of the models, and so on.\nStatistical models are a subset of mathematical models. A mathematical model abstracts a concrete system using mathematical concepts. For example, we can model mathematically how much space is in a box, the trajectory a baseball will travel, or how closely we can package items on a shelf. While a mathematical model describes a system in deterministic terms, statistical models are non-deterministic, they allow for the presence of uncertainty.\nThe non-determinism of statistical models is a feature and a strength. The random elements of the model allow us to develop models that are simpler (more parsimonious) than equivalent mathematical models that must capture the salient features deterministically. Randomness is an antidote to complexity as long as the random elements can be described through their distributional properties. It does come at a cost, however.\nStatistical models are a subset of mathematical models, and they also belong to the class of stochastic models. A stochastic model describes the probability distribution of outcomes by allowing one or more of the model elements to be random variables. Like stochastic models, statistical models use random variables and their probability models to capture the effects of random processes. Like mathematical models, statistical models involve mathematical relationships that are governed by parameters; unknown constants that are determined based on training data.\nWhat did we gain and what did we give up in the coin flip example by using a probabilistic formulation instead of a deterministic model? We gained a much simpler model that is easily understood and is very general. It applies to pretty much all coin flips, regardless of the conditions. It is a very parsimonious model that depends on just one parameter: the probability that heads (or tails) will turn up. And we have a simple recipe to estimate that parameter if we need to.\nWe lost, however, the ability to determine the outcome of any one coin toss. We cannot say with certainty whether a coin toss results in heads or tails. We can only state how likely the outcome will be. By incorporating uncertainty (randomness) in the model we made the model simpler but we transferred uncertainty into the model output. In data science applications we are just fine with that. Because our input material—data—is inherently uncertain we cannot expect to produce deterministic models. Quantifying the uncertainty in the model outputs is an important part of modeling data with statistical approaches.\nThe process of training a statistical model to a set of data is called statistical learning. Here is a more precise definition:\nIf data science is concerned with statistical models and uses statistical learning to understand the structure and relationships in data, then what about machine learning models? Do they fall outside of data science?",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "models/model_intro.html#statistical-models",
    "href": "models/model_intro.html#statistical-models",
    "title": "17  Introduction",
    "section": "",
    "text": "Definition: Statistical Model\n\n\nA statistical model is a stochastic model that contains random variables and unknown constants. The unknown constants are called parameters and are estimated based on data. Parameters are constants, not random variables. The estimator of a parameter that depends on data is a random variable since the data are random.\n\n\n\n\nExample: Modeling Coin Flips\n\n\nSuppose that you want to develop a model that can predict whether a coin lands on heads or tails. A deterministic mathematical model would capture the physics of the coin itself, the medium through which it travels, and the surface on which it lands. It would take into account the shape of the coin and the condition of its surface. It would account for direction and strength of air movement, the angle and force with which the coin is released, and so on. The model would be very complex. If the various forces and variables that affect the coin—as well as their interactions—are captured correctly, the model could be very accurate. However, change an element of the process, for example, make the coin travel through moist air rather than dry air or land on grass, and you would have to revisit the model.\nA much simpler model is one that describes the coin behavior in a probabilistic (non-deterministic) way. The various forces affecting whether the coin lands on heads or tails are essentially random affects that balance each other out. The end result is the following: if the coin is fair (balanced), it will land on heads or tails with equal probability. This model is extremely simple, it has no parameters. The behavior of the coin is described not in terms of what happens in any one coin toss, but in terms of the distribution of results that is realized when the random effects play out: the coin toss is modeled as a Bernoulli (binary) distribution with an event probability of 0.5. On aveage you will observe as many coins landing on heads and tails, but you cannot say for sure what the outcome of the next coin toss will be. We have to guess it. That guess is a prediction under the model. Since the outcome itself is uncertain, the prediction carries with it also some uncertainty. If the coin is tossed twenty times we would predict 10 heads and 10 tails and would not be surprised if 9 heads or 11 heads turned up.\nIf there are doubts whether the coin in question is fair, we can run a little experiment. Flip the coin a hundred times and observe the number of heads (or tails) that appear. If we see 65 out of 100 heads, we’ll model the distribution of coin flips as a Bernoulli distribution with an event probability of 0.65.\n\n\n\n\n\n\n\n\nDefinition: Statistical Learning\n\n\nStatistical Learning is the process of understanding data through the application of tools that describe structure and relationships in data. Models are formulated based on the structure of data to predict or classify outcomes from inputs, to test hypothesis about relationships, to group data, to find association, to reduce the dimensionality of a problem, and so on.\n\n\n\n\nStatistical Learning and Machine Learning\nMuch is being made of the difference between statistical models and machine learning models, or to be more precise, between statistical learning (SL) and machine learning (ML).\nStatistical learning emphasizes prediction more than the testing of hypothesis, as compared to classical statistical modeling. Many model classes used in statistical learning are the same models one uses to test hypothesis about patterns and relationships in data. Emphasis of prediction over hypothesis testing—or vice versa—flows from the nature of the problem we are trying to solve. The same model can be developed with focus on predictive capability or with focus on interpretability. We do not want to overdo the distinction between statistical learning and statistical modeling: statistical learning uses statistical models.\nLearning is the process of converting experience into knowledge and machine learning is an automated way of learning by using computers and algorithms. Rather than directly programming computers to perform a task, machine learning is used when the tasks are not easily described and communicated (e.g., driving, reading, image recognition) or when the tasks exceed human capabilities (e.g., analyzing large and complex data sets). Modern machine learning discovered data as a resource for learning and that is where statistical learning and machine learning meet.\nSL and ML have more in common, than what separates them:\n\nThe input to a learning algorithm is data; the raw material is the same.\nThe data are thought of as randomly generated, there is some sense of variability in the data that is attributed to random sources.\nBoth disciplines distinguish supervised and unsupervised forms of learning\nThey use many of the same models and algorithms for regression, classification, clustering, dimension reduction, etc.\n\nMachine learning uses observed data to describe relationships and “causes”; the emphasis is on predicting new and/or future outcomes. There is comparatively little emphasis on experimentation and hypothesis testing.\nA key difference between SL and ML is what Breiman (2001) describes as the difference between data modeling and algorithmic modeling and aligns closely with statistical and machine learning thinking. In data modeling, theory focuses on the probabilistic properties of the model and of quantities derived from it. In algorithmic modeling, the focus is on the properties of the algorithm itself: starting values, optimization, convergence behavior, parallelization, hyperparameter tuning, and so on. Consequently, statisticians are concerned with the asymptotic distributional behavior of estimators and methods as \\(n \\rightarrow \\infty\\). Machine learning focuses on finite sample properties and asks what accuracy can be expected based on the available data.\nThe strong assumptions statisticians make about the stochastic data-generating mechanism that produced the data set in hand are not found in machine learning. That does not mean that machine learning models are free of stochastic elements and assumptions—quite the contrary. It means that statisticians use the data-generating mechanism as the foundation for conclusions rather than the data alone.\nWhen you look at a p-value in a table of parameter estimates, you rely on all assumptions about distributional properties of the data, correctness of the model, and (asymptotic) distributional behavior of the estimator. These flow explicitly from the data-generating mechanism or implicitly from somewhere else. Otherwise, the p-value does not make much sense. (Many argue that p-values are not very helpful to begin with and possibly even damaging to decision making but this is not the point of the discussion here.)\nIf you express the relationship between a target variable \\(Y\\) and inputs \\(x_1, \\cdots, x_p\\) as\n\\[\nY = f(x_1,\\cdots,x_p) + \\epsilon\n\\]\nwhere \\(\\epsilon\\) is a random variable, it does not matter whether you perform data modeling or algorithmic modeling. We need to think about \\(\\epsilon\\) and its properties. How does \\(\\epsilon\\) affect the algorithm, the prediction accuracy, the uncertainty of statements about \\(Y\\) or \\(f(x_1, \\cdots, x_p)\\)? That is why all data professionals need to understand about stochastic models and statistical models.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "models/model_intro.html#sec-model-intro-logist",
    "href": "models/model_intro.html#sec-model-intro-logist",
    "title": "17  Introduction",
    "section": "17.2 Example: Logistic Regression Model",
    "text": "17.2 Example: Logistic Regression Model\nSuppose we are charged with developing a model to predict recurrence of cancer. There are many possible aspects that influence the outcome. These are potential input variables to our model:\n\nAge, gender\nMedical history\nLifestyle factors (nutrition, exercise, smoking, …)\nType of cancer\nSize of the largest tumor\nSite of the cancer\nTime since diagnostic, time from treatment\nType of treatment\nand so on\n\nIf we were to try and build a deterministic model that predicts cancer recurrence perfectly, all influences would have to be taken into account and their impact on the outcome would have to be incorporated correctly. That would be an incredibly complex—and impractical—model.\nBy taking a stochastic approach we acknowledge that there are processes that affect the variability in cancer recurrence we observe from patient to patient. The modeling can now focus on the most important factors and how they drive cancer recurrence. The other factors are included through random effects. If the model captures the salient factors and their impact correctly, and the variability contributed by other factors is not too large, and not systematic, the model is very useful. It possibly is much more useful than an inscrutably complex model that tries to accommodate all influences perfectly.\nThe simplest stochastic model for cancer recurrence is to assume that the outcome is a Bernoulli (binary) random variable taking on two states (cancer recurs, cancer does not recur) with probabilities \\(\\pi\\) and \\(1-\\pi\\). If we code the two states numerically, cancer recurs as 1, cancer does not recur as 0, the probability mass function of cancer recurrence is that of the random variable \\(Y\\),\n\\[\n\\Pr(Y=y) = \\left \\{ \\begin{array}{cl} \\pi & y=1 \\\\ 1-\\pi & y = 0\\end{array} \\right .\n\\]\nThe parameter in our cancer model is \\(\\pi\\), the probability that \\(Y\\) takes on the value 1. Unlike the coin flip example earlier in the chapter, we do not know \\(\\pi\\) and must estimate it from data. And \\(\\pi\\) will not be a single number. The probability of cancer recurrence will be a function of some or all of the input variables.\n\n\n\n\n\n\nSuccess and Failure\n\n\n\nIn statistics, the probability \\(\\pi\\) for the event coded as \\(Y=1\\) is frequently called the “success” probability and its complement is called the “failure” probability. We prefer to call them the “event” and “non-event” probabilities instead to avoid the awkward situation to label cancer recurrence a “success”. The event is the binary outcome coded as a 1.\n\n\nBecause we cannot visit with all cancer patients, a sample of patients is used to estimate \\(\\pi\\). This process introduces uncertainty into the estimator of \\(\\pi\\), a larger sample will lead to a more precise (a less uncertain) estimator.\nThe model is overly simplistic in that it captures all possible effects on cancer recurrence in the single quantity \\(\\pi\\). Regardless of age, gender, type of cancer, etc., we would predict a randomly chosen cancer patient’s likelihood to experience a recurrence as \\(\\pi\\). To incorporate input variables that affect the rate of recurrence we need to add structure to \\(\\pi\\). A common approach in statistical learning and in machine learning is that inputs have a linear effect on a transformation of the probability \\(\\pi\\):\n\\[\ng(\\pi) = \\beta_0 + \\beta_1 x_1+\\cdots + \\beta_p x_p\n\\tag{17.1}\\]\nThe interpretation of Equation 17.1 is that the effects of the inputs manifest in an additive fashion, each input is scaled by a coefficient \\(\\beta\\). But the effects do not act additively (linearly) on the event probability. They act additively on a transformation of the event probability.\nWhen \\(g(\\pi)\\) is the logit function\n\\[\\log\\left \\{ \\frac{\\pi}{1-\\pi} \\right\\}\\]\nthe model is called a logistic regression model. \\(x_1,\\cdots,x_p\\) are the inputs of the model, \\(\\beta_0, \\cdots, \\beta_p\\) are the parameters of the model.\nIf we accept that the basic structure of the logistic model applies to the problem of predicting cancer occurrence, we use our sample of patient data to\n\nestimate the parameters \\(\\beta_0, \\cdots, \\beta_p\\);\ndetermine which inputs and how many inputs are adequate: we need to determine \\(p\\) and the specific input variables;\ndetermine whether the logit function is the appropriate transformation that achieves linearity of the input effects.\n\nThe effect of the inputs is called linear on \\(g(\\pi)\\) if \\(g(\\pi)\\) is a linear function of the parameters. To test whether this is the case take derivatives of the function with respect to all parameters. If the derivatives do not depend on parameters, the effect is linear.\n\\[\n\\begin{align*}\n\\frac{\\partial g(\\pi)}{\\partial\\beta_{0}} &= 1 \\\\\n\\frac{\\partial g(\\pi)}{\\partial\\beta_{1}} &= x_{1}\\\\\n\\frac{\\partial g(\\pi)}{\\partial\\beta_{p}} &= x_{p} \\\\\n\\end{align*}\n\\]\nNone of the derivatives depends on any of the \\((\\beta_{0},\\ldots,\\beta_{p})\\). We conclude that \\(g(\\pi)\\) is linear in the parameters. A non-linear function is non-linear in at least one parameter.\n\n\nExample: Plateau (hockey stick) Model\n\n\nA plateau model reaches a certain amount of output and remains flat afterwards. When the model prior to the plateau is a simple linear model, the plateau model is also called a hockey-stick model.\n\n\n\n\n\nThe point at which the plateau is reached is called a change point. Suppose the change point is denoted \\(\\alpha\\). The hockey-stick model can be written as\n\\[\n\\text{E} \\lbrack Y\\rbrack = \\left\\{\n\\begin{matrix}\n\\beta_{0} + \\beta_{1}x      & x \\leq \\alpha \\\\\n\\beta_{0} + \\beta_{1}\\alpha & x &gt; \\alpha\n\\end{matrix} \\right.\n\\]\nIf \\(\\alpha\\) is an unknown parameter, this is a non-linear model.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "models/model_intro.html#model-components",
    "href": "models/model_intro.html#model-components",
    "title": "17  Introduction",
    "section": "17.3 Model Components",
    "text": "17.3 Model Components\nA statistical model has random elements, expressed through the distribution of random variables in the model and systematic components that describe the mathematical relationship between inputs and target variables. Another way of phrasing this: a statistical model contains noise and signal(s). The task of modeling data is to figure out which is which.\nThe expression for the logistic regression model\n\\[\ng(\\pi) = \\beta_{0} + \\beta_{1}x_{1} + \\ldots + \\beta_{p}x_{p}\n\\]\nlooks quite different from the model introduced earlier,\n\\[\nY = f\\left( x_{1},\\ldots,x_{p} \\right) + \\epsilon\n\\]\nWhere is the connection?\nThe error term \\(\\epsilon\\) is a random variable and we need to specify some of its distributional properties to make progress. At a minimum we provide the mean and variance of \\(\\epsilon\\). If the model is correct—correct on average—then the error terms should have a mean of zero and not depend on any input variables (whether those in the model or other inputs). A common assumption is that the variance of the errors is a constant and not a function of other effects (fixed or random). The two assumptions are summarized as \\(\\epsilon \\sim \\left( 0,\\sigma^{2} \\right)\\); read as \\(\\epsilon\\) follows a distribution with mean 0 and variance \\(\\sigma^{2}\\).\n\nMean Function\nNow we can take the expected value of the model and find that\n\\[\n\\text{E}\\lbrack Y\\rbrack = \\text{E}\\left\\lbrack f\\left( x_{1},\\ldots,x_{p} \\right) + \\epsilon \\right\\rbrack = f\\left( x_{1},\\ldots,x_{p} \\right) + \\text{E}\\lbrack\\epsilon\\rbrack = f\\left( x_{1},\\ldots,x_{p} \\right)\n\\]\nBecause the errors have zero mean and because the function \\(f\\left( x_{1},\\ldots,x_{p} \\right)\\) does not contain random variables, \\(f\\left( x_{1},\\ldots,x_{p} \\right)\\) is the expected value (mean) of \\(Y\\). \\(f\\left( x_{1},\\ldots,x_{p} \\right)\\) is thus called mean function of the model.\n\n\nExample: Curvilinear Models\n\n\nPolynomial models such as a quadratic model \\[\nY = \\beta_{0} + \\beta_{1}x + \\beta_{2}x^{2} + \\epsilon\n\\] or cubic model \\[\nY = \\beta_{0} + \\beta_{1}x + \\beta_{2}x^{2} + \\beta_{3}x^{3} + \\epsilon\n\\] have a curved appearance when \\(Y\\) is plotted against \\(x\\). They are linear models in the parameters and they are non-linear models in the \\(x\\)s.\nTo test this, take derivatives of the mean function with respect to the parameters. For the quadratic model the partial derivatives with respect to \\(\\beta_{0}\\), \\(\\beta_{1}\\), and \\(\\beta_{2}\\) are 1, \\(x\\), and \\(x^{2}\\), respectively. The model is linear in the parameters.\nTo emphasize that the models are not just straight lines in \\(x\\), a linear model with curved appearance is called curvilinear.\n\n\nWhat does the mean function look like in the logistic regression model? The underlying random variable \\(Y\\) has a Bernoulli distribution. Its mean is\n\\[\n\\text{E}\\lbrack Y\\rbrack = \\sum y\\, \\Pr(Y = y) = 1 \\times \\pi + 0 \\times (1 - \\pi) = \\pi\n\\]\nThe logit function \\[\ng(\\pi) = \\log \\left\\{ \\frac{\\pi}{(1 - \\pi)} \\right\\}\n\\] is invertible and the model \\[\ng(\\pi) = \\beta_0 + \\beta_1 x_{1} + \\ldots + \\beta_p x_p\n\\]\ncan be written as\n\\[\n\\text{E}\\lbrack Y\\rbrack = \\pi = g^{- 1}\\left( \\beta_{0} + \\beta_{1}x_{1} + \\ldots + \\beta_{p}x_{p} \\right)\n\\]\nThe mean function of the logistic model is also a function of the inputs.\n\n\nExercise\n\n\nShow that if \\(g(\\pi)\\) is the logit function, the mean function can be written as\n\\[\n\\pi = \\frac{1}{1 + \\text{E}\\left\\{ - \\beta_0 - \\beta_1 x_1 - \\cdots - \\beta_p x_p \\right\\}}\n\\] Show that although \\(g(\\pi)\\) is linear in the parameters, \\(\\pi\\) is a non-linear function of the parameters.\n\n\n\n\nSystematic Component\nThe mean functions \\[\nf\\left( x_{1},\\ldots,x_{p} \\right)\n\\] and \\[\\frac{1}{1 + \\text{E}\\left\\{ - \\beta_{0} - \\beta_{1}x_{1} - \\ldots - \\beta_p x_p \\right\\} }\n\\] look rather different, except for the input variables \\(x_{1},\\ldots,x_{p}\\).\nWith the model \\(Y = f\\left( x_{1},\\ldots,x_{p} \\right) + \\epsilon\\) we did not specify how exactly the mean function depends on parameters. There are three general approaches.\n\nLinear predictors\nThe systematic component has the form of a linear predictor, that is, a linear combination of the inputs. The linear predictor is frequently denoted as \\(\\eta\\): \\[\n\\eta = \\beta_{0} + \\beta_1 x_1 + \\cdots + \\beta_p x_p\n\\]\nThe parameter \\(\\beta_0\\) is called the intercept of the linear predictor. Although optional, it is included in most models to capture the effect on the mean if no input variables are present. Models with a linear predictor and an intercept have \\(p + 1\\) parameters in the mean function. The interpretation of \\(\\beta_0\\) is simply as the value of \\(\\eta\\) when all \\(x\\)s are 0: \\(x_1 = x_2 = \\cdots = x_p =0\\). This might be an impossible configuration. Even so, we usually leave the intercept in the model; if the \\(x\\)s can be zero simultaneously, we would not necessarily predict \\(\\eta = 0\\).\nThe logistic regression model in Section 17.2 also contains a linear predictor. Depending on whether you write the model in terms of \\(\\pi\\) or \\(g(\\pi)\\), the expressions are\n\\[\n\\begin{align*}\ng(\\pi) &= \\eta \\\\\n\\pi &= \\frac{1}{1 + \\text{E}\\{ - \\eta \\}}\\\\\n\\end{align*}\n\\]\n\n\nNon-linear functions\nThe mean function can be a general non-linear function of the parameters. The number of input variables and the number of parameters can be quite different.\n\n\nExample: Mitscherlich Model\n\n\nThe Mitscherlich model is popular in agricultural studies to model plant growth as a function of an input such as a fertilizer. If \\(Y\\) denotes plant yield and \\(x\\) denotes the amount of input, the Mitscherlich model is\n\\[\nY = f(x,\\xi,\\lambda,\\kappa) + \\epsilon = \\lambda + (\\xi - \\lambda)\\text{E}\\left\\{ - \\kappa x \\right\\} + \\epsilon\n\\]\nThe mean function \\(f\\)() depends on one input variable \\(x\\) and three parameters \\((\\xi,\\lambda,\\kappa)\\). Taking derivatives, it is easily established that the mean function is non-linear (in the parameters):\n\\[\n\\frac{\\partial f(x,\\xi,\\lambda,\\kappa)}{\\partial\\xi} = \\text{E}\\{ - \\kappa x \\}\n\\]\nThe derivative with respect to \\(\\xi\\) depends on the \\(\\kappa\\) parameter.\n\n\nNon-linear equations like the Mitscherlich model are appealing because they are easily interpretable. The parameters have meaning in terms of the subject domain:\n\n\\(\\xi\\) is the crop yield if no fertilizer is applied, the mean of \\(Y\\) at \\(x = 0\\). This is the baseline yield.\n\\(\\lambda\\) is the upper yield asymptote as \\(x\\) increases.\n\\(\\kappa\\) relates to a rate of change, how quickly the yield increases from \\(\\xi\\) and reaches \\(\\kappa\\).\n\nFigure 17.1 shows the Mitscherlich model fitted to a set of plant yield data, the input variable is the nitrogen rate applied (in kg/ha). Visual estimates for the baseline yield and the asymptotic yield are \\(\\widehat{\\xi} = 40\\) and \\(\\widehat{\\lambda} = 80\\).\n\n\n\n\n\n\nFigure 17.1: Mitscherlich yield equation for plant yield as a function of nitrogen rate fitted to a set of data.\n\n\n\nInterpretability of the parameters allows us to answer research questions in terms of model parameters:\n\nIs the asymptotic yield greater than 75? This can be answered with a confidence interval for the estimate of \\(\\lambda\\). If the confidence interval includes 75 the hypothesis \\(H: \\text{Yield } &gt; 75\\) is rejected.\nAt what level of \\(x\\) does yield achieve 75% of the maximum? This is an inverse prediction problem. Set yield to 75% of \\(\\lambda\\) and solve the model for \\(x\\).\nThe rate of change in yield is less than ½ unit once \\(x = 100\\) are applied. This can be answered with a hypothesis test for \\(\\kappa\\).\n\n\n\nSmooth functions and local models\nThe third method of specifying the systematic component is to not write it as a function of inputs and parameters. This is common for non-parametric methods such as kernel regression models, regression splines, smoothing splines, and generalized additive models. These models still have parameters, but the relationship between inputs and parameters is implied through the method of training the models.\nFor example, LOESS is a local polynomial regression method. A LOESS model of degree 2 fits a quadratic polynomial model to a portion of the data (a window). The window is created by centering a weight function at the location where you want to predict; call this value \\(x_0\\). Data points close to \\(x_0\\) receive more weight in the analysis than data points far from \\(x_0\\). The model at \\(x_0\\) is the following:\n\\[\nY(x_0) = \\beta_{00} + \\beta_{10}x_0 + \\beta_{20}x_0^{2} + \\epsilon\n\\]\n\\(\\beta_{00}\\) is the intercept of the quadratic model fit at location \\(x_0\\), \\(\\beta_{10}\\) is the slope with respect to \\(x\\) at location \\(x_0\\), \\(\\beta_{20}\\) is the slope of the quadratic term \\(x^2\\) at location \\(x_0\\).\nTo predict at a different location, \\(x_j\\), say, the weight function is moved to \\(x_j\\) and the local quadratic model $$ Y(x_j) = {0j} + {1j}x_j + _{2j}x_j^{2} + \n$$ is fit again.\nA LOESS model cannot be written as a single model with a single set of parameters. A different set of parameters applies to each prediction location \\(x_j\\). The underlying model in each window is a known parametric model, however: a quadratic polynomial in this example.\n\n\n\nRandom Component\nThe random components of a statistical model are the stochastic elements that describe the distribution of the target variable \\(Y\\). By now we are convinced that most data we work with are to some degree the result of random processes and that incorporating randomness into models makes sense. The model does not need to be correct for every observation, but it needs to be correct on average—an additive zero-mean random error is OK. Even if all influences on the output \\(Y\\) were known, it might be impossible to measure them, or to include them correctly into the model.\nRandomness is often introduced deliberately by sampling observations from a population or by randomly assigning treatments to experimental units. Finally, stochastic models are often simpler and easier to explain than other models. Among competing explanations, the simpler one wins (Occam’s Razor).\nWe encountered two basic approaches to reflect randomness in a statistical model:\n\nAdding an additive error term to a mean function\nDescribing the distribution of the target variable\n\nThe Mitscherlich model is an example of the first type of specification:\n\\[\nY = f(x,\\xi,\\lambda,\\kappa) + \\epsilon = \\lambda + (\\xi - \\lambda)\\text{E}\\left\\{ - \\kappa x \\right\\} + \\epsilon\n\\]\nUnder the assumption that \\(\\epsilon \\sim \\left( 0,\\sigma^{2} \\right)\\), it follows that \\(Y\\) is randomly distributed with mean \\(f(x,\\xi,\\lambda,\\kappa)\\) and variance \\(\\sigma^{2}\\); \\(Y \\sim \\left( f(x,\\xi,\\lambda,\\kappa),\\sigma^{2} \\right)\\). If the model errors were normally distributed, \\(\\epsilon \\sim N\\left( 0,\\sigma^{2} \\right)\\), then \\(Y\\) would also be normally distributed. Randomness is contagious.\nThe logistic regression model is an example of the second type of specification:\n\\[\ng\\left( \\text{E} \\lbrack Y\\rbrack \\right) = \\beta_{0} + \\beta_{1}x_{1} + \\ldots + \\beta_{p}x_{p}\n\\]\nand \\(Y\\) follows a Bernoulli distribution.\nIt does not make sense to write the model with an additive error term unless the target variable is continuous.\nModels can have more than one random element. In the cancer recurrence example, suppose we want to explicitly associate a random effect with each patient: \\(b_{i} \\sim \\left( 0,\\sigma_{b}^{2} \\right)\\). The modified model is now\n\\[\ng\\left( \\pi\\ |\\ b_{i} \\right) = \\beta_{0} + b_{i} + \\ \\beta_{1}x_{1} + \\ldots + \\beta_{p}x_{p}\n\\]\nConditional on the patient-specific value of \\(b_{i}\\), the model is still a logistic model with intercept \\(\\beta_{0} + b_{i}\\). Because the parameters \\(\\beta_{0}, \\cdots,\\beta_{p}\\) are constants (not random variables), they are referred to as fixed effects. Models that contain both random and fixed effects are called mixed models.\nMixed models occur naturally when the sampling process is hierarchical.\nFor example, you select apples on trees in an orchard to study the growth of apples over time. You select at random 10 trees in the orchard and chose 25 apples at random on each tree. The apple diameters are then measured in two-week intervals. To represent this data structure, we need a few subscripts.\nLet \\(Y_{ijk}\\) denote the apple diameter at the \\(k\\)th measurement of the \\(j\\)th apple from the \\(i\\)th tree. A possible decomposition of the variability of the \\(Y_{ijk}\\) could be \\[\nY_{ijk} = \\beta_{0} + a_{i} + \\eta_{ijk} + \\epsilon_{ijk}\n\\]\nwhere \\(\\beta_{0}\\) is an overall (fixed) intercept, \\(a_{i} \\sim ( 0,\\sigma_{a}^{2})\\) is a random tree effect, \\(\\eta_{ijk}\\) is an effect specific to apple and measurement time, and \\(\\epsilon_{ijk} \\sim ( 0,\\sigma_{\\epsilon}^{2})\\) are the model errors.\nThis is a mixed model because we have multiple random effects (\\(a_{i}\\) and \\(\\epsilon_{ijk}\\)). In addition, we need to decide how to parameterize \\(\\eta_{ijk}\\). Suppose that a simple linear regression trend is reasonable for each apple over time. Estimating a separate slope and intercept for each of the 10 x 25 apples would result in a model with over 500 parameters. A more parsimonious parameterization is to assume that the apples share a tree-specific (fixed) intercept and slope and to model the apple-specific deviations from the tree-specific trends with random variables:\n\\[\n\\eta_{ijk} = \\left( \\beta_{0i} + b_{0ij} \\right) + {(\\beta}_{1i} + b_{1ij})t_{ijk}\n\\]\n\\(t_{ijk}\\) is the time that a given apple on a tree is measured. The apple-specific intercept offsets from the tree-specific intercepts \\(\\beta_{0i}\\) are model as random variables \\(b_{0ij} \\sim ( 0,\\sigma_{b_{0}}^{2})\\). Similarly, \\(b_{1ij} \\sim ( 0,\\sigma_{b_{1}}^{2} )\\) models the apple-specific offset for the slopes as random variables. Putting everything together we obtain\n\\[\nY_{ijk} = \\beta_{0} + \\left( \\beta_{0i} + b_{0ij} \\right) + {(\\beta}_{1i} + b_{1ij})t_{ijk} + \\epsilon_{ijk}\n\\]\nNote that \\(a_{i}\\) was no longer necessary in this model, that role is now played by \\(\\beta_{0i}\\).\nThe total number of parameters in this model is 24 (1 overall intercept, 10 tree-specific intercepts, 10 tree-specific slopes, and 3 variances (\\(\\sigma_{\\epsilon}^{2}, \\sigma_{b_{0}}^{2}\\), \\(\\sigma_{b_{1}}^{2}\\)).\nThis is a relatively complex model and included here only to show how the sampling design can be incorporated into the model formulation to achieve interpretable and parsimonious models and how this naturally leads to multiple random effects.\nA further refinement of this model is to recognize that the measurements over time for each apple are likely not independent. Furthermore, diameter measurements on the same apple close in time are more strongly correlated than measurements further apart. Incorporating this correlation structure into the models leads to a mixed model with correlated errors.\n\n\nResponse (Target) Variable\nA model has inputs that are processed by an algorithm to produce an output. When the output is a variable to be predicted, classified, or grouped, we refer to it with different—but interchangeable—names as the response variable, or the target variable, the output or the dependent variable. We are not very particular =about what you call the variable, as long as we agree on what we are talking about—the left-hand side of the model.\n\n\n\n\n\n\nDependent and Independent Variables\n\n\n\nThe terms dependent variable for the target variable and independent variable(s) for the input variable(s) are common in statistics. Calling the target the dependent variable makes sense because we are modeling its dependence on the input variables. Calling the inputs independent variables makes no sense. What are they independent of? In most applications the input variables are very much related to each other so they cannot be independent of each other. In most predictive applications we condition the model on the observed values of the inputs, so the \\(x\\)s are not random variables—the probabilistic concept of independence applies only to random variables.\nFor these reasons you will not find us referring to input variables as independent variables.\n\n\nThe target variable is a random variable and can be of different types; see Section 4.2.2 for a list of data types. Selecting an analytic method that matches the data type of the target variable matters greatly.\nApplying an analytic method designed for continuous variables that can take on infinitely many values to a binary variable that takes on two values is ill advised. However, it happens. A lot. Treating an ordinal variable that is defined through greater–lesser relationships of its values, rather than differences, as a continuous variable for which such differences are meaningful, is ill advised. However, it happens. A lot.\nFortunately, we are equipped today with a rich set of tools and can find the appropriate tool for the type of response variable.\n\n\n\nFigure 17.1: Mitscherlich yield equation for plant yield as a function of nitrogen rate fitted to a set of data.\n\n\n\nBreiman, Leo. 2001. “Statistical Modeling: The Two Cultures.” Statistical Science 16 (3): 199–231.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "models/model_concepts.html",
    "href": "models/model_concepts.html",
    "title": "18  General Concepts",
    "section": "",
    "text": "18.1 Signal and Noise\nThe signal represents the systematic, non-random effects in the data. The noise is the unpredictable, unstructured and non-systematic, randomness around the signal.\nA slightly different, and also useful, definition of noise stems from intelligence analysis. The signal is the information we are trying to find, the noise is the cacophony of other information that obscures the signal. That information might well be a signal for something else but it is irrelevant or useless for the event the intelligence analyst is trying to predict.\nInformation not being relevant for the signal we are trying to find is the key. In the view of the data scientist, that information is due to random events.\nFinding the signal is not trivial, different analysts can arrive at different models to capture it. Signals can be obscured by noise. Multiple signals can hide in the data, for example, short-term and long-term cyclical trends in time series data. What appears to be a signal might just be random noise that we mistake for a systematic effect.\nFinding the signal in noisy data is not trivial. The opposite can also be difficult: not mistaking noise for a signal. Figure 18.2 is taken from Silver (2012, 341) and displays six “trends”. Four of them are simple random walks, the result of pure randomness. Two panels show the movement of the Dow Jones Industrial Average (DJIA) during the first 1,000 trading days of the 1970s and 1980s. Which of the panels are showing the DJIA and which are random noise?\nWhat do we learn from this?\nBy the way, panels D and F in Figure 18.2 are from actual stock market data. Panels A, B, C, and E are pure random walks. It would not be surprising if some investors would bet money on “trend” C.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>General Concepts</span>"
    ]
  },
  {
    "objectID": "models/model_concepts.html#sec-model-signal-noise",
    "href": "models/model_concepts.html#sec-model-signal-noise",
    "title": "18  General Concepts",
    "section": "",
    "text": "Example: Theophylline Concentration\n\n\nFigure 18.1 shows the concentration of the drug theophylline over 24 hours after administration of the drug in two groups of patients. There are 98 data points of theophylline concentration and measurement time. What are the signals in the data? What is noise?\n\n\n\n\n\n\n\n\nFigure 18.1: Theophylline concentration over time in two groups of patients.\n\n\n\n\n\nThe first observation is that the data points are not all the same over time, otherwise they would fall on a horizontal straight line: there is variability in the data. Separating signal and noise means attributing this variability to different sources: some systematic, some random.\nFocusing on either the open circles (group 1) or the triangles (group 2), you notice that points that are close in time are not necessarily close in the concentration measurement. Not all patients were measured at exactly the same time points, but at very similar time points. For example, concentrations were measured after about 7, 9, and 12 hours. The differences in the concentration measurements among the patients receiving the same dosage might be due to patient-to-patient variability or measurement error.\nFocusing on the general patterns of open circles and triangles, it seems that the triangles appear on average below the average circle a few hours after administration. Absorption of theophylline into the body and elimination from the body appear to be different in the two groups.\nMuch of the variability in the data seems to be a function of time. Shortly after administering the drug the concentration rises, reaches a maximum level, and declines as the drug is eliminated from the body. Note that this sentence describes a general overall trend in the data.\nWhich of these sources of variability are systematic—the signals in the data— and which are random noise?\n\nPatient-to-patient variability within a group at the same time of measurement: we attribute this to random variation among the participants.\nPossible measurement errors in determining the concentrations: random noise\nOverall trend of drug concentration over time: signal\nDifferences among the groups: signal\n\nThese assignments to signal and noise can be argued. For example, we might want to test the very hypothesis that there are no group-to-group differences. If that hypothesis is true, any differences between the groups we discern in Figure 18.1 would be due to chance; random noise in other words.\nThe variability between patients could be due to factors such as age, gender, medical condition, etc. We do not have any data about these attributes. By treating these influences as noise, we are making important assumptions that their effects are irrelevant for conclusions derived from the data. Suppose that the groups refer to smokers and non-smokers but also that group 1 consists of mostly men and group 2 consists of mostly women. If we find differences in theophylline concentration over time among the groups, we could not attribute those to either smoking status or gender.\n\n\n\n\n\n\n\n\n\nFigure 18.2: Figure 11-4 from Silver (2012). Random walk or stock market data?\n\n\n\n\n\nEven purely random data can appear non-random over shorter sequences. We can easily fall into the trap of seeing a pattern (a signal) where there is none. Sometimes there is no signal at all. After drawing two unlikely poker hands in a row there is not a greater chance of a third unlikely hand unless there is some systematic effect (cards not properly shuffled, game rigged). Our brains ignore that fact and believe that we are more lucky than is expected by chance. Paul the Octopus predicted the winner of 12 out of 14 soccer matches correctly; I would argue purely by chance.\nData that contains clear long-run signals—the stock market value is increasing over time—can appear quite random on shorter sequences. One a day-to-day basis predicting whether the market goes up or down is very difficult. In the long run ups and downs are almost equally likely. Upswings have a slight upper hand and are on average greater than the downswings, increasing the overall value in the long term. Traders who try to beat the market over the short run have their work cut out for them.\n\n\n\n\nExercise: Southern Oscillation Index (SOI)\n\n\nThe Southern Oscillation Index (SOI) is a standardized index based on the observed sea level pressure differences between Tahiti and Darwin, Australia. The SOI is one measure of the large-scale fluctuations in air pressure occurring between the western and eastern tropical Pacific (i.e., the state of the Southern Oscillation) during El Niño and La Niña episodes.\nIn general, smoothed time series of the SOI correlate highly with changes in ocean temperatures across the eastern tropical Pacific. The negative phase of the SOI represents below-normal air pressure at Tahiti and above-normal air pressure at Darwin. Prolonged periods of negative (positive) SOI values coincide with abnormally warm (cold) ocean waters across the eastern tropical Pacific typical of El Niño (La Niña) episodes (Figure 18.3).\nAccording to Wikipedia, there have been about 30 El Niño episodes since 1950 with strong El Niño events in 1982–83, 1997–98, and 2014–16. You recognize El Niño when the SOI dips negative for a period of time. La Niña is marked by periods of positive SOI values.\n\n\n\n\n\n\n\n\nFigure 18.3: Monthly SOI data from 1951 to mid-2023 according to NOAA.\n\n\n\n\n\n\nWhat is the signal and the noise in these data?\nIs it possible that there are multiple signals in these data, associated with different time horizons?",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>General Concepts</span>"
    ]
  },
  {
    "objectID": "models/model_concepts.html#types-of-statistical-learning",
    "href": "models/model_concepts.html#types-of-statistical-learning",
    "title": "18  General Concepts",
    "section": "18.2 Types of Statistical Learning",
    "text": "18.2 Types of Statistical Learning\nThe primary distinction of learning methods in data science is between supervised and unsupervised methods of learning (Figure 18.4). However, there are other important forms of learning from data, for example, self-supervised learning and reinforcement learning (RL), which are important for machine learning and artificial intelligence applications.\n\n\n\n\n\n\nFigure 18.4: Important types of statistical learning\n\n\n\n\nSupervised Learning\n\n\nDefinition: Supervised Learning\n\n\nSupervised learning trains statistical learning models through a target variable.\n\n\nSupervised learning is characterized by the presence of a target variable (dependent variable, response variable, output variable); it is the attribute we wish to model. The training and test data sets contain values for the target variable, also called the labels in machine learning. The other variables in the data set are potentially input variables. This is the predominant form of learning in data science applications.\nGoals of supervised learning include to\n\nPredict the target variable from input variables.\nClassify observations into categories of the target variable based on the input variables.\nDevelop a function that approximates the underlying relationship between inputs and outputs.\nUnderstand the relationship between inputs and outputs.\nGroup the observations into sets of similar data based on the values of the target variable and based on values of the inputs.\nReduce the dimensionality of the problem by transforming target and inputs from a high-dimensional to a lower-dimensional space.\nTest hypotheses about the target variable.\n\nStudies can pursue one or more of these goals. For example, you might be interested in understanding the relationship between target and input variables and use that relationship for predictions as well as testing of hypotheses.\nThe name supervised learning comes from thinking of learning in an environment that is supervised by a teacher. The teacher asks questions for which they know the correct answer (the ground truth) and judge a student’s response to the questions. The goal is to increase students’ knowledge as measured by the quality of their answers. But we do not want students to just memorize answers, we want to teach them to be problem solvers, to apply the knowledge to new problems, to be able to generalize.\nThe parallel between the description of supervised learning in a classroom and training an algorithm on data is obvious: the problems asked by the teacher, the learning algorithm, are the data points, \\(Y\\) is the correct answer, the inputs \\(x_{1},\\cdots,x_{p}\\) are the information used by the students to answer the question. The discrepancy between question and answer is measured by a loss function: \\[\n(y - \\widehat{y})^2 = (y - \\widehat{f}\\left( x_{1},\\cdots x_{p} \\right))^2\n\\] or some such metric. Teaching the students reduces the loss as their answers get closer to the correct answers, the ground truth. Training stops when the students can transfer knowledge to solve previously unseen problems. We are not interested in teaching students to memorize the answers to the questions asked in class. If that were the case we would just train until the loss function reaches zero. We do not know whether the students have learned the concepts or just memorized the answers. To avoid the latter, we need to validate the students’ knowledge on new questions and not overdo memorization.\nTable 18.1 contains a non-exhaustive list of algorithms and models you encounter in supervised learning.\n\n\n\nTable 18.1: A sampling of supervised learning methods.\n\n\n\n\n\n\n\n\n\n\nLinear regression\nNonlinear regression\nRegularized regression\n(Lasso, Ridge, Elastic nets)\n\n\nLocal polynomial regression (LOESS)\nSmoothing splines\nKernel methods\n\n\nLogistic regression (binary & binomial)\nMultinomial regression (nominal and ordinal)\nPoisson regression (counts and rates)\n\n\nDecision trees\nRandom forests\nBagged trees\n\n\nAdaptive boosting\nGradient boosting machine\nExtreme gradient boosting\n\n\nNaïve Bayes classifier\nNearest-neighbor methods\nDiscriminant analysis (linear and quadratic)\n\n\nPrincipal component regression\nPartial least squares\nGeneralized linear models\n\n\nGeneralized additive models\nMixed models (linear and nonlinear)\nModels for correlated data (spatial, time series)\n\n\nSupport-vector machines\nNeural networks\nExtreme gradient boosting\n\n\n\n\n\n\nThere is a lot to choose from, and for good reason. The predominant application of data analytics is supervised learning with batch (or mini-batch) data. In batch data analysis the data already exist as a historical data source in one place. We can read all records at once or in segments (called mini-batches). If we have to read the data multiple times, for example, because an iterative algorithm passes through the data at each iteration, we can do so.\n\n\nUnsupervised Learning\n\n\nDefinition: Unsupervised Learning\n\n\nIn unsupervised learning methods a target variable is not present.\n\n\nUnsupervised learning does not utilize a target variable; hence it cannot predict or classify observations. However, we are still interested in discovering structure, patterns, and relationships in the data.\nThe term unsupervised refers to the fact that we no longer know the ground truth because there is no target variable. The concept of a teacher who knows the correct answers and supervises the learning progress of the student does not apply. In unsupervised learning there are no clear error metrics by which to judge the quality of an analysis, which explains the proliferation of unsupervised methods and the reliance on heuristics. For example, a 5-means cluster analysis will find five groups of observations in the data, whether this is the correct number or not, and it is up to us to interpret what differentiates the groups and to assign group labels.\nOften, unsupervised learning is used in an exploratory fashion, improving our understanding of the joint distributional properties of the data and the relationships in the data. The findings then help lead us toward supervised approaches.\nA coarse categorization of unsupervised learning techniques also hints at their application:\n\nAssociation analysis: which values of the variables \\(x_{1},\\cdots,x_{p}\\) tend to occur together in the data? An application is market basket analysis, where the \\(X\\)s are items are in a shopping cart (or a basket in the market), and \\(x_{i} = 1\\) if the \\(i\\)th item is present in the basket and \\(x_{i} = 0\\) if the item is absent. If items frequently appear together, bread and butter, or beer and chips, for example, then maybe they should be located close together in the store. Association analysis is also useful to build recommender systems: shoppers who bought this item also bought the following items \nCluster analysis: can data be grouped based on \\(x_{1},\\cdots,x_{p}\\) into sets such that the observations within a set are more similar to each other than they are to observations in other sets? Applications of clustering include grouping customers into segments. Segmentation analysis is behind loyalty programs, lower APRs for customers with good credit rating, and churn models.\nDimension reduction: can we transform the inputs \\(x_{1},\\cdots,x_{p}\\) into a set \\(c_{1},\\cdots,c_{k}\\), where \\(k \\ll p\\) without losing relevant information? Applications of dimension reduction are in high-dimensional problems where the number of inputs is large relative to the number of observations. In problems with wide data, the number of inputs \\(p\\) can be much larger than \\(n\\), which eliminates many traditional methods of analysis from consideration.\n\nMethods of unsupervised learning often precede supervised learning; the output of an unsupervised learning method can serve as the input to a supervised method. An example is dimension reduction through principal component analysis (PCA) prior to supervised regression. Suppose you have \\(n\\) observations on a target variable \\(Y\\) and a large number of potential inputs \\(x_{1},\\cdots,x_{p}\\) where \\(p\\) is large relative to \\(n\\). PCA computes linear combinations of the \\(p\\) inputs that account for decreasing amounts of variability among the \\(X\\)s. These linear combinations are called the principal components. For example, the first principal component explains 70% of the variability in the inputs, the second principal component explains 20% and the third principal component 5%. Rather than building a regression model with \\(p\\) predictors, we use only the first three principal components as inputs in the regression model. The resulting regression model is called a principal component regression (PCR) because its inputs are the result of a PCA. The PCA is an unsupervised model because it does not use information about \\(Y\\) in forming the principal components. If \\(p = 250\\), using the first three principal components replaces\n\\[\nY = \\beta_{0} + \\beta_{1}{\\ x}_{1} + \\beta_{2}x_{2} + \\beta_{3}x_{3} + \\beta_{4}x_{4} + \\cdots + \\beta_{250}x_{250} + \\epsilon\n\\] with \\[\nY = \\alpha_{0} + \\alpha_{1}c_{1} + \\alpha_{2}c_{2} + \\alpha_{3}c_{3} + \\epsilon\n\\]\nwhere \\(c_{1}\\) denotes the first principal component, itself a linear combination of the 250 inputs\n\\[\nc_{1} = \\gamma_{1}x_{1} + \\gamma_{2}x_{2} + \\gamma_{3}x_{3} + \\cdots + \\gamma_{250}x_{250}\n\\]\n\n\nSelf-supervised Learning\nSelf-supervised learning (SSL) is a form of learning that combines elements of supervised and unsupervised learning. Like unsupervised learning, the data is not labeled, that is, there is no target variable. Like the majority of supervised learning applications, the goal is to predict or classify observations.\nSSL accomplishes these seemingly contradictory features by implicitly and autonomously extracting relationships, patterns, and knowledge from the training data. Generative pretrained transformers (GPT), for example, are trained in a self-supervised way as autoregressive language model. During training the model learns the relevant patterns and relationships in the text input data.\nIn a second step, applications are built on top of the foundation models trained in a self-supervised fashion. For example, ChatGPT is a question-answer applications built on top of the GPT foundation model.\n\n\nReinforcement Learning\nReinforcement learning (RL) is unique to machine learning and does not fall neatly in the supervised/unsupervised learning categories. It is a powerful method that received much attention when algorithms were trained on data to play games.\nThe approach, based on reinforcement learning, was fundamentally different from the expert system-based approach used so far to teach computers how to play games. An expert system translates the rules of the game into machine code and adds strategy logic. For example, the Stockfish open-source chess program, released first in 2008, has developed with community support into (one of) the best chess engines in the world. In 2017, Google’s DeepMind released AlphaZero, a chess system trained using reinforcement learning. After only 24 hours of training, the data-driven AlphaZero algorithm crushed Stockfish, the best chess engine humans have been able to build over 10 years.\nPreviously, Google’s DeepMind had developed AlphaGo, a reinforcement-trained system that beat the best Go player in the world, Lee Sedol, four to one. This was a remarkable achievement as Go had been thought to be so complex and requiring intuition that would escape computerization at the level of expert players.\nIn reinforcement learning, an agent (a player) is taking actions (makes moves) in an environment (the game). The agent learns by interacting with the environment by receiving feedback on the moves. Actions are judged by a reward function (a score) and the system is trained to maximize the sum of future rewards. In other words, given your current position in the game, choose the next move to maximize the score from here on out.\nAn interesting difference between AlphaGo and AlphaZero is the nature of the training data. Both systems are trained using reinforcement learning. AlphaGo was trained on records of many expert-level games. It was trained to play against historic experts. Success was getting better compared to how human experts played. AlphaZero was trained by playing against itself. Success was beating its former self.\nUnlike supervised learning, inputs and outputs do not need to be present in reinforcement learning. The technique is commonly used in robotics, gaming, and recommendation systems.\nIn 2017, when AlphaGo beat Lee Sedol, it was thought that reinforcement learning would change the world. Despite its remarkable achievement in gameplay and robotics, the impact of RL fell short of expectations.\n\n\n\n\n\n\nFigure 18.5: Yann LeCun weighs in on the impact of reinforcement learning (RL).\n\n\n\nWhy did RL fall short? Developing and training reinforcement learning models is an expensive undertaking. The barrier to entry is very high, limiting RL research and development to large tech-savvy organizations. The main reason is the Sim2Real problem mentioned in the tweet above. Reinforcement learning trains an agent in a simulated, artificial environment. The real world is much more complex and transferring training based on simulation to reality is difficult. The RL agents end up performing poorly in real applications.\n\nUntil recently, a limitation of RL was the need for a good reward function. It is important that actions in the environment are properly judged. In situation where the result of a move is difficult to judge, reinforcement learning was difficult to apply. For example, in natural language processing, where an action produces some prose, how do we rate the quality of the answer?\nThis was the problem faced by systems like ChatGPT. How do you score the answer produced during training to make sure the algorithm continuously improves? The solution was a form of reinforcement learning modified by human intervention. RLHF, reinforcement learning with human feedback, uses human interpreters to assign scores to the actions (the Chat-GPT answers).",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>General Concepts</span>"
    ]
  },
  {
    "objectID": "models/model_concepts.html#regression-and-classification",
    "href": "models/model_concepts.html#regression-and-classification",
    "title": "18  General Concepts",
    "section": "18.3 Regression and Classification",
    "text": "18.3 Regression and Classification\n\nRegression and the Regression to the Mean Fallacy\nThe term regression was coined by Sir Francis Galton in 1877 in his study of genetics. Galton observed a relationship between physical attributes of offspring and their parents. He found that offspring deviated less from the mean value of the population than their parents did. Taller parents tend to have taller children, but children of taller parents tend to be shorter than their parents and children of shorter parents tend to be taller than their parents.\nThe cause Galton attributed to this phenomenon—which he called regression toward mediocrity—is controversial, his arguments had implications about natural selection and eugenics. As a statistical phenomenon, it is well understood. Attributes are distributed randomly; if you draw an extreme observation from a symmetric distribution, then a subsequent draw is likely to be less extreme, there is a regression to the mean.\nFigure 18.6 displays this phenomenon for the distribution of HDL cholesterol values in a single person. Our cholesterol levels vary from day to day and if the distribution is normally distributed, it might look like the density in the figure, centered at a mean of 50 mg/dl. Suppose you measure a person’s HDL cholesterol, and it results in a first measurement of 30 mg/dl. The value is on the low side of the distribution, but it is not implausible. A follow-up measurement is more likely to be observed near the center of the distribution where most of the probability density is located. A second measurement might thus return a value of 55 mg/dl. A regression to the mean occurred between the first and second measurements.\n\n\n\n\n\n\nFigure 18.6: An example of regression to the mean. An extreme observation is more likely to be followed by a less extreme observation, one that falls near the center of the distribution.\n\n\n\n\n\nAssignment: Random Multiple-choice Answers\n\n\nSuppose you administer a multiple-choice exam to a group of students who choose their answers completely at random.\nThe students with the top 10% of the scores are asked to take the exam a second time. Again, they answer the multiple-choice questions at random.\n\nWhat percentage of the questions will the students taking the second exam get correct?\nHow does this relate to the regression to the mean phenomenon? \n\n\n\nThe term regression has evolved to describe statistical methods that model the mean behavior of an attribute and separate it from non-systematic, random behavior. The regression to the mean phenomenon does not mean regression methods are bad. It is a fact of random variation and a real fallacy when interpreting data. In the cholesterol example, is the change from 30 to 50 mg/dl of HDL due to natural variation or due to a systematic effect, for example, a medical intervention? Regression is intended to separate signal from noise. The regression to the mean fallacy is to misinterpret random variation as a signal.\n\n\nExample: Testing a Treatment Effect\n\n\nYou want to determine whether a change in diet reduces the risk of heart disease. From a group of individuals, you select those at greatest risk of heart disease and put them on the diet. After some time, you reassess their risk of heart disease.\nBecause of regression to the mean, the follow-ups will likely show an improvement even if the diet has no effect at all.\nThe correct way of studying whether the diet has an effect is to randomly divide the individuals into two groups and assign the diet to one group (the treated group) while the other group stays on their normal diet. If the individuals in the treated group improve more than the untreated group, to a degree that cannot be attributed just to chance, then we can make a statement that the diet is effective in reducing the risk of heart disease.\n\n\n\n\nRegression Problems\n\n\nDefinition: Regression Model\n\n\nA regression model is a statistical model that describes how the mean of a random variable depends on other factors. The factors are often called inputs, predictor variables, or regressors.\nThe variable whose mean is modeled is called the target variable, response variable, or dependent variable.\n\n\nRegression models are not just for continuous response data, they apply to all response types. The defining characteristic is to model the mean as a function of inputs:\n\\[\n\\text{E}\\lbrack Y\\rbrack = f\\left( x_{1},\\cdots,x_{p},\\theta_{1},\\cdots,\\theta_{k} \\right)\n\\]\nThis expression explicitly lists parameters \\(\\theta_{1},\\cdots,\\theta_{k}\\) in the mean function. All regression models involve the estimation of unknown, fixed quantities (=parameters). As seen in the non-linear regression example in the introduction, the number of parameters and the number of inputs do not have to be directly related.\nEven if the target variable is categorical, we might be interested in modeling the mean of the variable. The simplest case of categorical variables are binary variables with two levels (two categories). This is the domain of logistic regression. As seen earlier, if the categories are coded numerically as \\(Y = 1\\) for the category of interest (the “event”) and \\(Y = 0\\) for the “non-event” category, the mean of \\(Y\\) is a probability. A regression model for a binary target variable is thus a model to predict probabilities. It is sufficient to predict one of the probabilities in the binary context, we call this the event probability \\(\\pi\\). The complement can be obtained by subtraction, \\(1 - \\pi\\).\nExtending this principle to more than two categories leads to regression models for multinomial data. If the category variable has \\(k\\) levels with labels \\(C_{1}, \\cdots, C_{k}\\), we are dealing with \\(k\\) probabilities; \\(\\pi_{j}\\) is the probability to observe category \\(C_{j}\\). Suppose that we are collecting data on ice cream preferences on college campuses. A random sample of students are given three ice cream brands in a random order and report the taste as \\(C_{1} =\\)’yuck’, \\(C_{2} =\\)’meh’, and \\(C_{3} =\\)’great’. Modeling these data with regression techniques, we develop a model for the probability to observe category \\(j\\) as a function of inputs. A multinomial version of logistic regression looks like the following:\n\\[\n\\text{Pr}\\left( Y = j \\right) =\n\\frac{\\exp\\left\\{ \\beta_{0j} + \\beta_{1j}x_1 + \\cdots + \\beta_{pj}x_p \\right\\}}\n     {\\sum_{l=1}^k \\exp\\{\\beta_{0l} + \\beta_{1l}x_1 + \\cdots + \\beta_{pl}x_p\\}}\n\\]\nThis is a rather complicated model, but we will see later that it is a straightforward generalization of the two-level case. Instead of one linear predictor we now have separates predictors for the categories. The point of introducing the model here is to show that even in the categorical case we can apply regression methods—they predict category probabilities rather than the mean of a continuous variable.\n\n\nClassification Problems\nClassification applies to categorical target variables that take on a discrete number of categories, \\(k\\). In the binary case \\(k = 2\\), in the multinomial case \\(k &gt; 2\\).\nThe classification problem is to predict not the mean of the variable but to assign a category to an observation. In image classification, for example, an algorithm is trained to assign the objects seen on an image to one of \\(k\\) possible categories. In the ImageNet competition, \\(k=1000\\).\nThe algorithm that maps from input variables to a category is called a classifier. The logic that turns the model output into a category assignment is called the classification rule. Applications of classifications occur in many domains, for example,\n\nMedical diagnosis: Given a patient’s symptoms, assign a medical condition.\nFinancial services: Determine whether a payment transaction is fraudulent.\nCustomer intelligence: Assign a new customer to a customer profile (segmentation).\nComputer vision: Detect defective items on an assembly line.\nComputer vision: Identify objects in an image.\nText classification: Categorize incoming emails as spam.\nText classification: Categorize the sentiment of a document as positive, neutral, or negative.\nDigital marketing: predict which advertisement a user is most likely to click.\nSearch engine: Given a user’s query and search history predict what link they will follow.\n\nClassification problems can be presented as prediction problems; rather than the mean of a random variable, they predict the membership in a category.\n\nFrom probabilities to classification\nTo determine category membership many classification models first go through a predictive step, predicting the probabilities that an observation belongs to the \\(k\\) categories. The classification rule is then based to assign an observation to one category based on the predicted probabilities; usually the category with the highest predicted probability.\nSuppose we have a three-category problem with \\(\\pi_{1} = 0.7,\\ \\pi_{2} = 0.2,\\pi_{3} = 0.1\\), and you are asked to predict the category of the next randomly drawn observation. The most likely category to appear is \\(C_{1}.\\)\nThis classification rule is known as the Bayes classifier.\n\n\nDefinition: Bayes Classifier\n\n\nThe Bayes classifier assigns an observation with inputs \\(x_{1},\\cdots,x_{p}\\) to the class \\(C_{j}\\) for which\n\\[\n\\Pr(Y = j \\, | \\, x_{1},\\cdots,x_{p})\n\\]\nis largest.\n\n\nThe Bayes classifier is written as a conditional probability, the probability to observe category \\(C_{j}\\), given the values of the input variables. The reason for this will become clearer when we cover different methods for estimating category probabilities. Some methods for deriving category probabilities assume that the \\(X\\)s are random. In regression problems it is assumed that they are fixed, so there is no difference between the unconditional probability \\(\\Pr\\left( Y = j \\right)\\) and the conditional probability \\(\\Pr( Y = j \\, | \\, x_1,\\cdots,x_p)\\).\nWe can now see the connection between regression and classification. Develop first a regression model that predicts the category probabilities \\(\\Pr( Y = j \\, | \\, x_1,\\cdots,x_p)\\). Then apply a classification rule to assign a category based on the predicted probabilities. If you side with the Bayes classifier, you choose the category that has the highest predicted probability. For a 2-category problem where events are coded as \\(Y=1\\) and non-events are coded as \\(Y=0\\), this means classifying an observation as an event if\n\\[\n\\Pr\\left( Y = 1\\, | \\,x_1,\\cdots,x_p \\right) \\geq 0.5\n\\]\n\n\nMisclassification rate\nWhile the mean-squared prediction error is the standard measure of model performance in regression models, the performance of a classification model is measured by statistics that contrast the number of correct and incorrect classifications. The most important of these metrics is the misclassification rate (MCR).\n\n\nDefinition: Misclassification Rate\n\n\nThe misclassification rate (MCR) of a classifier is the proportion of observations that are predicted to fall into the wrong category. If \\(y_{i}\\) is the observed category of the \\(i\\)th data point, and \\({\\widehat{y}}_{i}\\) is the predicted category, the MCR for a sample of \\(n\\) observations is\n\\[\n\\text{MCR} = \\frac{1}{n}\\sum_{i = 1}^{n}{I\\left( y_{i} \\neq {\\widehat{y}}_{i} \\right)}\n\\]\n\\(I(x)\\) is the indicator function,\n\\[\nI(x) = \\left\\{ \\begin{matrix} 1 & \\text{if }x\\text{ is true} \\\\ 0 & \\text{otherwise} \\end{matrix} \\right.\n\\]\n\n\nThe misclassification rate is simply the proportion of observations we predicted incorrectly. Since the target is categorical we do not use differences \\(y_i - \\widehat{y}_i\\) to measure the closeness of target and prediction. Instead, we simply check whether the observed and predicted categories agree (\\(y_i = \\widehat{y}_i\\)). The term \\(\\sum_{i = 1}^{n}{I\\left( y_{i} \\neq {\\widehat{y}}_{i} \\right)}\\) counts the number of incorrect predictions. The complement of MCR, the proportion predicted correctly, is called the accuracy of the classification model.\n\n\nConfusion matrix\nThe confusion matrix in a classification model with \\(k=2\\) categories (a binary target) is a 2 x 2 matrix that contrasts the four possibilities of observed and predicted outcomes.\n\n\nDefinition: Confusion Matrix\n\n\nThe confusion matrix in a classification problem is the cross-classification between the observed and the predicted categories. In a problem with two categories, the confusion matrix is a 2 x 2 matrix.\nThe cells of the matrix contain the number of data points that fall into the cross-classification when the model’s decision rule is applied to \\(n\\) observations. If one of the categories is labeled positive and the other is labeled negative, the cells give the number of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions.\n\n\nThe following table shows the layout of a typical confusion matrix for a 2-category classification problem. A false positive prediction, for example, is to predict a positive (“Yes”) result when the true state (the observed state) was negative. A false negative result is when the decision rule assigns a “No” to an observation with positive state.\n\n\n\nTable 18.2: Confusion matrix for a classification problem with two states.\n\n\n\n\n\n\nObserved Category\n\n\n\n\n\nPredicted Category\nYes (Positive)\nNo (Negative)\n\n\nYes (Positive)\nTrue Positive (TP)\nFalse Positive (FP)\n\n\nNo (Negative)\nFalse Negative (FN)\nTrue Negative (TN)\n\n\n\n\n\n\nAccuracy of the model is calculated as the proportion of observations that fall on the diagonal of the confusion matrix. The misclassification rate is the complement: the proportion of off-diagonal cells.\nBased on the four cells in the confusion matrix many statistics can be calculated (Table 18.3).\n\n\n\nTable 18.3: Statistics calculated from a 2 x 2 confusion matrix.\n\n\n\n\n\n\n\n\n\n\nStatistic\nCalculation\nNotes\n\n\n\n\nFalse Positive Rate (FPR)\nFP / (FP + TN)\nThe rate of the true negative cases that were predicted to be positive\n\n\nFalse Negative Rate (FNR)\nFN / (TP + FN)\nThe rate of the true positive cases that were predicted to be negative\n\n\nSensitivity\nTP / (TP + FN) = 1 – FNR\nThis is the true positive rate; also called Recall\n\n\nSpecificity\nTN / (FP + TN) = 1— FPR\nThis is the true negative rate\n\n\n\n\n\n\n\nAccuracy\n(TP + TN) / (TP + TN + FP + FN)\nOverall proportion of correct classifications\n\n\nMisclassification rate\n(FP + FN) / (TP + TN + FP + FN)\nOverall proportion of incorrect classifications, 1 – Accuracy\n\n\nPrecision\nTP / (TP + FP)\nRatio of true positives to anything predicted as positive\n\n\nDetection Rate\nTP / (TP + TN + FP + FN)\n\n\n\nNo Information Rate\n\\(\\frac{\\max(TP+FN,FP+TN)}{TP+TN+FP+FN}\\)\nThe proportion of observations in the larger observed class\n\n\nF Score\n\\(\\frac{2\\text{TP}}{2\\text{TP} + \\text{FP} + \\text{FN}}\\)\nHarmonic mean of precision and recall\n\n\n\n\n\n\nThe model accuracy is measured by the ratio of observations that were correctly classified, the sum of the diagonal cells divided by the total number of observations. The misclassification rate is the complement of the accuracy, the sum of the off-diagonal cells divided by the total number of observations.\nThe sensitivity (recall) is the ratio of true positives to what should have been predicted as positive. The specificity is the ratio of true negatives to what should have been predicted as negative. These are not complements of each other; they are calculated with different denominators.\nThe problem with focusing solely on accuracy to measure the quality of a classification model is that the two possible errors, false positives and false negatives might not be of equal consequence. For example, it might not matter how accurate a model is unless it achieves a certain sensitivity—the ability to correctly identify positives.\nConsider the data in the Table 18.4, representing 1,000 observations and predictions.\n\n\n\nTable 18.4: Example of a confusion matrix for 1,000 observations.\n\n\n\n\n\n\nObserved Category\n\n\n\n\n\nPredicted Category\nYes (Positive)\nNo (Negative)\n\n\nYes (Positive)\n9\n7\n\n\nNo (Negative)\n26\n958\n\n\n\n\n\n\nThe classification has an accuracy of 96.7%, which seems impressive. Its false positive and false negative rates are very different, however: FPR = 0.0073, FNR = 0.7429. The model is much less likely to predict a “Yes” when the true state is “No”, than it is to predict a “No” when the true state is “Yes”. Whether we can accept a model with such low sensitivity (100 – 74.29) = 25.71% is questionable, despite the high accuracy. An evaluation of this model should consider whether the two errors, false positive and false negative predictions, are of equal importance and consequence.\nIt is also noteworthy that the accuracy of 96.7% is not as impressive if you check the no-information rate of the data. The proportion of observations in the larger observed class is (958 + 7)/1,000 = 0.965. The accuracy of the decision rule is only slightly larger. In other words, if you were to take a naïve approach and predict all observations as “No” without looking at the data, that naïve decision rule would have an accuracy of 96.5%. The model did not buy ads much over the naïve decision rule.\nThe precision of the binary classifier is the number of true positive predictions divided by all positive predictions.\nThe F-score (also called the \\(F_1\\) score) is the harmonic mean of precision and recall: \\[\nF_1 = \\frac{2}{\\text{Precision}^{-1} + \\text{Recall}^{-1}} =\n\\frac{2\\text{TP}}{2\\text{TP} + \\text{FP} + \\text{FN}}\n\\] The \\(F\\) score is popular in natural language applications but is not without problems. The goal of the score is to balance both precision and recall, but the relative importance of the two depends on the problem. The \\(F\\) score does not take into account true negatives and is problematic if positive and negative outcomes are unbalanced.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>General Concepts</span>"
    ]
  },
  {
    "objectID": "models/model_concepts.html#prediction-and-explanation",
    "href": "models/model_concepts.html#prediction-and-explanation",
    "title": "18  General Concepts",
    "section": "18.4 Prediction and Explanation",
    "text": "18.4 Prediction and Explanation\nThe goal in developing models is to perform inference, to reach conclusions and make decisions based on data. Broadly, the goals fall into two categories:\n\nPredictive inference: concerned with developing an algorithm that predicts the target variable well and generalizes to observations not used in training the model.\nExplanatory inference: also called confirmatory inference, it is concerned with understanding the relationship between target and input variables, understanding the relevance of the inputs, and testing hypotheses about the target variable.\n\nIn machine learning, the term inference is used to describe the process of predicting new observations after training a model. Statisticians call this part of data analytics scoring the model. The predicted value is the “score” associated with the new observation. Our view of inference is broader than just predicting (scoring) observations. It includes any application of the trained model to derive information of interest: hypothesis testing, confidence and prediction intervals, predicted values, forecasts, etc.\nData projects are not necessarily either predictive or confirmatory. Many projects have elements of both, as in the following example.\n\n\nExample: Dose-response Study of Insect Mortality\n\n\nFigure 18.7 shows logits of sample proportions in a dose-response study of insect larvae mortality as a function of the concentration of an insecticide. Suppose \\(y_{i}\\) denotes the number of larvae out of \\(n_{i}\\) that succumb to the insecticide at concentration \\(x_{i}\\). The right panel of the figure shows the logit of the sample proportion \\(p_{i} = \\frac{y_{i}}{n_{i}}\\),\n\\[\n\\log\\left\\{ \\frac{p_{i}}{1 - p_{i}} \\right\\}\n\\]\nas a function of the log insecticide concentration. A simple linear model seems appropriate,\n\\[\n\\log\\left\\{ \\frac{\\text{E}\\lbrack p_{i}\\rbrack}{1 - \\text{T}\\lbrack p_{i}\\rbrack} \\right\\} = \\beta_0 + \\beta_1\\log_{10}x_i\n\\]\nNote that the expected value \\(\\text{E}\\left\\lbrack p_{i} \\right\\rbrack\\) is a probability. This model is a generalization of logistic regression for binary data (a 0/1 response) to binomial sample proportions. \\(\\text{E}\\left\\lbrack p_{i} \\right\\rbrack = \\pi_{i}\\) is the probability that an insect dies when \\(\\log_{10}x_{i}\\) amount of insecticide is applied.\n\n\n\n\n\n\nFigure 18.7: Logits of larvae mortality sample proportion against insecticide concentration and log concentration.\n\n\n\nThe investigators want to understand the relationship between larvae mortality and insecticide concentration. The parameter estimate for \\(\\beta_{1}\\) is of interest, it describes the change in logits that corresponds to a unit-level change in the log concentration. A hypothesis test for \\(\\beta_{1}\\) might compare the dose-response in this study with the known dose-response slope \\(c\\) of a standard insecticide. The null hypothesis of this test specifies that the insecticide is as effective as the standard:\n\\[\nH_{0}:\\beta_{1} = c\n\\]\nAnother value of interest in dose-response studies is the concentration that achieves a specified effect. For example, the lethal dosage \\(LD_{50}\\) is the concentration that kills 50% of the subjects. Determining the \\(LD_{50}\\) value is known as an inverse prediction problem: rather than predicting \\(\\text{E}\\lbrack Y\\rbrack\\) for a given value of \\(X\\), we are interested in finding the value \\(X\\) that corresponds to a given a value of \\(\\text{E}\\lbrack Y\\rbrack\\).\n\n\n\n\n\n\nFigure 18.8: Fitted dose-response curve and inverse prediction of $LD_{50}$. The $LD_{50}$ is calculated from the value where the vertical line intersects the horizontal axis.\n\n\n\nThe \\(LD_{50}\\) value can be calculated from the model equation. More generally, we can find any value on the x-axis that corresponds to a particular mortality rate \\(\\alpha\\) by solving the following equation for \\(\\alpha\\):\n\\[\n\\text{logit}(\\alpha) = \\log\\left\\{ \\frac{\\alpha}{1 - \\alpha} \\right\\} = \\beta_0 + \\beta_1\\log_{10}x_\\alpha\n\\]\nThe solution is\n\\[\nx_{\\alpha} = 10^{\\frac{\\left( \\text{logit}(\\alpha) - \\beta_{0} \\right)}{\\beta_{1}}}\n\\]\nFor the special value \\(\\alpha = 0.5\\), the \\(LD_{50}\\) results,\n\\[\nLD_{50} = 10^{\\frac{- \\beta_{0}}{\\beta_{1}}}\n\\]\nIn addition to hypothesis testing about \\(\\beta_{1}\\) and calculating the \\(LD_{50}\\), the investigators are also interested in predicting the mortality rate at concentrations not used in the study.\nThe inference in the study has explanatory (confirmatory) and predictive elements.\n\n\nMany studies are not completely confirmatory or predictive, because models that are good at confirmatory inference are not necessarily good at predicting. Similarly, models that predict well are not necessarily good at testing hypotheses. Interpretability of the model parameters is important for confirmatory inference because hypotheses about the real world are cast as statements about the model parameters. Many disciplines place a premium on interpretability, e.g., biology, life sciences, economics, physical sciences, geosciences, natural resources, financial services. Experiments designed to answer specific questions rely on analytic methods designed for confirmatory inference.\nInterpretability of the model parameters might not be important for a predictive model. A biased estimator that reduces variability and leads to a lower mean squared prediction error (see the next section) can be appealing in a predictive model but can be unacceptable in a project where confirmatory inference is the primary focus.\n\n\n\nFigure 18.4: Important types of statistical learning\nFigure 18.6: An example of regression to the mean. An extreme observation is more likely to be followed by a less extreme observation, one that falls near the center of the distribution.\n\n\n\nSilver, Nate. 2012. The Signal and the Noise: Why so Many Predictions Fail–but Some Don’t. Penguin Books.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>General Concepts</span>"
    ]
  },
  {
    "objectID": "models/correlation.html",
    "href": "models/correlation.html",
    "title": "19  Correlation and Causation",
    "section": "",
    "text": "19.1 Introduction\nOne of the key tasks in analyzing data is uncovering relationships between things. Describing the statistical behavior of a single variable is interesting. Studying how two or more variables behave together is really interesting. That is how we uncover mechanisms, relationships, associations, and patterns. And if things go really well, we might discover cause-and-effect relationships.\nYou might have heard the saying\nWhat do we mean by that?\nCausation implies that one thing is the result of another thing; they stand in a cause-and-effect relationship to each other. The gravitational pull of the moon on earth’s oceans causes the tides. An accident causes a traffic jam. Smoking causes an increase in the risk of developing lung cancer.\nCorrelation, on the other hand, is about establishing association between attributes. The weight of a person is correlated with their height. Taller people tend to be heavier but height alone is not the only factor affecting someone’s weight. Smoking is correlated with alcoholism but does not cause it.\nFigure 19.1 displays the relationship between highway accidents in the U.S. and lemon imports from Mexico for a period of five years, from 1996–2001. The U.S. Dept. of Agriculture tracks agricultural imports and exports, the U.S. National Highway Traffic Safety Administration (NHTSA) tracks highway fatalities. Neither federal agency probably thought much about the data collected by the other agency. But when put together, voilà. A clear trend emerges!\nIf the relationship in Figure 19.1 is causal, public policy to reduce highway fatalities is very clear: reduce fresh lemon imports from Mexico! Clearly, this is not a causal relationship. There must be another explanation why the variables in Figure 19.1 appear related.\nCompare this situation to John Snow’s investigation of the relationship between water quality and cholera incidences in 19th century London (Section 2.8). Snow found higher cholera incidences in houses closer to the Broad Street public water pump. There was a strong association between cholera cases and proximity to the pump. But did the pump—or more precisely the water from the pump or something in the water—cause cholera? Today we know that cholera is caused by the bacterium Vibrio cholerae, but that discovery was not made until 1883.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Correlation and Causation</span>"
    ]
  },
  {
    "objectID": "models/correlation.html#introduction",
    "href": "models/correlation.html#introduction",
    "title": "19  Correlation and Causation",
    "section": "",
    "text": "correlation does not imply causation\n\n\n\n\n\n\n\n\n\n\n\nFigure 19.1: Relationship between highway fatalities and lemon imports from Mexico.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Correlation and Causation</span>"
    ]
  },
  {
    "objectID": "models/correlation.html#correlation-and-the-correlation-coefficient",
    "href": "models/correlation.html#correlation-and-the-correlation-coefficient",
    "title": "19  Correlation and Causation",
    "section": "19.2 Correlation and the Correlation Coefficient",
    "text": "19.2 Correlation and the Correlation Coefficient\nWe experience correlation when one attribute changes with another. When the attributes are continuous, we can display their association with a scatterplot. A positive correlation then implies that the point cloud has a positive slope, as one attribute increases the other one tends to increase as well (Figure 19.2). When the correlation is negative, an increase in one attribute is associated with a decrease in the other attribute (Figure 19.3).\n\n\n\n\n\n\nFigure 19.2: Positive correlation.\n\n\n\n\n\n\n\n\n\nFigure 19.3: Negative correlation.\n\n\n\nWhile the direction of the point cloud indicates whether the correlation (association) is positive or negative, the tightness of the point cloud indicates the strength of the association (Figure 19.4).\n\n\n\n\n\n\nFigure 19.4: Correlations of different strength and directions. The numbers above the point clouds indicate the strength and direction of the correlation\n\n\n\nWhen we are dealing with discrete attributes, the association cannot be revealed through a point cloud. Instead, we cross-tabulate the frequency of occurrence of the attributes.\n\n\nExample: Rater Agreement\n\n\nTable 19.1 shows the results of a study where insect damage on 236 agricultural fields was classified into 5 damage categories by two different inspectors.\n\n\n\nTable 19.1: Results of rating 236 experimental units by 2 raters\n\n\n\n\n\nRater 2\n1\n2\n3\n4\n5\nTotal\n\n\n\n\n1\n10\n6\n4\n2\n2\n24\n\n\n2\n12\n20\n16\n7\n2\n57\n\n\n3\n1\n12\n30\n20\n6\n69\n\n\n4\n4\n5\n10\n25\n12\n56\n\n\n5\n1\n3\n3\n8\n15\n30\n\n\nTotal\n28\n46\n63\n62\n37\n236\n\n\n\n\n\n\nFor example, 16 experimental units were assigned to damage category 3 by rater 1 and to damage category 2 by rater 2. There is relatively strong association between the ratings, the majority of the counts fall on the diagonal of the table and in the cells immediately off the diagonal (where the raters disagree by one damage category).\n\n\nAnother example of cross-tabulating counts to demonstrate association is Table 2.3 in John Snow’s cholera study. It shows that cholera deaths are 10 times more likely to occur in homes supplied by the Southward & Vauxhall water company than in homes supplied by the Lambeth water company.\n\nThe Correlation Coefficient\nThe correlation between variables \\(X\\) and \\(Y\\) is a feature of the joint probability distribution \\(f(x,y)\\). It is a function of the expected values of \\(X\\), \\(Y\\), and \\(XY\\).\n\n\nDefinition: Correlation\n\n\nThe correlation between random variables \\(X\\) and \\(Y\\), denoted \\(\\rho_{xy}\\) or \\(\\text{Corr}(X,Y)\\), is the ratio of their covariance, \\(\\text{Cov}(X,Y)\\), and the product of their standard deviations:\n\\[\n\\text{Cov}(X,Y) = \\text{E}\\lbrack\\left( X - \\text{E}\\lbrack X\\rbrack \\right)\\left( Y - \\text{E}\\lbrack Y\\rbrack \\right) = \\text{E}\\lbrack XY\\rbrack - \\text{E}\\lbrack X\\rbrack\\text{E}\\lbrack Y\\rbrack\n\\]\n\\[\n\\rho_{xy} = \\text{Corr}(X,Y) = \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}\\lbrack X\\rbrack\\text{Var}\\lbrack Y\\rbrack}}\n\\]\nWhen the correlation between \\(X\\) and \\(Y\\) is non-zero, we say that \\(X\\) and \\(Y\\) are correlated or are associated with each other. The covariance measures how \\(X\\) and \\(Y\\) vary jointly: as \\(X\\) deviates from its mean, how does \\(Y\\) change relative to its mean? The correlation is positive when above-average values of \\(X\\) are associated with above-average values of \\(Y\\). Dividing by the product of the standard deviations scales the correlation so that \\(-1 \\leq \\rho_{xy} \\leq 1\\).\n\n\nThe correlation—like the covariance—is a function of expected values, it describes long-run behavior of the joint distribution of \\(X\\) and \\(Y\\). The correlation is not directly knowable and is estimated from pairs of observations \\((x_1, y_1),\\cdots,\\ (x_n, y_n)\\). The most common estimator when \\(X\\) and \\(Y\\) are continuous random variables is the Pearson product-moment correlation coefficient.\n\n\nDefinition: Pearson Product-moment Correlation Coefficient\n\n\nThe Pearson product-moment estimate of the correlation \\(\\text{Corr}(X,Y),\\) based on a sample \\((x_1, y_1),\\cdots,(x_n,y_n),\\) is given by\n\\[\n{\\widehat{\\rho}}_{xy} = \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}\n\\]\n\\[\n\\begin{align*}\nS_{xy} &= \\sum_{i = 1}^{n}{\\left( x_{i} - \\overline{x} \\right)\\left( y_{i} - \\overline{y} \\right) = \\sum_{i = 1}^{n}{x_{i}y_{i}} - n\\overline{x}\\overline{y}}\\\\\nS_{xx} &= \\sum_{i = 1}^{n}{\\left( x_{i} - \\overline{x} \\right)^{2} = \\sum_{i = 1}^{n}x_{i}^{2} - n{\\overline{x}}^{2}}\\\\\nS_{yy} &= \\sum_{i = 1}^{n}{\\left( y_{i} - \\overline{y} \\right)^{2} = \\sum_{i = 1}^{n}y_{i}^{2} - n{\\overline{y}}^{2}}\\\\\n\\end{align*}\n\\]\n\\(S_{xx}\\) and \\(S_{yy}\\) are called the observed sum of squares of \\(X\\) and \\(Y\\), respectively. \\(S_{xy}\\) is the observed sum of cross-products between the variables.\n\n\nLike the correlation \\(\\text{Corr}(x,y)\\) the correlation coefficient ranges from \\(-1\\) to \\(1\\). Both of these extremes are called perfect correlations and occur when all points fall on a perfect line, without any variability about the line. The relationship is deterministic.\nLinear mathematical relationships exhibit such patterns, for example, the relationship between degree Celsius and degree Fahrenheit (Figure 19.5): \\[\n^\\circ F = {^\\circ C} \\times \\frac{9}{5}  + 32\n\\]\n\n\n\n\n\n\n\n\nFigure 19.5: Linear relationship between \\(^\\circ F\\) and \\(^\\circ C\\).\n\n\n\n\n\nThis leads us to another important point about correlation metrics: the relationship between attributes can be quite strong, but their degree of linear relationship can be low. Figure 19.6 shows two variables with a strong nonlinear relationship. \\(Y\\) decreases with increasing \\(C\\) for values \\(X &lt; 0\\) and \\(Y\\) increases with \\(X\\) for values \\(X &gt; 0\\). When the standard linear correlation coefficient is calculated, it turns out to indicate a very weak relationship between the variables—a very weak linear relationship.\nThe takeaway is not to only focus on reported measures of association but to also examine the relationships visually.\n\n\n\n\n\n\n\n\nFigure 19.6: Strong nonlinear relationship with small (linear) correlation coefficient.\n\n\n\n\n\n\n\nSpurious Correlation\nCorrelation itself is not a reliable concept either. Correlation can be the result of a direct relationship between the variables, or it can be induced by mediating or latent (confounding) variables. Correlations that are not the result of direct relationships are called spurious.\n\n\n\n\n\n\nFigure 19.7: Chocolate consumption and number of Nobel laureates.\n\n\n\nFigure 19.7 displays the number of Nobel laureates per 10 million population against the chocolate consumption (in kg per capita and year) for various countries (Messerli 2012). An upward trend is clearly noticeable. A greater per-capita chocolate consumption is associated with a lager number of Nobel laureates. Whoa! If the two variables stand in a cause-effect relationship then we have a simple recipe to increase Nobel prizes: we all should eat more chocolate. While one can argue the benefits of chocolate for cognitive function, what we have here is a simple correlation. The two attributes, number of Nobel laureates per 10 million population and chocolate consumption per capita, are related. If one increases so does the other. But why?\nThis is an example of a spurious correlation. The variables are not really dependent on each other, a relationship is induced in some other way. In this particular example, the correlation was “found” by cherry-picking the data: only four years of chocolate consumption were considered on a limited number of chocolate products and no data prior to 2002 was used. The number of Nobel laureates is a cumulative measure that spans a much longer time frame.\nIt appears that the data were organized in such a way as to suggest a relationship between the variables.\nYou can find spurious correlations everywhere, without manipulating the data. Here are some examples and the reasons why the data appear correlated.\n\nCoincidence\nForecasting economic conditions is difficult and highly valuable. Since the end of World War II there have been only eleven economic recessions. On the other hand we are producing thousands of economic indicators. The government alone generates 45,000 economic statistics each year (Silver 2012, 185).\nSo it should not be surprising that when sifting through all those variables we find some that appear to go up or down together, just by coincidence.\nA famous example is the Super Bowl winning conference as an indicator of economic performance. Between 1967 and 1997, in years when the team from the original NFL won, the stock market went up by 14%. When a team from the original AFL won the stock market decreased by almost 10%. Through 1997, the Super Bowl winner “predicted” correctly the direction of the stock market in 28 out of 31 years (Silver 2012). Since 1998 the trend has reversed and the stock market is doing better when an AFL team won the Super Bowl. Coincidence during a period of time leads to mistake noise for signal.\nAnother example of coincidence being mistaken for correlation is Paul, the Octopus who correctly predicted the winner in 2008–2010 international soccer matches 12 out of 14 times, an 85% accuracy. Since this success rate is unlikely to happen by chance, it was determined that Paul the octopus has divine powers. When Paul got it wrong in a 2010 FIFA World Cup game between Germany and Spain, the German fans called for Paul to be eaten and the Spanish Prime Minister offered Paul asylum.\n\n\nLatent variables\nA correlation between variables A and C can be induced by another variable, say B. If A is correlated with (or caused by) B and C is correlated with (or caused by) B, then plotting A versus C indicates a correlation between the two variables. However, the relationship is induced by the latent variable B.\nLatent variables are often the real reason why things appear related when we deal with variables that depend on population size or common factors such as the weather or time.\n\n\nExample: Donuts and High School Graduation\n\n\nThe number of high school graduates and donut consumption are positively correlated—with increasing donut consumption the number of high school graduates increases (Figure 19.8). This is a spurious correlation induced by the latent variable population size. Both variables increase over time with an increasing population. Even if the proportion of high school graduates and the donut consumption per person are unrelated, the total numbers will be higher in a larger population.\n\n\n\n\n\n\nFigure 19.8: Spurious relationship between donut consumption and high school graduation. Source.\n\n\n\nNotice that we are not plotting graduation rates, these would most likely not have any relationships with donut consumption. The latent variable at work here is the size of the population over time. As the population increases, more donuts are consumed and more people graduate from high school.\n\n\nA similar spurious correlation is that between ice cream sales and forest fires. Both increase during the summer heat and decrease in the winter.\nYou can imagine the horrible public policy decisions one would make by mistaken those spurious correlations for cause and effect relationships.\n\n\nInduced correlation\nCorrelations can also be induced by confounding variables that distort relationships or by mathematical manipulations that induce dependencies.\n\n\nExample: Confounded Customer Churn\n\n\nA common request to the data science team is to use data analytics to improve customer retention. That starts with understanding why customers leave the company (churn).\nAn analysis of historical customer data reveals a positive correlation between churn rate and discounts offered. This association is induced by a confounding factor: customer satisfaction. Customers who are dissatisfied are more likely to complain to customer service, which results in discount offers to entice the complaining customer to stay with the vendor. The discount offer cannot overcome the customer’s dissatisfaction and they churn the company. Without understanding the confounding factor—customer satisfaction—an analysis of the raw data could suggest that higher discount rates lead to higher customer attrition.\n\n\nAnother interesting mechanism to induce correlation is by introducing a mathematical dependence between two attributes. A famous example is the relationship between birth rate and the density of storks.\n\n\nExample: Storks and Babies\n\n\n\n\n\n\n\n\nFigure 19.9: Storks and Babies\n\n\n\nIn central Europe a persistent myth is that storks bring babies. The origin of the association probably goes back to medieval days when conception was more common in mid-summer during the celebration of the summer solstice which is also a pagan holiday of marriage and fertility. The white stork is a migratory bird that flies to Africa in the fall and returns to Europe nine months later. The return of the storks coincided with the arrival of newborns; the connection was made that storks brought the babies.\nAlthough the myth has been debunked, there have been several studies of the connection between fertility and the stork abundance. Neyman (1952) describes a study of 54 counties that comprises the following attributes:\n\\(W\\): Number of women of child-bearing age in the county (in 10,000)\n\\(S\\): Number of storks in the county\n\\(B\\): Number of babies born in the county\nSince it is likely that these numbers increase with the size of the county, the variables analyzed were \\(Y = B/W\\) and \\(X = S/W\\), the birth rate per 10,000 women and the density of storks per 10,000 women. Figure 4.6 reproduces the scatterplot of \\(Y\\) vs \\(X\\).\nThere seems to be a clear trend, the birth rate increases with the stork density. Is that evidence the myth is correct after all?\nThe reason for the apparent trend is the use of \\(W\\) in the denominator of both \\(X\\) and \\(Y\\). Even if \\(S\\) and \\(B\\) are unrelated, the ratio with a common variable induces a correlation between \\(X\\) and \\(Y\\).\n\n\nThere are many examples of spurious correlations—from hilarious to frightening—and these are often trotted out to try and explain the difference between correlation and causation. The debate between correlation and causation is not because correlations can be spurious. The rooster crows when the sun rises, this is not a spurious relationship. But the rooster does not cause the sun to rise. Correlation of any kind, true dependence or spurious, does not imply causation—but then what does?\nSmoking causes lung cancer but smoking does not cause alcoholism. However, there is an association (correlation) between smoking and alcoholics. How did we establish causation in one instance and correlation in the other? The battle for the statement “smoking causes lung cancer” raged for many years and it was not straightforward to settle the question based on the established methodology for establishing causation.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Correlation and Causation</span>"
    ]
  },
  {
    "objectID": "models/correlation.html#establishing-causality",
    "href": "models/correlation.html#establishing-causality",
    "title": "19  Correlation and Causation",
    "section": "19.3 Establishing Causality",
    "text": "19.3 Establishing Causality\nEstablishing a causal link between factors is the holy grail of scientific study. When we prove that one event is the result of another event, we have established new, irrefutable knowledge, beyond a reasonable doubt.\nEstablishing cause and effect is also quite difficult. Spiegelhalter (2021, 97) calls causation a “deeply contested subject”. You get a splinter in your finger and it hurts. Did the splinter cause the pain? Is it possible that the finger would have abruptly started to hurt if it wasn’t for the splinter? If it was me, it would be pretty obvious to me that the splinter caused the pain.\nBut wait. Not everyone has the same level of pain tolerance. The splinter that causes me much agony might be a mere scratch, hardly noticeable, for someone else. How do we take this variability in the population into account in making statements about causality?\nWhen we say that smoking causes lung cancer, we do not claim that every smoker will get the disease. Some smokers do not get lung cancer and there are lung cancer patients who never smoked. The second leading cause of lung cancer deaths in the U.S. is exposure to Radon gas. That is why in many regions Radon inspections are required prior to purchasing homes. A more precise statement would be that smoking causes an increase in the likelihood of getting lung cancer. It is not a statement about what happens to Joe or Diana. The statistical notion of causation between \\(X\\) and \\(Y\\) means that if \\(X\\) occurs, \\(Y\\) tends to occur more or less often. The statistical notion of causation is not deterministic (Spiegelhalter 2021, 99).\nIn a study about the association between repeated head impacts (RHI) and chronic traumatic encephalopathy (CTE), Nowinski et al. (2022) state that causation is an interpretation, not an entity. In studies involving complex environmental exposures causation is a continuum from highly unlikely to highly likely, and no single study can prove causation.\nIn the presence of uncertainty some scientific standard needs to be met in order for us to claim that something has been proven and even then, we are not making statements that the something will happen every time, only that the proportion of times that it will happen has been affected.\n\nConfounding\nLet’s return to the 1854 cholera outbreak and the question before John Snow: did something in the water of the Broad Street public water pump cause cholera? If so, this would explain the higher incidence rate of cholera in residences near the pump and it would also explain the other anomalies he found in the data (see Section 2.8).\nThe map Snow drew in 1854 (Figure 2.3) might be convincing to us, his contemporaries did not feel that way. For one, he could not prove that the Broad Street well water caused the cholera cases. And his hypothesis was inconsistent with the prevailing theory of the time, that cholera was caused by airborne particles (miasma) from dirty or decaying biological material.\nThe analysis of the cholera map established a correlation rather than causation because of the possibility of confounding factors: variables that can mask or distort the effect of other variables. In the 1960s it was shown that coffee drinkers had higher rates of lung cancer than non-coffee drinkers. Some thought this was implication of coffee as a cause of lung cancer. That is incorrect. The association is due to a confounding factor: coffee drinkers at the time were more likely to be also smokers. Coffee drinking was associated with lung cancer but does not cause the disease.\nA confounding factor is related to both the cause and the effect and can mislead us into attributing too much or too little importance to the potential cause. Variables such as age, time, temperature, population size are often confounding factors because they act on the factor of interest and on the outcome of interest. For example, age is a confounding factor in studies of exposure to harmful agents. If damage from the agent is more prevalent in older people, age can be a confounding factor because older people have been exposed longer.\nWhen spurious correlations are induced by latent variables, the latent variable is a confounder. The apparent correlation between ice cream sales and shark attacks is explained by the confounder temperature. Ice cream sales and shark attacks increase with temperature as more people buy ice cream and more people go to the beach.\n\n\nAssignment: 1854 Cholera Outbreak\n\n\nIn the case of the 1854 cholera outbreak, there could have been confounding factors that caused cholera incidences in the Broad Street area to be higher, whether the water was or was not the cause of the disease. Maybe the residents of that poorer neighborhood had a different diet that caused the disease. Maybe they had occupations that made it more likely to be exposed to a harmful agent. Maybe. Maybe. Maybe.\nWhat other confounding factors can you think of that would mask, amplify, or suppress the incidence of cholera?\n\n\nIn order to establish a causal link between two variables, the confounding factors must be accounted for—at least beyond a reasonable doubt. Otherwise there will always be some reason to believe another mechanism was at work. There are a few principal mechanisms to deal with confounding variables:\n\nAdjustment\nStratification\nRandomization\n\nWe will discuss these in turn, but note that they are not mutually exclusive. A study might involve experimentation with randomly assigned treatments as well as model adjustments for confounding variables.\n\n\nAdjustment\nAdjusting for confounding variables means to include the variables in models that describe the relationship between the factor of interest and the outcome of interest. Consider the Radon exposure example above. A model to describe the relationship between exposure \\(x\\) (in picocuries per liter air) and cancer risk could be written in two parts: \\[\n\\begin{align}\n\\eta &= \\beta_0 + \\beta_1 x + \\beta_2 \\text{ age} \\\\\n\\Pr(\\text{Develops cancer}) &= \\frac{1}{1+\\exp\\{-\\eta\\}} \\\\\n\\end{align}\n\\tag{19.1}\\]\nThe first term of the model, \\(\\eta\\), is called the linear predictor and is a function of the variables that materially determine the cancer risk. In addition to the exposure \\(x\\), the linear predictor also contains a term for a person’s age. The second expression in Equation 19.1 transforms the linear predictor into a probability—it is called the logistic transformation.\nThis model allows us to estimate how much of the cancer risk is due to the level of radon exposure and how much is due to the age of the person. With the variable of interest (\\(x\\)) and the confounding variable (age) disentangled we can make statements about the risk of cancer as a function of radon exposure and age.\nAn important aspect of adjusting for confounding variables is the functional relationship between the variables. In Equation 19.1 the two variables enter the linear predictor in an additive fashion. Maybe this is not the appropriate adjustment. If the effect of age on cancer risk changes with the level of radon exposure then we say that the two variables interact. A model that includes a multiplicative interaction tterm then might be more appropriate: \\[\n\\eta = \\beta_0 + \\beta_1 x + \\beta_2 \\text{ age} + \\beta_3 x\\,\\text{age}\\\\\n\\]\nIn other words, determining how to model the relationship of confounding variables on other variables and on the outcome of interest, is of great importance.\n\n\nStratification\nStratification is an approach to deal with confounding factors that are qualitative. It means to examine relationships separately for each level of the confounder. To see if there is a general relationship between ice cream sales and shark attacks we can examine the association for different temperature ranges. As a surrogate for that we can look at the association by month or by season.\nWhen performing analyses overall and comparing them to analyses within groups (within strata) we can run into situations where the two seem to provide contradictory results. This is known as Simpson’s paradox.\n\nSimpson’s Paradox\nThe paradox is named after Edward Simpson who described it in a technical paper in 1951, but the phenomenon has been known much longer. It is also not a paradox as it does not lead to nonsensical outcomes or a contradiction. What we know as Simpson’s Paradox is simply the result of looking at an aspect from two viewpoints: The trend that we see in combined data can reverse when we look at the data in groups.\nFigure 19.10 displays a scatter plot of two variables and a linear regression trend. The slope is positive, the average value of \\(Y\\) increases with the value of \\(X\\).\nThe data in Figure 19.10 consists of three groups and when the regression analysis is performed separately for each group, a different picture emerges. Within each group the regression relationship indicates a negative slope, the opposite of the trend in the ungrouped data (Figure 19.11).\n\n\n\n\n\n\n\n\nFigure 19.10: Combined data and regression trend.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 19.11: Grouped data and group-specific regression trends.\n\n\n\n\n\nThe apparent contradiction comes about because the three groups have different centers and orientation. When “stacked” they create a single point cloud with a positive slope. As mentioned previously, this is not really a paradox, both ways of looking at the data are sensible and the results are meaningful either way. The “paradox” lies in the fact that the two views lead to seemingly different conclusions about the relationship between the variables.\nExamples of Simpson’s Paradox with qualitative data can be found in college admissions data. Wikipedia shows the apparent gender bias effect for 1973 data from UC Berkeley. Spiegelhalter (2021) shows data for Cambridge from 1996. The table in Spiegelhalter (2021, 111) is reproduced below as two separate tables.\n\n\n\nTable 19.2: Application and acceptance rates at Cambridge in 1996 in STEM disciplines for men and women.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWomen\n\n\nMen\n\n\n\n\n\n\n\nApplied\nAccepted\n%\nApplied\nAccepted\n%\n\n\nTotal\n1,184\n274\n23%\n2,740\n584\n24%\n\n\n\n\n\n\nTable 19.2 shows the overall acceptance rates for men and women, with the former slightly higher by one percent.\n\n\n\nTable 19.3: Application and acceptance rates at Cambridge in 1996 by STEM discipline.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWomen\n\n\nMen\n\n\n\n\n\n\n\nApplied\nAccepted\n%\nApplied\nAccepted\n%\n\n\nComputer Science\n26\n7\n27%\n228\n58\n25%\n\n\nEconomics\n240\n63\n26%\n512\n112\n22%\n\n\nEngineering\n164\n52\n32%\n972\n252\n26%\n\n\nMedicine\n416\n99\n24%\n578\n140\n24%\n\n\nVeterinary Medicine\n338\n53\n16%\n180\n22\n12%\n\n\n\n\n\n\nTable 19.3 shows the applications and acceptance rates by discipline. In each discipline the acceptance rate for women is at least as high as that for men, in fact it is higher than that for men, except for Medicine.\nHow do we explain this apparent contradiction? In each discipline the acceptance rate for women is as high or higher than that for men, but overall the acceptance rat for women is lower. The explanation is that women were more likely to apply for subjects that have high application numbers and are more difficult to get in. 64% of the applications from women went to Medicine and Veterinary Medicine (416 + 338 out of 1,184). Only 30% of the male applications went to those subjects. Men applied disproportionately more to Engineering, which has a higher acceptance rate than the other STEM disciplines. It seems that there is no gender bias in these admission numbers, the lower overall admission rate for women is the result in women applying in larger numbers to STEM disciplines that are difficult to get into.\n\n\n\nRandomization\nRandomization is a simple and effective mechanism to create probabilistic equivalence in data and takes two forms: random selection and random assignment. Random does not imply haphazard or arbitrary, the way in which selection or assignment is randomized obeys probability distributions. Because we specify the distribution according to which selection or assignment occurs, we understand the probabilistic behavior of the data we collect.\nFor example, we select a random sample of people from a population such that each person has the same chance of entering the sample. This guarantees that the attributes of the population are represented properly in the sample. The proportion of persons of different age, gender, race, etc. in the sample will be representative of the population from which the sample is drawn, although the exact proportions will not be identical in a particular sample. If we were to draw a sample of \\(n\\) people by a non-random mechanism, for example, by taking the first \\(n\\) folks who drive through an intersection, or \\(n\\) airline passengers on a given day, or \\(n\\) residents of a particular neighborhood, or the first \\(n\\) entries in the list of the U.S. Census Bureau, our sample would not be representative of the population we are interested in studying. The sample would suffer from selection bias, conclusions drawn from analyzing the data in the sample would not apply to the population as a whole. Note that random sampling from a non-representative list also suffers from selection bias. Randomly sampling social media posts does not give insight into the opinions of the general population because social media users are not representative of the general population. Asking questions of a random sample of drivers passing through an intersection does not properly represent those who do not drive or live elsewhere.\nThe random mechanism in random selection has a balancing property. It ensures that sub-groups are not over represented or under represented in the sample, provided that the sample is sufficiently large. This balancing property is also key for the second form of randomization: random assignment when conditions are manipulated on purpose. This leads us to experimentation.\n\n\nExperimentation–The Randomized Controlled Trial (RCT)\n\nExample: RCBD in Agriculture\nSuppose you wish to study how poppies grow on an agricultural field. The poppies are subject to varying growing conditions due to differences in the soil characteristics, topography, weather, plant-to-plant variations, and so on. We are particularly interested to see how six different fertilizer applications affect the poppy growth. The conditions that affect poppy growth can be grouped into three categories:\n\nConditions we manipulate\nConditions we do not manipulate but know about\nConditions we do not know about (lurking factors)\n\nThe fertilizer treatments fall into the first category. We can apply parts of the field and apply fertilizer A to it, other parts receive fertilizer B, and so on. Suppose that the field is large and sloped and we suspect a gradient in soil nutrients and water. This falls into the second category. We do not create or manipulate the soil and water conditions, but we are aware of them. They are a confounding factor. All other potential influences of poppy growth fall into the third category.\nFigure 19.12 displays a popular layout for running such a fertilizer experiment in agricultural science. It is called a randomized complete block design (RCBD). The experimental area is divided up into separate blocks, in this case four of them.\n\n\n\n\n\n\nFigure 19.12: Experimental layout for poppy experiment.\n\n\n\nThe blocks are chosen so that the fields within a block—we call them experimental units—are homogeneous with respect to the conditions we know about. This is the technique to control confounding factors in category 2 by stratification and adjustment. Within each block the fertilizer treatments are assigned randomly to the six experimental units. This random assignment balances out the effects of the confounding factors we do not know about (category 3). If we were to assign treatments to experimental unit in a deterministic way—for example, treatment A always to the upper left unit, treatment B right next to it, etc.—it is possible that confounding factors associated with the position in the block mask or distort the effect of the treatment. We would then not really compare treatment A to treatment B, but a blend of treatment effect and location effect.\nFinally, because the world is uncertain, we do not apply the treatments only once. We use multiple blocks, each with a separate assignment of treatments to experimental units to replicate the per-block layout. This replication allows us to measure the inherent variability in poppy growth, unaffected by treatment or confounding factors.\nIf the experimental units in our agricultural science experiment contain more poppies than we can harvest and analyze in the lab, we would select some of them for lab analysis. This would be done by random selection to make sure that the plants analyzed in the lab are representative of the plants growing on the experimental unit.\nIn this RCBD with four blocks and six treatments we encounter all techniques to manage confounding factors: stratification, adjustment (because block effects will be included in the model during analysis), and randomization. As a result, we are allowed to make cause-and-effect statements about the treatment factor we manipulated, the fertilizer. If plants grown under the fertilizer A treatment are taller than those grown under fertilizer B, then there are only two possible explanations:\n\nFertilizer A causes poppies to grow taller compared to fertilizer B\nCoincidence: the height difference we are seeing is due to chance\n\nOther explanations can be ruled out because we controlled the experiment for other factors.\nIn order to separate the two remaining explanations, we analyze the size of the treatment differences relative to the inherent variability in poppy growth. That is the reason why the experiment uses multiple blocks rather than a single block. The replication of treatment assignments allows us to estimate that inherent variation. If poppy growth varies widely, then a small difference in height between plants from treatment A and treatment B will not surprise us. If inherent growth variation is small, observed differences between treatments point at the fertilizer as the cause.\n\n\nOther Experiments\nExperimentation with random assignment of conditions has a long history. It started in agricultural experiments and has since permeated many domains. Randomized clinical trials are common to test medical drugs, devices, and treatments. Industrial experimentation is used to find the best manufacturing conditions for products.\nRandomized trials are also used in the social sciences. Spiegelhalter (2021, 107) cites the Study of the Therapeutic Effects of Intercessory Prayer (STEP) by Benson H. et al. (2006) to answer the question whether being prayed for improves the recovery of patients after coronary artery bypass graft (CABG) surgery. The Methods section of the paper describes the randomized experiment:\n\nPatients at 6 US hospitals were randomly assigned to 1 of 3 groups: 604 received intercessory prayer after being informed that they may or may not receive prayer; 597 did not receive intercessory prayer also after being informed that they may or may not receive prayer; and 601 received intercessory prayer after being informed they would receive prayer. Intercessory prayer was provided for 14 days, starting the night before CABG. The primary outcome was presence of any complication within 30 days of CABG. Secondary outcomes were any major event and mortality.\n\nThe study concludes\n\nIntercessory prayer itself had no effect on complication-free recovery from CABG, but certainty of receiving intercessory prayer was associated with a higher incidence of complications.\n\nKnowing that they were being prayed for might have made patients more uncertain, wondering whether they are so sick that they had to call in the prayer team.\n\n\nA/B Testing\nExperimentation with random assignment is also used frequently in the technology industry, the technique is known as A/B testing. Only two treatments are being evaluated (A and B), one is typically a current product or design. Users of the product/design are randomly directed to the A or B option and the attribute of interest is measured (click-through rate, time on page, use of features on page, checkout, purchase amount, etc.). We are all participating in these ongoing A/B experiments when we operate online. Google is said to run about 10,000 A/B experiments every year.\n\n\n\nWhen Experimentation is Not Possible\nExperimentation with random assignment of treatments is the gold standard to establish cause and effect. But it is not always possible to go down that path.\nSome systems defy manipulation with treatments. We can only observe the weather we cannot change it. The process of manipulation can alter how a system behaves in ways that are not related to the treatment application, so we cannot really study just the treatment effects. Epidemiological studies, like John Snow’s investigation of the 1854 Cholera epidemic are by definition observational studies: we observe what is happening, not conditions we create deliberately.\nWhile one can assign conditions, ethical considerations might prevent us from doing so. How can we justify assigning harmful things and ask a person to smoke two packs of cigarettes a day for the next 10 years? In the case of testing a medical breakthrough against a horrible disease, how can we justify assigning patients to a placebo group and withholding a potentially life-saving treatment? To show that repeated head impact (RHI) causes chronic traumatic encephalopathy (CTE), it would not be ethical to randomize subjects and hit those assigned to the RHI arm of the study repeatedly over the head.\nWhen experimentation is not possible, we rely on observational data, analyzing the data we can collect, and it is often the best we can do. How can we then explain the association we find in the data and get closer to establishing causality?\nTo link RHI to CTE, we can study data on subjects who have been exposed to repeated head trauma, such as boxers and football players, and compare their likelihood of developing CTE to individuals who did not experience such head trauma. Comparisons must be made with care. We would not want to compare star athletes who experience head trauma with non-athletes who did not experience head trauma; there would be too many confounding factors. Maybe we could follow a group of athletes over time and record accumulated head impacts along with brain scans. While we cannot design an experiment, we can design how to collect observational data.\n\n\nAssignment: Why Do Old Men Have Big Ears?\n\n\nSpiegelhalter (2021, 108–9) asks this question based on his personal experience that older men seem to have big ears. This question cannot be answered with a randomized controlled trial, we cannot assign ear lengths. It is what it is.\nObservational studies in the UK and Japan collected cross-sectional data, that is, a sample from the current population, which will include men of different ages. Analyzing the data the studies concluded that there is a positive correlation between age and ear length. For example, the figure below appeared in Heathcote (1995). The study concluded that the regression trend was significant. The slope of the regression line is 0.22mm per year with a 95% confidence interval of [0.17, 0.27] mm per year. Because the confidence interval does not cover the value 0, the trend is statistically significant. It seems that as we get older our ears get bigger by an average by 0.22 mm per year.\n\n\n\n\n\n\nFigure 19.13: Ear length. From Heathcote (1995).\n\n\n\n\nTry and explain the association between age and ear length in men. What are possible reasons ears are/appear larger in older men?\nIf you were to conduct a follow-up study to test the possible reasons in 1., what would the study look like? What kind of data would you collect? What kind of men would you recruit for the study?\n\n\n\n\nQuasi-experiments\nIf you cannot manipulate and control factors in an experiment, maybe you are lucky to find data where the confounding factors have already been controlled for you. Sometimes real life runs these quasi-experiments for us and eliminates the confounding factors. Although the data is observational rather than experimental, it can go a long way toward establishing causality. In Section 2.8.4 we discussed a second, deeper analysis John Snow conducted in which he compared cases between customers of the Lambeth and the Southward & Vauxhall water companies. For all intents and purposes the groups serviced by the two companies were identical except for the source of the water. Lambeth’s water was drawn upriver from sewage discharge into the River Thames and was cleaner than the water from Southwark & Vauxhall, which drew water below the sewage discharge. The much higher cholera incidence in the group supplied by Southwark & Vauxhall was sufficient evidence to implicate the water.\n\n\nAssignment: Fluoride Exposure and IQ\n\n\nThe National Toxicology Program of the U.S. Department of Health and Human Services conducted a meta-analysis of the relationship between fluoride intake and IQ.\nAmong the findings of the analysis, the article states (emphasis in original):\n\nThe NTP monograph concluded, with moderate confidence, that higher levels of fluoride exposure, such as drinking water containing more than 1.5 milligrams of fluoride per liter, are associated with lower IQ in children. The NTP review was designed to evaluate total fluoride exposure from all sources and was not designed to evaluate the health effects of fluoridated drinking water alone. It is important to note that there were insufficient data to determine if the low fluoride level of 0.7 mg/L currently recommended for U.S. community water supplies has a negative effect on children’s IQ. The NTP found no evidence that fluoride exposure had adverse effects on adult cognition.\n\n…\n\nThe determination about lower IQs in children was based primarily on epidemiology studies in non-U.S. countries such as Canada, China, India, Iran, Pakistan, and Mexico where some pregnant women, infants, and children received total fluoride exposure amounts higher than 1.5 mg fluoride/L of drinking water.\n\nPlease read the full article from the National Toxicology Program here and answer the following questions:\n\nDid the study establish correlation or causation between fluoride intake and children’s IQ?\nDoes the article make it clear whether to take the results as an indication of causation or correlation?\nWhat is meta-analysis?\nHow do you interpret the fact that the study did not analyze data from the U.S.? Does that affect whether the results are applicable to U.S. children?\nCan you think of confounding factors that limit transfer of the results to the U.S?\nAre you surprised that there is no evidence of adverse effects on adults?\n\n\n\n\n\nHill’s Criteria\nEstablishing causality from observational data, as in the case of Snow’s water quality study, is a common problem in epidemiology, the study of the distribution of diseases and health risks. Establishing designed experiments with randomized control of factors is often not possible in those studies.\nThe English epidemiologist and statistician Sir Author Bradford Hill established nine principles that allow one to move from association to causation. These are known as Hill’s criteria, developed to establish causation involving environmental exposure. Hill was part of the research team that confirmed the link between smoking and lung cancer. The criteria are:\n\nStrength: strong association is stronger evidence of causality. A small association does not rule out a causal effect, however.\nConsistency: similar studies by others in different places with different samples give consistent findings.\nSpecificity: the more specific the association between a factor and an effect, the more likely we are dealing with cause and effect. The association is specific when the cause leads to only one outcome and the outcome can only come from the one cause.\nTemporarlity: the effect comes after the cause.\nGradient: an increase in level, intensity, duration or total level of exposure to the potential cause leads to progressive increase in (the likelihood of) the outcome.\nPlausibility: the association is plausible based on known scientific facts.\nCoherence: agreement between epidemiological and laboratory findings. The interpretation of the data does not seriously conflict with what is already known about the disease or exposure.\nExperimentation: if experimentation is possible, it provides results in support of the causal hypothesis (provides strong evidence).\nAnalogy: similarity between things that are otherwise different. Scientists can use prior knowledge and patterns to infer similar causal associations.\n\nHill’s criteria should be viewed as a guideline for establishing causality based on association. Proving causality is not done by clicking check boxes. Meeting the criteria increases the likelihood that a factor causes an effect.\n\n\nAssignment: Causal Link between RHI and CTE\n\n\nTiaina Baul “Junior” Seau was an outstanding linebacker who played in the National Football League for 20 years, mostly with the San Diego Chargers and also the Miami Dolphins and New England Patriots. He committed suicide in 2012 by shooting himself in the chest. Junior did not leave a suicide note but a piece of paper with lyrics from the country song “Who I Ain’t”. An autopsy confirmed that Junior Seau had suffered from chronic traumatic encephalopahty (CTE) believed due to repeated head trauma he experienced as a football player.\n\n\n\n\n\n\nFigure 19.14: Junior Seau playing for the New England Patriots. Source: Wikipedia\n\n\n\nNowinski et al. (2022) applied the Hill criteria to establish causality between repeated head impacts (RHI) and CTE. The authors discuss previous research on the association of the two and resistance to calling the link between RHI and CTE causal. Interestingly, it appears that the causal link between the two was settled throughout the 20th century when causality was called into question again. The article is available online.\n\nWhat reasons were given by some of the organizations involved in the debate (like CISG) to resist declaring a causal link between RHI and CTE?\nWhat do you believe was their motivation to do so?\nIn the section Understanding Causation, the article examines each of Hill’s nine criteria. Cite one argument from the article in support of each criterion.\nPer the Discussion section of the article, what are valid reasons why scientists might remain skeptical of a causal link between RHI and CTE?\nWhat language did the authors use in the Conclusion to indicate a causal relationship between RHI and CTE?\nBased on the evidence provided, should the conclusions drawn from data about adult athletes be applied to children? Argue for or against and why or why not?\n\n\n\nIn domains and applications where experimentation is not possible and confounding factors are present, we try to establish causation by a process called causal inference. By studying which variables act on each other, causality can be inferred. The Ladder of causation is an important tool in causal inference.\n\n\n\nThe Ladder of Causation\nThe difference between the experimental and the observational study is doing versus seeing. Designed experiments are the statistician’s way of “doing”, but it is not always possible to manipulate and intervene with systems in this way. Ethical concerns might rule out giving harmful treatments. Some effects can only be assessed over long periods of time and maintaining control of other effects over time can be difficult. Some systems are altered by interventions in ways that make inferences about the original state meaningless. Some systems defy randomization. If you cannot randomize the stock market, how can we establish that higher returns were caused by a change in trading algorithm? How can we establish that human activity causes climate change? Traditional designed experimentation cannot be used. A prohibition to think about causation unless we are in a randomized controlled trial is not helpful.\nIn our daily lives we make causal inferences, not statistical ones. Human intuition is grounded in causal logic. When I gradually push a book over the edge of a table it will eventually fall off the table, caused by gravity. I do not need to repeat this 20 times to convince me that what was observed—the book fell—was just incredibly unlikely. Human intuition is sufficient to conclude that the physical therapy reduced the pain from tennis elbow. Something was done (to us, physical therapy) and we see the effect (on us, pain reduction). We do not require a statistical experiment to figure out that we have been helped. On the other hand, if we decide among treatment alternatives for tennis elbow, the existence of such experiments can be helpful in deciding on a treatment plan.\nIn their influential (cult) text “The Book of Why”, Judea Pearl and Dana MacKenzie introduce the Ladder of Causation, three distinct levels of cognitive ability: seeing, doing, and imagining (Pearl and MacKenzie 2018).\n\n\n\n\n\n\nFigure 19.15: The Ladder of Causation according to Pearl and MacKenzie (2018).\n\n\n\n\nFirst rung—seeing\nSeeing (observing) means detection of regularities and irregularities in the environment and acting on it. Most animals have this cognitive ability. An owl observes the movement of its prey and reacts to it. It recognizes regular and irregular patterns—healthy versus unhealthy prey—and changes its reaction to the environment. The first rung of the ladder of causation is where questions are answered by observing. The typical question is “What if I observe …?” A this rung of the ladder we find relationships and associations but cannot establish causality. We cannot determine why something happened.\nModifying an example given by Pearl and MacKenzie, suppose the manager of a grocery store wonders how likely someone who buys diapers also buys a 6-pack of beer. From the database of store sales, we can estimate the probability Pr(customer buys a 6-pack of beer) and the conditional probability Pr(customer buys a 6-pack of beer | customer also bought diapers).\nThe conditional probability is a measure of the association of the two events: buying diapers and buying beer. We cannot learn from this information whether increasing the price of diapers would affect beer sales. Even if the database of store sales contains sales where diapers were more expensive, we cannot conclude an effect on beer sales because the prices could have been higher in the past for other reasons, maybe a diaper supply shortage. The differences we see in the data are not due to the interventions we should have taken to answer the question of interest.\n\n\nSecond rung—doing\nThe second rung of the ladder of causation is reached when we deliberately change the world. As Pearl and MacKenzie put it\n\nSeeing smoke tells us a totally different story about the likelihood of fire than making smoke.\n\nQuestions we answer at this level are “What if we do…?” and “What happens if …?”. The store manager now raises the price of diapers under the current market condition and observes how the sale of beer reacts to the intervention. In particular, they are interested if the conditional probability Pr(beer | diaper) changes. If the store is part of a larger chain, they can run an A/B experiment, raising the price of diapers at some randomly selected stores, and comparing the sales numbers across stores with and without price increase.\n\n\nThird rung—imagining\nThe third rung of the ladder of causation answers a different set of questions, one that requires not just interventions, but theory of interventions that allows us to imagine worlds that have not happened. These “What if…?” questions are called counterfactuals. So we raised the price of diapers and beer sales dropped. Why? What caused that? Was it the change in price of diapers? Was it the cooler weather? Was it the change in the NFL schedule? To answer these questions, we need to imagine and reason about a world where we did not change the price of diapers.\nExperiments cannot answer questions such as “What if I had done…?”; the first two rungs deal with observable phenomenon, either through observing what is or observing an intervention. The final rung of the ladder of causation requires models for the underlying processes, understanding that manifests itself in theories and what we call laws of nature.\n\n\nThe role of data\nInterestingly, Pearl and MacKenzie place artificial intelligence and machine learning, as depicted by the robot, on the first rung of the ladder of causation. By simply observing what is we cannot answer causal “What if” questions at the upper rungs of the ladder: “What if I do …?”, “What if I had done …?”.\nDoes that mean we can never use observational data to make causal statements? Yes, unless the data are supplemented with models and theories that fall outside of the data. The store manager could answer the question “What happens if I change the price of diapers?” without experimentation, based on a model of consumer behavior and market conditions. Combining this model with the observed data on beer—diaper sales can produce a better prediction of the effect on beer sales than the observational data alone, subject to the correctness of the assumed market model.\nWhy are we discussing all this in the context of data science?\nMost of the data you work with is likely observational data, not experimental data. The analysis of observational data with statistical techniques is on the first rung of the ladder of causation. You cannot answer rung-2 “What if I do..?” questions from this data alone; no matter the sophistication of the analytic method. In order to climb the ladder of causation and ask more interesting questions you need to collect data under manipulation or apply an external model that implies manipulation (an economic theory, for example).\nThe belief that with more data we can answer more sophisticated questions and climb the ladder of causality is fundamentally flawed. Answering more sophisticated questions from a higher rung of the ladder requires different cognitive abilities. You cannot answer questions about interventions by analyzing patterns and associations. Understanding and imagination of an autonomous driving system does not come from training on data. It comes from explicit programming—augmenting the information in the data with abilities from a higher rung of the ladder. AI systems trained on data can learn impressive tasks but are limited to tasks that can be learned by watching. They cannot learn tasks based on learning by doing. And they cannot answer counterfactual questions (“What if I had instead done …?”) without understanding and reasoning. AI derived from observational data cannot achieve intelligence.\n\n\n\nFigure 19.1: Relationship between highway fatalities and lemon imports from Mexico.\nFigure 19.2: Positive correlation.\nFigure 19.3: Negative correlation.\nFigure 19.7: Chocolate consumption and number of Nobel laureates.\nFigure 19.12: Experimental layout for poppy experiment.\nFigure 19.13: Ear length. From Heathcote (1995).\nFigure 19.15: The Ladder of Causation according to Pearl and MacKenzie (2018).\n\n\n\nBenson H., Dusek J. A., Sherwood J. B., P. Lam, C. F. Bethea, W. Carpenter, S. Levitsky, et al. 2006. “Study of the Therapeutic Effects of Intercessory Prayer (STEP) in Cardiac Bypass Patients: A Multicenter Randomized Trial of Uncertainty and Certainty of Receiving Intercessory Prayer.” American Heart Journal 151 (4): 934–42.\n\n\nHeathcote, James A. 1995. “Why Do Old Men Have Big Ears?” BMJ 311 (7021): 1668. https://doi.org/10.1136/bmj.311.7021.1668.\n\n\nMesserli, F. H. 2012. “Chocolate Consumption, Cognitive Function, and Nobel Laureates.” New England Journal of Medicine 367: 1562–64.\n\n\nNowinski, Christopher J., Samantha C. Bureau, Michael E. Buckland, Maurice A. Curtis, Daniel H. Daneshvar, Richard L. M. Faull, Lea T. Grinberg, et al. 2022. “Applying the Bradford Hill Criteria for Causation to Repetitive Head Impacts and Chronic Traumatic Encephalopathy.” Frontiers in Neurology 13. https://doi.org/10.3389/fneur.2022.938163.\n\n\nPearl, Judea, and Dana MacKenzie. 2018. The Book of Why. Basic Books, New York.\n\n\nSilver, Nate. 2012. The Signal and the Noise: Why so Many Predictions Fail–but Some Don’t. Penguin Books.\n\n\nSpiegelhalter, David. 2021. The Art of Statistics. How to Learn from Data. Basic Books.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Correlation and Causation</span>"
    ]
  },
  {
    "objectID": "models/bias_variance.html",
    "href": "models/bias_variance.html",
    "title": "20  The Bias-Variance Tradeoff",
    "section": "",
    "text": "20.1 Introduction\nIn The Signal and the Noise, Silver (2012, 52) cites a study at the University of Pennsylvania that found when political scientists claim that a political outcome had no chance of occurring, it happened about 15% of the time. And of the absolutely sure things they proclaimed, 25% failed to occur. Now we know that predictions are themselves uncertain, that is why polling results have margins of error. But when you predict that something has a zero chance of happening, then you ought to be pretty confident in that, the margin of error should be small, definitely not 15%.\nThis is an example of a prediction that is not very good.\nIf a plane had a 29% of crashing, you would probably consider the risk of flying too high and stay on the ground. In the run-up to the 2016 presidential election, FiveThirtyEight predicted a 71% chance for Clinton to win the Electoral College and a 29% chance for Trump to win. This was a much higher chance of a Trump victory than the 1% to 15% chance many other models produced. As it turned out, the FiveThirtyEight model was much better than the models that treated the Clinton victory as a near certainty. But both models were wrong about the outcome of the election.\nSilver (2012) points out\nIt is much easier to sift through intelligence after a terrorist attack and to point out what was missed than it is finding the signals in the cacophony of data before the attack.\nIt seems that we make a lot of predictions and are not very good at it. And even if our predictions are spot-on, we might not act on them or ignore them. Predictions that contradict our intuition or preferred narrative can be ignored or explained away. Our personal judgment is not as good as you might think. We tend to overvalue our own opinion and this trend increases the more we know. 80% of doctors believe they are in the top 20% of their profession. More than half of them are clearly wrong!\nSo when a prediction does not come true, does the fault lie with the model of the world or the world itself? If there is a 80% chance of rain tomorrow, then you might see sunny skies. If, in fact, the long run ratio of days that have sunny skies when the forecast calls for an 80% chance of rain is 1 in 5, then the forecast model is correct. We cannot fault the model that calls for a 80% chance of rain for the occasional sunny skies.\nNow compare this scenario to the following. In the build-up of the the 2008 financial crisis, Standard & Poor gave CDOs, a type of mortgage-backed securities, a stellar AAA credit rating, meaning that there is only a 0.0012 probability that they would fail to pay out over the next 5 years (Silver 2012, 20). In reality, 28% of the AAA-rated CDOs defaulted. Had the world of financial markets drastically changed to bring about such a massive change in default rates (200x!)? Or is it more likely that the default models of the rating agencies were wrong? It was the latter.\nWe predict all the time. On the drive to work we choose this route over that route because we predict it has less traffic, fewer red lights, or we are less likely to get caught behind a school bus. We probably make this choice instinctively, without much deliberation, based on experience, instantaneously processing information about the time of day, weather, etc.\nYou might choose Netflix over Paramount+ one evening because you think (predict!) it is more likely that you’ll find content that interests you. This is a prediction problem. You are also on the receiving end of predictions every day. A company offers you a discount because it predicts that without an incentive you might shop at a competitor. We are being served weather forecasts (predictions!) on the local news and apps every day.\nIf we are not good at predicting, can we learn from what bad predictions have in common? Silver (2012, 20) lists attributes of bad predictions:\nIf we want to avoid these mistakes with predictions based on modeling data we need to\nThe bias-variance tradeoff in statistical learning is the tension between models that are overly simple and those that are overly complex, finding models that generalize well and do not follow the training data too closely. Resolving the bias-variance tradeoff is our weapon to build models that satisfy 1. and 2. in the list.\nTo demonstrate the tradeoff we use a simulation.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>The Bias-Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "models/bias_variance.html#sec-bias-var--intro",
    "href": "models/bias_variance.html#sec-bias-var--intro",
    "title": "20  The Bias-Variance Tradeoff",
    "section": "",
    "text": "Most people fail to recognize how much easier it is to understand an event after the fact when you have all the evidence at your disposal. […] But making better first guesses under conditions of uncertainty is an entirely different enterprise than second-guessing.\n\n\n\n\n\n\n\n\n\n\n\n\nFocus on the signals that tell a story about the world as we would like it to be, not how it really is.\nIgnore the risks that are most difficult to measure, although they pose the greatest risk to our well-being.\nMake approximations and assumptions that are much cruder than we realize.\nDislike (abhor) uncertainty, even if it is an integral part of the problem.\n\n\n\nBuild models that are useful abstractions, not too complicated and not too simple\nFind the signal that the data is trying to convey about the problem under study, allowing for generalization beyond the data at hand.\nBe honest about the quality of the data and the limitations to capture complex systems through quantification.\nQuantify the uncertainty in the conclusions drawn from the model.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>The Bias-Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "models/bias_variance.html#a-simulation",
    "href": "models/bias_variance.html#a-simulation",
    "title": "20  The Bias-Variance Tradeoff",
    "section": "20.2 A Simulation",
    "text": "20.2 A Simulation\nSuppose that one hundred observations on variables \\(Y\\) and \\(X\\) are sampled from a population. We suspect that the variables are related and wish to predict the difficult-to-measure attribute \\(Y\\) from the easy-to-measure attribute \\(X\\). Figure Figure 20.1 displays the data for the random sample, inspired by Notes on Predictive Modeling by Eduardo García-Portugués at Carlos III University of Madrid.\n\n\n\n\n\n\n\n\nFigure 20.1: 100 observations sampled from a population where \\(Y\\) and \\(X\\) are related.\n\n\n\n\n\nThere is noise in the data but there is also a signal, a systematic change in \\(Y\\) with \\(X\\). How should we go about extracting the signal? Figure 20.2 shows three possible models for the signal.\n\n\n\n\n\n\n\n\nFigure 20.2: Observed data and three possible models for the signal.\n\n\n\n\n\nThe three models differ in their degree of smoothness. The solid (blue) line is the most smooth, followed by the dotted (red) line. The dashed (green) line is the least smooth, it follows the observed data points more closely. The solid (blue) line is probably not a good representation of the signal, it does not capture the trend in the data for small or large values. This model exhibits bias; it overestimates for small values of \\(X\\) and underestimates for large values of \\(X\\). On the other hand, the dashed (green) model exhibits a lot of variability.\nThe question in modeling these data becomes: what is the appropriate degree of smoothness? In one extreme case the model interpolates the observed data points the model reproduces the 100 data points. Such a model fits the observed data really well but you can imagine that it does not generalize well to a new data point that was not used in training the model. Such a model is said to be overfitting the data. The dashed (green) line in Figure 20.2 approaches this extreme.\nIn another extreme case, the model is too rigid and does not extract sufficient signal from the noise. Such a model is said to be underfitting the data. The solid (blue) line in Figure 20.2 is likely a case in point.\n\nMore Simulations\nThe concept of model bias and model variability relates not to the behavior of the model for the sample data at hand, although in practical applications this is all we have to judge a model. Conceptually, we imagine repeating the process that generated the sample. Imagine that we draw two more sets of 100 observations each. Now we have 300 observations, 3 sets of 100 each. Figure 20.3 overlays the three samples an identifies observations belonging to the same sample with colors and symbols.\n\n\n\n\n\n\n\n\nFigure 20.3: Three realizations of 100 observations each.\n\n\n\n\n\nThe process of fitting the three models displayed in Figure 20.2 can now be repeated for the two other samples. Figure 20.4 displays the three versions of the solid (blue) model. Figure 20.4 displays the three versions of the dotted (red) model and Figure 20.6 the versions of the dashed (green) model.\n\n\n\n\n\n\n\n\nFigure 20.4: The solid (blue) model fit to each of the three samples.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 20.5: The dotted (red) model fit to each of the three samples.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 20.6: The dashed (green) model fit to each of the three samples.\n\n\n\n\n\nComparing the same model type for the three sets of 100 observations, it is clear that the blue model shows the most stability from set to set, the green model shows the least stability (most variability), and the red model falls between the two.\nWe also see now why the highly variable green model would not generalize well to a new observation. It follows the training data too closely and is sensitive to small changes in the data.\n\n\nThe True Model\nSince this is a simulation study we have the benefit of knowing the underlying signal around which the data were generated. This is the same signal for all three sets of 100 observations and we can compare the models in Figure 20.4 through Figure 20.6 against the true model (Figure 20.7 through Figure 20.9)\n\n\n\n\n\n\n\n\nFigure 20.7: The solid (blue) model fit to each of the three samples and the true signal.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 20.8: The dotted (red) model fit to each of the three samples and the true signal.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 20.9: The dashed (green) model fit to each of the three samples and the true signal.\n\n\n\n\n\nAgainst the backdrop of the true signal, the solid (blue) and dashed (green) models do not look good. The former is biased, it is not sufficiently flexible to capture the signal. The latter is too flexible and overfits the signal.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>The Bias-Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "models/bias_variance.html#the-mean-squared-error",
    "href": "models/bias_variance.html#the-mean-squared-error",
    "title": "20  The Bias-Variance Tradeoff",
    "section": "20.3 The Mean Squared Error",
    "text": "20.3 The Mean Squared Error\nThe simulation is unrealistic for two reasons:\n\nWe do not know the true signal in practice, otherwise there would not be a modeling problem.\nWe have only a single sample of \\(n\\) observations and cannot study the behavior of the model under repetition of the data collection process.\n\nThe considerations are the same, however. We do not want a model that has too much variability or a model that has too much bias. Somehow, the two need to be balanced based on a single sample of data. We need a method of measuring bias and variance of a model and to balance the two.\nMathematically, we can express the problem of predicting a new observation as follows. Since the true function is unknown, it is also unknown at the new data location \\(x_{0}\\). However, we observed a value \\(y\\) at \\(x_{0}\\). Based on the chosen model we can predict at \\(x_{0}\\). But since we do not know the true function \\(f(x)\\), we can only measure the discrepancy between the value we observe and the value we predicted; this quantity is known as the error of prediction (Table 20.1).\n\n\n\nTable 20.1: Components that contribute to bias and variance of an estimator. The last column designates whether the quantity can be measured in data science applications.\n\n\n\n\n\n\n\n\n\n\n\nQuantity\nMeaning\nMeasurable\n\n\n\n\n\n\\(f(x)\\)\nThe true but unknown function\nNo\n\n\n\n\\(f\\left( x_{0} \\right)\\)\nThe value of the function at a data point \\(x_{0}\\) that was not part of fitting the model | No\n\n\n\\(\\widehat{f}\\left( x_{0} \\right)\\)\nThe estimated value of the function at the new data point \\(x_{0}\\)\nYes\n\n\n\n\\(f\\left( x_{0} \\right) - \\widehat{f}\\left( x_{0} \\right)\\)\nThe function discrepancy\nNo\n\n\n\n\\(y -\\widehat{f}\\left( x_{0} \\right)\\)\nThe error of prediction\nYes\n\n\n\n\n\n\n\nMultiple components contribute to the prediction error: the variability of the data \\(y\\), the discrepancy between \\(f\\left( x_{0} \\right)\\) and \\(\\widehat{f}\\left( x_{0} \\right)\\), and the variability of the function \\(\\widehat{f}\\left( x_{0} \\right)\\). The variability of \\(y\\) is also called the irreducible variability or the irreducible error because the observations will vary according to their natural variability. Once we have decided which attribute to observe, how to sample it, and how to measure it, this variability is a given. The other two sources relate to the accuracy and precision of the prediction; or, to use statistical terms, the bias and the variance.\n\nAccuracy and Precision\nIn the context of measuring devices, accuracy and precision are defined as\n\nAccuracy: How close are measurements to the true value\nPrecision: How close are measurements to each other\n\nTo demonstrate the difference between accuracy and precision, the dart board bullseye metaphor is helpful. The following figure shows four scenarios of shooting four darts each at a dart board. The goal is to hit the bullseye in the center of the board; the bullseye represents the true value we are trying to measure. A is the result of a thrower who is neither accurate nor precise. The throws vary greatly from each other (lack of precision), and the average location is far from the bullseye. B is the result of a thrower who is inaccurate but precise. The throws group tightly together (high precision) but the average location misses the bullseye (the average distance from the bullseye is not zero). The thrower with pattern C is not precise, but accurate. The throws vary widely (lack of precision) but the average distance of the darts from the bullseye is close to zero—on average the thrower hits the bullseye. Finally, the thrower in D is accurate and precise; the darts group tightly together and are centered around the bullseye.\n\n\n\n\n\n\nFigure 20.10: Accuracy and precision—the dart board bullseye metaphor.\n\n\n\nWe see that both accuracy and precision describe not a single throw, but a pattern over many replications. The long-run behavior is described by the expected value. The accuracy of a statistical estimator is the proximity of its expected value from the target value. An estimator that is not accurate is said to be biased.\n\n\nDefinition: Bias\n\n\nAn estimator \\(h\\left( \\text{Y} \\right)\\) of the parameter \\(\\theta\\) is said to be biased if its expected value does not equal \\(\\theta\\).\n\\[\n\\text{Bias}\\left\\lbrack h\\left( \\textbf{Y} \\right);\\theta \\right\\rbrack = \\text{E}\\left\\lbrack h\\left( \\textbf{Y} \\right) - \\theta \\right\\rbrack = \\text{E}\\left\\lbrack h\\left( \\textbf{Y} \\right) \\right\\rbrack - \\theta\n\\]\n\n\nThe last equality in the definition follows because the expected value of a constant is identical to the constant. In the dartboard example, \\(\\theta\\) is the bullseye and \\(h\\left( \\text{Y} \\right)\\) is the distance of the dart from the bullseye. The bias is the expected value of that distance, the average across many repetitions (dart throws).\n\n\nCombining Accuracy and Precision\nWe can now combine the concepts of accuracy and precision of a prediction into a single statistic, the mean squared error. But first we have to take another small detour to be clear about what we want to predict.\nNow consider a generic model of the form \\[\nY = f(x) + \\epsilon\n\\]\n\\(\\epsilon\\) is a random variable with mean 0 and variance \\(\\sigma^{2}\\), the irreducible variability. For a new observation with input value \\(x_0\\) the model looks the same \\[\nY_{0} = f\\left( x_{0} \\right) + \\epsilon\n\\] but only \\(x_{0}\\) is known.\nIf we make predictions under this model there are two possible targets:\n\n\\(f\\left( x_{0} \\right)\\)\n\\(f\\left( x_{0} \\right) + \\epsilon\\).\n\nThe first is the expected value of \\(Y_{0}\\): \\[\n\\text{E}\\left\\lbrack Y_{0} \\right\\rbrack = f\\left( x_{0} \\right) + \\text{E}\\lbrack\\epsilon\\rbrack = f\\left( x_{0} \\right)\n\\]\nThis is a fixed quantity (a constant), not a random variable. The second is a random variable. Interestingly, the estimator of both quantities is the same, \\(\\widehat{f}\\left( x_{0} \\right)\\). The difference comes into play when we consider the uncertainty associated with estimating \\(f\\left( x_{0} \\right)\\) or with predicting \\(f\\left( x_{0} \\right) + \\epsilon\\)—more on this later.\n\n\nDefinition: Mean Squared Error (MSE)\n\n\nThe mean squared error of estimator \\(h\\left( \\textbf{Y} \\right)\\) for target \\(\\theta\\) is\n\\[\\begin{align*}\n\\text{MSE}\\left\\lbrack h\\left( \\textbf{Y} \\right);\\ \\theta \\right\\rbrack &= \\text{E}\\left\\lbrack \\left( h\\left( \\textbf{Y} \\right) - \\theta \\right)^{2} \\right\\rbrack \\\\\n&= \\text{E}\\left\\lbrack \\left( h\\left( \\text{Y} \\right) - \\text{E}\\left\\lbrack h\\left( \\textbf{Y} \\right) \\right\\rbrack \\right)^{2} \\right\\rbrack + \\left( \\text{E}\\left\\lbrack h\\left( \\textbf{Y} \\right) \\right\\rbrack - \\theta \\right)^{2}\\\\\n\n&= \\text{Var}\\left\\lbrack h\\left( \\textbf{Y} \\right) \\right\\rbrack + \\text{Bias}\\left\\lbrack h\\left( \\textbf{Y} \\right);\\theta \\right\\rbrack^{2}\n\\end{align*}\\]\n\n\nThe mean squared error is the expected square deviation between the estimator and its target. That is akin to the definition of the variance, but the MSE is only equal to the variance if the estimator is unbiased for the target. As the last line of the definition shows, the MSE has two components, the variability of the estimator and the squared bias. The bias enters in squared terms because the variance is measured in squared units and because negative and positive bias discrepancies should not balance out.\nIf we apply the MSE definition to the problem of using estimator \\(\\widehat{f}\\left( x_{0} \\right)\\) to predict \\(f\\left( x_0 \\right)\\),\n\\[\n\\text{MSE}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);f\\left( x_{0} \\right)\\  \\right\\rbrack = \\text{Var}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right) \\right\\rbrack + \\text{Bias}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);f\\left( x_{0} \\right) \\right\\rbrack^{2}\n\\]\nwe see how the variability of the estimator and its squared bias contribute to the overall MSE. Similarly, if the target is to predict the new observation, rather than its mean, the expression becomes\n\\[\n\\text{MSE}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);Y_{0} \\right\\rbrack\\text{ = MSE}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);f\\left( x_{0} \\right) + \\epsilon\\  \\right\\rbrack = \\text{Var}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right) \\right\\rbrack + \\text{Bias}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);f\\left( x_{0} \\right) \\right\\rbrack^{2} + \\sigma^{2}\n\\]\nYou now see why \\(\\sigma^{2}\\) is called the irreducible error. Even if the estimator \\(\\widehat{f}\\left( x_{0} \\right)\\) would have no variability and be unbiased, the mean squared error in predicting \\(Y_{0}\\) can never be smaller than \\(\\sigma^{2}\\).\n\n\nExample: \\(k\\)-Nearest Neighbor Regression\n\n\nThe \\(k\\)-nearest neighbor (\\(k\\)-NN for short) regression estimator is a simple estimator of the local structure between a target variable \\(y\\) and an input variable \\(x\\). The value \\(k\\) represents the number of values in the neighborhood of some target input \\(x_{0}\\) that are used to predict \\(y\\). The extreme case is \\(k = 1\\), the value of \\(f\\left( x_{0} \\right)\\) is predicted as the \\(y\\)-value of the observation closest to \\(x_{0}\\).\nSuppose our data come from a distribution with mean \\(f(x)\\) and variance \\(\\sigma^{2}\\). The mean-square error decomposition for the \\(k\\)-NN estimator is then\n\\[\n\\text{MSE}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);Y_{0} \\right\\rbrack\\text{ = }\\frac{\\sigma^{2}}{k}{+ \\left\\lbrack f\\left( x_{0} \\right) - \\frac{1}{k}\\sum_{}^{}Y_{(i)} \\right\\rbrack}^{2} + \\sigma^{2}\n\\]\nwhere \\(y_{(i)}\\) denotes the \\(k\\) observations in the neighborhood of \\(x_{0}\\).\nThe three components of the MSE decomposition are easily identified:\n\n\\(\\sigma^{2}/k\\) is the variance of the estimator, \\(\\text{Var}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right) \\right\\rbrack\\). Not surprisingly, it is the variance of the sample mean of \\(k\\) observations drawn at random from a population with variance \\(\\sigma^{2}\\).\n\\(\\left\\lbrack f\\left( x_{0} \\right) - \\frac{1}{k}\\sum Y_{(i)} \\right\\rbrack^{2}\\) is the squared bias component of the MSE.\n\\(\\sigma^2\\) is the irreducible error, the variance in the population from which the data are drawn.\n\nWhile we cannot affect the irreducible error \\(\\sigma^{2}\\), we can control the magnitude of the other components through the choice of \\(k\\). The variance contribution will be largest for \\(k = 1\\), when prediction relies on only the observation closest to \\(x_{0}\\). The bias contribution for this 1-NN estimator is \\(\\left\\lbrack f\\left( x_{0} \\right) - Y_{(1)} \\right\\rbrack^{2}\\).\nAs \\(k\\) increases, the variance of the estimator decreases. For a large enough value of \\(k\\), all observations are included in the “neighborhood” and the estimator is equal to \\(\\overline{Y}\\). If \\(f(x)\\) changes with \\(x\\), the nearest neighbor method will then have smallest variance but large bias.\n\n\n\n\nStriking the Balance\nIf we want to minimize the mean-squared error, we can strive for estimators with low bias and low variance. If we cannot have both, how do we balance between the bias and variance component of an estimator? That is the bias-variance tradeoff.\nStatisticians resolve the tension with the UMVUE principle. Uniformly minimum-variance unbiased estimation requires to first identify unbiased estimators, those which \\[\n\\text{Bias}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);f\\left( x_{0} \\right) \\right\\rbrack = 0\n\\] and to select the estimator with the smallest variance among the unbiased estimators. According to the UMVUE principle, a biased estimator is not even considered. It is comforting to know that on average the estimator will be on target. This principle would select estimator C in the dartboard example over estimator B because the latter is biased. If you have only one dart left and you need to get as close to the bullseye as possible, would you ask player B or player C to take the shot for your team?\nUMVU estimators are not necessarily minimum mean-squared error estimators. It is possible that a biased estimator has a sharply reduced variance so that the sum of variance and squared bias is smaller than the variance of the best unbiased estimator. If we want to achieve a small mean-square error, then we should consider estimators with some bias and small variance. Resolving the bias-variance tradeoff by eliminating all biased estimators does not lead to the “best” predictive models. Of course, this depends on our definition of “best”.\n\n\nEstimated Mean Squared Error\nIn practice, \\(f\\left( x_{0} \\right)\\) is not known and the bias component \\(\\text{Bias}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right);f\\left( x_{0} \\right) \\right\\rbrack\\) cannot be evaluated by computing the difference of expected values. For many modeling techniques we can calculate—or at least estimate— \\(\\text{Var}\\left\\lbrack \\widehat{f}\\left( x_{0} \\right) \\right\\rbrack\\), the variance component of the MSE. Those derivations depend on strong assumptions about distributional properties and the correctness of the model. So, we essentially need to treat the MSE as an unknown quantity. Fortunately, we can estimate it from data.\n\n\nDefinition: Mean Squared Prediction Error (MSPE)\n\n\nThe mean squared prediction error (MSPE) is the average squared prediction error in a sample of \\(n\\) observations,\n\\[\n\\widehat{\\text{MSE}} = \\frac{1}{n}\\sum_{i=1}^n\\left( y_i - \\widehat{f}\\left( x_i \\right) \\right)^{2}\n\\tag{20.1}\\]\n\n\nTaking the sample average replaces taking formal expectations over the distribution of \\(( Y - \\widehat{f}(x) )^2\\).\nWe have one more complication to resolve. By increasing the flexibility of the model the difference between observed and predicted values can be made arbitrarily small. In the extreme case where the model interpolates the observed data points the differences \\(y_i - \\widehat{y}_i\\) are all zero and \\(\\widehat{\\text{MSE}}\\) computed for the training data is zero. If we were to choose to minimize Equation 20.1, we would end up with highly variable models. And this defeats the purpose.\nThe remedy is to compute the estimated MSE not by comparing observed and predicted values based on the values in the training data set, but to predict the values of observations that did not participate in training the model. This test data appears to the algorithm as new data it has not seen before and is a true measure for how well the model generalizes:\n\\[\n\\widehat{\\text{MSE}}_\\text{Test} = \\frac{1}{m} \\sum (\\text{Observed} - \\text{Predicted})^2\n\\tag{20.2}\\]\nThe main difference between Equation 20.1 and Equation 20.2 is that the former is computed based on the \\(n\\) observations in the training data and the latter is computed based on \\(m\\) observations in a separate test data set. We refer to mean squared errors calculated on test data sets as the test error. This quantity properly balances accuracy and precision, choosing the model with the smallest test error resolves the bias-variance tradeoff.\n\n\nCross-validation\nBecause it is expensive to collect a separate data set to compute the test error the collected data is often split randomly into a training set and a test set. For example, you might use split the data 50:50 or 80:20 or 90:10 into training:test sets. If you have a lot of data, then splitting into separate training and test sets is reasonable.\nThere is a clever way in which the same observations can be used sometimes for training and sometimes for testing, that uses the collected information more economically. This procedure is called cross-validation.\nWe give here a brief introduction to a specific cross-validation method, \\(k\\)-fold cross-validation to round up the application in this chapter. More details about test and validation data sets and about cross-validation follow in the next chapter.\nDuring cross-validation, an observation is either part of the test set or part of the training set. The process repeats until each observation was used once in a test set. One technique of cross-validation randomly assigns observations to one of \\(k\\) groups, called folds. Figure 20.11 is an example of creating 5 folds from 100 observation for 5-fold cross-validation.\n\n\n\n\n\n\nFigure 20.11: Example of 5-fold cross-validation for 100 observations. Numbers in the cells represent observation numbers. The records were randomly arranged prior to assigning the folds.\n\n\n\nThe numbers in the cells of Figure 20.11 are the observation numbers. For example, the first fold includes observations #4, #16, #90, etc. The second fold includes observations #31, #94, #38, etc.\nThe process of calculating the test error through \\(k\\)-fold cross-validation is as follows (Figure 20.12):\n\nSet aside the data in the first fold. It serves as the test data.\nFit the model to the remaining data.\nUsing the model trained in step 2., predict the values in the test data and calculate the test error\nReturn the set-aside fold to the training data and set aside the next fold as the test data. Return to step 2. and continue until each fold has been set aside once.\n\n\n\n\n\n\n\nFigure 20.12: \\(k\\)-fold cross-validation scheme.\n\n\n\nAt the end of the \\(k\\)-fold cross-validation procedure, the model has been trained \\(k\\) times, and we have \\(k\\) estimates of the test error, one for each of the folds. These test errors are then combined into one overall test error.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>The Bias-Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "models/bias_variance.html#overfitting-and-underfitting",
    "href": "models/bias_variance.html#overfitting-and-underfitting",
    "title": "20  The Bias-Variance Tradeoff",
    "section": "20.4 Overfitting and Underfitting",
    "text": "20.4 Overfitting and Underfitting\nThe preceding discussion might suggest that flexible models such as the smoothing spline have high variability and that rigid models such as the simple linear regression model have large bias. This generalization does not necessarily hold although in practice it often works out this way. The reason for this is not that simple linear regression models are biased—they can be unbiased. The reason why flexible models tend to have high variance and low bias and rigid models tend to have low variance and high bias has to do with overfitting and underfitting.\nAn overfit model follows the observed data \\(Y_{i}\\) too closely and does not capture the mean trend \\(f(x)\\). The overfit model memorizes the training data too much. When you predict a new observation with an overfit model that memory causes high variability. Remember that the variability we are focusing on here is the variability across repetitions of the sample process. Imagine drawing 1,000 sets of \\(n\\) observations, repeating the model training and predicting from each model at the new location \\(x_{0}\\). We now have 1,000 predictions at \\(x_{0}\\). Because the overfit model follows the training data too closely, its predictions will be variable at \\(x_{0}\\).\nAn underfit model, on the other hand, lacks the flexibility to capture the mean trend \\(f(x)\\). Underfit models result, for example, when important predictor variables are not included in the model.\nThe most extreme case of overfitting a model is the saturated model. It perfectly predicts the observed data. Suppose you collect only two pairs of \\((x,y)\\) data: (1,0) and (2,1) (Figure 20.13). A two-parameter straight line model will fit these data perfectly. The straight line has an intercept of –1 and a slope of +1. It passes through the observed points and the MSPE is zero.\n\n\n\n\n\n\nFigure 20.13: A straight line model saturates a data set with two $(x,y)$ pairs. The difference between observed values (the dots) and the predicted values (values on the line) is zero at each point. The saturated model has a MSPE of zero.\n\n\n\nSaturated models are not very interesting, they are just a re-parameterization of the data, capturing both signal \\(f(x)\\) and noise \\(\\epsilon\\). A useful model separates the signal from the noise. Saturated models are used behind the scenes of some statistical estimation methods, for example to measure how much of the variability in the data is captured by a model—this type of model metric is known as the deviance. Saturated models are never the end goal of data analytics.\nOn the other extreme lies the constant model; it does not use any input variables. It assumes that the mean of the target variable is the same everywhere:\n\\[Y_{i} = \\beta_0 + \\epsilon_{i}\\]\nThis model, also known as the intercept-only model, is slightly more useful than the saturated model. It is rarely the appropriate model in data science applications; it expresses the signal as a flat line, the least flexible model of all.\nIn our discussion of the model building process during the data science project life cycle we encountered an example of pharmacokinetic data, 500 observations on how a drug is absorbed and eliminated by the body over time (\\(t\\)). The data are replayed in the next figure along with the fit of the constant model. The constant model under-predicts the drug concentration between times \\(t = 3\\) and \\(t = 12\\) and overpredicts everywhere else.\n\n\n\n\n\n\nFigure 20.14: Concentration of a drug in patient’s bodies over time.\n\n\n\nSuppose we draw 1,000 sets of \\(n = 500\\) observations, fit the constant model to each, and predict at the new time \\(t_{0}\\). Because the constant model does not depend on time, we get the same predicted value regardless of the value of \\(t_{0}\\). In each sample of size \\(n\\), the predicted value will be the sample mean, \\(\\overline{y} = \\frac{1}{500}\\sum_{}^{}y_{i}\\). The variability of the 1,000 predictions will be small; it is the variance of the sample mean:\n\\[\n\\text{Var}\\left\\lbrack \\widehat{f}\\left( x_0 \\right) \\right\\rbrack = \\frac{\\sigma^2}{500}\n\\]\nIf the true model does depend on \\(t\\)—and the plot of the data suggests this is the case—the bias of the predictions will be large. The mean-squared prediction error is dominated by the squared bias component in this case.\nSomewhere between the two extremes of a hopelessly overfit saturated model and a hopelessly underfit constant model are models that capture the signal \\(f(x)\\) well enough without chasing the noisy signal \\(f(x) + \\epsilon\\) too much. Those models permit a small amount of bias if that results in a reduction of the variance of the predictions.\nTo summarize,\n\nOverfit models do not generalize well because they follow the training data too closely. They tend to have low bias and a large variance.\nUnderfit models do not generalize well because they do not capture the salient trend (signal) in the data. They tend to have high bias and low variance.\nA large mean squared prediction error can result in either case but is due to a different reason.\nTo achieve a small mean squared prediction error you need to have small bias and small variance.\nIn practice, zero-bias methods with high variance are rarely the winning approaches. The best MSPE is often achieved by allowing some bias if the variance of the method is decreased.\n\nThe danger of overfitting is large when models contain many parameters, and when the number of parameters \\(p\\) is large relative to the sample size \\(n\\). When many attributes (inputs) are available and you throw them all into the model, the result will likely be an overfit model that does not generalize well. It will have a large prediction error. In other words, there is a cost to adding unimportant information to a model. Methods for dealing with such high-dimensional problems play an important role in statistics and machine learning and are discussed in detail in a more advanced section. We mention here briefly:\n\nFeature Selection: Structured approaches that use algorithms to determine which subset of the inputs should be in the model. The decision is binary in that an input is either included or excluded. Also known as variable selection.\nRegularization: Deliberately introducing some bias in the estimation through penalty terms that control the variability of the model parameters which in turn controls the variability of the predictions. The parameters are shrunk toward zero in absolute value compared to an unbiased estimator—regularization is thus also known as shrinkage estimation. The Lasso methods can shrink parameters to zero and thus combines regularization with feature selection. The Ridge regression methods also applies a shrinkage penalty but allows all inputs to contribute.\nEnsemble Methods: Ensemble methods combine multiple methods into an overall, averaged prediction or classification. Ensembles can be homogeneous, where the methods are the same, or heterogeneous. An example of a homogeneous ensemble is a bagged decision tree, where several hundred individual trees are trained independently and the predictions from the trees are averaged to obtain an overall predicted value. Due to averaging, the variance of the ensemble estimator is smaller than any individual estimator. Bagging and boosting are common ensemble methods to reduce variance.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>The Bias-Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "models/bias_variance.html#putting-it-all-together",
    "href": "models/bias_variance.html#putting-it-all-together",
    "title": "20  The Bias-Variance Tradeoff",
    "section": "20.5 Putting It All Together",
    "text": "20.5 Putting It All Together\nLet us now put together everything we have covered in this chapter about building a predictive model based on data for the simulated data set in Figure 20.1.\n\nWe build a model that captures the relationship between \\(Y\\) and \\(X\\).\nThe result should be an algorithm that we can use to predict \\(Y\\) based on values of \\(X\\) (\\(Y\\) is the target variable, \\(X\\) is the input variable).\nThe model should generalize well to new observations, that is, it should not overfit or underfit. Use cross-validation to determine the appropriate amount of flexibility in the model.\nWe should be able to quantify the uncertainty in predictions from the model.\n\n\nCreating the Data (Quantification)\nThe following R code creates the data frame shown in Figure 20.1.\n\n1set.seed(12345)\n2n &lt;- 100\n3eps &lt;- rnorm(n, sd = 2)\n4X &lt;- rnorm(n, sd = 2)\n5Y &lt;- X^2 * cos(X) + eps\n\n6simData &lt;- data.frame(X=X, Y=Y)\n\n\n1\n\nSetting a seed for the random number generator ensures that the program generates the same values every time it is run. The value for the seed is chosen here as 12345 and can be any integer.\n\n2\n\nThe number of observations drawn is set to \\(n = 100\\).\n\n3\n\neps is a vector of Gaussian (normal) random variables with mean 0 and variance 4 (std. dev 2). This represents the noise in the system.\n\n4\n\nThe values of \\(X\\) are drawn randomly from a Gaussian distribution with mean 0 and variance 4\n\n5\n\nThe signal is \\(x^2 \\, \\cos(x)\\). The noise (eps) is added to the signal.\n\n6\n\nA data frame is constructed from X and Y.\n\n\n\n\n\n\nCross-validating the Model\nThe steps of training the model and selecting the best flexibility by cross-validation can be done in a single computational step using the train function in the caret package. We need to decide on the general family of model we are going to entertain. Here we choose what is known as a regression spline. These model the relationship between two variables and their flexibility is governed by a single parameter, called the degrees of freedom of the model. With increasing degrees of freedom the models become more flexible. Cross-validation is used to determine the best value for the degree of freedom parameter in this model family.\n\n1library(caret)\n\n2cv_results &lt;- train(Y ~ X,\n3                    data     =simData,\n4                    method   =\"gamSpline\",\n5                    tuneGrid =data.frame(df=seq(2, 25, by=1)),\n6                    trControl=trainControl(method=\"cv\", number=10))\n\n\n1\n\nThe caret library is loaded into the R session. If caret is not yet installed in your system, execute the command install.packages(\"caret\") once from the Console prompt.\n\n2\n\nThe train() function in the caret library is called. Y ~ X specifies the model we wish to train. The target variable is placed on the left side of the ~ symbol and the input variable is placed on the right side.\n\n3\n\nThe data frame where the variables in the model specifications can be found.\n\n4\n\nThe method= parameter specifies the model family we wish to train. gamTrain is the specification for the regression spline family.\n\n5\n\nThe tuneGrid parameter provides a list of the values to be considered in the cross-validation. We vary only the degrees of freedom parameter (df) of the gamSpline function and we evaluate all values from 2 to 25.\n\n6\n\nThe trControl= parameter specifies the computational nuances of the train function. Here we choose 10-fold cross-validation.\n\n\n\n\nAfter running the code above we can examine the results:\n\nprint(cv_results)\n\nGeneralized Additive Model using Splines \n\n100 samples\n  1 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 90, 91, 89, 89, 90, 89, ... \nResampling results across tuning parameters:\n\n  df  RMSE      Rsquared   MAE     \n   2  3.948383  0.2468568  3.021822\n   3  3.583394  0.4016653  2.751121\n   4  3.264671  0.5176447  2.559835\n   5  2.967739  0.5996185  2.371416\n   6  2.732739  0.6454729  2.218233\n   7  2.573920  0.6688788  2.107661\n   8  2.474458  0.6808445  2.036267\n   9  2.413180  0.6873204  2.001114\n  10  2.375289  0.6911981  1.983925\n  11  2.351995  0.6938019  1.976452\n  12  2.338419  0.6957038  1.971334\n  13  2.331878  0.6970928  1.970695\n  14  2.330844  0.6979920  1.971182\n  15  2.334393  0.6984189  1.972935\n  16  2.342029  0.6983416  1.975561\n  17  2.353220  0.6978141  1.978418\n  18  2.367892  0.6968446  1.981459\n  19  2.385770  0.6954992  1.984928\n  20  2.406903  0.6938330  1.990501\n  21  2.431180  0.6919134  1.998484\n  22  2.458785  0.6897954  2.007296\n  23  2.489828  0.6875293  2.017593\n  24  2.524311  0.6851669  2.028585\n  25  2.562596  0.6827171  2.041443\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was df = 14.\n\n\nThe output lists the values of the cross-validation parameter (df) along with three statistics computed for each. The statistic of interest to us is RMSE, the cross-validation root mean square error, the square root of our measure of test error. Note that caret reports the square root of the \\(\\widehat{\\text{MSE}}_\\text{Test}\\). In order to find the df with the smallest test error it does not matter whether we look at \\(\\widehat{\\text{MSE}}_\\text{Test}\\) or its square root. The minimum is achieved at the same value.\nWe can see that the smallest value in the RMSE column occurs at df=14. This is confirmed in the sentence at the bottom of the output.\nIt is customary to graph the cross-validation criterion (RMSE) against the values of the parameter. Figure 20.15 shows a typical pattern. The test error decreases to an optimal value and increases afterwards.\n\nplot(cv_results)\n\n\n\n\n\n\n\nFigure 20.15: Cross-validation root mean squared error as a function of model flexibility (degrees of freedom).\n\n\n\n\n\n\n\nTraining the Final Model\nOne final step remains. Now that we have chosen df=14 as the optimal parameter for this combination of data set and class of model, we train the model with those degrees of freedom on all the data to obtain the final model.\n\nfinal_model &lt;- train(Y ~ X,                   \n                     data     =simData,       \n                     method   =\"gamSpline\",   \n1                     tuneGrid =data.frame(df=cv_results$bestTune[1]),\n2                     trControl=trainControl(method=\"none\"))\n\n\n1\n\nInstead of a list of df values, we now pass only one value. The best value from cross-validation was stored automatically in the bestTune field of the result object in the previous step.\n\n2\n\nInstead of cross-validation we request none as the training nuance. train will simply train the requested model on the data frame.\n\n\n\n\n\n\nMaking a Prediction\nSuppose we want to predict \\(Y\\) (or more precisely the mean of \\(Y\\)) for a series of \\(X\\) values, say from \\(-5\\) to \\(6\\).\nComputing the predicted values is easy with the predict function in R. You simply pass it the model object and a data frame with the values for which you need predictions.\n\n1xGrid &lt;- data.frame(X=seq(-5, 6, length.out = 250))\n2predvals &lt;- predict(final_model,newdata=xGrid)\n\n3predvals[1:10]\n\n\n1\n\nCreate a data frame with 250 values for \\(X\\) ranging from \\(-5\\) to \\(6\\).\n\n2\n\nCompute the 250 predicted values.\n\n3\n\nDisplay the first 10 predicted values.\n\n\n\n\n        1         2         3         4         5         6         7         8 \n-2.658016 -2.947193 -3.236370 -3.525547 -3.814724 -4.103901 -4.393078 -4.682255 \n        9        10 \n-4.971432 -5.260609 \n\n\nAt this point we would like to see how the predicted values of the signal compare to the observed noisy data. Figure 20.16 overlays the predicted values and the data. Also displayed is the true signal. The model derived from the data does an excellent job capturing the signal without following the data points too closely or being too inflexible.\n\nplot(simData$X, simData$Y, \n     type=\"p\", \n     las =1,\n     bty =\"l\",\n     xlab=\"X\",\n     ylab=\"Y\")\nlines(xGrid$X,predvals,col=\"red\",lwd=2)\nlines(xGrid$X,m(xGrid$X),col=\"black\",lwd=2,lty=\"dashed\")\nlegend(\"topleft\",\n       legend=c(\"Cross-validated\",\"True\"),\n       lty   =c(\"solid\",\"dashed\"),\n       col   =c(\"red\",\"black\"),\n       lwd   =2)\n\n\n\n\n\n\n\nFigure 20.16: Predictions of the cross-validated spline model and observed data.\n\n\n\n\n\n\n\nQuantifying Uncertainty\nRecall the fourth antidote against bad predictions:\n\nQuantify the uncertainty in the conclusions drawn from the model.\n\nConceptually, we think of the bias and variability of the model under repetition of the data collection. In an earlier section we looked at models for three data sets of 100 observations each. There are several factors contributing to our uncertainty about the predicted values in this analysis.\nWe are not sure whether the regression spline is the correct model family to capature the relationship between \\(Y\\) and \\(X\\). Although the results of the cross-validation and the comparison with the true signal (which in practice we would not know) give us comfort that this model family does a good job here.\nThe second source of uncertainty comes from the fact that there is noise in the data. As we saw earlier, another set of data drawn from the same process gives slightly different data points that lead to a different best model. Fortunately, we can quantify this uncertainty by computing confidence intervals for the predicted values.\nA 95% confidence interval for a parameter is a range that with 95% probability will cover the value of the parameter. For example, a 95% confidence interval for the predicted value at \\(X=2.5\\) is the range into which the mean value of \\(Y\\) will fall in 95% of the sample repetitions.\nThe predict functions in R can compute the basic ingredients for confidence intervals for many models. Unfortunately, the predict.train function invoked by caret cannot do this. However, since we know that we fit a gamSpline model, and the predict.Gam function does compute the uncertainty of the predicted values for the values in the training data, we can repeat the final model fit with gam and use its predict function:\n\n1library(gam)\n\n2final &lt;- gam(Y ~ s(X,df=14), data=simData)\n\n3predvals &lt;- predict(final,se.fit=TRUE)\n\n4round(predvals$fit[1:10],4)\n\n5round(predvals$se.fit[1:10],4)\n\n\n1\n\nLoad the gam library.\n\n2\n\nFit the final model with the gam function. Y ~ s(X,df=14) is the formulation for a regression spline model with 14 degrees of freedom.\n\n3\n\nCompute the predicted values and request that the standard errors of the predicted values are also computed.\n\n4\n\nThe predicted values for the first 10 observations.\n\n5\n\nThe standard errors of the predicted values for the first 10 observations.\n\n\n\n\n      1       2       3       4       5       6       7       8       9      10 \n 0.2388 -2.4706  0.5433 -5.7746  0.5378 -0.0852  0.4096 -9.5366  0.4262  0.0101 \n [1] 0.6535 0.7064 0.7128 0.8453 0.6418 0.6865 0.7142 0.8642 0.5838 0.6757\n\n\nFor example, the first predicted value is 0.2388 with a standard error of 0.6535.\nTo compute the 95% confidence interval for the predicted values, add and subtract qnorm(0.975) times the standard error from the predicted value. This is done in the following code and the results are plotted.\n\npred_df &lt;- data.frame(X=simData$X, \n                      pred=predvals$fit,\n                      se=predvals$se.fit)\npred_df_sorted &lt;- pred_df[order(pred_df$X),]\npred_df_sorted$ci_up &lt;- pred_df_sorted$pred + qnorm(0.975)*pred_df_sorted$se\npred_df_sorted$ci_lo &lt;- pred_df_sorted$pred - qnorm(0.975)*pred_df_sorted$se\n\nplot(simData$X, simData$Y, \n     type=\"p\", \n     las =1,\n     bty =\"l\",\n     xlab=\"X\",\n     ylab=\"Y\")\nlines(xGrid$X,m(xGrid$X),col=\"black\",lwd=2,lty=\"dashed\")\nlines(pred_df_sorted$X,\n      pred_df_sorted$pred,col=\"red\",lwd=2)\nlines(pred_df_sorted$X,\n      pred_df_sorted$ci_up,col=\"blue\",lty=\"dotted\",lwd=1.5)\nlines(pred_df_sorted$X,\n      pred_df_sorted$ci_lo,col=\"blue\",lty=\"dotted\",lwd=1.5)\n\n\n\n\n\n\n\nFigure 20.17: Predictions and confidence intervals of the cross-validated spline model and observed data. The black line is the true signal in the data.\n\n\n\n\n\nThe confidence intervals (dotted blue) are tracing the predicted values and are narrower in the center of the \\(X\\)-data than near the edges. The further out we move the more uncertain our predictions will be. The dashed black line shows the true signal in the data, the 95% confidence interval covers it.\n\n\n\nFigure 20.1: 100 observations sampled from a population where \\(Y\\) and \\(X\\) are related.\nFigure 20.2: Observed data and three possible models for the signal.\nFigure 20.3: Three realizations of 100 observations each.\nFigure 20.4: The solid (blue) model fit to each of the three samples.\nFigure 20.5: The dotted (red) model fit to each of the three samples.\nFigure 20.6: The dashed (green) model fit to each of the three samples.\nFigure 20.7: The solid (blue) model fit to each of the three samples and the true signal.\nFigure 20.8: The dotted (red) model fit to each of the three samples and the true signal.\nFigure 20.9: The dashed (green) model fit to each of the three samples and the true signal.\nFigure 20.10: Accuracy and precision—the dart board bullseye metaphor.\nFigure 20.12: \\(k\\)-fold cross-validation scheme.\nFigure 20.16: Predictions of the cross-validated spline model and observed data.\nFigure 20.17: Predictions and confidence intervals of the cross-validated spline model and observed data. The black line is the true signal in the data.\n\n\n\nSilver, Nate. 2012. The Signal and the Noise: Why so Many Predictions Fail–but Some Don’t. Penguin Books.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>The Bias-Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "models/test_cv.html",
    "href": "models/test_cv.html",
    "title": "21  Testing, Validation, Cross-Validation",
    "section": "",
    "text": "21.1 Introduction\nEvaluating the performance of a model uses a variety of techniques: performance metrics, visualizations, goodness-of-fit criteria, hypothesis tests against alternative models, and so on. For regression and classification models, performance metrics are based on comparing observed and predicted values. In models with a continuous target the comparisons are usually be based on absolute differences \\[\n| y_i - \\widehat{y}_i |\n\\] or squared differences \\[\n(y_i - \\widehat{y}_i)^2\n\\] The aggregated performance metrics in these cases are the mean absolute error (MAE) \\[\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n | y_i - \\widehat{y}_i |\n\\] and the mean squared prediction error (MSPE) \\[\n\\text{MSPE} = \\widehat{\\text{MSE}} = \\frac{1}{n}\\sum_{i=1}^n\\left( y_i - \\widehat{f}\\left( x_i \\right) \\right)^{2}\n\\] In classification models the measure of model performance is one or more of the statistics based on the confusion matrix, see Section 18.3.3.3. Accuracy, or its complement, the misclassification rate, are commonly used, but precision, recall, \\(F\\) score, specificity and sensitivity are also highly relevant.\nWhen these metrics are used to resolve the bias-variance tradeoff, applying them to the training data set leads to overfitting because the metrics can be made arbitrarily small for the training data. We encountered this in the regression context with saturated models that interpolate the data points. In a saturated model \\(y_i = \\widehat{y}_i\\) and the MSPE or MAE will be zero. To resolve the bias-variance tradeoff properly, the metrics need to be applied to observations that did not participate in the analysis. This leads us to the distinction between training, testing, and validation data and to the practical mechanics of obtaining estimates of the prediction or classification error relevant for adjudicating bias and variance.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Testing, Validation, Cross-Validation</span>"
    ]
  },
  {
    "objectID": "models/test_cv.html#training-testing-and-validation-data",
    "href": "models/test_cv.html#training-testing-and-validation-data",
    "title": "21  Testing, Validation, Cross-Validation",
    "section": "21.2 Training, Testing, and Validation Data",
    "text": "21.2 Training, Testing, and Validation Data\nDeveloping a data science model can involve three different data sets. The training data is used to determine the parameters of the model. The testing data (or test data) is used to evaluate the performance of the model on observations not used in training to provide a reliable estimate of model error. The validation data is used to compare the model to benchmarks and to determine the values of hyperparameters.\n\nTraining Data\nTraining data is the set of \\(n\\) observations used to train the model. The training data is useful to diagnose whether model assumptions are met, for example,\n\ndoes the model adequately describe the mean trend in the (training) data,\nare distributional assumptions met, for example are the errors Gaussian-distributed and does the variance of the target not depend on the inputs\nis it reasonable to assume that the observations are uncorrelated (or even independent)\n\nWe can also use the training data after the model fit to detect data points that have a high influence of the analysis. These are data points that substantially affect an important aspect of the model. Based on the training data we can also study the interdependence of the model inputs and whether those relationships affect the model performance negatively.\nThe diagnostic techniques just mentioned rely on\n\nResidual diagnostics\nCase-deletion and influence diagnostics\nCollinearity diagnostics\n\nThese diagnostics are all very helpful, but they do not answer an important question: how well does the model generalize to observations not used in training the model; how well does the model predict new observations? We also need to figure out, given a single training data set, how to select the values for the hyperparameters of the various techniques.\n\n\nDefinition: Hyperparameter\n\n\nA hyperparameter is a tuning quantity that controls the overall configuration of a statistical model or machine learning technique. Hyperparameters are sometimes referred to as external parameters, whereas the parameters of the model function (slopes, intercepts, etc.) are called the internal parameters.\n\n\nHyperparameters need to be set before a model can be trained and their values impact the performance of the model. The process of determining the values for hyperparameters given a particular data set is called hyperparameter tuning.\nHyperparameters include, for example,\n\nThe number of terms in a polynomial model\nThe smoothing parameters in non-parametric regression models\nThe bandwidth in kernel-based estimation methods such as LOESS, kernel regression, local polynomial regression\nThe kernel function in local models, support vector machines, etc.\nThe shrinkage penalty in Lasso, Ridge regression, smoothing splines\nThe depth of decision trees\nThe number \\(k\\) in \\(k\\)-nearest neighbor methods or in \\(k\\)-means clustering\nThe convergence rate and other tolerances that affect numerical optimizations\nThe learning rate, number of units, and number of layers in neural networks\n\n\n\nTest Data\nTo measure the true predictive performance of a model we need to apply the model to a different set of observations; a set that was not used in training the model. This set of observations is called the test data set. With a test data set we can measure how well the model generalizes and we can also use it to select the appropriate amount of flexibility of the model. Figure 21.1 displays the general behavior of test and train mean squared prediction error as a function of model flexibility. Since increasing model flexibility comes at the expense of increased model complexity, choosing the \\(\\text{right}^{\\texttrademark{}}\\) degree of flexibility chooses the proper model complexity.\nThe MSPE of the test data set is on average higher than the MSPE of the training data set. Since these are random variables, it can happen in a particular application that the test error is lower than the training error, but this is rare. The model complexity/flexibility is measured here by the number of inputs in the model. As this number increases, the \\(\\widehat{\\text{MSE}}_{\\text{Train}}\\) decreases toward zero. The \\(\\widehat{\\text{MSE}}_{\\text{Test}}\\), on the other hand, first decreases, reaches a minimum, and then increases again. The \\(\\widehat{\\text{MSE}}_{\\text{Test}}\\) is high for models with few parameters because of bias and increases with model flexibility past the minimum because of variability.\nThe two contributors to the mean square prediction error—bias and variance—work at different ends of the spectrum—models that balance bias and variance fall somewhere in-between. That is why MSPE calculated on a test data set can help resolve the bias–variance tradeoff and the MSPE calculated on the training data set cannot.\n\n\n\n\n\n\nFigure 21.1: MSETr and MSETe as a function of model flexibility (complexity).\n\n\n\nThe big question is: where do we get the test data?\n\n\nValidation Data\nBefore discussing ways to obtain test data sets, a few words about another type of data set, the validation data. The terms test data and validation data are often used interchangeably, but there is a difference. Test data represents new data that should otherwise be representative of the training data. A test data set drawn at random from the training data set typically satisfies that.\nValidation data can be a separate data set with known properties, for example, a benchmark data set. Such a data set can be used to compare approaches from different model families, for example, a random forest and a neural network. It can be used to measure model performance against known conditions (typical and atypical) to ensure a model works properly.\n\n\nExample: Computer Vision\n\n\nImageNet is a data set of images organized according to the WordNet hierarchy. ImageNet provides an average of 1,000 images for each meaningful concept in WordNet. The data set is used as a benchmark for object categorization algorithms and currently contains over 14 million images that are labeled and annotated by humans.\nThe most used subset of ImageNet data is the Large Scale Visual Recognition Challenge (ILSVRC) data set. It is used to evaluate object classification algorithms since 2010. T he data sets for the challenges are themselves broken down into training, test, and validation sets.\nThe IARPA Janus Benchmark (IJB) data sets contain images and videos used in face detection and face recognition challenges. There are several data sets, for example IJB-B consists of 1,845 subjects with human-labeled face bounding boxes, eye & nose location, and metadata such as skin tone and facial hair for 21,798 still images and 55,026 video frames. The collection methodology for the IJB-B data set is documented .\n\n\nTest data tells us how well a model performs, validation data tells us which model is best.\n\n\nExample: Programming Competition\n\n\nSuppose we want to send one student from a group of students to a programming competition. The goal is to win the competition. During training the students encounter problems from past programming competitions.\nStudents that do well during training are not necessarily the best candidates for the competition. We need to find out whether a student does well because they memorized the solution or whether they truly understand how to solve the programming problem. To answer this a validation step is used and a set of new programming problems is presented, specifically designed to test student’s ability to apply general concepts in problem solving. At the end of the validation step we have identified the best student to represent the group at the competition.\nWe are not done, however. Does the best student in the group have a chance in the competition? We now enter the testing phase to answer the question: how well will the best student perform? After administering a real test with new problems, we find out that the student scores above 90%: they are ready for the competition. If, however, we find out that the student scores below 25%, we will not send them to the competition. Instead, we return to the drawing board with a new training procedure and/or a set of new training problems.\nNotice that in this situation the validation occurs prior to the testing phase. The validation data set is not a random sample of the training problems held back to test the students after training. The validation data set is a specifically designed set of problems that targets problem solving ability while the testing data set is a set of new problems that are akin to what the students might encounter during the competition.\n\n\nValidation and test data are often used interchangeably because the test data is often used as the validation data. The questions “which model is best?” and “how well does the model perform?” are answered simultaneously: the best model is the one that achieves the best metric on the test data set. Often that results in choosing the model with the lowest performance metric on the test data set.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Testing, Validation, Cross-Validation</span>"
    ]
  },
  {
    "objectID": "models/test_cv.html#hold-out-samples",
    "href": "models/test_cv.html#hold-out-samples",
    "title": "21  Testing, Validation, Cross-Validation",
    "section": "21.3 Hold-out Samples",
    "text": "21.3 Hold-out Samples\nLet’s return to the important question: where do we find the test data set?\nMaybe you just happen to have a separate set of data lying around that is just like the training data, but you did not use it. Well, that is highly unlikely.\nTypically, we use the data collected, generated, or available for the study to carve out observations for training and testing. This is called a hold-out sample, a subset of the observations is held back for testing and validation. If we start with \\(n\\) observations, we use \\(n - m\\) observation to train the model (the training data set), and \\(m\\) observations to test/validate the model.\nIn Python you can create this train:test split with the train_test_split() function in sklearn. The following statements load the fitness data from DuckDB into a Pandas DataFrame and split it into two frames of 15 and 16 observations.\n\nimport pandas as pd\nimport duckdb\n\ncon = duckdb.connect(database=\"../ads5064.ddb\")\nfit = con.sql(\"SELECT * FROM fitness\").df()\n\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(fit,random_state=235,train_size=0.5)\ncon.close()\n\nThe random_state= parameter sets the seed for the random number generator. By setting this to a non-zero integer, the random number generator is initialized with the seed value. That does not mean the initial random number is equal to the seed. It means that the initial state of the random number generator, the starting position for generating random numbers, is now determined.\nSetting the seed value of a random number generator makes the selection reproducible. Subsequent runs of the program will produce identical sequences of “random” numbers. We put random in quotes here since no computer generated random number sequence is truly random; these are pseudo-random number generators. Their quality is good enough, however, to treat the generated sequence of numbers as essentially random.\nThe train_size= parameter specifies the proportion of observations in the training set—if the value is between 0 and 1—or the number of observations in the training set—if the value is an integer &gt; 1.\n\ntrain.shape\n\n(15, 7)\n\ntrain.describe()\n\n             Age     Weight     Oxygen  ...  RestPulse   RunPulse    MaxPulse\ncount  15.000000  15.000000  15.000000  ...  15.000000   15.00000   15.000000\nmean   49.666667  75.539333  47.693067  ...  52.733333  171.00000  174.133333\nstd     4.654747   8.076112   4.516180  ...   7.731814   10.96097    9.210760\nmin    40.000000  59.080000  39.203000  ...  40.000000  148.00000  155.000000\n25%    48.000000  70.760000  45.215500  ...  48.000000  166.00000  169.000000\n50%    51.000000  76.320000  46.672000  ...  51.000000  170.00000  172.000000\n75%    53.000000  80.400000  49.772000  ...  58.500000  178.00000  180.500000\nmax    57.000000  91.630000  59.571000  ...  67.000000  186.00000  188.000000\n\n[8 rows x 7 columns]\n\n\n\ntest.shape\n\n(16, 7)\n\ntest.describe()\n\n             Age     Weight     Oxygen  ...  RestPulse    RunPulse    MaxPulse\ncount  16.000000  16.000000  16.000000  ...  16.000000   16.000000   16.000000\nmean   45.812500  79.230625  47.078375  ...  54.125000  168.375000  173.437500\nstd     5.140931   8.415590   6.125977  ...   7.701731    9.721968    9.408994\nmin    38.000000  61.240000  37.388000  ...  45.000000  146.000000  155.000000\n25%    43.750000  73.285000  43.665750  ...  48.000000  162.000000  167.500000\n50%    44.500000  80.170000  47.023500  ...  53.500000  169.000000  174.000000\n75%    48.500000  86.295000  50.040750  ...  59.000000  174.500000  180.000000\nmax    57.000000  91.630000  60.055000  ...  70.000000  186.000000  192.000000\n\n[8 rows x 7 columns]\n\n\nThe two data sets have very similar properties as judged by the descriptive statistics. If the goal is to develop a model that can predict the difficult to measure oxygen intake from easy to measure attributes such as age, weight, and pulse, then we would use the 15 observations in the train frame to fit the model and the 16 observations in the test frame to evaluate the model.\nIf we cull the test data from the overall data, how should we determine an appropriate size for the test data? Here we used a 50:50 split, would it have mattered if we had taken a 20:80 or a 90:10 split? For the two data sets to serve their respective functions, you need enough observations in the training data set to fit the model well enough so it can be tested, and you need enough observations in the test data set to produce a reliable estimate of \\(\\widehat{\\text{MSE}}_\\text{Test}\\). Splits that allocate between 50 and 90% of the observations to the training data set are common in practice.\nWith small training proportions you run the risk that the model cannot be fit and/or that the data does not support the intended model. For example, with a 10:90 train:test split in the fitness example, the training data contains only 3 observations and evaluating the effect of all input variables on oxygen intake is not possible—the model is saturated after three inputs are in the model. With categorical inputs, you need to make sure that the training and test data sets contain all the categories. For example, if you categorize age into four age groups and only three groups are present in the training data after the split, the resulting model no longer applies to a population with four age groups. If the categorical target variable is highly unbalanced, for example when a binary variable has very few events or non-events, then splitting the data can result in training or test data sets that contain only one of the outcomes. Training a model is then not possible and the confusion matrix breaks down.\nFrom this discussion we can glean some general advantages and disadvantages of hold-out test samples.\n\n\n\nTable 21.1: Advantages and disadvantages of hold-out samples generated by random train:test splits.\n\n\n\n\n\n\n\n\n\nAdvantages\nDisadvantages\n\n\n\n\nEasy to do\nInvolves a random selection; results change depending on which observations selected\n\n\nNo separate test data set needed\nPotentially large variability from run to run, especially for noisy data\n\n\nA general method that can be applied regardless of how model performance is measured\nMust decide how large to make the training (test) set\n\n\nReproducible if fixing random number seed\nAn observation is used either for testing or for training\n\n\n\nTends to overestimate the test error compared to cross-validation methods\n\n\n\n\n\n\nThe last two disadvantages in Table 21.1 weigh heavily. Since we cannot rely on the training error for model selection, we are sacrificing observations by excluding them from training. At least we expect then a good estimate of the test error. The reason for overestimating the true test error with a train:test hold-out sample is that models tend to perform worse when trained on fewer observations. Reducing the size of the training data set results in less precise parameter estimates which in turn increases the variability of predictions.\nTo compare the variability of the hold-out sample method with other techniques, we draw on the Auto data set from ISLR2 (James et al. 2021). The data comprise information on fuel mileage and other vehicle attributes of 392 automobiles. Suppose we want to model mileage as a function of horsepower. Figure 21.2 shows the raw data and fits of a linear and quadratic regression model\n\\[\n\\begin{align*}\n\\text{mpg}_{i} &= \\beta_{0} + \\beta_{1}\\text{horsepower}_{i} + \\epsilon_{i}\\\\\n\\text{mpg}_{i} &= \\beta_{0} + \\beta_{1}\\text{horsepower}_{i} + {\\beta_{2}\\text{horsepower}_{i}^{2} + \\epsilon}_{i}\n\\end{align*}\n\\]\n\n\n\n\n\n\nFigure 21.2: Simple linear and quadratic polynomial fit for miles per gallon versus horsepower in Auto data set.\n\n\n\nA simple linear regression—the red line in the figure—does not seem appropriate. The model does not pick up the curvature in the underlying trend. A quadratic model seems more appropriate. Can this be quantified? What about a cubic model\n\\[\n\\text{mpg}_{i} = \\beta_{0} + \\beta_1\\text{horsepower}_i + \\beta_2\\text{horsepower}_i^2 + \\beta_3\\text{horsepower}_i^3 + \\epsilon_{i}\n\\]\nFigure 21.3 shows the hold-out test errors for all polynomial models up to degree 10. The simple linear regression (SLR) model has degree 1 and is shown on the left. The test error is large for the SLR model and for the 10-degree polynomial. The former is biased as can be seen from the previous graph. The latter is too wiggly and leads to a poor test error because of high variability. The test error is minimized for the quadratic model but we note that the test error is also low for degrees 7—9.\n\n\n\n\n\n\nFigure 21.3: Hold-out test errors for polynomial models from first to tenth degree. The horizontal line marks the minimum, achieved at degree 2.\n\n\n\nBased on this result we would probably choose the second-degree polynomial. To what extent is this decision the result of having selected the specific 196 observations in the 50:50 split? We can evaluate this by repeating the sampling process a few more times. Figure 21.4 displays the results of 9 additional 50:50 train:test splits.\nThe variability in the results is considerable. Most replications would select a second-degree polynomial as the model with the lowest test error, but several replications achieve the smallest test error for higher degree polynomials (5th degree, 7th degree, etc.).\n\n\n\n\n\n\nFigure 21.4: Test errors in ten hold-out samples, 50:50 splits. The errors from the previous graph are shown in red.\n\n\n\n\nHaving spent time, energy, resources, money to build a great data set, it seems wasteful to use some observations only for training and the others only for testing. Is there a way in which we can use all observation for training and testing and still get a good estimate (maybe even a better estimate) of the test error?\nHow about the following proposal:\n\nSplit the data 50:50 into sets \\(t_1\\) and \\(t_2\\)\nUse \\(t_1\\) as the training data set and determine the mean-squared prediction error from \\(t_2\\), call this \\(\\widehat{\\text{MSE}}_{(t_2)}\\).\nReverse the roles of \\(t_1\\) and \\(t_2\\), using \\(t_2\\) to train the model and \\(t_1\\) to compute the test error \\(\\widehat{\\text{MSE}}_{(t_1)}\\).\nCompute the overall test error as the average \\[\n\\widehat{\\text{MSE}}_{\\text{Test}} = \\frac{1}{2} \\left (\n\\widehat{\\text{MSE}}_{(t_1)} +\n\\widehat{\\text{MSE}}_{(t_2)} \\right )\n\\]\n\nEach observation is used once for training and once for testing. Because of averaging, the combined estimate of test error is more reliable than the individual test errors.\nThis proposal describes a special case of cross-validation, namely 2-fold cross-validation.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Testing, Validation, Cross-Validation</span>"
    ]
  },
  {
    "objectID": "models/test_cv.html#cross-validation",
    "href": "models/test_cv.html#cross-validation",
    "title": "21  Testing, Validation, Cross-Validation",
    "section": "21.4 Cross-validation",
    "text": "21.4 Cross-validation\nCross-validation is a general method to measure the performance of a model. It is commonly used for predictive models to evaluate how well a model generalizes to new observations, but it can also be used to, for example, select hyperparameters. Cross-validation extends the concept of the hold-out sample to address the drawbacks of train:test splits. It is also a general method; you are not limited to MSE or MCR as performance measurements. Before we dive into cross-validation more deeply, a few words about loss functions.\n\nLoss Functions\n\n\nDefinition: Loss Function\n\n\nA loss function or cost function maps an event to a real number that reflects some loss or cost incurred from the event.\nIn data analytics, loss functions measure the discrepancy between observed and predicted values and the losses are typically referred to as errors.\n\n\nTable 21.2 displays common loss functions in data science.\n\n\n\nTable 21.2: Loss functions common in data science applications. \\(y\\) and \\(\\widehat{y}\\) denote observed and predicted value, respectively. \\(\\widehat{p}_j\\) denotes the sample proportion in category \\(j\\) of a classification problem with \\(k\\) categories.\n\n\n\n\n\n\n\n\n\n\nLoss Function\nExpression\nApplication Example\n\n\n\n\nSquared Error\n\\(\\left( y - \\widehat{y} \\right)^{2}\\)\nRegression with continuous response\n\n\nZero-one (0—1)\n\\(I\\left( y \\neq \\widehat{y} \\right)\\)\nClassification\n\n\nAbsolute Value\n\\(\\left| y - \\widehat{y} \\right|\\)\nRobust regression\n\n\nMisclassification\n\\(1 - \\max_{j}{\\widehat{p}}_{j}\\)\nPruning of decision trees\n\n\nGini Index\n\\(\\sum_{j = 1}^{k}{{\\widehat{p}}_{j}\\left( 1 - {\\widehat{p}}_{j} \\right)}\\)\nGrowing of decision trees, neural networks\n\n\nCross-entropy (Deviance)\n\\(- 2\\sum_{j = 1}^{k}{{n_{j}\\log}{\\widehat{p}}_{j}}\\)\nGrowing of decision trees, neural networks\n\n\nEntropy\n\\(- \\sum_{j = 1}^{k}{{\\widehat{p}}_{j}\\log{\\widehat{p}}_{j}}\\)\nGrowing of decision trees\n\n\n\n\n\n\nSquared error and zero-one loss dominate data science work in regression and classification problems. For specific methods you will find additional loss functions used to optimize a particular aspect of the model, for example, growing and pruning of decision trees or exponential loss in adaptive boosting.\nSuppose the loss associated with an observation is denoted \\(\\mathcal{l}_{i}\\). Cross-validation estimates the average loss for each of \\(k\\) sets of observations and averages the \\(k\\) estimates into an overall cross-validation estimate of the loss.\nSuppose we create two random sets of (near) equal size for the 31 observations in the fitness data set; \\(k = 2\\). The sets will have \\(n_1 = 15\\) and \\(n_2 = 16\\) observations. This leads to a cross-validation estimate of the loss function for each set:\n\\[\n\\begin{align*}\n{CV}_1\\left( \\mathcal{l} \\right) &= \\frac{1}{n_1}\\sum_{i = 1}^{n_1}\\mathcal{l}_i\\\\\n{CV}_2\\left( \\mathcal{l} \\right) &= \\frac{1}{n_2}\\sum_{i = 1}^{n_2}\\mathcal{l}_i\n\\end{align*}\n\\]\nThe overall cross-validation loss is the average of the two:\n\\[\nCV\\left( \\mathcal{l} \\right) = \\frac{1}{2}\\left( {CV}_1\\left( \\mathcal{l} \\right) + {CV}_2\\left( \\mathcal{l} \\right) \\right)\n\\]\nThis is a special case of \\(k\\)-fold cross-validation; the sets are referred to as folds. The other special case is leave-one-out cross-validation.\n\n\n\\(K\\)-fold Cross-validation\nThe set of \\(n\\) observations is divided randomly into \\(k\\) groups of (approximately) equal size. The groups are called the \\(k\\) folds. The model is fit \\(k\\) times, holding out a different fold each time. After computing the loss in each fold\n\\[\n{CV}_j\\left( \\mathcal{l} \\right) = \\frac{1}{n_{j}}\\sum_{i = 1}^{n_j}\\mathcal{l}_i\n\\] the overall loss is calculated as the average\n\\[\nCV\\left( \\mathcal{l} \\right) = \\frac{1}{k}\\sum_{j = 1}^{k}{{CV}_j\\left( \\mathcal{l} \\right)}\n\\]\nFigure 21.5 displays 5-fold cross-validation for \\(n = 100\\) observations. The observations are randomly divided into 5 groups of 20 observations each. The model is trained five times. The first time around, observations in fold 1 serve as the test data set, folds 2—5 serve as the training data set. The second time around, fold 2 serves as the test data set and folds 1, 3, 4, and 5 are the training data set; and so forth. Each time, the average loss is calculated for the 20 observations not included in training. At the end, five average cross-validation losses are averaged to calculate the overall loss.\n\n\n\n\n\n\nFigure 21.5: Example of 5-fold cross-validation for 100 observations. Numbers in the cells represent observation numbers. The records were randomly arranged prior to assigning the folds.\n\n\n\n\n\n\nTable 21.3: Advantages and disadvantages of \\(k\\)-fold cross-validation.\n\n\n\n\n\n\n\n\n\nAdvantages\nDisadvantgages\n\n\n\n\nNot as variable as the train:test hold-out sample\nStill has a random element due to randomly splitting the data into \\(k\\) sets\n\n\nLess bias in test error than train:test hold-out sample\nCan becomputationally intensive if the model must be fit \\(k\\) times\n\n\nNot as computationally intensive as leave-one-out cross-validation (see below)\nMust decide on the number of folds\n\n\nEvery observation is used for training (\\(k - 1\\) times) and testing (once)\n\n\n\nReproducible if fixing random number seed\n\n\n\nA general method that can be applied regardless of how model performance is measured\n\n\n\n\n\n\n\nThe most common values for \\(k\\) found in practice are 5, 10, and \\(n\\). \\(k = n\\) is a special case, called leave-one-out cross-validation; see below. Values of 5 and 10 have shown to lead to good estimates of loss while limiting the variability of the results. The averaging of the losses from the folds has a powerful effect of stabilizing the results.\nFor the Auto data set, the following figures show the results of repeating 5-fold and 10-fold cross-validation ten times. The results vary considerably less than the ten repetitions of the 50:50 hold-out sample in Figure 21.4. This reduced variability demonstrates the effect of averaging the error estimates from the folds.\n\n\n\n\n\n\nFigure 21.6: Ten repetitions of 5-fold cross-validation for polynomials of degree 1—10; Auto data set.\n\n\n\n\n\n\n\n\n\nFigure 21.7: Ten repetitions of 10-fold cross-validation for polynomials of degree 1—10; Auto data set.\n\n\n\nThe results of 10-fold cross-validation vary less than those of 5-fold CV (compare Figure 21.7 to Figure 21.6). This is the effect of averaging 10 quantities rather than 5. The effect of averaging the results from the folds is stronger than the averaging of observations within the folds. If training a model is computationally intensive, 5-fold cross-validation is still a good solution.\n\n\nLeave-One-Out Cross-validation\nAbbreviated LOOCV, this method removes the random element from cross-validation. Rather than randomly grouping the data set into \\(k\\) folds, each observation functions as a fold with a single element. In other words, LOOCV is a special case of \\(k\\)-fold CV where \\(k = n\\). Each observation is used once as a test set of size 1 and the model is fit to the remaining \\(n - 1\\) observations. The observation is put back and the next observation is removed from the training set.\nLOOCV thus estimates the model \\(n\\) times, each time removing one of the observations. It is a special case of \\(k\\)-fold cross-validation where \\(k = n\\).\nA pseudo-algorithm for LOOCV is as follows:\nStep 0: Set \\(i = 1\\)\nStep 1: Set the index of the hold-out observation to \\(i\\)\nStep 2. Remove observation \\(i\\) and fit the model to the remaining \\(n - 1\\) observations\nStep 3. Compute the loss \\(\\mathcal{l}_{i}\\) for the held-out observation\nStep 4. Put the observation back into the data. If \\(i = n\\), go to Step 5. Otherwise, increment \\(i\\) and return to Step 1.\nStep 5. Compute the LOOCV loss as the average of the \\(n\\) losses: \\[\n{CV}\\left( \\mathcal{l} \\right) = \\frac{1}{n}\\sum_{i=1}^n\\mathcal{l}_{i}\n\\]\n\n\n\nTable 21.4: Advantages and disadvantage of leave-one-out cross-validation.\n\n\n\n\n\n\n\n\n\nAdvantages\nDisadvantages\n\n\n\n\nNo randomness involved. Identical results upon repetition.\nCan become computationally intensive if fitting a model is expensive and no closed-form expressions (or approximations) are available to compute the loss per observation based on a single fit\n\n\nEvery observation is used in training (\\(n - 1\\) times) and in testing (once)\n\n\n\nA general method that can be applied to any loss function and model\n\n\n\nGood estimate of test error\n\n\n\n\n\n\n\nThe results of LOOCV for the Auto data set are shown in Figure 21.8. Based on LOOCV we would select the seventh-degree polynomial.\n\n\n\n\n\n\nFigure 21.8: Leave-one-out cross-validation for polynomials in the Auto data set.\n\n\n\nTraining a model \\(n\\) times on \\(n-1\\) observations each can be computationally demanding. LOOCV thus might not be practically feasible for some models. Fortunately, the leave-one-out cross-validation error can be computed for important model families without fitting the model \\(n\\) times. For linear regression models, for example, formulas exist to compute the leave-one-out prediction error from information available after training the model once on all observations. Wait, what?\nSuppose we are predicting the target value of the \\(i\\)th observation in the LOO step when that observation is not in the training set and denote this predicted value as \\(\\widehat{y}_{-i}\\). The LOO cross-validation error using a squared error loss function is then\n\\[\n\\frac{1}{n}\\sum_{i = 1}^{n}\\left( y_{i} - {\\widehat{y}}_{- i} \\right)^{2}\n\\]\nThe sum in this expression is called the PRESS statistic (for prediction sum of squares). The interesting result is that \\(\\widehat{y}_{-i}\\) can be calculated as\n\\[\ny_{i} - \\widehat{y}_{-i} = \\frac{y_i - \\widehat{y}_i}{1 - h_{ii}}\n\\tag{21.1}\\]\nwhere \\(h_{ii}\\) is the leverage of the \\(i\\)th observation. We will discuss the leverage in more detail in the context of linear model diagnostics. At this point it is sufficient to note that the leverage measures how unusual an observation is with respect to the input variables of the model and that \\(0 &lt; h_{ii} &lt; 1\\).\nThe term in the numerator of Equation 21.1 is the regular residual for \\(y_{i}\\). We can thus calculate the leave-one-out prediction error from the difference between observed and predicted values in the full training data by adjusting for the leverage. Since \\(0 &lt; h_{ii} &lt; 1\\), it follows that\n\\[\ny_{i} - \\widehat{y}_{- i} &gt; y_{i} - \\widehat{y}_i\n\\]\nPredicting an observation that was not used in training the model cannot be more precise than predicting the observation if it is part of the training set.\n\nWe are now equipped with a number of approaches to estimate the test error: a separate test data set, a hold-out sample, \\(k\\)-fold cross-validation, and leave-one-out cross-validation. But wait, there is more.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Testing, Validation, Cross-Validation</span>"
    ]
  },
  {
    "objectID": "models/test_cv.html#out-of-bag-error",
    "href": "models/test_cv.html#out-of-bag-error",
    "title": "21  Testing, Validation, Cross-Validation",
    "section": "21.5 Out-of-bag Error",
    "text": "21.5 Out-of-bag Error\nSome data science models are based on applying a basic learning algorithm repeatedly to samples drawn from the training data. The trick is that the samples are of the same size as the training data and that they are drawn with replacement. Suppose that your training data consists of \\(n=20\\) observations. A sample of the same size drawn with replacement could be the following observation numbers:\n\nset.seed(234)\nsample(seq(1,20),replace=TRUE)\n\n [1]  1  2 14  2 18 13  1  4  4 15 14 11 18 10 13  8  1 16  3 19\n\n\nThere are still \\(n=20\\) observations in the sample, but not all of the original observations are present. Instead, observation #1 appears three times, #2 appears twice, observation #4 appears twice, #14 appears twice, and so on.\nSuch a sample of the same size as the original data, drawn with replacement, is called a bootstrap sample. The procedure to draw multiple bootstrap samples and to apply a learning method to each is called bootstrapping. It is a defining characteristic of important statistical learning methods such as bagged trees and random forests. Like cross-validation, bootstrapping is a general method that can be applied more widely.\nWe mention bootstrapping here because it naturally lends itself to estimating the test error of a model. Since sampling is with replacement, some observations will appear once in the bootstrap sample, some will appear more than once, and others will not appear at all.\nConsider a single bootstrap sample from a data set with \\(n\\) observations. The probability that the \\(n\\)th observation selected into the bootstrap sample is the \\(j\\)th observation in the data frame is \\(1/n\\), because sampling is with replacement. (The probability that a particular observation is selected at any step is \\(1/n\\).) The complement, the probability that the \\(n\\)th bootstrap observation is not the \\(j\\)th obs is \\(1-1/n\\). It follows that in a bootstrap sample of size \\(n\\), the probability that a particular observation is not included is \\[\n(1 - 1/n)^n \\approx 1/3\n\\]\nAs \\(n\\) grows, this probability quickly approaches 1/3. Figure 21.9 displays \\((1-1/n)^n\\) as a function of \\(n\\). The asymptote near 1/3 is reached for small data set sizes (&gt; 10).\n\n\n\n\n\n\n\n\nFigure 21.9: Probability that an observation is excluded from a bootstrap sample.\n\n\n\n\n\nA bootstrap sample will contain about 2/3 of the observations from the original data set, some with repeated values. The other third of the observations are a natural hold-out sample for this bootstrap sample. The bootstrap procedure provides a mechanism to estimate the test error similar to a cross-validation procedure.\nTwo methods come to mind to use the bootstrap samples to estimate the test error based on bootstrap samples.\nThe first method computes the test error based on the \\(m\\) observations not included in a particular bootstrap sample. This set of observations is called the out-of-bag set. If the criterion is mean-squared prediction error, this would yield for the \\(j\\)th bootstrap sample \\[\n\\widehat{\\text{MSE}}_{\\text{Test}}^{(j)} = \\frac{1}{m} \\sum_{k=1}^m (y_k - \\widehat{y}_k)^2\n\\] Repeats this for each of the \\(B\\) bootstrap samples and compute the overall test error estimate as the average of the \\(B\\) out-of-bag errors: \\[\n\\widehat{\\text{MSE}}_{\\text{Test}} = \\frac{1}{B}\\sum_{j=1}^B \\widehat{\\text{MSE}}_{\\text{Test}}^{(j)}\n\\]\nThe second method of computing the out-of-bag error is to compute the predicted value for an observation whenever it is out-of-bag. This yields about \\(B/3\\) predicted values for each observation, and when averaged, an overall out-of-bag prediction for \\(y_i\\). The overall error is then computed from the \\(n\\) out-of-bag predictions. This estimate, for \\(B\\) sufficiently large, is equivalent to the leave-one-out prediction error, but it is not identical to the leave-one-out error because bootstrapping involves a random element and LOOCV is deterministic.\nThe following function computes the out-of-bag error estimates both ways for the Auto data and the model \\[\n\\text{mpg} = \\beta_0 + \\beta_1\\text{horsepower} + \\beta_2\\text{horsepower}^2 + \\epsilon\n\\] and compares them to the LOOCV error:\n\nComputing \\(\\widehat{\\text{MSE}}_{\\text{Test}}^{(j)}\\) for each bootstrap sample and averaging those (a mean across \\(B\\) quantities)\nAveraging the individual \\(B/3\\) out-of-bag predictions and computing the mean of those (a mean across \\(n\\) quantities)\n\n\nlibrary(ISLR2)\ndata(Auto)\n\nOOB_error &lt;- function(B=1000) {\n    n &lt;- dim(Auto)[1]\n\n    # Compute LOOCV error first\n    reg &lt;- lm(mpg ~ poly(horsepower,2), data=Auto)\n    leverage &lt;- hatvalues(reg)\n    PRESS_res &lt;- reg$residuals / (1-leverage)\n    PRESS &lt;- sum(PRESS_res^2)\n    loocv_error &lt;- PRESS/length(leverage);\n\n    ind &lt;- seq(1,n,1)\n    MSE_Te &lt;- 0\n    oob_preds &lt;- matrix(0,nrow=n,ncol=2)\n    # draw the bootstrap samples\n    for(i in 1:B) {\n        bs &lt;- sample(n,n,replace=TRUE) # replace=TRUE is important here!\n        oob_ind &lt;- !(ind %in% bs)  # the index of out-of-bag observations\n        reg &lt;- lm(mpg ~ poly(horsepower,2), data=Auto[bs,])\n        # predict the response for the out-of-bag observations\n        oob_pred &lt;- predict(reg,newdata=Auto[oob_ind,])\n        # accumulate predictions of the out-of-bag observations\n        oob_preds[oob_ind,1] &lt;- oob_preds[oob_ind,1] + oob_pred\n        oob_preds[oob_ind,2] &lt;- oob_preds[oob_ind,2] + 1\n        # Accumulate mean-square prediction errors in the jth bootstrap sample\n        MSE_Te &lt;- MSE_Te + mean((oob_pred - Auto[oob_ind,\"mpg\"])^2)\n    }\n    # Average the MSE_Te^(j) across the B samples\n    MSE_Te &lt;- MSE_Te / B\n\n    # Compute the average predictions for the n observations\n    # oobs_preds[,2] will be approximately B/3 for each observation\n    oob_preds[,1] &lt;- oob_preds[,1] / oob_preds[,2]\n    oob_error &lt;- mean((oob_preds[,1]-Auto[,\"mpg\"])^2)\n\n    return(list(MSE_Te=MSE_Te, OOB_error=oob_error, LOOCV=loocv_error))\n}\n\nset.seed(765)\noe &lt;- OOB_error(B=1000)\noe\n\n$MSE_Te\n[1] 19.44796\n\n$OOB_error\n[1] 19.24568\n\n$LOOCV\n[1] 19.24821\n\n\nThe OOB_error estimate based on the averaged predictions is very close to the leave-one-out prediction error.\n\n\n\nFigure 21.5: Example of 5-fold cross-validation for 100 observations. Numbers in the cells represent observation numbers. The records were randomly arranged prior to assigning the folds.\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r, 2nd Ed. Springer. https://www.statlearning.com/.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Testing, Validation, Cross-Validation</span>"
    ]
  },
  {
    "objectID": "models/messydata.html",
    "href": "models/messydata.html",
    "title": "22  Messy Data",
    "section": "",
    "text": "22.1 Low Signal to Noise",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Messy Data</span>"
    ]
  },
  {
    "objectID": "models/messydata.html#unbalanced-data",
    "href": "models/messydata.html#unbalanced-data",
    "title": "22  Messy Data",
    "section": "22.2 Unbalanced Data",
    "text": "22.2 Unbalanced Data",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Messy Data</span>"
    ]
  },
  {
    "objectID": "models/messydata.html#outliers",
    "href": "models/messydata.html#outliers",
    "title": "22  Messy Data",
    "section": "22.3 Outliers",
    "text": "22.3 Outliers",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Messy Data</span>"
    ]
  },
  {
    "objectID": "models/messydata.html#missing-data",
    "href": "models/messydata.html#missing-data",
    "title": "22  Messy Data",
    "section": "22.4 Missing Data",
    "text": "22.4 Missing Data",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Messy Data</span>"
    ]
  },
  {
    "objectID": "models/featproc.html",
    "href": "models/featproc.html",
    "title": "23  Feature and Target Processing",
    "section": "",
    "text": "23.1 Standardization",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Feature and Target Processing</span>"
    ]
  },
  {
    "objectID": "models/featproc.html#standardization",
    "href": "models/featproc.html#standardization",
    "title": "23  Feature and Target Processing",
    "section": "",
    "text": "Centering and Scaling\nStandardizing a variable \\(X\\) comprises two steps, centering and scaling. Scaling does what it says, it replaces \\(x_i\\) with \\(c\\times x_i\\) where \\(c\\) is the scaling factor. Centering means shifting the variable so that it is centered at some specific value. The common parameters of standardization are centering at the mean \\(\\overline{y}\\) and scaling with the standard deviation \\(s_x\\): \\[\nz = \\frac{x-\\overline{x}}{s_x}\n\\] The transformed variable has a sample mean of 0 and a sample standard deviation of 1: \\[\n\\overline{z} = 0 \\qquad s_z = 1\n\\] Figure 23.1 shows the effect of standardization of 1,000 observations drawn from a Gamma(3,2) distribution. Centering shifts the data to have mean zero and compresses the dispersion of the data to a standard deviation of one.\n\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n\nnp.random.seed(234)\nx = np.random.gamma(shape=3,scale=2,size=1000)\nx = np.reshape(x,(-1,1))\n\nstder = preprocessing.StandardScaler().fit(x)\n\nstder.mean_\n\narray([6.02121806])\n\nstder.scale_\n\narray([3.47094438])\n\nx_scaled = stder.transform(x)\nplt.figure()\nplt.hist(x,bins=20,color='0.7',alpha=0.7,label='Original')\nplt.hist(x_scaled,color='magenta',alpha=0.5,label='Standardized')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFigure 23.1: Original and standardized data drawn from a Gamma(3,2) distribution.\n\n\n\n\n\nThe mean (center) of the original data is 6.021 and the scale applied by StandardScaler is 3.471. Note that this standard deviation calculation is based on np.std() which by default uses \\(n\\) instead of \\(n-1\\) as the denominator–using \\(n\\) instead of \\(n-1\\) in the calculation of a sample variance yields a biased estimator of the population variance.\n\n\n\n\n\n\nScaling called Standardization\n\n\n\nYou will find references in the literature and in software documentation to “scaling the data”. It is often understood to mean standardization of the data, that is, centering and scaling.\nBe also aware of the default behavior of software packages. The prcomp function for principal component analysis in R, for example, centers the data by default but does not scale by default. The loess function in R normalizes the predictor variables, using scaling to a robust estimate of the standard deviation, but only if there is more than one predictor. The glmnet::glmnet function standardizes the inputs by default (standardize=TRUE) and reports the results on the original scale of the predictors.\n\n\n\n\nRange Scaling\nThis form of scaling of the data transforms the data so it falls between a known lower and upper bound, often 0 and 1. Suppose that \\(\\text{min}(X) \\le X \\le \\text{max}(X)\\) and we want to create a variable \\(z_{\\text{min}} \\le Z \\le z_{\\text{max}}\\) from \\(X\\). \\(Z\\) can be computed by scaling and shifting a standardized form of \\(X\\): \\[\n\\begin{align*}\n  x^* &= \\frac{x-\\min(x)}{\\max(x)-\\min(x)} \\\\\n  z &= z_{\\text{min}} + x^* \\times (z_{\\text{max}} - z_{\\text{min}})\n\\end{align*}\n\\]\nThe following Python code scales the Gamma(3,2) sample to lie between 0 and 2.\n\nrange_scaler = preprocessing.MinMaxScaler(feature_range=(0,2)).fit(x)\n\nprint(range_scaler.data_min_)\n\n[0.25157731]\n\nprint(range_scaler.data_max_)\n\n[21.55883927]\n\nx_range_scaled = range_scaler.transform(x)\n\nx_range_scaled.min()\n\n0.0\n\nx_range_scaled.max()\n\n2.0\n\n\nAn advantage of range scaling is that the same range can be applied to training and test data. This makes it easy to process test data prior to applying a model derived from range-scaled data.\n\n\nTo Scale or not to Scale\nSome data scientists standardize the data by default. When should you standardize and when should you not? Analytic techniques that depend on measures of distance to express similarity or dissimilarity between observations, or that depend on partitioning variability, typically benefit from standardized input variables.\nExamples of the former group are \\(K\\)-means clustering and hierarchical clustering. Here, the data are grouped into clusters based on their similarity, which is expressed as a function of a distance measure. Without scaling the data, large items tend to be further apart than small items, distorting the analysis toward inputs with large scale. Whether lengths are measured in meters, centimeters, or inches should not affect the analysis. Principal component analysis (PCA) is an example of a method in the second group, apportioning variability to linear combinations of the input variables. Without scaling, inputs that have a large variance can dominate the PCA to the point of not being interpretable or meaningful. Larger things tend to have larger variability compared to small things.\nBinary variables, are usually not scaled. This includes variables derived from encoding factor variables, see Section 23.3.1.\n\n\n\n\n\n\nTrain and Test Data\n\n\n\nAny preprocessing steps such as standardization, scaling, normalization, are applied separately to train and test data set. In the case of standardization, for example, it means that you split the data into train and test data sets first, then standardize each separately. The training data set is centered and scaled by the means and sample standard deviations in the training data set. The test data set is centered and scaled by the means and standard deviations in the test data set. Both data sets thus have the same properties of zero mean and standard deviation one.\nIf we were to standardize first and split into training and test data set second, neither data set would have zero mean and unit standard deviation. Furthermore, information from the training data (the mean and standard deviation) would leak into the test data set.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Feature and Target Processing</span>"
    ]
  },
  {
    "objectID": "models/featproc.html#normalization",
    "href": "models/featproc.html#normalization",
    "title": "23  Feature and Target Processing",
    "section": "23.2 Normalization",
    "text": "23.2 Normalization\nNormalization is sometimes (often, actually) described as a transformation of data to normality; the transformed data appears Gaussian distributed. This is not what we consider normalization here. A variable \\(z\\) that is normalized with respect to the \\(L_2\\) norm has the property \\[\n||\\textbf{z}|| = \\sqrt{\\sum_{i=1}^n z_i^2}  = 1\n\\]\nA variable \\(x\\) can be normalized with respect to any norm by dividing each value by the norm of the vector. In the case of \\(L_2\\), \\[\nz_i = \\frac{x_i}{||\\textbf{x}||}\n\\]\nNormalization is a special case of scaling so that the resulting vector has unit length with respect to some norm. Any norm can be used to normalize the data, for example, the \\(L_1\\) norm \\(|\\textbf{x}|\\). Note that scaling by the standard deviation does not yield a vector with unit length, but a vector with unit standard deviation.\nThe following code normalizes the Gamma(3,2) sample with respect to \\(L_2\\).\n\nx_norm_l2 = preprocessing.normalize(x, norm='l2', axis=0)\n\nplt.figure()\nplt.hist(x_norm_l2,bins=20,color='0.7',alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\nFigure 23.2: Normalized data from the Gamma(3,2) distribution.\n\n\n\n\n\nNormalization of the sample from the Gamma(3,2) distribution maintains the general shape of the distribution and changes its scale (Figure 23.2).",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Feature and Target Processing</span>"
    ]
  },
  {
    "objectID": "models/featproc.html#sec-encoding",
    "href": "models/featproc.html#sec-encoding",
    "title": "23  Feature and Target Processing",
    "section": "23.3 Encoding",
    "text": "23.3 Encoding\nData processed by algorithms are represented as numbers, integers and real numbers. The information we collect is often not in the form that can be directly processed by a data science algorithm. Treatments in an experiment could be identified with numeric values, “1”, “2”, “3”, etc. We need to somehow tell the algorithm that these are simply identifying labels and should not be treated as numbers for which differences are meaningful. Treatment “2” is simply different from treatment “1”, it is not necessarily twice as much as treatment “1”. Whether the treatments are labeled “1”, “2”, “3”, or “A”, “B”, “C”, the same analysis must result.\nMany pieces of information come to us as in the form of character strings, either as structured or as unstructured text. A case of structured text data is when we use strings as labels for things. The treatment labels “A”, “B”, “C” are an example. An example of unstructured text is free-form text such as the quote\n\nThe strength of a bureaucracy is measured by its ability to resist change.\n\nAn algorithm that predicts the sentiment of the quote must translate these characters into a numerical representation. The process of transforming data from a human-readable form into a machine-readable form is known as encoding in machine learning. The reverse, translating from a machine-readable to a human-readable representation is known as decoding.\nIn this section we take a somewhat broader view of data encoding.\n\n\nDefinition: Data Encoding and Decoding\n\n\nEncoding of data is the process of translating information into a numerical format for processing by a statistical or machine learning algorithm.\nDecoding is translating information from the numerical format used in analytic processing back into human-readable form.\n\n\nEncoding is more important to us at this stage. When dealing with natural language processing tasks such as sequence-to-sequence modeling, encoding and decoding are important. An example is the translation of text from one language into another using artificiall neural networks. The input text is encoded into numeric format, processed by a network, and the output of that network is decoded by another network into the target language of translation.\n\nFactor Variables\nThe first case of encoding information is the processing of factor input variables. Factors are categorical variables representing levels (or classes, or categories, or states). For example, the variable treatment with levels “A”, “B”, and “C” is a three-level factor variable. When this variable is used in a statistical model, it enters the model through its levels, not through its values. That is why some software refers to the process of encoding factor variables as levelization.\nTo encode factor variables, three decisions affect the representation of the variable and the interpretation of the results:\n\nThe order of the levels\nThe method of encoding\nThe choice of reference level—if any\n\nThe order of the levels is important because it affects the order in which the factor levels enter the model and how the \\(\\textbf{X}\\) matrix is constructed. Consider the following example, which we will use throughout this section, of two factors with 2 and 3 levels, respectively, and a numeric variable \\(X\\).\n\n\n\nTable 23.2: Data with two factors \\(A\\) and \\(B\\) with 2 and 3 levels, respectively, and a numeric variable. To distinguish the values of the variables from subsequent encodings, the values are shown in boldface.\n\n\n\n\n\nY\nA\nB\n\\(X\\)\n\n\n\n\n1\n1\n1\n1\n\n\n2\n1\n2\n0.5\n\n\n3\n1\n3\n0.25\n\n\n4\n2\n1\n1\n\n\n5\n2\n2\n0.5\n\n\n6\n2\n3\n0.25\n\n\n\n\n\n\n\nOne-hot encoding\nIf we choose one-hot encoding, also called standard encoding or dummy encoding, then the \\(k\\) levels of each variable are transformed into \\(k\\) columns of 0/1 values. The value in the \\(j\\)th column is 1 if the variable is at level \\(j\\), 0 otherwise. The one-hot encoding for \\(A\\) and \\(B\\) in Table 23.2 is shown in Table 23.3.\n\n\n\nTable 23.3: One-hot encoding of \\(A\\) and \\(B\\).\n\n\n\n\n\nA\n\\(A_1\\)\n\\(A_2\\)\nB\n\\(B_1\\)\n\\(B_2\\)\n\\(B_3\\)\n\n\n\n\n1\n1\n0\n1\n1\n0\n0\n\n\n1\n1\n0\n2\n0\n1\n0\n\n\n1\n1\n0\n3\n0\n0\n1\n\n\n2\n0\n1\n1\n1\n0\n0\n\n\n2\n0\n1\n2\n0\n1\n0\n\n\n2\n0\n1\n3\n0\n0\n1\n\n\n\n\n\n\n\\(A_1\\) and \\(A_2\\) are the first and second columns of the encoded 2-level factor \\(A\\). So why does the order in which these columns enter a statistical model matter? Suppose our model is a linear model with inputs \\(X\\), a numeric variable, and \\(A\\) (\\(\\textbf{x} = [x, A_1, A_3]\\)): \\[\nf(\\textbf{x}) = \\beta_0 + \\beta_1x + \\beta_2 A_1 + \\beta_2 A_2\n\\] For the six observations in Table 23.2, the \\(\\textbf{X}\\) matrix of the model is \\[\n\\textbf{X} = \\left [\\begin{array}{rrrr}\n1 & 1    & 1 & 0 \\\\\n1 & 0.5  & 1 & 0 \\\\\n1 & 0.25 & 1 & 0 \\\\\n1 & 1    & 0 & 1 \\\\\n1 & 0.5  & 0 & 1 \\\\\n1 & 0.25 & 0 & 1 \\\\\n\\end{array} \\right]\n\\] There is a problem, the first column of \\(\\textbf{X}\\) is the sum of the third and forth column. This perfect linear dependency makes the \\(\\textbf{X}\\) matrix singular, the \\(\\textbf{X}^\\prime\\textbf{X}\\) matrix cannot be inverted, and the ordinary least squares solution cannot be computed.\n\nimport numpy as np\nX = np.array([[1,1,1,0], [1,0.5,1,0], [1,0.25,1,0], [1,1,0,1], [1, 0.5,0,1], [1,0.25,0,1]])\n\nXpX = X.transpose() @ X\nprint(XpX)\n\n[[6.    3.5   3.    3.   ]\n [3.5   2.625 1.75  1.75 ]\n [3.    1.75  3.    0.   ]\n [3.    1.75  0.    3.   ]]\n\nnp.linalg.inv(XpX)\n\nnumpy.linalg.LinAlgError: Singular matrix\n\n\nTo remedy this problem many software packages implicitly make one of the levels of a one-hot encoded factor a reference level. For example, R takes the first level of a factor as the reference level. The following code creates a data frame from the data in Table 23.2, declares the numeric variable A as a factor and assigns the second level as the reference level.\n\ndf &lt;- data.frame(y=c(1,2,3,4,5,6),\n                 x=c(1,0.5,0.25,1,0.5,0.25),\n                 A=c(1,1,1,2,2,2))\n\ndf$A &lt;- relevel(as.factor(df$A),ref=2)\ncoef(summary(lm(y ~ x + A, data=df)))\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  6.500000  0.2089277  31.11124 7.296355e-05\nx           -2.571429  0.2857143  -9.00000 2.895812e-03\nA1          -3.000000  0.1781742 -16.83746 4.561983e-04\n\n\nThe analysis now succeeds but the coefficients reported seem to be for a model without the reference level, namely \\[\nf(\\textbf{x}) = \\beta_0 + \\beta_1x + \\beta_2 A_1\n\\]\nYou can easily verify by specifying this model directly:\n\ndf2 &lt;- data.frame(y=c(1,2,3,4,5,6),\n                  x=c(1,0.5,0.25,1,0.5,0.25),\n                  A1=c(1,1,1,0,0,0))\n\ncoef(summary(lm(y ~ x + A1, data=df2)))\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  6.500000  0.2089277  31.11124 7.296355e-05\nx           -2.571429  0.2857143  -9.00000 2.895812e-03\nA1          -3.000000  0.1781742 -16.83746 4.561983e-04\n\n\nSo where did the information about \\(A_2\\) end up? To see how this works, consider the model formula for the cases \\(A_1 = 0\\) and \\(A_1 = 1\\):\n\\[\\begin{align*}\n    A_1 = 0 &: f(\\textbf{x}) = \\beta_0 + \\beta_1 x \\\\\n    A_1 = 1 &: f(\\textbf{x}) = \\beta_0 + \\beta_1 x + \\beta_2 A_1\n\\end{align*}\\]\nBut the value \\(A_1=0\\) corresponds to \\(A_2 = 1\\). The simple linear regression \\(\\beta_0 + \\beta_1 x\\) is the response for the case where \\(A_2 = 1\\). The coefficient \\(\\beta_2\\) now measures the difference between the two levels of \\(A\\), with \\(A = 2\\) serving as the reference level. The model has two intercepts, one for \\(A=1\\), and one for \\(A=2\\), their values are \\(6.5 - 3 = 3.5\\) and \\(6.5\\), respectively.\nYou can also see this from the coefficient estimate in the R output: the sample mean of \\(Y\\) when \\(A = 1\\) is \\(\\overline{y}_{A=1} = 3\\) and \\(\\overline{y}_{A=2} = 6\\). Their difference is \\(\\widehat{\\beta}_2 = -3\\).\nIf the first level is chosen as the reference level, this estimate has the opposite sign:\n\ndf &lt;- data.frame(y=c(1,2,3,4,5,6),\n                 x=c(1,0.5,0.25,1,0.5,0.25),\n                 A=c(1,1,1,2,2,2))\n\ndf$A &lt;- relevel(as.factor(df$A),ref=1)\ncoef(summary(lm(y ~ x + A, data=df)))\n\n             Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept)  3.500000  0.2089277 16.75220 0.0004631393\nx           -2.571429  0.2857143 -9.00000 0.0028958122\nA2           3.000000  0.1781742 16.83746 0.0004561983\n\n\n\\(A=1\\) now serves as the reference level and the coefficient associated with A2 is the difference between the intercept at \\(A=2\\) and at \\(A=1\\). The absolute intercepts of the two level remain the same, \\(3.5\\) for \\(A=1\\) and \\(3.5 + 3 = 6.5\\) for \\(A=2\\).\nWhen a factor has more than two levels, reference levels in one-hot encoding works the same way, the effects of all levels is expressed as the difference of the level with respect to the reference level.\n\n\n\n\n\n\nCaution\n\n\n\nIf a statistical model contains factors and you interpret the coefficient estimates you must make sure to understand the encoding method–including level ordering and reference levels–otherwise you might draw wrong conclusions about the effects in the model. Whn making predictions, the choice of order and reference level does not matter, predictions are invariant, as you can easily verify in the example above.\n\n\nThe issue of a singular cross-product matrix \\(\\textbf{X}^\\prime\\textbf{X}\\) could have been prevented in the model with \\(X\\) and \\(A\\) by dropping the intercept. This model has a full-rank \\(\\textbf{X}\\) matrix \\[\n\\textbf{X} = \\left [\\begin{array}{rrr}\n1    & 1 & 0 \\\\\n0.5  & 1 & 0 \\\\\n0.25 & 1 & 0 \\\\\n1    & 0 & 1 \\\\\n0.5  & 0 & 1 \\\\\n0.25 & 0 & 1 \\\\\n\\end{array} \\right]\n\\]\nEven without an intercept, the problem resurfaces as soon as another factor enters the model. Suppose we now add \\(B\\) in one-hot encoding to the model. The \\(\\textbf{X}\\) matrix gets three more columns:\n\\[\n\\textbf{X} = \\left [\\begin{array}{rrrrrr}\n1    & 1 & 0 & 1 & 0 & 0\\\\\n0.5  & 1 & 0 & 0 & 1 & 0\\\\\n0.25 & 1 & 0 & 0 & 0 & 1\\\\\n1    & 0 & 1 & 1 & 0 & 0\\\\\n0.5  & 0 & 1 & 0 & 1 & 0\\\\\n0.25 & 0 & 1 & 0 & 0 & 1\\\\\n\\end{array} \\right]\n\\] Again, we end up with perfect linear dependencies among the columns. The sum of columns 2–3 equals the sum of columns 4–6. It is thus customary to not drop the intercept when factors are present in the model.\n\n\nAverage effect encoding\nAn encoding that avoids the singularity issue creates only \\(k-1\\) columns from the \\(k\\) levels of the factor and expresses the effect of the levels as differences in the effects from the average effect across all levels. This encoding also relies on the choice of a reference level. Suppose we apply average effect encoding to \\(B\\) and choose \\(B=3\\) as the reference level.\n\nAverage effect encoding for \\(B\\) with reference level \\(B=3\\).\n\n\nB\n\\(B_1\\)\n\\(B_2\\)\n\n\n\n\n1\n\\(1\\)\n\\(0\\)\n\n\n2\n\\(0\\)\n\\(1\\)\n\n\n3\n\\(-1\\)\n\\(-1\\)\n\n\n1\n\\(1\\)\n\\(0\\)\n\n\n2\n\\(0\\)\n\\(1\\)\n\n\n3\n\\(-1\\)\n\\(-1\\)\n\n\n\n\n\nBaseline encoding\nIn this encoding, \\(k-1\\) columns are created for the \\(k\\) levels of the factor. The first level is taken as the baseline and the effects are expressed in terms of differences between successive levels. Baseline encoding of \\(B\\) results in the following encoding:\n\nBaseline encoding for \\(B\\) with reference level \\(B=3\\).\n\n\nB\n\\(B_1\\)\n\\(B_2\\)\n\n\n\n\n1\n\\(0\\)\n\\(0\\)\n\n\n2\n\\(1\\)\n\\(0\\)\n\n\n3\n\\(1\\)\n\\(1\\)\n\n\n1\n\\(0\\)\n\\(0\\)\n\n\n2\n\\(1\\)\n\\(0\\)\n\n\n3\n\\(1\\)\n\\(1\\)\n\n\n\n\n\nExample: Baseline and Average Encoding\n\n\nFor our little data set, we compute the baseline and average effect encoding here. Before doing so, let’s fit a model with factor \\(B\\), and without intercept. The coefficients associated with \\(B\\) are the raw effects of the three levels of \\(B\\). The intercept is automatically added in model formula expressions in R. You can remove it with -1 from the model.\n\ndf &lt;- data.frame(y=c(1,2,3,4,5,6),\n                 x=c(1,0.5,0.25,1,0.5,0.25),\n                 A=c(1,1,1,2,2,2),\n                 B=c(1,2,3,1,2,3))\n\nmodB &lt;- lm(y ~ as.factor(B) -1, data=df)\ncoef(summary(modB))\n\n              Estimate Std. Error  t value   Pr(&gt;|t|)\nas.factor(B)1      2.5        1.5 1.666667 0.19417135\nas.factor(B)2      3.5        1.5 2.333333 0.10183797\nas.factor(B)3      4.5        1.5 3.000000 0.05766889\n\n\nThe coefficients of the factor effects are \\(\\widehat{\\beta}_{B1} =\\) 2.5, \\(\\widehat{\\beta}_{B2} =\\) 3.5, and \\(\\widehat{\\beta}_{B3} =\\) 4.5.\nNext we fit the model using baseline encoding for \\(B\\).\n\nX &lt;- matrix(c(0,1,1,0,1,1,0,0,1,0,0,1),nrow=6,ncol=2)\n\nmodB_baseline &lt;- lm(df$y ~ X)\n\ncoef(summary(modB_baseline))\n\n            Estimate Std. Error   t value  Pr(&gt;|t|)\n(Intercept)      2.5    1.50000 1.6666667 0.1941713\nX1               1.0    2.12132 0.4714045 0.6695150\nX2               1.0    2.12132 0.4714045 0.6695150\n\n\nThe (Intercept) of this model represents the level \\(B=1\\), X1 is the difference of level \\(B=2\\) to level \\(B=1\\) and X2 measures the difference between \\(B=3\\) and \\(B=2\\).\nIf \\(B\\) is expressed in average effect encoding we obtain the following:\n\nX &lt;- matrix(c(1,0,-1,1,0,-1,0,1,-1,0,1,-1),nrow=6,ncol=2)\n\nmodB_ave_eff &lt;- lm(df$y ~ X)\n\ncoef(summary(modB_ave_eff))\n\n                 Estimate Std. Error       t value   Pr(&gt;|t|)\n(Intercept)  3.500000e+00  0.8660254  4.041452e+00 0.02726185\nX1          -1.000000e+00  1.2247449 -8.164966e-01 0.47402139\nX2          -2.110977e-16  1.2247449 -1.723605e-16 1.00000000\n\n\nWe see from the initial analysis that the average effect of \\(B\\) is \\(1/3 \\times (2.5+3.5+4.5) = 3.5\\). The (Intercept) estimate represents this average in the average effect encoding. X1 and X2 represent the differences of \\(B=1\\) and \\(B=2\\) from the average. If we place the \\(-1\\) row at level \\(B=2\\), then X1 and X2 are the differences of \\(B=1\\) and \\(B=3\\) from the average effect of \\(B\\):\n\nX2 &lt;- matrix(c(1,-1,0,1,-1,0,0,-1,1,0,-1,1),nrow=6,ncol=2)\n\nmodB_ave_eff &lt;- lm(df$y ~ X2)\n\ncoef(summary(modB_ave_eff))\n\n            Estimate Std. Error    t value   Pr(&gt;|t|)\n(Intercept)      3.5  0.8660254  4.0414519 0.02726185\nX21             -1.0  1.2247449 -0.8164966 0.47402139\nX22              1.0  1.2247449  0.8164966 0.47402139\n\n\n\n\n\n\n\nContinuous Variables\nWhy would you encode continuous variables, that are already in numerical form?\nSuppose you are modeling a target as a function of time and the measurements are taken in minute intervals. Maybe such a granular resolution is not necessary and you are more interested in seasonal or annual trends. You can convert the time measurements easily into months, seasons, or years, using date-time functions in the software. Should you now treat the converted values as continuous measurements or as categories of a factor?\nWhen working with age variables we sometimes collect them into age groups. Age groups 0-9 years, 10-24 years, and 25-40 years are not evenly spaced and are no longer continuous. Age has been discretized into a factor variable. Other terms used for the process of discretizing continuous variables are binning and quantizing.\nThe advantage of using factors instead of the continuous values lies in the introduction of nonlinear relationships while maintaining interpretability. If you model age as a continuous variable, then \\[\nY = \\beta_0 + \\beta_1 \\text{age} + \\cdots + \\epsilon\n\\] implies a linear relationship between age and the target. Binning age into three groups, on the other hand, leads to the model \\[\nY = \\beta_0 + \\beta_1\\text{[Age=0-9]} + \\beta_2\\text{[Age=10-24]} + \\beta_3\\text{[Age=25-40]} + \\cdots + \\epsilon\n\\] This model allows for different effects in the age groups.\nThe function KBinsDiscretizer in sklearn.preprocessing bins vectors and matrices of continuous variables using different encoding methods. encode='ordinal' returns a vector or matrix of integers identifying the bin a value belongs to. The default binning strategy (strategy='quantile') is to form bins that contain the same number of points.\n\nbinner = preprocessing.KBinsDiscretizer(n_bins=[5], encode='ordinal').fit(x)\n\nx_binned = binner.transform(x)\nx_binned[1:10,]\n\narray([[0.],\n       [2.],\n       [0.],\n       [2.],\n       [4.],\n       [0.],\n       [4.],\n       [4.],\n       [2.]])\n\n\nencode='onehot-dense returns a dense matrix of one-hot encoded variables. The following code uses the uniform binning strategy in which all bins have the same widths.\n\nbinner_q = preprocessing.KBinsDiscretizer(n_bins=[5], \n              encode='onehot-dense',\n              strategy=\"uniform\",\n              subsample=None).fit(x)\n\nx_binned = binner_q.transform(x)\nx_binned[1:10,]\n\narray([[1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 1., 0., 0., 0.]])\n\n\n\n\nUnstructured Text\nIf the values of a variable are structured text in the sense that the values are a limited number of single character strings, the previous encoding methods apply. An example is names of U.S. states or cities, names of active ingredients, medications, places, and so on.\nWhen text is unstructured, it can vary in length and content. How can we encode a sentence such as\n\nThe strength of a bureaucracy is measured by its ability to resist change.\n\nThe encoding itself is just one of possibly many steps in structuring unstructured data.\nParsing and tokenization breaks unstructured text into smaller units (tokens) for further analysis. A token can be a word, a punctuation symbol, an n-gram such as “Covid 19 pandemic”. Entity recognition finds named entities such as places, persons, businesses in text data. Feature extraction pulls relevant characteristics and attributes from text data and forms numeric columns from it. For example, feature extraction finds temperature or precipitation values in weather reports. Normalization transforms unstructured data into a standardized format, removes duplicates, applies consistent rules to spelling, capitalization, etc.\nAfter all that, suppose we have some unstructured data that needs to be encoded.\n\nOne-hot encoding\nOne-hot encoding can be applied to unstructured text as well, but now it is not immediately obvious how many levels to apply in the encoding. Typically, the unstructured text is compared to a dictionary of tokens. The dictionary can be made up of the tokens found in the text to be encoded, or comprise a larger corpus of text. Clearly, dictionaries tend to be large and one-hot encoding of text leads to data matrices with many columns.\nIf the dictionary has 10,000 entries, each word or token is represented by a 10,000-element vector that contains 9,999 zeros and a single one that matches the position of the word in the dictionary. This leads to very high-dimensional problems that are also very sparse.\nTo avoid those problems it is popular to represent words in a lower-dimensional space of real numbers. In other words, rather than choosing 9,999 zeros and a single 1 to represent a particular word, we might choose a 20-dimensional vector of real numbers. This technique is known as word embedding.\n\n\nWord embeddings\nhttps://www.turing.com/kb/guide-on-word-embeddings-in-nlp\nWord embeddings encode words in a low-dimensional space as real-valued vectors. A big advantage of embeddings is that they can be pre-trained and they can capture semantic and syntactic information. Words with similar meaning should have similar representation in the vector space. The distance between two embedding vectors can then be used to measure the similarity between text expressions.\nOne of the most common embeddings is Word2Vec, developed by Google. It measures the semantic similarity of words based on cosine similarity of the embedding vector. Cosine similarity between two \\((n \\times 1)\\) vectors \\(\\textbf{a}\\) and \\(\\textbf{b}\\) is the cosine of their angle, \\[\n\\cos(\\theta) = \\frac{\\sum_{i=1}^n a_i b_i}{\\sqrt{\\sum_{i=1}^n a_i^2}\\sqrt{\\sum_{i=1}^n b_i^2}}\n\\] A cosine similarity of \\(1\\) indicates complete similarity, \\(0\\) indicates lack of correlation, and \\(-1\\) indicates complete dissimilarity.\nWord2Vec is based on two types of neural network models, a continuous bag-of-word (CBOW) model and a skip-gram model. The following Python code trains Word2Vec on a sequence of quotes about change and generates embeddings of length 20. In practical applications the corpus of text you train the model on is much larger, the size of the embedding is 100 or more. You can also download existing Word2Vec embeddings and use those instead of training your own.\n\nfrom gensim.models import Word2Vec\nimport gensim\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize, punkt \n\n\ns = \"The strength of a bureaucracy is measured by its ability to resist change.\\nThe only constant is change.\"\ns = s + \"Change is the law of life. And those who look only to the past or present are certain to miss the future\\n\"\ns = s + \"To improve is to change; to be perfect is to change often\\n\"\ns = s + \"Change before you have to\\n\"\ns = s + \"Not everything that is faced can be changed, but nothing can be changed until it is faced\\n\"\ns = s + \"People who are crazy enough to think they can change the world are the ones who do\"\n\nf = s.replace(\"\\n\", \" \")\n\ndata = []\n \n# iterate through each sentence to tokenize the sentence\nfor i in sent_tokenize(f):\n    temp = []\n    for j in word_tokenize(i):\n        temp.append(j.lower())\n    data.append(temp)\n \n# Create and train a continuous bag-of-words model\nmodel1 = gensim.models.Word2Vec(data, \n                                min_count=1,\n                                vector_size=20, \n                                window=5)\n \nprint(model1.wv[\"strength\"])\n\n[ 0.0170561   0.02584315  0.03140133 -0.01401705  0.03660801  0.01414236\n  0.01435158 -0.01175447 -0.01568154 -0.01181616  0.02141082  0.00037154\n -0.04789285 -0.04828245 -0.03068198 -0.00066577  0.01007916  0.04715109\n  0.02784073 -0.02150913]\n\nprint(model1.wv[\"ability\"])\n\n[ 0.03692275 -0.03363638  0.02789302 -0.04761012 -0.0040024  -0.04343542\n -0.02548623  0.04648114 -0.0093174   0.01458398  0.04536819  0.04468612\n -0.04102621 -0.01507848  0.04946372  0.02549913 -0.00791798 -0.04344965\n  0.01477943 -0.03337966]\n\nprint(model1.wv[\"change\"])\n\n[-0.00833561  0.00161311 -0.02078105 -0.03846731 -0.00756165  0.01230773\n -0.00439966  0.02841706 -0.01395713  0.01146699  0.02746079  0.04177966\n -0.0072043  -0.04605786  0.02223997  0.00267907  0.03758679 -0.00414909\n -0.01355227 -0.04394563]\n\nprint(\"Cosine similarity between 'strength and 'ability' - CBOW : \",\n      model1.wv.similarity('strength', 'ability'))\n\nCosine similarity between 'strength and 'ability' - CBOW :  0.04198855\n\n \nprint(\"Cosine similarity between 'strength and 'change' - CBOW : \",\n      model1.wv.similarity('strength', 'change'))\n\nCosine similarity between 'strength and 'change' - CBOW :  0.20074466\n\nprint(\"Cosine similarity between 'past and 'future' - CBOW : \",\n      model1.wv.similarity('past', 'future'))\n\nCosine similarity between 'past and 'future' - CBOW :  0.2608111\n\n \n# Create and train a skip Gram model\nmodel2 = gensim.models.Word2Vec(data, \n                                min_count=1, \n                                vector_size=20,\n                                window=5, sg=1)\n\nprint(model2.wv[\"strength\"])\n\n[ 1.6670765e-02  2.5719611e-02  3.1497952e-02 -1.4134813e-02\n  3.6302831e-02  1.3883009e-02  1.4506035e-02 -1.1257441e-02\n -1.5849181e-02 -1.1934853e-02  2.1613920e-02  5.8639314e-05\n -4.7993530e-02 -4.8156403e-02 -3.0682029e-02 -4.1891963e-04\n  1.0360058e-02  4.7084000e-02  2.7559893e-02 -2.1798888e-02]\n\nprint(model2.wv[\"ability\"])\n\n[ 0.03672264 -0.03360568  0.02790979 -0.04771556 -0.00409749 -0.04356595\n -0.02543401  0.04678207 -0.00931965  0.0144831   0.04550748  0.04467208\n -0.04109946 -0.0150124   0.04958136  0.02554292 -0.00773899 -0.04348198\n  0.01469189 -0.03355295]\n\nprint(model2.wv[\"change\"])\n\n[-0.01012116  0.00203569 -0.02054695 -0.03874388 -0.00886681  0.01132476\n -0.00387686  0.03110312 -0.01429986  0.01094422  0.0288924   0.04091645\n -0.0075365  -0.04540935  0.02317034  0.00313406  0.03954622 -0.00476322\n -0.0144198  -0.04520554]\n\nprint(\"Cosine similarity between 'strength' and 'ability' - Skip Gram : \",\n      model2.wv.similarity('strength', 'ability'))\n\nCosine similarity between 'strength' and 'ability' - Skip Gram :  0.043875016\n\n \nprint(\"Cosine similarity between 'strength' and 'change' - Skip Gram : \",\n      model2.wv.similarity('strength', 'change'))\n\nCosine similarity between 'strength' and 'change' - Skip Gram :  0.19225661",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Feature and Target Processing</span>"
    ]
  },
  {
    "objectID": "models/featproc.html#transforming-the-target",
    "href": "models/featproc.html#transforming-the-target",
    "title": "23  Feature and Target Processing",
    "section": "23.4 Transforming the Target",
    "text": "23.4 Transforming the Target\nThe previously discussed preprocessing steps can be applied to any variable, input or target variable.\nThe purpose of transforming the target variable is to meet distributional assumptions of the analysis. For example, many statistical estimators have particularly nice properties when the data are Gaussian distributed. A common transformation of \\(Y\\) is thus to morph its distribution so that the transformed \\(Y^*\\) is closer to Gaussian distributed than \\(Y\\). Sometimes transformations just aim to create more symmetry or to stabilize the variance across groups of observations. Transformations change the relationship between (transformed) target and the inputs, it is common to transform targets to a scale where effect are linear. We can then model additive effects on the transformed scale instead of complex nonlinear effects on the original scale of the data.\n\nDiscretizing\nDiscretizing a continuous target variable is commonly done to classify the target. If observations are taken with measurement error, we might not be confident in the measured values but we are comfortable classifying an outcome as high or low based on a threshold for \\(Y\\) and model the transformed data using logistic regression.\n\n\nNonlinear Transformations\nNonlinear transformations of a continuous target variable apply functions to modify the distributional properties. A common device to transform right-skewed distributions (long right tail) of positive values is to take logarithms or square roots. If \\(Y\\) has a log-normal distribution, then the log transformation yields a Gaussian random variable.\nA random variable \\(Y\\) has a log-normal distribution with parameters \\(\\alpha\\) and \\(\\beta\\) if its p.d.f. is given by \\[\np(y) = \\frac{1}{y\\sqrt{2\\pi\\beta}}\\exp\\left\\{-\\frac{(\\log y - \\alpha)^2}{2\\beta} \\right\\}, \\qquad y &gt; 0\n\\] \\(Y\\) has mean \\(\\mu = \\exp\\{\\alpha + \\beta/2\\}\\) and variance \\(\\sigma^2 = (e^\\beta - 1) \\exp\\{2\\alpha + \\beta\\}\\). If \\(Y \\sim \\text{Lognormal}(\\alpha,\\beta)\\), then \\(\\log Y\\) has a Gaussian distribution with mean \\(\\alpha\\) and variance \\(\\beta\\). We can also construct a log-normal variable from a standard Gaussian: if \\(Z \\sim G(0,1)\\), then \\(Y = \\exp\\{\\alpha + \\beta Z\\}\\) has a log-normal distribution. Figure 23.3 shows the histogram of 1,000 random samples from a Lognormal(2,1/2) distribution and the histogram of the log-transformed values.\n\n\n\n\n\n\n\n\nFigure 23.3: Histograms of 1,000 random samples from \\(Y \\sim \\text{Lognormal}(2,1/2)\\) and of \\(\\log Y\\).\n\n\n\n\n\n\nBox-Cox family\nThe Box-Cox family of transformations is a special case of power transformations that was introduced by Box and Cox (1964). Most commonly used is the one-parameter version of the Box-Cox transformation \\[\ny^{(\\lambda)} = \\left \\{\n\\begin{array}{ll}\n\\frac{y^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\ne 0 \\\\\n\\log y & \\text{if } \\lambda = 0\n\\end{array}\\right.\n\\tag{23.1}\\]\nThis transformation applies if \\(y &gt; 0\\). For the more general case, \\(y &gt; -c\\), the two-parameter Box-Cox transformation can be used \\[\ny^{(\\lambda)} = \\left \\{\n\\begin{array}{ll}\n\\frac{(y+c)^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\ne 0 \\\\\n\\log (y+c) & \\text{if } \\lambda = 0\n\\end{array}\\right.\n\\]\n\\(c\\) is simply a value by which to shift the distribution so that \\(y+c &gt; 0\\). The key parameter in the Box-Cox transformation is \\(\\lambda\\).\nThe term \\(- 1\\) in the numerator and the division by \\(\\lambda\\) for \\(\\lambda \\ne 0\\) in Equation 23.1 are linear operations that do not affect the proximity to normality of \\(y^{(\\lambda)}\\). An abbreviated version of Equation 23.1 is\n\\[\ny^{(\\lambda)} = \\left \\{\n\\begin{array}{ll}\ny^\\lambda & \\text{if } \\lambda \\ne 0 \\\\\n\\log y    & \\text{if } \\lambda = 0\n\\end{array}\\right.\n\\tag{23.2}\\]\nFor a given value of \\(\\lambda\\), the transformation is monotonic, mapping large values to large values if \\(\\lambda &gt; 0\\) and large values to small values if \\(\\lambda &lt; 0\\). The transformation thus retains the ordering of observations (Figure 23.4).\n\n\n\n\n\n\n\n\nFigure 23.4: Box-Cox transformations for various values of \\(\\lambda\\)\n\n\n\n\n\nHow do we find \\(\\lambda\\)? If the assumption is that for some value \\(\\widehat{\\lambda}\\) the distribution of \\(y^{(\\widehat{\\lambda})}\\) is Gaussian, then \\(\\lambda\\) can be estimated by maximum likelihood assuming a Gaussian distribution–after all, we are looking for the value of \\(\\lambda\\) for which the Gaussian likelihood of the transformed data is highest.\nThe following Python code simulates 1,000 samples from a Gamma(3,2) distribution and from a Lognormal(2,1) distribution.\n\nnp.random.seed(234)\nx1 = np.random.gamma(shape=3,scale=2,size=1000)\nx1 = np.reshape(x1,(-1,1))\n\n# numpy's lognormal distribution uses the mean and standard deviation\n# of the underlying normal distribution as parameters \nx2 = np.random.lognormal(mean=2,sigma=1,size=1000)\nx2 = np.reshape(x2,(-1,1))\n\nThe PowerTransformer method in sklearn.preprocessing estimates the \\(\\lambda\\) parameter of the Box-Cox transformation. We expect the estimate for the log-normal data to be close to zero, since a log transformation (\\(\\lambda=0\\)) of a log-normal random variable yields a Gaussian random variable.\n\npt = preprocessing.PowerTransformer(method='box-cox', standardize=False)\nx1_boxcox = pt.fit_transform(x1)\npt.lambdas_\n\narray([0.273836])\n\nx2_boxcox = pt.fit_transform(x2)\npt.lambdas_\n\narray([0.01351602])\n\n\nThe estimate of \\(\\lambda\\) to transform the Gamma(3,2) data is \\(\\widehat{\\lambda} = 0.2738\\) and the estimate to transform the log-normal data is \\(\\widehat{\\lambda} = 0.0135\\). The impact on the distribution of the transformed Gamma data is shown in Figure 23.5.\n\n\n\n\n\n\n\n\nFigure 23.5: 1,000 samples from Gamma(3,2) and Box-Cox transformation with \\(\\widehat{\\lambda} = 0.2738\\).\n\n\n\n\n\n\n\nQuantile transformation\nThe Box-Cox transformation does a good job to transform the data closer to a Gaussian distribution in the previous examples. It is not fail safe, however. Data that appear uniform or data with multiple modes do not transform to normality easily with the Box-Cox family.\nHowever, a transformation based on the quantiles of the observed data can achieve this. In fact, you can use quantiles to generate data from any distribution.\nIf \\(Y\\) is a random variable with c.d.f. \\(F(y) = \\Pr(Y \\le y)\\), then the quantile function \\(Q(p)\\) is the inverse of the cumulative distribution function; for a given probability \\(p\\), it returns the value \\(y\\) for which \\(F(y) = p\\).\nNow here is a possibly surprising result: if \\(Y\\) has c.d.f. \\(F(y)\\), then we can think of the c.d.f as a transformation of \\(Y\\). What would its distribution look like? The following R code draws 1,000 samples from a G(\\(2,1.5^2\\)) distribution and plots the histogram of the c.d.f. values \\(F(y)\\).\n\nset.seed(45)\nyn &lt;- rnorm(1000,mean=2,sd=1.5)\nF_yn &lt;- pnorm(yn,mean=2,sd=1.5)    \nhist(F_yn,main=\"\")\n\n\n\n\n\n\n\n\nThe distribution of \\(F(y)\\) is uniform on (0,1)—this is true for any distribution, not just the Gaussian.\nWe can combine this result with the following, possibly also surprising, result: If \\(U\\) is a uniform random variable, and \\(Q(p)\\) is the quantile function of \\(Y\\), then \\(Q(u)\\) has the same distribution as \\(Y\\). This is how you can generate random numbers from any distribution if you have a generator of uniform random numbers: plug the uniform random numbers into the quantile function.\n\nnorm_rv &lt;- qnorm(runif(1000),mean=2,sd=1.5)\nhist(norm_rv)\n\n\n\n\n\n\n\n\nThese are the key results behind quantile transformation, mapping one distribution to another based on quantiles.\nThe QuantileTransformer in sklearn.preprocessing can transform data based on quantiles to a uniform (default) or a Gaussian distribution. The results of transformation to normality for the Gamma(3,2) data are shown in Figure 23.6. These are not very different from the results of the Box-Cox transformation. However, the quantile transformation method always works, even in cases where the Box-Cox family of transformation fails to get close to normality.\n\nnp.random.seed(234)\nquantizer = preprocessing.QuantileTransformer(\n    output_distribution='normal')\n\nx1_trans = quantizer.fit_transform(x1)\n\nplt.figure()\nplt.hist(x1,bins=20,color='0.7',alpha=0.7,label='Original')\nplt.hist(x1_trans,color='magenta',alpha=0.5,label='Quantile transformed')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFigure 23.6: Quantile transformation to normality for Gamma(3,2) data.\n\n\n\n\n\n\n\nTo Transform or not to Transform\nWhy do we not always apply a nonlinear transform to a continuous target variable to achieve greater symmetry, stable variance, greater normality? After all, the results of the transformations are impressive. Figure 23.7 shows a quantile transformation to normality for data from a two-component mixture of Gamma densities. The bimodal distribution maps to a standard Gaussian like magic.\n\n\n\n\n\n\n\n\nFigure 23.7: Quantile transformation to normality for bimodal data from a mixture distribution.\n\n\n\n\n\nWhat are you giving up by analyzing the transformed target instead of the original target?\nWhen the goal of data analytics is testing research hypotheses, we usually observe the target variable of interest and formulate hypotheses about its properties. What do these hypotheses mean in terms of the transformed data? This is not always clear, in particular when highly nonlinear transformations such as quantiles are involved.\nThe parameters of the distribution of \\(Y^*\\), the transformed target, are not estimating the parameters of the distribution of \\(Y\\). The mean of \\(\\log Y\\) is not the log of the mean of \\(Y\\). When the transformation can be inverted, we usually end up with biased estimators. For example, if \\(Y\\) has a Lognormal(\\(\\alpha,\\beta\\)) distribution, then \\(\\overline{Y}\\) is an unbiased estimate of \\[\n\\text{E}[Y] = \\exp\\{\\alpha + \\beta/2\\}\n\\]\nIf we log-transform the data the sample mean of the transformed data \\(y^* = \\log y\\) estimates \\[\n\\text{E}[\\overline{Y}^*] = \\frac{1}{n}\\sum_{i=1}^n \\text{E}[\\log Y_i]\n\\] The exponentiation of this estimator is not an unbiased estimator of \\(\\text{E}[Y]\\).\n\n\n\nFigure 23.4: Box-Cox transformations for various values of \\(\\lambda\\)\nFigure 23.7: Quantile transformation to normality for bimodal data from a mixture distribution.\n\n\n\nBox, G. E. P., and D. R. Cox. 1964. “An Analysis of Transformations.” Journal of the Royal Statistical Society. Series B (Methodological) 26 (2): 211–52. http://www.jstor.org/stable/2984418.",
    "crumbs": [
      "Module IV. Modeling Data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Feature and Target Processing</span>"
    ]
  },
  {
    "objectID": "integ/ds_softwareeng.html",
    "href": "integ/ds_softwareeng.html",
    "title": "24  Data Science versus Software Engineering,",
    "section": "",
    "text": "24.1 Software Engineering Principles\nData scientists write statistical programs. That is software, but it is not bona fide software engineering. Because the result of statistical programming is software, data scientists need to know the principles of good software engineering:",
    "crumbs": [
      "Module VI. Operationalization",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Data Science versus Software Engineering,</span>"
    ]
  },
  {
    "objectID": "integ/ds_softwareeng.html#software-engineering-principles",
    "href": "integ/ds_softwareeng.html#software-engineering-principles",
    "title": "24  Data Science versus Software Engineering,",
    "section": "",
    "text": "Modularity: Separate software into components according to functionality and responsibility (functions, modules, public and private interfaces)\nSeparation of Concerns: Human beings have a limited capacity to manage contexts. Breaking down a larger task into units and abstractions that you can deal with one at a time is helpful. Interface and implementation are separate concerns. Data quality and data modeling are separate concerns. Not in the sense that they are unrelated, low quality data leads to low quality models—garbage in, garbage out. But in the sense that you can deal with data quality prior to the modeling task. Code efficiency (runtime performance) is sometimes listed as an example for separating concerns: write the code first to meet criteria such as correctness and robustness, then optimize the code for efficiency, focusing on the parts of the code the run spends most time in.\nAbstraction: Separate the behavior of software components from their implementation. Look at each component from two points of views: what it does and how it does it. A client-facing API (Application Programming Interface) specifies what a module does. It does not convey the implementation details. By looking at the function interface of prcomp and princomp in R, you cannot tell that one function is based on singular-value decomposition and the other is based on eigenvalue decomposition.\nGenerality: Software should be free from restrictions that limit its use as an automated solution for the problem at hand. Limiting supported data types to doubles and fixed-size strings is convenient, but not sufficiently general to deal with today’s varied data formats (unstructured text, audio, video, etc.). The “Year 2000” issue is a good example of lack of generality that threatened the digital economy: to save memory, years were represented in software products as two-digit numbers, causing havoc when 1999 (“99”) rolled over to “00” on January 1, 2000.\nAnticipation of Change: Software is an automated solution. It is rarely finished on the first go-around; the process is iterative. Starting from client requirements the product evolves in a back and forth between client and developer, each side refining their understanding of the process and the product at each step. Writing software that can easily change is important and often difficult. When software components are tightly coupled and depend on each other, it is unlikely that you can swap out for another without affecting both.\nConsistency: It is easier to do things within a familiar context. Consistent layout of code and user interfaces helps the programmer as well as the user as well as the next programmer. Consistency in code formatting, comments, naming conventions, variable assignments, etc. makes it easier to read and modify code and helps to prevent errors. When you are consistent in initializing all local variables in C functions, you will never have uninitialized variable bugs.",
    "crumbs": [
      "Module VI. Operationalization",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Data Science versus Software Engineering,</span>"
    ]
  },
  {
    "objectID": "integ/ds_softwareeng.html#dealing-with-uncertainty",
    "href": "integ/ds_softwareeng.html#dealing-with-uncertainty",
    "title": "24  Data Science versus Software Engineering,",
    "section": "24.2 Dealing with Uncertainty",
    "text": "24.2 Dealing with Uncertainty\nThe important differences between statistical programming and general software engineering stem to a large part from the inherent uncertainty and unpredictability of data and the ambiguity of solutions to more open-ended questions. Software engineers are comfortable with definite logic, data scientists are comfortable thinking in terms of probabilities.\n\nInput inherently unpredictable and uncertain. Statistical code is different from non-analytic code in that it is processing an uncertain input. A JSON parser also processes variability, each JSON document is different from the next. Does it not also deal with uncertain input? If the parser is free of bugs, the result of parsing is known with certainty. For example, we are convinced that the sentence “this book is certainly concerned with uncertainty” has been correctly extracted from the JSON file. Assessing the sentiment of the sentence, however, is a data science task: a sentiment model is applied to the text and returns a set of probabilities indicating how likely the model believes the sentiment of the text is negative, neutral, or positive. Subsequent steps taken in the software are based on interpreting what is probable.\nUncertainty about methods. Whether a software developer uses a quicksort or merge sort algorithm to order an array has impact on the performance of the code but not on the result. Whether you choose a decision tree or a support vector machine to classify the data in the array impacts the performance and the result of the code. A chosen value for a tuning parameter, e.g., the learning rate, can produce stable results with one data set and highly volatile results with another.\nRandom elements in code. Further uncertainty is introduced through analytic steps that are themselves random. Splitting data into training and test data sets, creating random folds for cross-validation, drawing bootstrap samples in random forests, random starting values in clustering or neural networks, selecting the predictors in random forests, Monte Carlo estimation, are some examples where data analysis involves drawing random numbers. The statistical programmer needs to ensure that random number sequences that create different numerical results do not affect the quality of the answers. The results are frequently made epeatable by fixing the seed or starting value of the random number generator. While this makes the program flow repeatable, it is yet another quantity that affects the numerical results. It is also a potential source for misuse: “let me see if another seed value produces a smaller prediction error.”\nData are messy. Data contains missing values and can be full of errors. There is uncertainty about how disparate data sources represent a feature (a customer, a region, a temperature) that affects how you integrate the data sources. These sources of uncertainty can be managed through proper data quality and data integration. As a data scientist you need to be aware and respectful of these issues; they can doom a project if not properly addressed. In an organization without a dedicated data engineering team resolving data quality issues might fall on your shoulders. If you are lucky to work with a data engineering team you still need to be mindful of these challenges and able to confirm that they have been addressed or deal with some of them (missing values).",
    "crumbs": [
      "Module VI. Operationalization",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Data Science versus Software Engineering,</span>"
    ]
  },
  {
    "objectID": "integ/ds_softwareeng.html#other-differences",
    "href": "integ/ds_softwareeng.html#other-differences",
    "title": "24  Data Science versus Software Engineering,",
    "section": "24.3 Other Differences",
    "text": "24.3 Other Differences\nOther differences between data science projects and software engineering projects are\n\nData science problems are inherently explorative. Finding insights in data is a discovery operation. Software engineering solves problems with specific requirements and less ambiguity.\nThe use of high-level languages. Statistical programming uses languages like R and Python. The software is written at a high level of abstraction, calling into existing packages and libraries. Rather than writing your own implementation of a random forest, you use someone else’s implementation. Instead, your concern shifts to how to use the hyperparameters of the random forest to the greatest effect for the particular data set. You can perform statistical programming in C, C++, or Rust. These system-level languages are best for implementing efficient algorithms, that are then called from a higher-level interface in R or Python.\nThe length of the programs. Statistical programs are typically short, a few hundred to a few thousands lines long. While a thousand lines of Python code may sound like much, it is not much compared to the size of large software engineering projects.\nThe programs are often standalone. A single file or module can contain all the code you need for a statistics project. That is good and bad. Good because it is easy to maintain. Bad because we often skip steps of good software hygiene such as documentation and source control.",
    "crumbs": [
      "Module VI. Operationalization",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Data Science versus Software Engineering,</span>"
    ]
  },
  {
    "objectID": "integ/coding_practices.html",
    "href": "integ/coding_practices.html",
    "title": "25  Coding Best Practices",
    "section": "",
    "text": "25.1 Version Control\nVersion control refers to the management and tracking of changes in digital content; mostly files and mostly code. Any digital asset can be placed under version control. Even if you are working (mostly) by yourself, using a version control system is important. Employers consider it a non-negotiable skill and you do not want to stand out as the applicant who does not know how to use Git. The benefits of version control systems are many, even the solo programmer would be remiss not to use it.\nWhat does a version control system like Git do for you:\nThe list goes on and on. The main point is that these capabilities and benefits are for everyone, whether you work on a project alone or as a team member.\nThere are many version control systems, Git, Perforce, Beanstalk, Mercurial, Bitbucket, Apache Subversion, AWS CodeCommit, CVS (Concurrent Versions System, not the drugstore chain), and others.\nThe most important system today is Git. GitHub and GitLab are built on top of Git. What is the relationship? Git is a local version control system, it runs entirely on the machine where it is installed and manages file changes there. GitHub and GitLab are cloud-based systems that allow you to work with remote repositories. In addition to supporting Git remotely, GitHub adds many cool features to increase developer productivity. The files for the pages you are reading are managed with Git and stored in a remote repository on GitHub (the URL is https://github.com/oschabenberger/oschabenberger-github.io-bn). GitHub also hosts the web site for the text through GitHub Pages. GitHub Actions can be set up so that the web site (the book) automatically rebuilds if any source files changes. And all of that comes with the free capabilities of GitHub.",
    "crumbs": [
      "Module VI. Operationalization",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Coding Best Practices</span>"
    ]
  },
  {
    "objectID": "integ/coding_practices.html#sec-repro-vc",
    "href": "integ/coding_practices.html#sec-repro-vc",
    "title": "25  Coding Best Practices",
    "section": "",
    "text": "It keeps track of files and their changes over time.\nIt saves changes to files without duplicating the contents, saving space in the process.\nIt groups content in logical units (branches) that are managed together. For example, all files associated with a particular build of a software release are kept in a branch.\nIt is a time machine, allowing you to reconstruct a previous state of the project and to see the complete history of the files.\nIt is a backup machine, making sure you have access to older versions of files and that changes do not get lost.\nIt allows you to perform comparisons between versions of files and to reconcile their differences.\nIt allows you to safely experiment with code without affecting code others depend on.\nIt allows you to see which parts of a project are worked on most/least frequently.\nIt is a collaborative tool, that reconciles changes to files made by multiple developers. Version control systems allow you to submit changes to someone else’s code.\nBy supporting modern continuous integration/continuous deployment (CI/CD) principles, a version control system can automate the process of testing and deploying software.\n\n\n\n\n\n\n\n\nTip\n\n\n\nOh how I wish there were easily accessible version control systems when I did my Ph.D. work. It involved a lot of programming algorithms and the analysis of real data sets. Developing the code took months to years and went through many iterations. I made frequent backups of the relevant files using really cool storage technology using special 1GB-size cartridges and a special reader. There were disks labeled “January 1993”, “March 1993”, “December 1993”, “Final”, “Final-V2”, and so forth. The storage technology was discontinued by the manufacturer and the cartridges are useless today. I am not able to access the contents even if the bits have not rotted on the media by now.\nTo study how the algorithm I needed to write for the dissertation evolved over time, I would have to go through all the backups and compare files one by one. A version control system will show me the entire history of changes in one fell swoop.\nUsing a cloud-based version control system would have avoided that headache. Alas, that did not exist back then.\n\n\n\n\n\nA Crash Course in Git\nGit is installed on your machine, it is a local tool for versioning files. You can perform all major Git operations (clone, init, add, mv, restore, rm, diff, grep, log, branch, commit merge, rebase, etc.) without an internet connection. The collaborative aspect of version control comes into play when you use a Git service provider such as GitHub or GitLab. Besides making Git a tool for multi-user applications, using GitHub or GitLab also gives you the ability to work with remote repositories; you can push your local changes to a server in the cloud, making it accessible to others and making it independent of the local workstation. Just because you push a repository to GitHub does not necessarily give everyone on the internet access to it—you manage whether a repository is private or public.\n\nInstalling Git\nThere are several ways to get Git on your machine, see here. On MacOS, installing the XCode Command Line tools will drop git on the machine. To see if you already have Git, open a terminal and check:\n➜ which git\n/usr/bin/git\nThe executable is installed in /usr/bin/git on my MacBook.\n\n\nBasic configuration\nThere are a million of configuration options for Git and its commands. You can see the configuration with\n➜ git config --list\nTo connect to GitHub later, add your username and email address to the configuration:\n➜ git config --global user.name \"First Last\"\n➜ git config --global user.email \"first.last@example.com\"\nYou can have project-specific configurations, simply remove the --global option and issue the git config command from the project (repository) directory.\n\n\nRepositories\nA repository is a collection of folders and files. Repositories are either cloned from an existing repository or initialized from scratch. To initialize a repository, change into the root directory of the project and issue the git init command:\n➜ cd \"STAT 5014\"\n➜ STAT 5014 pwd\n/Users/olivers/Documents/Teaching/Data Science/STAT 5014\n➜ STAT 5014 git init\nInitialized empty Git repository in /Users/olivers/Documents/Teaching/Data Science/STAT 5014/.git/\n➜ STAT 5014 git:(main)\nTo get help on git or any of the git commands, simply add --help:\n➜ git --help\n➜ git status --help\n➜ git add --help\n\n\nStages of a file\nA file in a Git repository goes through multiple stages (Figure 25.1). At first, the file is unmodified and untracked. A file that was changed in any way is in a modified state. That does not automatically update the repository. In order to commit the change, the file first needs to be staged with the git add command.\nWhen you issue a git add on a new file or directory, it is being tracked. When you clone a repository, all files in your working directory will be tracked and unmodified.\n\n\n\n\n\n\nFigure 25.1: The lifecycle of a file in Git. Source\n\n\n\nA file that is staged will appear under the “Changes to be committed” heading in the git status output.\nOnce you commit the file it goes back into an unmodified and tracked state.\n\nTracking files\nTo track files in a repository, you need to explicitly add them to the file tree with git add. This does not commit the file or push the file into a branch or a remote repository, it simply informs Git which files you care about.\n➜ git add LeastSquares.R\n➜ git add *.Rmd\n➜ git add docs/\nThe previous commands added LeastSquares.R, all .Rmd files in the current directory, and all files in the docs subfolder to the Git tree. You can see the state of this tree any time with\n➜ git status\ngit status shows you all files that have changed as well as files that are not tracked by Git and are not ignored. For example, after making some changes to the quarto.yml and to reproducibility.qmd files since the last commit, the status of the repository for this material looks as follows:\n➜  StatProgramming git:(main) ✗ git status\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    modified:   _quarto.yml\n    modified:   docs/reproducibility.html\n    modified:   docs/search.json\n    modified:   reproducibility.qmd\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    .DS_Store\n    .gitignore\n    .nojekyll\n    .python-version\n    StatProgramming.Rproj\n    _book/\n    ads.ddb\n    customstyle.scss\n    data/\n    debug_ada.R\n    debug_ada.Rmd\n    images/\n    latexmacros.tex\n    sp_references.bib\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nTwo more files have been noted by Git as modified, docs/reproducibility.html and docs/search.json. These files are generated by Quarto when the content of the modified files is being rendered. They will be added to the next commit to make sure the website is up to date and not just the source (.qmd) files.\ngit add can be a bit confusing because it appears to perform multiple functions: to track a new file and to stage a file for commit. If you think of git add as adding precisely the content to the next commit, then the multiple functions roll into a single one.\n\nAn ignored file is one for which you explicitly tell Git not to worry about. You list those files in a .gitignore file. (You can have multiple .gitignore files in the directory hierarchy, refer to the Git documentation on how they interact. The typical scenario is a .gitignore file in the root of the repository.)\nThe contents of the following .gitignore file state that all .html files should be ignored, except for foo.html. Also, StatLearning.Rproj will be ignored.\n➜ cat .gitignore\n*.html\n!foo.html\nStatLearning.Rproj\nFiles that are listed in .gitignore are not added to the repository and persist when a repository is cloned. However, if a file is already being tracked, then adding it to .gitignore does not untrack the file. To stop tracking a file that is currently tracked, use\ngit rm --cached filename \nto remove the file from the tree. The file name can then be added to the .gitignore file to stop the file from being reintroduced in later commits.\nFiles that you want to exclude from tracking are often binary files that are the result of a build or compile, and large files. Also, if you are pushing to a public remote repository, make sure that no files containing sensitive information are added.\n\n\nCommitting changes\nOnce you track a file, Git keeps track of the changes to the file. Those changes are not reflected in the repository until you commit them with the commit command. A file change will not be committed to the repository unless it has been staged. git add will do that for you.\nIt is a good practice to add a descriptive message to the commit command that explains what changes are committed to the repository:\n➜ git commit -m \"Early stopping criterion for GLMM algorithm\"\nIf you do not specify a commit message, Git will open an editor in which you must enter a message.\n\n\n\n\n\n\nTip\n\n\n\nIf Git opens an editor for you and this is the first time you find yourself in vi or vim, you might struggle with those editors. To set a different default editor on MacOS or Linux, set the EDITOR environment variable.\n➜ echo $EDITOR\ntells you whether a default text editor has been set.\n\n\nSince only files that have been added with git add are committed, you can ask Git to notice the changes to the files whose contents are tracked in your working tree and do corresponding git adds for you by adding the -a option to the commit:\n➜ git commit -a -m \"Early stopping criterion for GLMM algorithm\"\nWhat happens when you modify a file after you ran git add but before the net commit? The file will appear in git status as both staged and ready to be committed and as unstaged. The reason is because Git is tracking two versions of the file now: the state it was in when you first ran git add and the state it is in now, which includes the modifications since the last git add. In order to stage the most recent changes to the file, simply run git add on the file again.\n\n\nRemote repositories\nThe full power of Git comes to light when you combine the local work in Git repositories with a cloud-based version control service such as GitHub or GitLab. To use remote repositories with Git, first set up an account, say with GitHub.\nThe Git commands to interact with a remote repository are\n\ngit pull: Incorporates changes from a remote repository into the current branch. If the current branch is behind the remote, then by default it will fast-forward the current branch to match the remote. The result is a copy of changes into your working directory.\ngit fetch: Copies changes from a remote repository into the local Git repository. The difference between fetch and pull is that the latter also copies the changes into your working directory, not just into the local repo.\ngit push: Updates remote references using local references, while sending necessary objects.\ngit remote: Manage the set of remote repositories whose branches you track.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have used other version control systems, you might have come across the terms pushing and pulling files. In CVS, for example, to pull a file means adding it to your local checkout of a branch, to push a file means adding it back to the central repository.\nWith Git, push and pull command only come into play when you work with remote repositories. As long as everything remains on your machine, you do not need those commands. However, most repositories these days are remote, so the initial interaction with a repository is often a clone, pull, or fetch.\n\n\nStart by creating a new repository on GitHub by clicking on the New button. You have to decide on a name for the repository and whether it is public or private. Once you created a remote repository, GitHub gives you alternative ways of addressing it, using https, ssh, etc.\n\n\n\n\n\n\nTip\n\n\n\nDepending on which type of reference you use on the command line, you also need different ways of authenticating the transaction. GitHub removed passwords as an authentication method for command-line operations some time ago. If you use SSH-style references you authenticate using the passphrase of an SSH key registered with GitHub. If you use https-style references you authenticate with an access token you set up in GitHub.\n\n\nBack on your local machine you manage the association between the local repository and the remote repository with the git remote commands. For example,\n➜ git remote add origin git@github.com:oschabenberger/oschabenberger-github.io-bn.git\nassociates the remote repository described by the ssh syntax git@github.com:oschabenberger/oschabenberger-github.io-bn.git with the local repository. Using html syntax, the same command looks like this:\n➜ git remote add origin https://github.com/oschabenberger/oschabenberger-github.io-bn\nGitHub provides these strings to you when you create a repository.\nTo update the remote repository with the contents of the local repository, issue the git push command:\n➜ git push",
    "crumbs": [
      "Module VI. Operationalization",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Coding Best Practices</span>"
    ]
  },
  {
    "objectID": "integ/coding_practices.html#structure-and-organization",
    "href": "integ/coding_practices.html#structure-and-organization",
    "title": "25  Coding Best Practices",
    "section": "25.2 Structure and Organization",
    "text": "25.2 Structure and Organization\n\nNaming\n\nChoose names for variables and functions that are easy to understand. Variable and function names should be self explanatory. Most modern programming languages and tools no longer limit the length of function or variable names, there is no excuse for using a1, a2, b3 as variable names. Use nouns for names of variables and objects that describe what the item holds; for example, originalData and randomForestResult instead of d and out.\nStick with a naming convention such as snake_case, PascalCase, or camelCase. In snake_case, spaces between words are replaced with an underscore. In camelCase, words are concatenated and the first letter of the word is capitalized. PascalCase is a special case where the first letter of the entire name is also capitalized; camelCase is ambivalent about capitalizing the first letter of the name. The following are examples of names in camelCase.\n\naccountBalance\nthisVariableIsWrittenInCamelCase\nitemNumber\nsocialSN\nMasterCard\nAn issue with camelCase is that it is not entirely clear how to write names in that style that contain other names or abbreviations, for example, is it NASAAdminFiles or NasaAdminFiles? I am not sure it really matters.\nsnake_case is popular because it separates words with underscores—mimicking white space—while producing valid names for computer processing. The following are examples of names in snake_case:\naccount_balance\nACCOUNT_BALANCE\nhome_page\nitem_Number\nUsing upper-case letters in snake_case is called “screaming snake case”, situations where I have seen it used are the definition of global constants or macro names in C. kebab case is similar to snake case but uses a hyphen instead of an underscore. Here are examples of names in kebab case:\naccount-balance\nhome-page\nitem-Number\nAlthough it might look nice, it is a good idea to avoid kebab case in programs. Imagine the mess that ensues if the hyphen were to be interpreted as a minus sign! While the compiler might read the hyphen correctly, the code reviewer in the cubicle down the hall might think it is a minus sign.\nDo not assign objects to existing names, unless you really want to override them. This goes in particular for internal symbols and built-in functions. Unfortunately, R does not blink and allows you to do things like this:\nT &lt;- runif(20)\nC &lt;- summary(lm(y ~ x))\nThese assignments override the global variable whose value is set to TRUE for logical comparison and the function C() that defines contrasts for factors. If in doubt whether it is safe to assign to a name, check in the console whether the name exists or request help for it\n?T\n?C()\n\n\nComments\nUnless you write a literal program use comments throughout to clarify why code is written a certain way and what the code is supposed to accomplish. Even with literal programs, comments associated with code are a good practice because the code-portion of the literal program can get separated from the text material at some later point.\nComments frequently are intended by programmers to leave themselves some notes, for example, about functions yet to be written or to be refactored later. Make it clear with a “TODO” at the beginning of the comment where those sections of the program are and make the TODO comment stand out visually from other comments.\nIt is a good practice to have a standardized form for writing comments. For example, you can have a standard comment block at the beginning of functions. Some organizations will require you to write very detailed comment blocks that explain all inputs and outputs down to length of vectors and data types.\n# -------------------------------------------------------------------\n# TODO: Add check whether it is safe to perform the division before\n#       returning from the function. Variances can be zero or near zero.\n# -------------------------------------------------------------------\n\n# ###################################################################\n# Function: getIRLSWeights\n# Purpose: retrieve the vector of weights for iteratively \n#          reweighted least squares\n# Arguments:\n#       eta: numeric vector of linear predictors\n#       link: character string describing the link function\n#       dist: character string describing the distribution of the response \n#\n# Returns: the vector of weights, same length as the eta vector (input)\n#\n# Notes: \n#   For efficiency, eta is not checked for NULL or missing values. \n#   These checks are in the deta_dmu() and get_var() functions.\n# ###################################################################\n\ngetIRLSWeights &lt;- function(eta, link=\"log\", dist=\"poisson\") {\n    var_z &lt;- deta_dmu(eta,link)^2 * get_var(get_mu(eta,link),dist)\n    return (1/var_z)    \n}\nIf you program in Python, you would add docstrings to the function. This also serves as the help information for the user.\n\n\nWhitespace\nJudicious use of whitespace makes code more readable. It helps to differentiate visually and to see patterns. Examples are indentation (use spaces, not tabs), alignment within code blocks, placement of parentheses, and so forth.\nWhich of the following two functions is easier to read? It does not matter for the R interpreter but it matters for the programmer.\nget_z &lt;- function(y, eta, link) {\nif (is.null(y) || is.null(eta)) {\nstop(\"null values not allowed\") }\nif (anyNA(y) || anyNA(eta)) {\nstop(\"cannot handle missing values\") }\nz &lt;- eta + (y - get_mu(eta,link)) * deta_dmu(eta,link)\nreturn(z)\n}\n\nget_z &lt;- function(y, eta, link) {\n    if (is.null(y) || is.null(eta)) {\n        stop(\"null values not allowed\")\n    }\n    if (anyNA(y) || anyNA(eta)) {\n        stop(\"cannot handle missing values\")\n    }\n    z &lt;- eta + (y - get_mu(eta,link)) * deta_dmu(eta,link)\n    return(z)\n}\nThe following code uses indentation to separate options from values and to isolate the function definition for handling the reference strip. The closing parenthesis is separated with whitespace to visually align with the opening parenthesis of xyplot.\nxyplot(diameter ~ measurement | Tree, \n       data    = apples,\n       strip   = function(...) {\n           # alter the text in the reference strip   \n           strip.default(..., \n                         strip.names  = c(T,T), \n                         strip.levels = c(T,T),\n                         sep          = \" \")\n       },\n       xlab    = \"Measurement index\",\n       ylab    = \"Diameter (inches)\",\n       type    = c(\"p\"),\n       as.table= TRUE,\n       layout  = c(4,3,1)\n      )\nWith languages such as Python, where whitespace is functionally relevant, you have to use spacing within the limits of what the language allows.\n\n\nFunctions\n\nR\nIn R almost everything is a function. When should you write functions instead of one-off lines of code? As always, it depends; a partial answer hides in the question. When you do something only once, then writing a bunch of lines of code instead of packaging the code in a function makes sense. When you write a function you have to think about function arguments (is the string being passed a single string or a vector?), default values, return values, and so on.\nHowever, many programming tasks are not one-offs. Check your own code, you probably write the same two or three “one-off” lines of code over and over again. If you do it more than once, consider writing a function for it. If you do a substantial task over and over, consider writing a package.\n\n\n\n\n\n\nNote\n\n\n\nIf you are interested in writing an R package, check out this chapter in the book by Peng, Kross, and Anderson (2020).\n\n\nFunction names should be verbs associated with the function purpose, e.g., joinTables(), updateWeights(). For functions that retrieve or set values, using get and set is common: getWeights(), setOptimizationInput().\nThe comment block for function should document the function purpose, required arguments, and returns.\nSome argue that it is good coding practice to have default values on function arguments. For example,\naddNumbers &lt;- function(a=1, b=2) {return(a+b)}\ninstead of\naddNumbers &lt;- function(a, b) {return(a+b)}\nAdding defaults ensures that all variables are initialized with valid values and simplifies calling the function. On the other hand, it can mask important ways to control the behavior of the function. Users will call a function as they see it being used by others and not necessarily look at the function signature. Take the duckload() function:\n\nduckload &lt;- function(tableName, whereClause=NULL, dbName=\"ads.ddb\") {\n    if (!is.null(tableName)) {\n        if (!(\"duckdb\" %in% (.packages()))) {\n            suppressWarnings(library(\"duckdb\"))\n            message(\"duckdb library was loaded to execute duckload().\")\n        }\n        con &lt;- dbConnect(duckdb(), dbdir=dbName, read_only=TRUE)\n        query_string &lt;- paste(\"SELECT * from \", tableName)\n        if (!is.null(whereClause)) {\n            query_string &lt;- paste(query_string, \" WHERE \", whereClause)\n        }\n        df_ &lt;- dbGetQuery(con, query_string)\n        dbDisconnect(con)\n        return (df_)\n    } else {\n        return (NULL)\n    }\n}\n\nWould you know from the following usage pattern that you can pass a WHERE clause to the SQL string?\n\nduckload(\"apples\")\n\nIf the function arguments had no defaults, the function call would reveal its capabilities:\n\nduckload(\"apples\", whereClause=NULL, dbName=\"ads.ddb\")\n# or\nduckload(\"apples\",NULL,\"ads.ddb\")\n\nOther good practices to observe when writing functions:\n\nAlways have an explicit return argument. It makes it much easier to figure out where you return from the function and what exactly is being returned.\nCheck for NULL inputs\nCheck for missing values unless your code can handle them.\nHandle errors (see below)\nPass through variable arguments (...)\nIf you return multiple values, organize them in a list or a data frame. Lists are convenient to collect objects that are of different types and sizes into a single object. The following function returns a list with three elements,\n\n\niterationWeight &lt;- function(Gm,wts,method=\"tree\") {\n    pclass &lt;- predict(Gm,type=\"vector\")\n    misclass &lt;- pclass != Gm$y\n    Em &lt;- sum(wts*misclass)/sum(wts)\n    alpha_m &lt;- log((1-Em)/Em)\n    return (list(\"misclass\"=misclass,\"Em\"=Em,\"alpha_m\"=alpha_m))\n}\n\n\nError Handling\nThink of a function as a contract between you and the user. If the user provides specified arguments, the function produces predictable results. What should happen when the user specifies invalid arguments or when the function encounters situations that would create unpredictable results or situations that keep it from continuing?\nYour opportunities to handle these situations include issue warning messages with warning(), informational messages with message(), stopping the execution with stop() and stopifnot() and try-catch-finally execution blocks. In general, stopping the execution of a function with stop or stopifnot is a last resort if the function cannot possibly continue. If the data passed are of the wrong type and cannot be coerced into the correct data type, or if coercion would result in something nonsensical, then stop.\nIn the event that inputs are invalid and you cannot perform the required calculations, could you still return NULL as a result? If so, do not stop the execution of the function. You can issue a warning message and then return NULL. Warning messages are also appropriate when the function behavior is changing in an unexpected way. For example, the input data contains missing values (NAs) and your algorithm cannot handle them. If you process the data after omitting missing values, then issue a warning message if that affects the dimensions of the returned objects.\nKeep in mind that R is used in scripts and as an interactive language. Messages from your code are intended for human consumption so they should be explicit and easy to understand. But avoid making your code too chatty.\nTo check whether input values have the expected types you can use functions such as\n\nis.numeric()\nis.character()\nis.factor()\nis.ordered()\nis.vector()\nis.matrix()\n\nand to coerce between data types you can use the as. versions\n\nas.numeric()\nas.character()\nas.factor()\nas.ordered()\nas.vector()\nas.matrix()\n\ntryCatch() is the R implementation of the try-catch-finally logic you might have seen in other languages. It is part of the condition system that provides a mechanism for signaling and handling unusual conditions in programs. tryCatch attempts to evaluate expression expr, and if it succeeds, executes the code in the finally block. You can add erorr and warning handlers with the error= and warning= options.\n\ntryCatch(expr,\n         error = function(e){\n           message(\"An error occurred:\\n\", e)\n         },\n         warning = function(w){\n           message(\"A warning occured:\\n\", w)\n         },\n         finally = {\n           message(\"Finally done!\")\n         }\n        )\n\ntryCatch is an elegant way to handle conditions, but you should not overdo it. It can be a drag on performance. For example, if you require input to be of numeric type, then it is easier and faster to check with is.numeric than to wrap the execution in tryCatch.\n\n\nDependencies\nIt is a good idea to check dependencies in functions. Are the required packages loaded? It is kind of you to load required packages on behave of the caller rather than stopping execution. If you do, issue a message to that effect. See duckload() above for an example.\nInstalling packages on behalf of the caller is a step too far in my opinion, since you are now changing the R environment. You can check whether a package is installed with require. The following code stops executing if the dplyr package is not installed.\n\ncheck_pkg_deps &lt;- function() {\n    if (!require(dplyr))\n        stop(\"the 'dplyr' package needs to be installed first\")\n}\n\nrequire() is similar to library(), but while the latter fails with an error if the package cannot be loaded, require returns TRUE or FALSE depending on whether the library was loaded and does not throw an error if the package cannot be found. Think of require as the version of library you should use inside of functions.\n\n\nDocumentation\nComments in code are not documentation. Documentation is a detailed explanation of the purpose of the code, how it works, how its functions work, their arguments, etc. It also includes all information someone would want to need to take over the project. In literal programs you have the opportunity to write code and documentation at the same time. Many software authoring frameworks include steps in programming that generate the documentation. For example, to add documentation to an R package, you need to create a “man” subdirectory that contains one file per function in the special R Documentation format (.Rd). You can see what the files look like by browsing R packages on GitHub. For example, here is the repository for the ada package.\nAt a minimum a README file in Markdown should accompany the program. The file has setup instructions and use instructions someone would have to follow to execute the code. It identifies author, version, major revision history, and details on the functions in the public API—those functions called by the user of the program.\nThere are great automated documentation systems such as doxygen which annotate the source code in such a way that documentation can be extracted automatically. An R package for generating inline documentation that was inspired by doxygen is roxygen2.\n\n\n\nPython\n\n\n\nFigure 25.1: The lifecycle of a file in Git. Source\n\n\n\nPeng, Roger D., Sean Kross, and Brooke Anderson. 2020. Mastering Software Development in r. https://bookdown.org/rdpeng/RProgDA/.",
    "crumbs": [
      "Module VI. Operationalization",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Coding Best Practices</span>"
    ]
  },
  {
    "objectID": "integ/tools.html",
    "href": "integ/tools.html",
    "title": "26  Data Science Tools",
    "section": "",
    "text": "26.1 Tech Stacks\nBecause the “T-shaped” role of the data scientist touches many disciplines and systems it can take many tools to do the work. From project management tools like JIRA or Asana to machine learning frameworks like TensorFlow, CNTK, PyTorch, or Keras, to multiple programming languages (R, Python, Scala, Java, JavaScript, C++, Go), multiple databases, visualization tools, cloud providers, ETL and ELT tools, DevOps tools, ModelOps tools, dashboard builders. The list goes on and on. The abundance of software tools and frameworks available to data scientists can be overwhelming—and it is growing every day.\nA few comments and recommendations:\nEmployers will have preferences and standards you must comply with. Expertise with one tool makes switching to another tool easier. If your employer is an AWS shop you will not convince them to switch to Google BigQuery. If you have basic SQL skills, you will be able to move from BigQuery to Amazon Redshift. If you are familiar with business intelligence tools like Tableau or Alteryx, adding Power BI to your toolbox is not a problem.\nYou will also find that organizations have invested heavily in systems in the past and are slow to move on although more modern and better performing options are available. For example, many companies have built data lakes and machine learning environments based on the Hadoop ecosystem. Although superseded by cloud-based object storage, migrating from a Hadoop cluster is costly and time consuming. Hadoop-based tools such as Hive, Impala, Kudu, Pig, Mahout, Sqoop, and Zookeeper will be around for a while longer. Be ready to work with tools that might not be on the Favorites list.",
    "crumbs": [
      "Module VI. Operationalization",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#tech-stacks",
    "href": "integ/tools.html#tech-stacks",
    "title": "26  Data Science Tools",
    "section": "",
    "text": "You do not need to be an expert at every tool in order to be an expert data scientist.\nYou can and should develop your own preferred tech stack.\nEvery tool has pros and cons, and everyone has preferences.\nLook at job postings to learn about the tech stack of employers. Some companies have published or described their tech stack online. This will give you an idea of the tools you will encounter at those employers and how companies make choices about their data stack.\nCheck Reddit, Stack Overflow, Stack Exchange, GitHub, etc. The degree of activity on these sites is a good indicator for the relevance of a tool.\nIn an organization with a small data science team, you will have a different set of tasks and tools than as part of a larger data science team with specialists in data engineering, visualization, etc.\nIn research settings the tech stack is smaller since the number of data science tasks is smaller.\nYou will find tools built in-house in larger companies. Many organizations use a blend of internal and 3rd-party tools. Some companies have their own forks of open-source software projects.\nBe ready to adapt and learn new tools, technologies, languages.\n\n\n\n\nExample Tech Stacks\nHere are some examples of tech stacks at companies. These include frontend and backend tools as well as data tools. We dive a bit deeper into the data engineering stack at Meta (formerly Facebook) in the next section.\nIn the video below from 2023, Chris Wiggins, Chief Data Scientist of the New York Times discusses the evolution of the data science tech stack at the New York Times:\n\nLike many other organizations, their data science tech stack is changing, the company is iterating to find what works best for them. They move from “write your own MapReduce jobs against S3 buckets” to “jobs in Hive and Pig” to “started our own Hadoop instance” to “all in BigQuery and GCP tech stack”: code in Python leveraging scikit-learn, when necessary code in Go, data are read from BigQuery, model output is pushed back to BigQuery, sometimes hosting an API, scheduling using Airflow instance on GCP, containerized.\nWhat are the takeaways:\n\nMoving away from Hadoop\nNot afraid of changing cloud providers (from AWS S3 to Google Cloud Platform and BigQuery)\nReading from and writing back to the same data platform: minimizing data movement, single format, SQL access\nAt every stage the tech stack was pretty modern and the company is not afraid to change—good attributes.\n\nHere are the tech stacks of some other companies:\n\nGoogle: Python, Java, AngularJS, Golang, C++, Dart, Preact, K8s, Android Studio, Bazel\nFacebook: React, PHP, GraphQL, Cassandra, Memcached, Presto, Flux, Tornado, RocksDB, Jenkins, Chef, Phabricator, Datadog, Confluence\nNetflix: Python, Node.js, React, Java, MySQL, PostgreSQL, Flask, AWS (S3, EC2, RDS, DynamoDB, EMR, CloudTrail), Cassandra, Oracle, Hadoop, Presto, Pig, Atlas-DB, GitHub, Jenkins, Gradle, Sumo Logic.\nUber: Python, jQuery, Node.js, React, Java, MySQL, NGINX, PostgreSQL, MongoDB, Redis, Amazon EC2, Kafka, Golang, Cassandra, Apache Spark, Hadoop, AresDB, Terraform, Grafana, Prometheus, Zookeeper. Also see this article on data science at Uber.\nShopify: Python, React, MySQL, NGINX, Redis, GraphQL, Kafka, Goang, Memcached, Apache Spark, Hadoop, dbt, Apache Beam, ElasticSearch, GitHub, Docker, K8s, Datadog, Chef, Zookeeper\nUdemy: Python, jQuery, Node.js, React, MySQL, NGINX, CloudFlare, AngularJS, Redis, Django, Spring Boot, Kafka, Kotlin, Memcached, ElasticSearch, GitHub, Docker, Jenkins, K8s, PyCharm, Ansible, Terraform, Sentry, Datadog\n\n\n\nThe Meta Data Engineering Stack\nWe picked the stack for data engineering at Meta (fka Facebook) for a deeper examination because their team provided a detailed article that discusses some of the characteristics of data engineering at large, modern companies (Meta 2023):\n\nVery large data warehouses\nComplex data pipelines\nA mix of commercial and open-source tools\nA mix of in-house and 3rd-party tools\n\nThe main data warehouse for analytics consists of a collection of millions of Hive tables stored in ORC (Optimized Row Columnar) format (see Section 10.1.4). Meta maintains its own fork of ORC, which suggests that they optimized the file format for their use cases.\nThe data warehouse is so large that it cannot be stored in one data center. The data are partitioned geographically and logically into namespaces—groups of tables that are likely used together. Tables in the same namespace are located together in the same data center location to facilitate merges and joins without sending data across geographies. If data needs to be accessed across namespaces, the data are replicated to another namespace so that they can be processed at the same location.\nYou really have a lot of data if the analytic data needs to be spread across multiple data centers in multiple geographies. The total size of the Meta data warehouse is measured in exabytes (millions of terabytes).\nMeta has a strict data retention policy, table partitions older than the table’s retention time are deleted or archived following anonymization of the data.\nTo find data in such a massive data warehouse, Meta developed its own tool, iData, to search for data by keyword. The iData search engine returns tables ranked by relevance, considering data freshness, number of uses, and number of mentions in posts of the table.\nTo query the data warehouse, Meta uses Presto and Spark. Presto is an open-source SQL querying engine originally developed by Meta. After open-sourcing Presto, Meta maintains its own internal fork. SQL (Presto SQL or Spark SQL) is key for querying the data at Meta. Presto is used for most day-to-day queries; a light query at Meta’s scale scans through a few billion rows of data. Spark is used for the heavy workloads.\nData exploration and analysis are based on internal tools, Daiquery is the internal tool for querying and visualizing any data source. Bento is an internal implementation of Jupyter notebooks for Python and R code.\nDashboards are created with another internal tool, Unidash.\nData pipelines are written in SQL, wrapped in Python, and orchestrated with Dataswarm, a predecessor of Airflow.\nVSCode is the IDE of choice for developing data pipelines and has been enhanced with custom plugins developed internally. For example, a custom linter checks SQL statements. On Save the internal VSCode extension generates the directed acyclic graph for the pipeline. The data engineer can then schedule a test run of the pipeline using real data, writing the result to a temporary result table.",
    "crumbs": [
      "Module VI. Operationalization",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#sec-tools-eda-bi",
    "href": "integ/tools.html#sec-tools-eda-bi",
    "title": "26  Data Science Tools",
    "section": "26.2 Exploratory Data Analysis and Business Intelligence",
    "text": "26.2 Exploratory Data Analysis and Business Intelligence\nBusiness Intelligence (BI) is the processing of organizational data and presenting it in reports and on dashboards. The goal is to help an organization’s operations by using relevant data. Key functions are to monitor, report, and analyze the business operations. BI overlaps with Exploratory Data Analysis (EDA) in that it is highly descriptive, relying on visualizations and summarizations to inform about what is and has been happening.\nHere is a list of some BI tools you will encounter in practice (in no particular order):\n\nMicrosoft Power BI\nTableau (acquired by Salesforce)\nHeap Analytics\nMetabase\nMode (acquired by ThoughtSpot)\nThoughtSpot\nQlik\nSisense\nSAP BusinessObjects\nOracle BI\nTIBCO Spotfire\nAWS QuickSights\nLooker (Looker Studio)\nDOMO\nIBM Cognos Analytics\nMicroStrategy\nYellowfin\nSAS Augmented Analytics & BI\nJMP",
    "crumbs": [
      "Module VI. Operationalization",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#data-engineering",
    "href": "integ/tools.html#data-engineering",
    "title": "26  Data Science Tools",
    "section": "26.3 Data Engineering",
    "text": "26.3 Data Engineering\nData engineers build pipelines that help to collect, merge, cleanse, prepare, and transform data for subsequent analytics. Data engineering defines, creates, and maintains the infrastructure that enables modern data analytics. Key steps in the data engineering workflow are pipelining, data replication, change-data-capture (CDC), ETL (Extract-Transform-Load) and/or ELT (Extract-Load-Transform).\nHere is a list of some common tools used in data engineering.\n\nDbt Labs. The “t” in dbt is the “T” in ELT. Dbt is a SQL-based data engineering tool that assumes the data is already loaded into the target system. It transforms data where it lives.\nFivetran. An ELT and data-movement platform with extensive data replication and change-data-capture capabilities.\nCData. Data connectivity, data movement, data sync (CDC).\nSpark. An engine for distributed big-data analytics with interfaces for Python (pySpark), SQL (spark-sql), Scala, Java, and R (sparkR)\nDask. A parallel-processing framework for Python\nApache Airflow. A Python-based tool to create and manage workflows. Often used to pipeline data.\nPrefect. Workflow orchestration for data engineers and ML engineers.\nApache Kafka. Open-source distributed event-streaming platform that is frequently used to move data through streaming pipelines.\nMatillion. Build and orchestrate data pipelines.\nDatabases (see Section 10.2)\nElasticSearch. Distributed search and analytics engine.\nPresto. An open-source, distributed SQL engine for analytic queries\nRedis. An open-source, in-memory data store. Often used as a memory cache.",
    "crumbs": [
      "Module VI. Operationalization",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#data-visualization",
    "href": "integ/tools.html#data-visualization",
    "title": "26  Data Science Tools",
    "section": "26.4 Data Visualization",
    "text": "26.4 Data Visualization\n\nPython-based\n\npandas, matplotlib, seaborn, plotly, Vega-altair, plotnine (ggplot)\n\nR-based\n\ntidyverse (dplyr, tidyr, ggplot2, shiny)\n\nMany of the tools listed in Section 26.2.",
    "crumbs": [
      "Module VI. Operationalization",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#data-analytics-and-machine-learning",
    "href": "integ/tools.html#data-analytics-and-machine-learning",
    "title": "26  Data Science Tools",
    "section": "26.5 Data Analytics and Machine Learning",
    "text": "26.5 Data Analytics and Machine Learning\n\nCloud Service Providers\n\nAWS SageMaker\nAzure Synapse Analytics\nGoogle BigQuery ML\n\n\n\nLanguages & Packages\n\nPython-based\n\nnumpy, scipy, pandas, polars, statsmodels, scikit-learn\nPyspark\n\nScala-based\n\nSpark\n\nR-based\n\nBasic modeling capabilities are built into the language\ndplyr, tidyr, caret, gam, glmnet, nnet, KernLab, E1071, RandomForest, tree, gbm, xgboost, lme4, boot, …. See the CRAN overview for Machine Learning and Statistical Learning\nsparkR\n\nJava: mostly used to put models into production and to build applications rather than model building.\n\nDeeplearning4j: open-source toolkit for Java to deploy deep neural nets.\nND4J: n-dimensional array objects for scientific computing\n\nGolang: Go is used mainly as a language for managing and orchestrating backend architecture but is finding more applications in data orchestration.\n\n\n\nCommercial Offerings\n\nAlteryx\nKNIME\nDomino Data Lab\nDataRobot\nDataiku\nH20.ai\nRapidMiner (acquired by Altair)\nMindsDB\nDatabricks\nSAS Viya\nJMP Pro\nMATLAB",
    "crumbs": [
      "Module VI. Operationalization",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#deep-learning",
    "href": "integ/tools.html#deep-learning",
    "title": "26  Data Science Tools",
    "section": "26.6 Deep Learning",
    "text": "26.6 Deep Learning\n\nTensorFlow\nKeras\nTorch\nPyTorch\nMicrosoft Cognitive Toolkit (CNTK)\nOpenAI\nOpenCV\nViso Suite from viso.ai\nDeepLearningKit for Apple tvOS, iOS, OS X\nH2O.ai\nCaffe from Berkeley AI Research",
    "crumbs": [
      "Module VI. Operationalization",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#ides-and-developer-productivity",
    "href": "integ/tools.html#ides-and-developer-productivity",
    "title": "26  Data Science Tools",
    "section": "26.7 IDEs and Developer Productivity",
    "text": "26.7 IDEs and Developer Productivity\n\nIPython: a command shell for interactive computing\nJupyterLab and Jupyter Notebook\nSpyder: IDE for Python\nVSCode: a code editor with IDE-like plugins\nVisual Studio (an IDE)\nDataSpell from JetBrains\nPyCharm: a commercial IDE for Python from JetBrains\nGoogle Colab(oratory)\nGit, GitLab, GitHub\nGitHub Copilot\nRStudio (free open-source edition)",
    "crumbs": [
      "Module VI. Operationalization",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#cloud-computing-devops",
    "href": "integ/tools.html#cloud-computing-devops",
    "title": "26  Data Science Tools",
    "section": "26.8 Cloud Computing & DevOps",
    "text": "26.8 Cloud Computing & DevOps\n\nAWS (Amazon Web Services)\nMicrosoft Azure\nGCP (Google Cloud Platform)\nDocker\nKubernetes (K8s)\nFly.io\nCloudflare\nJenkins (CI/CD)\nAnsible",
    "crumbs": [
      "Module VI. Operationalization",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#web-development",
    "href": "integ/tools.html#web-development",
    "title": "26  Data Science Tools",
    "section": "26.9 Web Development",
    "text": "26.9 Web Development\n\nSvelte\nVue.js\nAngular\nReact\nD3.js (data visualization)\nLaravel (PHP based)\nGraphQL (Graphene, Apollo etc.)\nNodeJS\nFlask\nDjango\nHeroku\nVercel, Next.js\nNetlify\nAWS Amplify\nAWS Lambda\nMongoDB Realm\nFirebase\nDigitalOcean",
    "crumbs": [
      "Module VI. Operationalization",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "integ/tools.html#programming-languages",
    "href": "integ/tools.html#programming-languages",
    "title": "26  Data Science Tools",
    "section": "26.10 Programming Languages",
    "text": "26.10 Programming Languages\n\nR\nPython\nSQL\nScala\nJulia\nPHP\nHTML\nCSS\nC/C++\nGo\nRust\nJava\nJavaScript\nTypeScript\n\n\n\n\n\nMeta, Analytics at. 2023. “Data Engineering at Meta: High-Level Overview of the Internal Tech Stack.” Medium. https://medium.com/@AnalyticsAtMeta/data-engineering-at-meta-high-level-overview-of-the-internal-tech-stack-a200460a44fe.",
    "crumbs": [
      "Module VI. Operationalization",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Data Science Tools</span>"
    ]
  },
  {
    "objectID": "ethics/intro.html",
    "href": "ethics/intro.html",
    "title": "27  Introduction",
    "section": "",
    "text": "27.1 Why Now?\nThe conversations about ethics, bias, and fairness in data science have intensified over the past decade. Yet, data analysis and decisions based on data are not a new phenomenon. What changed? Why is data ethics a hot button issue today?",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ethics/intro.html#why-now",
    "href": "ethics/intro.html#why-now",
    "title": "27  Introduction",
    "section": "",
    "text": "More data is being collected and data owners want to know how their data is being used.\nMore decisions are based on algorithms derived from data versus algorithms based on expert knowledge or business rules.\nAutomation enables us to apply algorithms at greater scale.\nMore data fall under the definition of personal data and it is easier to infer personal data by combining data sources.\nArtificial intelligence and machine learning has penetrated domains where it replaces sensing (reading, seeing, listening) rather than logic.\nArtificial intelligence is now capable of generating content that can easily be mistaken for human-generated material, and it can fake and impersonate.\nData-driven models are used in situations where much is riding on the decisions: sentencing guidelines, credit approvals, medical diagnosis, identification, …\nGreater complexity of data-based models leads to greater opaqueness; many models are inscrutable black boxes that are difficult to explain.\nThe consequences of bias or unfairness are severe. Beyond legal repercussions when organizations break the law, missteps are damaging to reputation and destructive to the business.\nUnintended consequences of using data. A well-intended application can have negative side effects that cause harm and jeopardize the business. We will see several examples.\nInternet-based data collection (web scraping, online questionnaires, social media feeds) can lead to large databases of unknown quality and representativeness. This raises questions about inherent bias in the data. It also raises questions about ownership and right to use these data.\n\n\n\nExample: Baker Institute: More Automation means More Inequality\n\n\nIn a 2020 study by the Baker Institute for Public Policy at Rice University, the authors conclude that increased automation does not lead to a loss of jobs as much as it leads to an increase in inequality. They separate workers into three coarse groups: skilled workers that represent the top-10% of wages, medium-skilled workers that represent the next 40%, and low-skilled workers that represent the lower half of the wage distribution. The macroeconomic model used in the study is pretty simple, assuming that medium- and low-skilled workers can be replaced by automation and robotics, but not highly skilled workers. In that scenario, the wage share of skilled workers increases, and that of other workers, in particular medium-skilled workers decreases.\nSince 2020, things have changed. With the rise of generative AI, skills targeted for possible automation have reached a different, much higher, level.\n\n\n\n\n\n\nLoukides, Mike, Hilary Mason, and D. J. Patil. 2018. Ethics and Data Science. O’Reilly Media. https://resources.oreilly.com/examples/0636920203964/.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ethics/gonewrong.html",
    "href": "ethics/gonewrong.html",
    "title": "28  How Things Go Wrong",
    "section": "",
    "text": "28.1 Cybersecurity\nWith respect to the consequences of missteps and unintended consequences, data systems are entering a phase that cybersecurity has gone through previously. Today, the cybersecurity posture of an organization is a top priority. CISOs, Chief Information Security Officers, are common among high-ranking executives, whereas Chief Data Officers and Chief Analytic Officers tend to be found in Director-level positions. That is changing as the strategic importance of data-related activity increases; CDOs and CAOs are on the way up on the corporate ladder.\nInformation security, the security of IT assets, data, and operations, is a modern form of risk management and mitigation. This includes safeguarding information about the organization and its clients (customers).",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>How Things Go Wrong</span>"
    ]
  },
  {
    "objectID": "ethics/gonewrong.html#cybersecurity",
    "href": "ethics/gonewrong.html#cybersecurity",
    "title": "28  How Things Go Wrong",
    "section": "",
    "text": "Example: Equifax Data Breach\n\n\nApache Struts is a web application framework to develop Java applications. On March 7, 2017, a patch was released for Struts to fix a security vulnerability. The national credit bureau company Equifax had not updated applications on its website when the vulnerability was exploited on its site on March 12, 2017. The hackers accessed information about Equifax employees and used their credentials to scan databases and extract personal information on over 160 million citizens. The breached information included names, birth dates, addresses, social security numbers, and driver’s license numbers.\nWhile it took only five days from the release of the security fix to an exploit, it took Equifax more than two months to discover the breach and it took until September 2017 to disclose the breach. A week later the Chief Information Officer and Chief Security Officer left the company. Hundreds of lawsuits were filed against Equifax after the breach became public. In 2019, Equifax settled with the Federal Trade Commission for $575 million, $475 million in restitution and $100 million in fines.\nAfter the Equifax data breach, the Chief Information Officer of a major international bank called Apache Struts “the CIO killer.”\nRead more about the Equifax case here including the criticism of the company due to inadequate response, fumbling notification websites, and executives selling stock before the breach was made public.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>How Things Go Wrong</span>"
    ]
  },
  {
    "objectID": "ethics/gonewrong.html#model-risk-management",
    "href": "ethics/gonewrong.html#model-risk-management",
    "title": "28  How Things Go Wrong",
    "section": "28.2 Model Risk Management",
    "text": "28.2 Model Risk Management\nData breaches are not the only source of data-related risk to be mitigated. You can now add the risk from applying models trained on data to the list—models can be incorrect, and they can be misapplied and misused. In financial services, model risk management (MRM) has been a staple for years.\n\n\nDefinition: Model Risk Management (MRM)\n\n\nModel risk management is the management and mitigation of risks that arise from the consequences of decisions based on models that fail to perform.\n\n\nIn general, MRM is concerned with risks from the consequences of decisions based on incorrect, misapplied, or misused models. The FDIC (Federal Deposit Insurance Corporation) provides guidance to banks about effective model risk management. Banks are relying routinely on models for valuations, underwriting credit, determining adequacy of reserves, safeguarding client assets, etc. The use of models invariably presents risk that can lead to financial loss, poor decision making, and damage to reputation. The FDIC notes two primary reasons for model risk:\n\nModels may be fundamentally wrong due to incorrect assumptions, underlying theories, choice of sample design, choice of numerical routines, implementation errors, inadequate or erroneous inputs, and so forth.\nThe model may be fundamentally sound but is used or applied incorrectly. Real-world events might prove the underlying assumptions of a model inappropriate—e.g., a global pandemic. The model might be applied to situations for which it was not developed, for example, to new products or markets.\n\nWe recognize in these points the concepts of data drift and model drift discussed early on. It is also clear that the concept of model risk management is not germane to the financial service industry—it applies everywhere. A key principle of effective MRM is to challenge models effectively, testing and validation play an important part. When models are developed and tested under assumptions that do not reflect their use in real life, model risk increases dramatically. This is what happened to Microsoft Tay.\n\n\nExample: Microsoft Tay\n\n\nMicrosoft released in 2016 a Twitter AI chat bot, called Tay (as in Thinking-about-you) that was trained to behave like a teenager in interactions with Twitter users and learn from them. Within less than a day online, Tay had turned into a hateful racist, claiming in offensive tweets that, for example, “Hitler was right”, recommending feminists should all die, and denying the Holocaust. Microsoft blamed Tay’s responses on trolls teaching the AI bot deliberately inflammatory content.\nTay was a successful AI project in that the bot learned how to communicate based on who it was communicating with. But the internet can be a dark and awful place and Tay learned awful things from the people that engaged with the bot. The bot was not a successful AI project in that it did not incorporate content moderation and boundaries. The developers probably assumed that users on the internet would interact with the bot in the way the developers were interacting with it.\nWhen systems can learn, how do we make sure that they do not learn bad things? Is it ethical to limit what a system can learn? Or is it unethical to expose users to what a system can possibly learn?\nMicrosoft did not break any laws with Tay. The bot was unethical in that it was developed without safeguards that prevented offensive and hurtful responses. AI bots are highly complex pieces of software; the unintended consequences arose from Tay still being good enough for the intended application. It probably worked very well in a sheltered test environment with benign interactions.\nMicrosoft CEO Satya Nadella gave the Tay episode credit for how Microsoft is approaching AI today.\n\n\nThe reasons for model risk stated by the FDIC are somewhat obvious: (i) the model is wrong, (ii) the model is correct but misapplied. We add another source of model risk: models that are trained well, almost too well. The following example reminds us that by merging, processing, and modeling data we can find out more than we should know. While it might not be illegal to have that knowledge, it is unethical to use that knowledge.\n\n\nExample: Target Pregnancy Prediction\n\n\nAn angry father walked into a Minneapolis Target store and demanded to know why the company was sending his teenage daughter coupons for pregnancy- and baby-related items. He wanted to know if Target is trying to encourage his daughter to become pregnant. It turns out Target had indeed sent pregnancy marketing material to her, and it turns out that she was indeed pregnant–unbeknownst to her father at the time he called on the store.\nIt is disputed whether this actually happened or is an anecdote. What is not disputed, as reported by the New York Times, is that Target had developed a predictive model that assigns customers a pregnancy score based on demographic data merged with data from customers who had baby registries at a store, and a set of products that indicated a high probability of a pregnancy when purchased together.\nBy analyzing data of customers who had a baby registry, they\ndeveloped a predictive model for all shoppers. The algorithm performed so well that Target could determine the trimester and estimate the due date. This enabled all kinds of targeted, personalized marketing efforts. From the New York Times:\n\n… women on the baby registry were buying larger quantities of unscented lotion around the beginning of their second trimester. Another analyst noted that sometime in the first 20 weeks, pregnant women loaded up on supplements like calcium, magnesium and zinc. Many shoppers purchase soap and cotton balls, but when someone suddenly starts buying lots of scent-free soap and extra-big bags of cotton balls, in addition to hand sanitizers and washcloths, it signals they could be getting close to their delivery date.\n\nTarget did not run afoul of any laws, they used internal and public data available to them. They developed a model that performed well by statistical standards. But just because you can do something does not mean you should. You can learn something legally about someone and it can still be ethically questionable or even inappropriate to use that information.\nTarget customers were actually creeped out when they learned that the retailer knew intimate personal details about them. The company adjusted its marketing approach to be more subtle. The predictive models are still being used.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>How Things Go Wrong</span>"
    ]
  },
  {
    "objectID": "ethics/gonewrong.html#unintended-consequences",
    "href": "ethics/gonewrong.html#unintended-consequences",
    "title": "28  How Things Go Wrong",
    "section": "28.3 Unintended Consequences",
    "text": "28.3 Unintended Consequences\nThe law of unintended consequences describes the phenomenon when an action leads to results that are not the intended purpose of the action. Typically, we think of the unintended consequences as detrimental; they can be beneficial a well.\nTechnology, all the inventions of the human mind, are full of unintended consequences. Comic Chuck Nice says that future technology comes with two things: promises and unintended consequences.\nYou can reduce the likelihood of unintended consequences—positive or negative ones—by deeply understanding the systems involved. Greater complexity likely leads to more unintended consequences as complex systems cannot be fully comprehended. Unfortunately, our capabilities through innovation and our ability to build complex systems increases more quickly than our ability to foresee the consequences. In his TED talk, Edward Tenner gives examples of unintended consequences from innovation throughout history. For example, the introduction of HVAC systems in the 1970s raised the temperature of water in these systems to the optimum for reproduction of Legionella bacillus, the bacteria causing Legionnaire disease—an unintended consequence. A bactericide was added to HVAC systems to prevent the disease. In the early 1980s it was observed that computer tape drives were failing in drives close to ventilation ducts. The bactericide contained traces of tin which destroyed the tape heads—an unintended consequence of the safety intervention.\nWhen we build more complex and more opaque systems from data, we should expect more unintended consequences when working with data. While we develop the systems with the best of intentions, we should always ask “What could go wrong?”\n\n\nExample: Facebook’s Year in Review\n\n\nWhen Facebook first rolled out the Year in Review feature, it was intended to digitally scrapbook the previous year based on highly liked and photo-heavy content. Cheerful iconography surrounded images through which Facebook users were prompted to create their digital scrapbook for the year.\nAuthor and designer Eric Meyer called it Inadvertent Algorithmic Cruelty in his blog, when Facebook invited him to celebrate the past year with an image of his daughter, who had died. He writes:\n\nThis inadvertent algorithmic cruelty is the result of code that works in the overwhelming majority of cases, reminding people of the awesomeness of their years, showing them selfies at a party or whale spouts from sailing boats or the marina outside their vacation house.\n\nBut for those of us who lived through the death of loved ones, or spent extended time in the hospital, or were hit by divorce or losing a job or any one of a hundred crises, we might not want another look at this past year.\n\nThe application is designed with an ideal user in mind, someone who is happy, had a great year, and would like to see a summary. It does not consider the worst-case scenario and lacks empathy for the far-from-ideal users. Meyer concludes:\n\nIf I could fix one thing about our industry, just one thing, it would be that: to increase awareness of and consideration for the failure modes, the edge cases, the worst-case scenarios.\n\n\n\nIt is easy to brush aside unintended consequences as not knowable, the so-called unknown unknowns. They will certainly remain unknowns if you do not want them to be known. Hindsight has always 20:20 vision, but some unintended consequences are in fact knowable and foreseeable. We have to make an effort to think about possible unintended consequences and be creative in imagining what could happen. Data sharing is an interesting case in point. Much good can be done by combining data sources that complement each other—the total can be more than the sum of the parts. Medical research can benefit by combining anonymized data from clinical trials that are conducted in isolation. Data collected in an app with user consent can be made available to a wider audience and make the world a better place—or it can backfire spectacularly.\n\n\nExample: Strava Exercise Maps\n\n\nExercise app Strava opened up its data on Strava.com in 2018 to allow users to find new places to exercise. The move was well-intended. Strava had collected at that time data on more than a billion runs, walks, bicycle rides, and swims all over the world. For someone looking for a place to exercise that information would be highly valuable. Maybe you can find a trail or bike path in your neighborhood that you were not aware of.\nHowever, the move turned into a privacy nightmare (Burgess, n.d.). Military researchers discovered that the heatmaps of exercise activity made it possible to study the workouts of military personnel around military bases. Although the data shared online was anonymized, it was possible to de-anonymize the data by requesting information for a specific geographic location. With this method it was even possible to see the names and heart rates of some individuals. If your daily run starts and ends at a military basis, then it is easy to infer that you are somehow connected to the military. By revealing common movements, someone with ill intent could use the information to figure out how a military basis operates.\nThe Guardian determined that by drilling into the data at Strava.com, they were able to reveal the names of 50 U.S. service members stationed at an airbase in Afghanistan. From changes in running profiles one can also glean when service members are transferred.\nRevealing information about military operations was an unintended consequence of the release of information on Strava.com. It was not a data breach, it was done deliberately, with other goals in mind. According to the Guardian, the company argued that the information was already made public by the users who uploaded it. In hindsight, it was a foreseeable consequence of sharing geotagged data.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>How Things Go Wrong</span>"
    ]
  },
  {
    "objectID": "ethics/gonewrong.html#measuring-performance",
    "href": "ethics/gonewrong.html#measuring-performance",
    "title": "28  How Things Go Wrong",
    "section": "28.4 Measuring Performance",
    "text": "28.4 Measuring Performance\nShould we accept a more accurate approach over a less accurate approach? Many would say “Yes”, greater accuracy is better. The more accurate method can have other shortcomings, for example, it may be inscrutable or very difficult to implement and the nod might go to the less accurate method that is easier to implement. In this situation we are making a deliberate choice to prefer one approach over another despite its inferiority with respect to a specific metric. Other considerations also matter.\nAll things being equal, accuracy is not necessarily the appropriate measure by which to judge the performance of a decision rule. The confusion matrix of a binary classification leads to many performance measures for the model, see Section 18.3.3.3.\n\n\nExample: COMPAS Tool to Predict Recidivism Risk\n\n\nJudges, parole, and probation officers are increasingly relying on algorithms to determine an offender’s propensity for recidivism—to re-offend. A higher likelihood to re-offend might result in a stiffer sentence. One such algorithm was developed by Northpointe, Inc. and is part of their COMPAS tool (Correctional Offender Management Profile for Alternative Sanctions).\nUpon being booked in jail, a questionnaire of 137 questions is filled out, answered in part by the defendants or by pulling data from criminal records. Questions include things like “How often did you get in fights while at school?”, “How often have you moved in the last 12 months?”, and whether they agree with statements such as “Some people just don’t deserve any respect and should be treated like animals”. The questionnaire is known, but the algorithm that uses the answers to score defendants on the likelihood to re-offend within two years, is proprietary. Two risk scores are produces by the COMPAS tool: the risk of general recidivism and the risk of committing a violent crime in the future.\nThe idea of using algorithms to make key decisions in the legal process, not guided by conscious or unconscious personal bias, is appealing. An accurate prediction of recidivism can make the justice system fairer. But that requires that the algorithm works, it cannot itself be biased. Someone said that even a slightly flawed algorithm is better than a bigoted judge, but the consequence of applying even a slightly biased algorithm at scale, across the nation, in thousands of cases can be disastrous.\nIn 2016, ProPublica investigated the performance of the proprietary algorithm using data from more than 7,000 criminal defendants in Broward County, Florida (Angwin et al. (2016), Larson et al. (2016)). For these defendants the predicted recidivism and whether the individual actually re-offended was known. This allows the investigators to probe how well the COMPAS model is actually doing in predicting outcomes.\nA notebook with data management and analysis using R and Python is available on GitHub, along with the data—made available by ProPublica.\nThe ProPublica analysis uses logistic regression and survival models to compare COMPAS scores among races and genders. The results include confusion matrices for recidivism classification (Low/High) and future violent crime (Low/High). Some of the important findings are:\n\n51% of defendants were Black, 34% White, 8% Hispanic\n81% of defendants were Male, 19% were Female.\nBlack defendants are 45% more likely than white defendants to receive a higher risk score when correcting for seriousness of the crime, previous arrests, and future criminal behavior. The risk score for future violent crime overpredicts recidivism for black defendants by 77% compared to white offenders.\nWomen are 19% more likely to receive a higher risk score than men.\nDefendants under 25 are 2.5 times more likely to receive a higher risk score than middle-aged defendants.\nThe predictive accuracy of the COMPAS model is only 63% (as measured by a Cox proportional hazard model).\nOnly 20% of those predicted to commit violent crimes went on to do so.\nThe false positive rate of the model is 32.3% overall, 44.8% for Blacks, and 23.4% for Whites.\nUnder COMPAS, black defendants are 91% more likely to get a higher score and not go on to commit more crimes over the next two years.\nCOMPAS scores misclassify white re-offenders as low risk 70.4% more often than black re-offenders.\nBlack defendants are twice as likely to be false positives for a higher violent crime score than white defendants.\nWhite defendants are 63% more likely to get a lower violent crime score and go on to commit another crime.\n\nWith an accuracy of just over 60%, the system is barely better than a coin flip, which would have an accuracy of 50%. The racial disparities in the predictions from the model are deeply troubling. In predicting who would re-offend, the algorithms made mistakes for Black and White defendants at similar rates but in very different ways.\n\n\n\n\n\n\n\n\n\nWhite\nBlack\n\n\nLabeled higher risk for recidivism but did not re-offend\n23.5%\n44.8%\n\n\nLabeled lower risk for recidivism and did re-offend\n47.9%\n28.0%\n\n\n\nFor Blacks, the algorithm has a high false positive rate (44.8%). For Whites, the algorithm has a high false negative rate (47.9%). When the algorithm predicts incorrectly, it leads to lighter consequences for Whites and to tougher consequences for Blacks.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>How Things Go Wrong</span>"
    ]
  },
  {
    "objectID": "ethics/gonewrong.html#automation-and-scale",
    "href": "ethics/gonewrong.html#automation-and-scale",
    "title": "28  How Things Go Wrong",
    "section": "28.5 Automation and Scale",
    "text": "28.5 Automation and Scale\nWe automate processes to achieve greater efficiency and productivity, to scale up operations, and to reduce time spent on mundane repetitive tasks. As someone explained,\n\nwhen I do something for the second time, I think about how I did it last time. If I do it for the third time, I think about how to automate it and never do it again.\n\nScaling a process through automation is wonderful if automation makes the process better or if the outcome is inherently good. If automation of tax refunds means that refunds reach taxpayers sooner, no one will complain. If automation of traffic signals means better traffic flow through the city and less time spent staring at brake lights, grumbling is unlikely.\nHowever, if automated decisions are of lesser quality, more biased and potentially harmful, then exercising them at scale can cause great harm.\nA cruel example where this scenario played out is the former “Robodebt” system of the Australian Government.\n\n\nExample: Robodebt Scheme of the Australian Government\n\n\nBetween 2016 and 2019 the government of Australia, through its social services agency Centrelink, wanted to save billions of dollars by recovering social security (welfare) overpayments to recipients. The process previously in place checked for compliance manually and resulted in about 20,000 cases per year. It was based on matching income reported in two different places: the tax office served as the system of record for annual income, and the self-reported bi-weekly income by the welfare recipient. If there was a discrepancy between reported incomes, a staff member would investigate and request that the recipient explain the difference.\nAn automated system, it was thought, could do this more efficiently, issue more debt notices, and hence recover more overpayments. The system would more than pay for itself. Such a system was put in place in 2016. In the first six months in operation, it issued already 169,000 debt notices. But that was not the only difference between the manual and the automated system.\nWhile the manual (human-based) system would issue a notice to the recipient to please explain the difference, the new automated system, when it found a discrepancy between tax-reported and self-reported Centrelink income, would automatically issue a debt notice and expect repayment—hence the name “Robodebt”. The presumption of innocence turned into presumption of guilt. The onus was suddenly placed on the individual to prove that they did not owe the government funds—without human verification or interaction.\nThis is a horrible practice, unethical and possibly illegal, even if the overpayments are correctly assessed. But that is another way in which the automated system went astray. Robodebt had a staggering false positive rate of at least 27%! If you were a social security recipient who received the appropriate payment from the government, you had more than a 1-in-4 chance to be issued a request for overpayment.\nOver the period of operation, 567,000 debts were raised through the use of the Robodebt algorithm.\nAfter the government responsible for Robodebt was voted out of office, the incoming administration killed the program and announced that 470,000 wrongly-issued debts would be repaid in full. The error rate is over 80%!\nIn the article “The Robodebt strategy” in the December 2023 issue of Significance, Trewin, Fisher, and Cressie (2023) note the following main statistical flaws of Robodebt:\n\nUsing annual tax income to match against bi-weekly self-reported income. Bi-weekly income can fluctuate greatly throughout the year and cannot be simply compared against distributing the annual income evenly.\nNo documented understanding of error sources with the data or the data matching process.\nNo sensitivity analysis and inadequate testing of algorithms\nNo understanding of error rates\nNo involvement of professional statisticians.\n\nA system that was supposed to save the government A$300 million cost it A$1.2 billion in repayments alone. The human toll was devastating: some recipients of debt notices had committed suicide.\nThe statistician Noel Cressie called Robodebt\n\na catastrophic program that was legally and ethically indefensible—an example of how technological overreach, coupled with dereliction of duty can amount to immense suffering for ordinary people.\n\nHe noted that Jensen’s inequality tells us that if the algorithm called out overpayments it should have also detected underpayments and issued credits (Cressie (2023)). Robodebt never issued any credits.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>How Things Go Wrong</span>"
    ]
  },
  {
    "objectID": "ethics/gonewrong.html#bad-data-flawed-algorithm-poor-implementation",
    "href": "ethics/gonewrong.html#bad-data-flawed-algorithm-poor-implementation",
    "title": "28  How Things Go Wrong",
    "section": "28.6 Bad Data, Flawed Algorithm, Poor Implementation",
    "text": "28.6 Bad Data, Flawed Algorithm, Poor Implementation\nImplementing a flawed algorithm poorly and feeding it bad data is a recipe for disaster. In the following example, that recipe led to the murder of victims of domestic violence. As a recent audit of the system suggests, at least some of these murders were preventable.\n\n\nVioGén Algorithm to Monitor Gender Violence\n\n\nSince 2007, Spain has been operating the VioGén system, a web application to help Spanish authorities to protect women and children from domestic violence. Based on an external audit by the Eticas Foundation and reported by Politico and others, the system is deeply flawed. (See here for the full report in Spanish).\nThe combination of numerous shortcomings, from bad data, to an opaque algorithm, to lack of human intervention, leads to an underestimation of the risk of violence. Women do not receive the proper protection, some are sent back into a domestic violence environment, and some have paid for it with their lives.\nThe underlying algorithm uses classical statistical models for risk classification, yielding a risk score based on the weighted sum of responses on an intake questionnaire. The system is designed as a recommendation system, assigning a “recommended” score to an individual with the opportunity for adjustment by a human. That human would usually be a police officer who runs the algorithm on the questionnaire answers provided by the victim of domestic violence. The Eticas audit revealed that in 95% of the cases the score returned by the algorithm is not being adjusted. Because of lack of human intervention, the recommendation system becomes the automated decisioning system.\nThe algorithm assigns a case to the risk categories “unappreciated”, “low”, “medium”, “high”, and “extreme”. Only one in seven women who reached out to police for protection actually received protection. Many women reporting violence receive an “unappreciated” risk score; 45% of the cases. Only 3% of women who are victims of gender violence receive a risk score of medium or higher. Furthermore, the system caps the number of higher risk scores at the level at which protection is funded in the budget.\nIs the algorithm biased and is it underestimating the risk? Probably so. The algorithm focuses on physical violence and under-values psychological and non-sociological violence, for example, through social media. The lack of transparency of the algorithm does not help. The fact that only 35% of the women are told their risk scores is suspicious.\nWhat is not in doubt are the flaws in the collection of data for the risk prediction algorithm. When a woman makes an official complaint, authorities present them with a standardized set of binary questions assessing 39 risk indicators as “present”/“not present”. Questions explore the severity of previous assaults (e.g., whether weapons were ever used), the features of the aggressor (jealous, bully, sexual abuser, unemployed, drug addict, etc.), the vulnerability of the victim (pregnant, foreign, economically dependent, etc.), and aggravating factors. The questions are blunt and direct, sometimes embarrassing. In the situation of the complaint, the victim of domestic violence is often terrified, under extreme duress, in a state of shock, emotionally charged. That is not the time to fill out a very consequential questionnaire. Police officers are reported to leave victims alone with the questionnaire and pressure them to finish it quickly.\nThe VioGén algorithm and questionnaire have been revised several times, the latest iteration seems to be in 2019. The Eticas Foundation audit was published in 2024. Recent efforts to improve the VioGén system aimed at introducing machine learning to first predict the probability of recidivism and then to adjust that probability based on indicators about the perpetrator. A former employer of mine is involved in this effort; see here and here.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>How Things Go Wrong</span>"
    ]
  },
  {
    "objectID": "ethics/gonewrong.html#from-revenue-management-to-price-fixing",
    "href": "ethics/gonewrong.html#from-revenue-management-to-price-fixing",
    "title": "28  How Things Go Wrong",
    "section": "28.7 From Revenue Management to Price Fixing",
    "text": "28.7 From Revenue Management to Price Fixing\nBecause algorithms can make decisions at scale they are often said to be the key to drive harm and discrimination out of systems affected by subjectivity and bias of human reasoning. We have seen numerous examples in this chapter where algorithms, even if developed with the best intentions, can cause harm. Any positive or negative effects are amplified at scale.\nRevenue Management Systems (RMS) are computerized decision systems that determine prices for goods or services. The goal is typically to optimize—read maximize—revenue for the operator of the RMS. Suppose two competitors operate in the same market, offering the same or a similar product. The consumer is likely to choose the lower-priced alternative, motivating the competitors to compete on price. The competition benefits the consumer. If, however, the companies agree to not undercut each other’s prices and instead raise prices in parallel, the consumer is at a disadvantage. Effectively, the competitors are colluding to fix prices above what a healthy market would lead to. This is called supra-competitive pricing.\nThe collusion can be indirect in that it can happen through a third party that computes the optimal pricing for a product in a certain market, when the competitors are all using the same pricing tool. The RMS is the pricing tool in this case, and when it is the dominant (or only) mechansim to determine prices we essentially approach monopoly conditions.\n\n\nExample: IDeaS RMS for Hotel Management\n\n\nA class action antitrust lawsuit filed in April 2024 in federal court in San Francisco alleges that SAS Institute, Inc., IDeaS, Inc., Hilton Worldwide Holdings Inc., Wyndham Hotels & Resorts, Inc., Four Seasons Hotels and Resorts US Inc., Omni Hotels and Resorts, Inc., Hyatt Hotel Corporation, and Choice Hotels International Inc. used price fixing to inflate the price of hotel rooms.\nThe agreement to fix prices might have been tacit, meaning the executives of the hotel chains did not get together in a room and agreed to manipulate prices for their benefit. Instead, the anti-competitive action resulted from all hotel chains using the same revenue management system and how that RMS operates. The RMS at the heart of the antitrust lawsuit is G3 RMS of IDeaS (Integrated Decisions and Systems, Inc.), a Minnesota-based subsidiary of SAS Institute. IDeaS was acquired by SAS in 2008.\nFull disclosure: SAS Institute is one of my former employers, I worked there from 2002–2021 and towards the end of my tenure served as a high-ranking company executive.\nThis article by CBS News details how the IDeaS algorithm is alleged to support anticompetitive price fixing:\n\nIDeaS’s clients, all competitors with each other, agree to supply IDeaS with proprietary, non-public, and sensitive information about their room availability, demand, and pricing. The information is provided continuously, in real time, so that IDeaS always knows what all of its client-competitors have available in a given market.\n\nLike other revenue management systems, the vendors claim that they can optimize prices better than a person could. The suit alleges that the hotels participate knowing which of their competitors are part of the conspiracy, that they know IDeaS is collecting similar information from participating competitors and that faithful adherence to IDeaS’ price recommendations will yield greater revenue for all participants. According to plaintiffs,\n\nby sending their sensitive confidential pricing and occupancy information to a third party to process, analyze, and develop supra-competitive prices, the [defendants] are able to achieve the same result as if they secretly met in a back room and exchanged their information and agreed to a supra-competitive price.\n\n\n\nFor another example how a revenue management algorithm leads to higher prices and how concentration in an industry can create anti-competitive conditions that put the consumer (here, the renter) at a disadvantage, check out the ProPublica investigation into the RealPage YieldStar software used to determine prices for rental housing.\nThe RealPage and IDeaS cases have things in common:\n\nA proprietary optimization algorithm targets maximum revenue through modeling metrics such as price elasticity: how much does a change in price affect the demand? There is nothing unethical or illegal in modeling price elasticity, it is a common economic metric.\nThe models become very effective across sub-markets with access to large databases of market data, actual prices that consumers pay under known conditions. In a normally functioning market, competitors would not share that kind of data with other competitors. How you determine your price and based on what information is a trade secret.\nThe software vendors act as clearinghouses for data sharing across competitors. While they might not see each other’s data directly, the sharing enables pricing algorithms that yield higher-than-competitive prices that the client’s would otherwise not be able to achieve.\nThe market concentration of the participating companies creates anti-competitive pricing across many markets.\n\n\n\n\n\nAngwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. “Machine Bias—Risk Assessment in Criminal Sentencing.” ProPublica, May. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.\n\n\nBurgess, Matt. n.d. “Strava’s Heatmap Was a ‘Clear Risk’ to Security, UK Military Warned.” https://www.wired.co.uk/article/strava-heat-maps-military-app-uk-warning-security.\n\n\nCressie, N. 2023. “Robodebt Not Only Broke the Law of the Land – It Also Broke Laws of Mathematics.” The Conversation. https://theconversation.com/robodebt-not-only-broke-the-laws-of-the-land-it-also-broke-laws-of-mathematics-201299.\n\n\nLarson, Jeff, Surya Mattu, Lauren Kirchner, and Julia Angwin. 2016. “How We Analyzed the COMPAS Recidivism Algorithm.” ProPublica, May. https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm.\n\n\nTrewin, D., N. Fisher, and N. Cressie. 2023. “The Robodebt Tragedy.” Significance 20 (6): 18–21. https://academic.oup.com/jrssig/article/20/6/18/7457250.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>How Things Go Wrong</span>"
    ]
  },
  {
    "objectID": "ethics/bias_harm.html",
    "href": "ethics/bias_harm.html",
    "title": "29  Bias and Harm in Algorithms",
    "section": "",
    "text": "29.1 Are Data Biased?\nThe previous section presented examples where things went wrong, where algorithms had unintended consequences and side effects. It is easy to say that the data is biased and thus the fault lies with the data used in training models. The data can be a major source of problems in data analytics. But the answer cannot be as simple as saying “garbage—in—garbage—out”.\nTraining models on data that are not representative of the situation to which the model will be applied is just one possible form of bias, called representation bias. It can occur even if the data was collected perfectly, measured without error, sampled adequately—for a different purpose than it is now used. This is not unlike our statistical understanding of bias, as the (average) difference between an estimator and its target. A perfectly qualified estimator can be biased for one target and unbiased for another target. The sample mean \\(\\overline{Y}\\) of a random sample is an unbiased estimator of the mean \\(\\text{E}[Y] = \\mu\\) of the sampled population and is a biased estimator of the median of the population unless the distribution of \\(Y\\) is symmetric (mean and median are then identical). Context matters.\nRather than taking a 30,000-foot view that “data are biased” or “data can be biased” and harm of data-dependent models is a possible consequence, it is worthwhile to drill deeper and ask what types of biases exist and how unintended consequences and harm flows from the way in which we build, test, and deploy models. Data can be biased. We need to ask in what way bias can be introduced into data, through data, and through processing data. Without this understanding we cannot apply remedies.\nAlgorithms are normally not purposefully built to be unfair or cause harm. Instead, they are the result of processes that lead to algorithms that make unfair or harmful decisions. So how does it happen that an algorithm in Austria determines that women have lower employability than men if all other characteristics are equal? How is it possible that female job seekers are less likely to see advertisements on Google for high-paying jobs than their male counterparts? We know that the algorithm is unfair because we can test it. That is exactly what researchers from Carnegie Mellon did; they put the advertisement algorithm through the ringer with a designed experiment. As reported by the Guardian:\nIn the excellent paper “A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle”, on which this discussion is partly based, Suresh and Guttag (2021) give the following example where the same intervention to correct bias works in one case and fails in another.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Bias and Harm in Algorithms</span>"
    ]
  },
  {
    "objectID": "ethics/bias_harm.html#are-data-biased",
    "href": "ethics/bias_harm.html#are-data-biased",
    "title": "29  Bias and Harm in Algorithms",
    "section": "",
    "text": "One experiment showed that Google displayed adverts for a career coaching service for “$200k+” executive jobs 1,852 times to the male group and only 318 times to the female group. Another experiment, in July 2014, showed a similar trend but was not statistically significant.\n\n\n\n\nExample: Sample More Women\n\n\nThe goal is to build a model to predict the risk of a heart attack. A researcher trains a model on medical records from prior patients at a hospital and observes that the false negative rate of the trained model is higher for women than for men. Assuming that the reason for the poorer predictive performance is a lack of data on women in the records, the researcher adds more data on women who suffered heart attacks. The re-trained model shows improved performance in predicting heart attacks in women.\nIn another application, a hiring model is developed to determine suitability of candidates based on their resume, augmented by candidate ratings supplied by human evaluators. It is observed that the model predicts women to be less likely as suitable candidates compared to men. To correct this shortcoming, more data is collected on women, but the model does not improve. In both scenarios the model builders recognized a deficiency in the model and tried to correct it—that is ethical behavior.\nBut the same intervention, collecting more data on the group for which the model performed poorly, was successful in one instance and pointless in the other. The reason is that the model deficiency is due to different sources of bias. In the medical example, the training data under-represented women, there was not enough information about women in the data to create a model that was as accurate for that group as it was for the group more heavily represented in the data—men.\nModifying the sampling process changed the distribution of genders in the training data. In the hiring example, the bias is not introduced through the representativeness of the sample, but through the human-assigned rating. When the human assessment of candidate suitability disadvantages women over men, adding more data will not correct the model.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Bias and Harm in Algorithms</span>"
    ]
  },
  {
    "objectID": "ethics/bias_harm.html#seven-eight-sources-of-harm",
    "href": "ethics/bias_harm.html#seven-eight-sources-of-harm",
    "title": "29  Bias and Harm in Algorithms",
    "section": "29.2 Seven (Eight) Sources of Harm",
    "text": "29.2 Seven (Eight) Sources of Harm\nSuresh and Guttag (2021) describe seven distinct sources of bias and associate them with steps in the modeling cycle, from data preparation to model deployment.\n\n\n\n\n\n\nFigure 29.1: The seven sources of harm in machine learning models (Suresh and Guttag 2021).\n\n\n\n\nHistorical Bias\nModels do not extrapolate what they learned from the training data to situations that do not reflect the training data. They can create associations only from what is in the training data: the past is prologue.\nA system that reflects accurately how the world is or was can inflict harm, either by unfairly withholding resources (allocation harm) or by perpetuating stereotypes (representational harm).\nHistorical bias, also called pre-existing bias, is rooted in social institutions, practices and attitudes that are reflected in training data. Baking these into the algorithm reinforces and materializes the bias. In the example of the Austrian employability algorithm, the data reflected an accurate representation of a discriminatory labor market, women, especially women with children, are less likely to be found in high-paying jobs. The algorithm trained on this data assigned a lower employability score to women.\nIf you train a model on data from textbooks, then it is more likely to identify engineers as men than as women, since that profession is historically more likely to be depicted by men. Word embeddings are numerical representations of text data used to train language models. Any stereotypes in the source texts on which the embeddings are based will live on in the models developed based on the embeddings.\nThe historical bias might not be evident right away. It can rear its ugly side when the model is applied in an unforeseen way. Google Translate got into trouble when documents were translated from Hungarian into English. Hungarian is a gender-neutral language, so the algorithm had to apply pronouns in the English translation. From the article on DailyMail.com:\n\n‘She’ was placed before domestic chores such as cleaning and sewing, while ‘he’ was used for intellectual pursuits such as being a politician or a professor.\n\n\n‘He teaches. She cooks. He’s researching. She is raising a child. He plays music. She’s a cleaner. He is a politician. He makes a lot of money. She is baking a cake. He’s a professor. She’s an assistant.’\n\nThe researcher who found the issue blamed Google. Google blamed the algorithm on society: the algorithm inadvertently replicated gender biases that exist in the information it scraped from the internet.\nHow can we fix this type of bias?\n\nChange the data. One could revisit the training data in the translation example and change gender pronouns to achieve better balance across attributes. The trained algorithm then would hopefully reflect that balance. Modifying data is considered unethical. There be dragons! What proportion of female pronouns should be used for, say, a profession. The currently prevailing proportion or a desired future state? In Hungary or the U.S? Who decides what that is?\nUse different data. Finding a data source that does not have historical bias baked into it sounds good but might not be possible.\nAdjust the model. Corrections can be made to the inner workings of the model. For example, instead of using gendered pronouns the translation can use gender-neutral language: “they cook, they teach”. Alternatively, the gender pronouns in the translation can be chosen randomly.\n\n\n\nRepresentation Bias\nRepresentation bias bias occurs because of a mismatch between sample, target, and application population. The target population is the universe we have in mind in developing an algorithm. The sample population is the population represented by the training data. The application population is the universe to which the model is applied. It is easy to create mismatches.\nA model developed for house values in Boston probably does not apply in Chicago without re-training on data from Chicago. The target population (Boston) and application population (Chicago) are mismatched.\nSampling bias—also called selection bias—is a form of representation bias where the sampling method is defective and does not represent the target population. A random sample is always representative of the target population but there are many ways in which the training data can deviate from the properties of a random sample from the target population:\n\nAvailable data can misrepresent the target population, for example, if medical data is only available for individuals having a condition.\nRandom sampling that does not reflect the distribution of groups in the target population, for example, drawing random samples from lists of users (website, mobile phone, phone book, tax records) does not accurately represent the overall population (over-/under-coverage).\nMissing values processes that are related to the objective of the study. When recipients in a mental health survey are less likely to respond because of their mental health status, the sample of survey responses is biased (non-response bias).\nSelf-selection: participants who exercise control over their inclusion in a study can bias the data. Folks calling into radio shows or participating in online polls are particularly motivated by issues and their responses overestimate strong opinions in the population.\nSurvivorship bias can occur when the sample focuses on those meeting a selection criterion. For example, studying current companies does not reflect companies that have previously failed.\nDrop-out: subjects drop out of a long-term study for reasons related to the objective of the study. Losing study participants in a longitudinal health study that move out of the area because of a new job probably does not bias the data. Losing study participants because of deteriorating health might bias the data.\n\n\n\nExample: Literary Digest Poll\n\n\nA famous example of sampling bias because of under-coverage and non-response bias was the 1936 poll by the Literary Digest magazine to predict the outcome of the presidential race between incumbent Democratic candidate Franklin D. Roosevelt and Republican candidate Alf Landon. The magazine sampled voters based on its own subscribers, phonebooks and car registration data and predicted that Alf Landon would win the election.\nInstead, Franklin D. Roosevelt defeated Landon in a landslide. Although it had sampled millions of voters, the magazine’s prediction was way off, by more than 19%. The embarrassment contributed to the magazine’s demise by 1938.\nSampling bias due to an inappropriate sampling frame was one factor for the imprecise survey. Subscribers to Literary Digest, phone users and automobile owners were on average wealthier than the average American. Their responses over-estimated the proportion of Republican voters. The second, and primary source of bias was non-response bias: those who disliked Roosevelt were more likely to respond in favor of Landon.\n\n\nIn an earlier chapter we mentioned ImageNet, a large database of images used in computer vision challenges and to benchmark object classification algorithms. ImageNet does not represent a random sample of all images that could be taken. Almost half of its images originate in the U.S., only 1—2% of the images come from China and India. A computer vision model trained on ImageNet data is expected to perform worse classifying objects from images from those geographic areas.\nThis is a special kind of bias. Groups that occur in the data less frequently are predicted with less accuracy, simply because there are fewer training samples in those groups compared to others. In the ImageNet example this is due to the way the data are collected from the internet. If a population is properly sampled randomly, minorities will appear less frequently in the sample than majorities. When model performance is analyzed stratified by groups, the model will be less accurate (have a larger prediction error or mis-classification rate) for groups that contributed fewer observations. Differentiated levels of accuracy are unfair and potentially harmful.\nThe cause of representation bias might not be immediately obvious, while its presence can be very obvious in the results.\n\n\nExample: “Delving” into ChatGPT\n\n\nJeremy Nguyen asked on X (Twitter) why there was such an increase in the recent use of “delve” in medical papers and presented Figure 29.2 as evidence. The tweet appeared in March 2024, so we still had a long way to go in that year to grow the bar on the right.\n\n\n\n\n\n\nFigure 29.2: Use of “delve” in papers on PubMed. Source:JeremyNguyenPhD on X\n\n\n\nUse of “delve” has been on the increase for some time, but in 2023 it took off dramatically. Do medical papers just do more delving since then? It has been noted that ChatGPT responses seem to use certain words more frequently than the internet at large. Examples are “delve”, “explore”, “tapestry”, “testament” and “leverage”. Frequently enough to be seen as a giveaway that text was generated by GPT. GPT 3.5 was released in late 2022 and 2023 is the first full year the world enjoyed ChatGPT. Is Figure 29.2 evidence that (many) medical papers are now being written—or at least augmented—with the help of ChatGPT?\nHow is it possible that GPT, which is trained on a massive corpus of text found on the internet, has such an affinity for “delve” and uses it more frequently than would be consistent with the training data? The reason is a special form of representation bias. ChatGPT is a question-answer application built on top of the foundation large-language model GPT. The ChatGPT training uses a form of reinforcement learning, called reinforcement learning with human feedback (RLFH). That is a fancy way of saying that in order to perform reinforcement learning, the algorithm needs a reward function with which to rate competing actions–that is, competing responses to the prompt. Scoring the quality of a textual response is difficult and this is where human evaluators come in. The human evaluation can alter or correct an initial response from the AI to create a response with a higher reward score. When this altered response is fed back into the algorithm, the model will eventually learn to respond to maximize the reward, and respond in a way similar to the human evaluator.\nThis is the first part of the story. The second part of the story is that human evaluation and feedback in training LLMs is time consuming and costly. That is why tech companies outsource this work to places where anglophonic knowledge workers are cheap. As the Guardian points out in this article, “delve” is much more frequent in business English in Nigeria than in the U.S. or the U.K. Outsourcing the task to provide human feedback to Nigeria creates a feedback loop that makes ChatGPT write more African English than its original training data.\n\n\n\n\nMeasurement Bias\nMeasurement bias—also called detection bias or information bias—is due to systematic errors in the process of data collection. Random data measurement errors do not contribute to measurement bias, they cause greater variability in the data. A scale that is not properly calibrated, on the other hand, will give systematically incorrect readings of weight—that is a biased measurement.\nIncorrect instruments are a source of this bias and insufficient accuracy or limitations of the instruments. Measuring distance that exceed 10 feet with a 10-foot tape measure causes observations to be censored. The true distance is larger than 10 feet, but we do not know the exact value. The accuracy of instruments can vary with its range; being most accurate in the mid-range and less accurate above or below.\nBias in measurements applies to qualitative variables as well, for example, when survey interviewers ask participants to rate their satisfaction for the wrong timeframe.\nThese sources of measurement bias are obvious. Suresh and Guttag also discuss a type of measurement bias that could be called proxy bias. The target variable of interest is often not directly observable. Instead, we use proxies to operationalize the concept of interest. Creditworthiness, for example, is not directly observable, and we use credit scores as the proxy. To model student success, we need to define what we mean by that. The choice of proxy affects the quality of the inference. An oversimplification such as GPA can bias the results because it does not apply to all students, does not capture learning outside of the classroom, does not capture skills not assessed as grades, etc.\nWhile the concept of interest is consistent across groups, e.g., has a disease, the technique for measuring its proxy, the diagnosis, can vary among groups. Regional or cultural differences lead to different interpretations of the same context when annotating images. The same 5-point rating scale is used differently depending on someone’s interpretation of the categories. Does the “best” category mean “best in my experience so far” or “the best one could ever hope for”?\nIn the COMPAS system for predicting recidivism discussed previously, variables like arrested and re-arrested are proxies for criminality. Measurement bias is introduced when certain communities are policed more intensely than others. If the extent to which a higher number of arrests reflects a higher policing intensity is not accounted for, the analysis will be biased and disadvantage the community where policing is higher.\n\n\nAggregation Bias\nAggregation bias occurs when data are grouped or aggregated in a way that ignores or obfuscates important subsets of the data. Trends and associations that we see in aggregated data are not necessarily present in the non-aggregated data. Similarly, trends and associations in non-aggregated data can be missed in aggregated data. For example, working with data at an annual level hides seasonal trends. This can be desirable. It can also lead to bad decisions by not accounting for systematic variation at a smaller level.\nConfusing the units of inference with the units of analysis is known as an ecological fallacy: to assume that what is true for a population is also true for the individual members of the population. One commits an ecological fallacy by concluding that individuals from families in poverty perform less well in school by studying the correlation between average poverty level in schools and school test averages. The analysis is based on aggregates at the school level, an association at that level does not imply an association at the individual level. A proper analysis takes a multi-level approach that models effects at the student, classroom, and school level.\n\n\nLearning Bias\nThis type of bias occurs when the process of training the model introduces bias by emphasizing some criteria more than others. For example, classification models are often trained to minimize the mis-classification rate (maximize the accuracy) on a test data set. Models with a high accuracy are not guaranteed to have a high true positive or true negative rate. If it is important to minimize for a low false positive or a low false negative rate, then the learning algorithm should focus on that criterion (probably at the expense of the overall accuracy).\nIf the purpose of deriving a model is confirmatory inference, then driving the mean-squared prediction error is unlikely to lead to the best model for hypothesis testing.\nTechniques that simplify models such as pruning decision trees or compressing neural networks can amplify performance disparities on data with under-represented groups because the models retain the most frequent features.\n\n\nEvaluation Bias\nEvaluation bias is related to learning bias in that it is caused by the ways in which we evaluate model performance. The use of benchmark data sets to judge the quality of a model is common in computer vision applications. This becomes problematic if the performance of the model against the benchmark is more important than solving the problem in the first place. In particular, if the benchmark data is not without issues. Training a face recognition algorithm against a benchmark data set that under-represents dark-skinned faces encourages models that do not as well for that group than for light-skinned faces.\nEvaluation bias can also occur if we focus model evaluation on individual metrics and compare models on metrics that do not sufficiently differentiate. An example is the use of global goodness-of-fit criteria such as AIC (Akaike’s Information Criterion), BIC (Bayesian Information Criterion). Or Adjusted-\\(R^2\\). These statistics are frequently used to compare models that are not nested—that is, models that cannot be reduced by applying a hypothesis. An example is comparing a random forest and a regression model. Choosing the “best” model based on a single, overall number is straightforward and dangerous. You might end up with the best of a bunch of bad alternatives.\n\n\nDeployment Bias\nThis type of bias occurs when the model is used in a way that does not match the problem the model was intended to solve. This can be due to additional decisions that are made in processing the model output.\nSuppose a model predicts the employability of an individual on a scale from 0—1. The model fulfills its purpose: to provide an indication of employability based on characteristics of an individual. To operationalize the model in the administration of municipal resources for training and employment support, its predictions are classified into three categories, based on cutoffs between 0 and 1:\n\nHighly employable individuals \\(\\rightarrow\\) they do not need any assistance.\nMedium employable individuals \\(\\rightarrow\\) they can receive assistance.\nPoorly employable individuals \\(\\rightarrow\\) they cannot receive assistance.\n\nThe cutoffs are likely chosen based on budgetary considerations, what the city can afford. The way in which the model is deployed will withhold resources from those in the “lost cause” category, and probably unfairly so.\nThe COMPAS recidivism model is another example of deployment bias. Originally intended to predict the risk of predicting a future crime, the scores are used to determine the length of sentences.\n\n\nEmergent Bias\nThis source of bias is not discussed in the paper by Suresh and Guttag, but plays an increasingly important role in the era of models that continuously learn and adjust, such as AI models.\nEmergent bias does not exist when a data product is first released. It emerges because the product changes (evolves) as users interact with it.\nA good example of emergent bias and harm is Microsoft’s Tay chatbot—discussed earlier. It was not a hateful racist when it was first released. Through interaction with users, it was trolled into adopting hateful rhetoric and opinions. Microsoft did not program Tay to be racist, but Microsoft programmed Tay to learn and adapt through interactions.\nEmergent bias is also at work when we like things explicitly—hit the “Like” button—or implicitly—post about a topic—on social media. The algorithms analyzing our engagement are more likely to recommend to us similar topics and add those to our social media feed. A bias bubble emerges that over-emphasizes information we are likely to like.\n\n\n\nFigure 29.1: The seven sources of harm in machine learning models (Suresh and Guttag 2021).\nFigure 29.2: Use of “delve” in papers on PubMed. Source:JeremyNguyenPhD on X\n\n\n\nSuresh, H., and J. Guttag. 2021. “A Framework for Understanding Sources of Harm Throughout the Machine Learning Life Cycle.” https://arxiv.org/pdf/1901.10002.pdf.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Bias and Harm in Algorithms</span>"
    ]
  },
  {
    "objectID": "ethics/privacy.html",
    "href": "ethics/privacy.html",
    "title": "30  Personal Information and Personal Data",
    "section": "",
    "text": "30.1 Privacy and Confidentiality\nWorking with data requires compliance with data privacy regulations, an increasingly complex field. It is our job to safeguard all information, not just information about individuals. In the everyday use of the words, we might not make a distinction between privacy and confidentiality. Legally, and technically, there is an important distinction. Privacy is about people; it is about the individual’s rights regarding their information. Confidentiality is about an individual’s data and how it is protected from others.\nConfidentiality can be required explicitly: confidentiality clauses in employee agreements can limit information we are allowed to share about our employer. In the course of employment, we are learning trade secrets and proprietary information about products that constitute a competitive advantage. Data collection in research projects follows detailed protocols to ensure integrity and trust in the information. Non-disclosure agreements (NDAs) place restrictions on information that can be shared.\nBesides explicit confidentiality agreement, there is an expectation of confidentiality beyond privacy. Many consumers understand that organizations have access to their data. Their concern is how the organization uses the data, and ensures that it remains a secret. An organization that meets the letter of data privacy regulations can still run afoul of the confidentiality expectations of its employees and customers.\nThe Code of Conduct of the Data Science Association has this to say about confidential information:\nSpecial safeguards and regulations apply to personal data and personal information—information that is about people or relatable to people. This is the realm of data privacy. Not safeguarding personal information can lead to harm for both sides: identity theft, fraud, violation of rights, etc., for the individual whose personal data is mishandled; fines, legal action, loss of license, embarrassment, loss of business, etc., for the individual mishandling the personal information.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Personal Information and Personal Data</span>"
    ]
  },
  {
    "objectID": "ethics/privacy.html#privacy-and-confidentiality",
    "href": "ethics/privacy.html#privacy-and-confidentiality",
    "title": "30  Personal Information and Personal Data",
    "section": "",
    "text": "(a) Confidential information is information that the data scientist creates, develops, receives, uses or learns in the course of employment as a data scientist for a client, either working directly in-house as an employee of an organization or as an independent professional. It includes information that is not generally known by the public about the client, including client affiliates, employees, customers or other parties with whom the client has a relationship and who have an expectation of confidentiality. The data scientist has a professional duty to protect all confidential information, regardless of its form or format, from the time of its creation or receipt until its authorized disposal.\n\n\n(b) Confidential information is a valuable asset. Protecting this information is critical to a data scientists reputation for integrity and relationship with clients, and ensures compliance with laws and regulations governing the client’s industry.\n\n\n…\n\n\n(h) A data scientist shall protect client confidential information after termination of work for the client.\n\n\n(i) A data scientist shall return any and all confidential information in possession or control upon termination of the data scientist - client relationship and, if requested, execute an affidavit affirming compliance with obligations relating to confidential information.\n\n\n\n\n\nExample: Henrietta Lacks and the HeLa Cell Line\n\n\nHenrietta Lacks was born on August 1, 1920, in Roanoke, VA. She is probably one of the most important medical pioneers of the 20th century, with contributions that study the effect of toxins, drugs, hormones, and viruses on the growth of cancer cells. Not through her own medical work, but through the cells that were taken from her—without consent.\nMrs. Lacks had moved to Maryland and in 1951 visited Johns Hopkins Hospital in Baltimore complaining of vaginal bleeding. A malignant tumor was found on her cervix, and she started radiation therapy. On October 4, 1951, the cervical cancer took her life.\nCancer cells retrieved from her through a biopsy were sent to the tissue lab of Dr. George Gey. To his astonishment, Dr. Gey found that Mrs. Lacks’ cells behaved differently than cancer cells from other patients. While other cells would die within days of culturing, Mrs. Lacks’s cells continued to live and doubled about once a day.\nThe cells, called the HeLa cell line, is the first immortalized human cell line and continues to live to today. It has been used in countless medical research projects. As was practice at the time, no consent was required from the patient to culture the cells, and to distribute them. This would be unthinkable today. Even so, it was an unethical practice back then.\nAfter her death, Dr. Gey instructed his lab assistant to collect more cells while her body was in the autopsy facility.\nJohns Hopkins claims to have never sold or profited from the HeLa cell line and has given them away freely and widely for scientific research. Johns Hopkins admits that it could have done more to work with the Lacks family. They were not informed until 1975 about the existence of the HeLa cell line—another disturbing aspect about handling medical information without consent. The Lacks family found out about the cells because researchers contacted them and asked for blood samples hoping to use those to distinguish HeLa cells from other cell lines and through a conversation at a dinner party.\nMore than 10,000 patents involve HeLa cells. They were used to develop the polio vaccine, and in everything from AIDS research to gene mapping to testing cosmetics.\nIn October 2023, a statute of Henrietta Lacks was dedicated at Lacks Plaza in Roanoke. Small-scale statutes were dedicated at the Fralin Biomedical Research Institute at VTC, Virginia Tech Carilion School of Medicine, and Carilion Clinic. HeLa cells live at Fralin Biomedical Research Institute and in labs all over the world.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Personal Information and Personal Data</span>"
    ]
  },
  {
    "objectID": "ethics/privacy.html#data-privacy-regulations",
    "href": "ethics/privacy.html#data-privacy-regulations",
    "title": "30  Personal Information and Personal Data",
    "section": "30.2 Data Privacy Regulations",
    "text": "30.2 Data Privacy Regulations\nSince the late 2010s, data privacy regulations have appeared in many parts of the world. They are often similar but frequently different enough to create a messy landscape of laws that can be difficult to navigate. The most important regulations are\n\nGDPR: the General Data Protection Regulations of the European Union\nCCPA: the California Consumer Privacy Act\nCPRA: the California Privacy Rights Act\nVCDPA: the Virginia Consumer Data Protection Act\nCPA: the Colorado Privacy Act\n\nThis article in Bloomberg Law provides a good overview of the four pieces of legislation.\n\n\n\n\n\n\nFigure 30.1: Basics of major data privay regulations. Source.\n\n\n\n\n\n\n\n\n\nFigure 30.2: Data protected by major data privay regulations. Source.\n\n\n\nThere is substantial overlap between the regulations, and also important differences. For example, the U.S. laws, which are based on bottom-up common law, are opt-out laws, whereas the top-down GDPR is an opt-in law. Consider consent, for example. An opt-out law assumes that consent is automatically given unless someone opts out. Under GDPR, consent is automatically withheld unless someone opts in explicitly (Figure 30.1).\nThe type of personal data protected under the regulations differs as well. The California statutes extend personal data to data about a household. When modeling consumer behavior, for example, modeling at the aggregate household level does not relieve you from worrying about personal data privacy of California residents. The U.S. laws exclude publicly available data from the definition of personal data, while GDPR includes publicly available data (Figure 30.2).\n\n\n\n\n\n\nFigure 30.3: Who must comply with major data privay regulations. Source.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Personal Information and Personal Data</span>"
    ]
  },
  {
    "objectID": "ethics/privacy.html#what-is-personal-data",
    "href": "ethics/privacy.html#what-is-personal-data",
    "title": "30  Personal Information and Personal Data",
    "section": "30.3 What is Personal Data?",
    "text": "30.3 What is Personal Data?\nThings get complicated quickly because the definition of personal data varies and because there are many regulations to pay attention to. The General Data Protection Regulations (GDPR) of the European Union applies a very broad and non-prescriptive definition of personal information. HIPAA, The U.S. Health Insurance Portability and Accountability Act defines items specifically as PII, personally identifiable information. The primary focus of HIPAA is to protect personal information related to health. The National Institute of Standards and Technology, NIST, applies a different definition to PII. For example, your computer’s IP address is not PII under NIST standards but is personal data under GDPR. In 2011, the California State Supreme Court ruled that a person’s ZIP code is PII. The California Consumer Privacy Protection Act (CPPA) distinguishes personal information such as internet browsing history from sensitive personal information, for example, an account log-in or union membership.\nWe can never cover all the bases and thus recommend taking a conservative stance and a broad definition of personal information. If in doubt, consider a piece of data that can be linked to a person as personal information that is protected and needs to be safeguarded. The broad coverage of GDPR is a good place to start. In addition, you need to know the data privacy regulations that apply in a particular domain or to data from a specific group. The following are some federal laws that govern specific types of data:\n\nThe Privacy Act of 1974 governs how federal agencies can collect and use data about individuals in its system of records.\nThe Health Insurance Portability and Accounting Act (HIPAA) governs the collection of health information.\nThe Children’s Online Privacy Protection Act (COPPA) governs the collection of information about minors.\nThe Gramm Leach Bliley Act (GLBA) governs personal information collected by banks and financial institutions.\nThe Fair Credit Reporting Act (FCRA), which regulates the collection and use of credit information.\nThe Family Educational Rights and Privacy Act (FERPA), which protects the privacy of student education records.\n\nThe GDPR defines personal data in Article 4:\n\n‘personal data’ means any information relating to an identified or identifiable natural person (‘data subject’);\n\nThere is a lot to unpack here. What does “relating to” mean? What is the difference between an “identified” and an “identifiable” natural person?\nAn identifiable person can be identified by reference to a name, ID, location data, or by one or more other attributes specific to the identity of that person, such as genetic, mental, physical, cultural, or social information. To be identifiable, it is sufficient that you can distinguish them from other individuals. Personal data is not just what makes us unique, it is sufficient for the data to distinguish us from others. Your online shopping cart makes you an identifiable person because it can be used to distinguish you from people with a different shopping cart. It does not matter that there could be other shoppers whose cart looks exactly like yours.\nThe name is an identifier, but it might not make you identifiable—there are many “John Smiths”. It is still personal information.\nWhat is the meaning of “relating to”? A careful reading of the GDPR definition of personal data suggests that identifying data might not be personal data if it does not relate to the person. Wait, what? A few examples will make this clearer:\n\nInformation about an individual or about their activities is obviously personal data, it explicitly relates to the individual. Your birthday is your personal data. However, March 28, 2003, is just a date. It is not personal data until it is someone’s birthday, anniversary, wedding day, arrest day, etc.\nConsider a job listing that includes salary information. The salary is not personal information, it relates to no specific individual. As soon as the position is filled, the salary information becomes personal information because it now relates to the new employee.\nAn organization has an org chart with job titles. The job title in the org chart is not personal data until it is related to a specific individual or a group of employees.\nData about a house is not, by itself, personal data. But an individual’s tax record or utility bill are personal data. As soon as the information about the house is tied to an owner, it becomes personal data—it now relates to the owner. You can conduct analyses of house values in a geographical area. The houses in the study are selected because of their properties—four bedrooms and a double garage—not because they are occupied by a specific entity. But if the data about the house is used with respect to an individual, then it is personal data.\nA factory collects information about a machine on the factory floor. This is not personal data if the purpose of analysis is to understand the operating characteristics of the machine. If the data is used to assess the productivity of the machine operator, then it is personal data.\n\nThe definition of personal data in GDPR is non-prescriptive. The regulations do not explicitly state that a social security number is personal information—it is. Whether information is personal data depends on the context. Does it relate to distinguishable individuals? If so, it is personal data. GDPR is adaptable because of this approach, if we determine that an IP address is personal data in a context, then a MAC address is as well. The downside of a non-prescriptive definition is that context matters, and we need to know how to apply it.\nData does not have to be correct to constitute personal data. A social-security number entered into a database is still personal data, even if someone fat-fingered some digits.\n\n\nExample: Inaccurate Personal Data\n\n\nJohn and William live in an apartment building; John lives on the ground floor and William lives on the top floor. They both wear glasses.\nThe landlord receives a complaint that the man wearing glasses who lives on the ground floor has engaged in bad behavior. However, the complaint actually is about William’s activity.\nThe landlord records the information about the behavior relating to John. The information is inaccurate, but it is nevertheless personal data related to John. At the same time, this is also personal data about William, even though it’s been recorded about John.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Personal Information and Personal Data</span>"
    ]
  },
  {
    "objectID": "ethics/privacy.html#safeguarding-personal-information",
    "href": "ethics/privacy.html#safeguarding-personal-information",
    "title": "30  Personal Information and Personal Data",
    "section": "30.4 Safeguarding Personal Information",
    "text": "30.4 Safeguarding Personal Information\nIf you google how to safeguard personal information you get a wealth of similar responses—almost all of them are about how you can safeguard your personal information: use strong passwords, use two-factor authentication, don’t overshare online, turn off location services, watch out for phishing attempts, don’t follow unknown or suspicious links, keep your social security card in a safe place at home, …\nBut how do we safeguard other’s personal information that makes it into data sets we work with? There are a number of principles you can follow as an ethical data scientist. These (and more) are enshrined as the data protection principles of GDPR.\n\nLawful, Fair, and Transparent\nAct in the best interest of the data subjects, always on a legal basis and with best intentions. That means you had consent to collect and analyze the data and the scope of analysis does not exceed what can reasonably be expected.\n\n\nData Minimization\nOnly collect and keep the amount of personal data that is needed, in terms of the number of records and the number of variables. If you gather data to deliver a newsletter electronically, then all you need is the name and email address. Maybe you do not even need to know the name of the subscriber. Having their job title might be an interesting piece of information but it is not necessary to send out the newsletter—it should not be collected.\nSuppose you analyze data on houses to develop a predictive model for residential real estate prices, and the data set contains information about the owners of the properties. This information is not necessary for the purpose and should be removed from the data frame you work with.\n\n\nConfidentiality\nOnly people who should have access to data are working with the data. If you are in control of data, you have a responsibility to protect the personal data you are processing. A common means of violating this principle is sharing data with others not knowing that the data contains personal information. If you receive a request for access to data that you have access to, ask yourself whether the requestor is authorized to use the data. If in doubt, put security concerns ahead of trying to be helpful.\n\n\nStorage Limitation\nDo not store personal data you do not need anymore. This is related to the data minimization principle, do not keep what you do not need.\nDestroying data is trickier than you might think. There can be multiple copies of records, on multiple devices, including backups in the cloud, backups on tapes, etc. The same information can be stored at different levels of data preparation, for example in different layers of a data lake with medallion architecture. You can destroy data by removing it from storage devices using erasure, degaussing (destroying the magnetic field), or overwriting. Or you can destroy the storage device, for example by shredding hard drives.\nThe GDPR include a right to data deletion, any data subject can ask that their personal data be deleted. You need to know what personal data you have on someone, where it is, and how to destroy that data.\nIf you gather email addresses for the purpose of sending an electronic newsletter, and you decide to stop sending newsletter, you have to delete the email addresses because the purpose for which the personal data was given no longer exists.\n\n\nPurpose Limitation\nUse the data only for the purpose it was intended for. If the purpose expands or changes you need to re-secure consent. You cannot reuse personal data for other purposes. If you tell an online user that you store their IP address to document their consent, then you cannot use the IP address to send them marketing material.\n\n\nAnonymization and Pseudonymization\nAnonymization: remove personal information from a data set. The anonymized data should eliminate direct re-identification, for example, by combining other information in the data set.\nPseudonymization: pseudonymizing data is a weaker safeguard than anonymizing it. Pseudonymization replaces the original value of personal information with a mask or by encryption. If the pseudonymization is reversible, for example with an encryption key, the data is still considered personal information.\n\n\n\nFigure 30.1: Basics of major data privay regulations. Source.\nFigure 30.2: Data protected by major data privay regulations. Source.\nFigure 30.3: Who must comply with major data privay regulations. Source.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Personal Information and Personal Data</span>"
    ]
  },
  {
    "objectID": "ethics/genAI.html",
    "href": "ethics/genAI.html",
    "title": "31  Ethics of Generative AI",
    "section": "",
    "text": "31.1 A Brief History of AI\nArtificial Intelligence (AI) is in every conversation. Everywhere you turn you hear “AI this”, “AI that”, and (too) many people are now claiming to be AI experts. This has not always been the case. Some decades ago, when you’d tell anyone you work in artificial intelligence they would not take you serious. If you’d try to get a research grant you’d better not mention the term artificial intelligence. This was the time of one of the the AI winters that followed a period of overblown expectations and exuberant predictions what machines would be capable of doing.\nPeriods of AI hype (AI summers) and AI disappointment (AI winters) are cyclic. The peak of inflated expectations in a hype cycle is followed by the trough of disillusionment. Industry analyst firm Gartner makes a living from this cycle. Since the arrival of capable foundation models and GPT-based large language models in late 2022 we find ourselves in another AI summer. And once more you hear voices claiming machines have/are becoming sentient, that all our jobs are on the line, and that the era of artificial general intelligence, when machines can think for themselves, is just around the corner. We heard the same hype in the 2010s when deep learning-based neural networks bested us at image recognition and at playing games such as Go.\nDo we know more about AI now then we did back then? Is the hype now more justified? Are we better at predicting the future of technology now than we were back then?\nArtificial intelligence (AI) refers to building systems that can perform tasks or make decisions that a human can make. The systems can be built on multiple technologies, mechanization, robotics, software engineering, among them. Many AI systems today are entirely software based, large language models (ChatGPT, Gemini, Claude Sonnet) for text processing or diffusion-based systems for image generation are examples.\nFigure 31.1 shows an example of a mechanized AI system, called the Mechanical Turk. It was an automaton, a device made to imitate an human action. The action in this case was to play chess.\nYou can imagine that building a purely analog machine that plays chess is difficult. To accomplish this in the 18th century is really remarkable. Well, it turned out to be impossible. The Mechanical Turk was a hoax. The cabinet concealed an human player who operated the chess pieces from below the board.\nPerforming human tasks by non-human means is as old as humanity. Goals are increased productivity through automation, greater efficiency and strength, elimination of mundane, boring, or risky tasks, increasing safety, etc.\nThe two major AI winters occurred during the periods 1974–1980 and 1987–2000. One was triggered by disappointment with progress in natural language processing, in particular machine translation. The other was triggered by disappointment with expert systems.\nDuring the Cold War the government was interested in the automatic, instant translation of documents from Russian to English. Neural network architectures were proposed to solve the task. Today, neural network-based algorithms can perform language translation very well, it is just one of the many text analytics tasks that modern AI is good at. In the 1950s and 1960s progress was hindered by a number of factors:\nThe expectations for the effort were sky high, however. Computers were described as “bilingual” and predictions were made that within the next 20 years essentially all human tasks could be done by machines. These expectations could not be met.\nAn expert system is a computerized system that solves a problem by reasoning through a body of knowledge (called the knowledge base), akin to an expert who uses their insight to find answers based on their expertise. Expert systems were an attempt to create software that “thinks” like a human. The problem is that computers are excellent at processing logic, but not at reasoning. The reasoning system of these expert systems, called the inference engine, consisted mainly of rules and conditional (if-else) logic. We are still using expert systems today, but a very special kind, those that can operate with captured logic rather than asking them to reason. Tax software is an example of such an handcrafted knowledge system—a very successful one at that. Most taxpayers would not think twice to use software such as TurboTax or TaxSlayer to prepare their income tax return.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Ethics of Generative AI</span>"
    ]
  },
  {
    "objectID": "ethics/genAI.html#a-brief-history-of-ai",
    "href": "ethics/genAI.html#a-brief-history-of-ai",
    "title": "31  Ethics of Generative AI",
    "section": "",
    "text": "Figure 31.1: Mechanical Turk, a chess-playing automaton in the 18th century.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCynics might say that 2 1/2 centuries removed, we are still operating by a similar principle. When you ask a large language model to write a poem in the style of Edgar Allan Poe, there is no real poet behind the curtain crafting words. However, there are digital poets behind the curtain, represented by the volumes of literature used to train the language model so that it can respond in the requested style.\n\n\n\n\n\n\nlack of computing power to build large, in particular, deep networks\nlack of large data sets to train the networks well\nneural networks specifically designed for text data had not been developed\n\n\n\n\nEasy for Computers, Easy for Humans\nTax software works well and is a successful AI effort because it performs a task that is easy for computers that is intellectually difficult for humans. The tax code is essentially a big decision tree with many branches and conditions. Such a structure is easily converted into machine instructions. There is no reasoning involved, we just have to make sure to get all the inputs and conditionals right. If the adjusted gross income is $X, and the deductions are $Y, and the payer is filing jointly with their spouse, …, then the tax is this amount. Figuring out the correct tax is trivial based on the program. On the other hand it is impossible for us to memorize the entire logic and execute it without errors.\nAn expert system that performs logic reasoning is the exact opposite: it performs a task that is easy for us but very difficult to perform for a computer. Imagine to create an expert system that can operate a car by converting how a human operator drives a car into machine instructions. We instantly recognize an object in the road as a deer and plan an evasive action. A machine would need to be taught how to recognize a deer in the first place. It would have to be taught to choose an action when a deer appears in the road, or when a deer is in one lane of traffic and an oncoming car is in the other lane.\nHumans excel solving problems that require a large amount of context and knowledge about the world. We look at a photo or glance out the window and instantly see what is happening. We choose between hitting the deer, hitting the other car, and running off the road almost immediately, intuitively. Our value system and humanity drive the decision. Seeing, sensing, speaking, operating machinery are such problems. Unlike the tax code, they are very difficult to describe formally.\nThis changed—to some degree—in the mid 2000s. Computers were suddenly getting much better at these hard to formalize tasks such as sensing the world. You could call this period the AI spring before the ChatGPT AI summer we are in now. Our ability to solve problems that require knowledge about the world increased by orders of magnitude. The key was a new discipline, deep learning, which turned out to be a renaissance of decade-old ideas.\n\n\nNeural Networks–Again\nImagine writing computer software to recognize objects on images, for example facial recognition software. Explicitly programmed software had been around and was doing an OK’ish job at that. Algorithms were specifically designed to discover edges such as the outline of the face, identify eyes and noses and so on. We call them explicitly programmed algorithms because software developers created the algorithms that took an image as input and processed the pixels to discover faces.\nIn an implicit program, on the other hand, the software developer does not need to handle all aspects of the program. Many programming languages have implicit features. For example, a language can infer the data type of a variable without it being explicitly declared.\nAn extreme form of implicit programming is when the algorithm is generated as the result of other instructions. That is the case with deep neural networks trained on large volumes of data.\nA neural network is essentially an algorithm to predict or classify an input. The input could be a photo, the output of the algorithm are bounding boxes around the objects it classified on the photo, along with their labels and a photo caption. Neural networks are made up of many nonlinear functions and lots of parameters, quantities that are unknown and whose value is determined by training the network on data. Networks with tens of thousands or millions of parameters are not unusual. The layers of a neural network are related to levels of abstraction of the input data. Each layer processes a different aspect of the structural information in the input data. Whereas an explicit programmer knows when they write code that detects edges and which step of the program is locating the eyes of a face, what structure a particular layer of a neural network is abstracting is not known.\nFigure 31.2 shows a schema of the popular AlexNet network for image processing. It is a special kind of neural network, called a convolutional neural network, and won the ImageNet competition in 2012, classifying objects into 1,000 categories with a smaller error rate than a human interpreter.\n\n\n\n\n\n\nFigure 31.2: AlexNet, a convolutional neural network (Mallick and Nayak 2018).\n\n\n\nThe various layers of AlexNet tell us what their role is, some layers convolve the results of previous layers, others aggregate by pooling neighboring information, yet others flatten the structure, and so on. But they do not tell us exactly how the information in an input pixel is transformed as the data flows through the network. Nearly 63 million parameters act on the data as it is processed by the network; it is a big black box. Yet once the 63 million parameters are determined based on many training images, the neural network has turned into an algorithm with which new images can be processed. The process of training the network on labeled data, that is, images where the objects were identified to the network, implicitly programmed the classification algorithm.\nThis process of training deep neural networks on large data sets, made possible by the availability of large data and compute resources, overcame the limitations that held back neural networks previously. Implicitly programmed prediction and scoring algorithms were handily beating the best algorithms humans had been able to write explicitly. In the area of game play, deep learning algorithms implicitly programmed based on reinforcement learning were beating the grand masters and the best traditional computer algorithms.\n\n\n\n\n\n\nNote\n\n\n\nStockfish, one of the most powerful chess engines in the world is an open-source software project that has been developed since 2008. Many view it as the best chess engine humans have been able to build.\nIn 2017, Google’s DeepMind released AlphaZero, a system trained using reinforcement learning, a machine learning technique in which an agent (player) optimizes decisions in an environment (moves in a game) by maximizing the sum of future scores (rewards). Earlier, a Go system that was trained against millions of recorded expert-level games beat the best human Go player handily. What made AlphaZero special is that it was trained entirely by self-play, it improved by playing against itself.\nAfter only 24 hours of training, this data-driven system, crushed Stockfish, the best chess engine humans have been able to build.\n\n\nWhen decades-old neural network technology met up with Big Data and massive computing resources, capabilities made a huge leap forward in areas such as natural language understanding, image processing, autonomous driving, etc. The resulting hype was predictable: AI is coming for our jobs, the machines are out to get us, yada yada yada. Since deep learning algorithms could read images, it was predicted that radiologists would be replaced within a few years by machines. Yet not one radiologist has lost their job because of AI. Not one New York City cab driver has lost their job to a robo cab. Instead, they lost jobs to Uber and Lyft. Autonomous driving is still not fully possible. The ability to translate language based on recurrent neural networks and its cousins remained limited to relatively short sequences.\nWith the rise of deep learning neural networks and implicit programming algorithms pushed deep into domains we felt were uniquely human. The change feels more personal when machines replace brain function rather than muscle (brawn) function. We also gave up a very important attribute of decision systems: interpretability. The black box models do not make themselves understood, they do not explain how they work. We can only observe how they perform and try to make corrections when they get it wrong. These models do not tell us why they decide that an animal on a photo is more likely a cat than a dog. They do not understand “catness” or “dogness”. They are simply performing pixel pattern matching, comparing what we feed them to the patterns they have encountered during the training phase.\nWith the arrival of generative AI this seemed to have changed. AI algorithms appeared much smarter and to understand much more about the world. The length of text generated by GPT models seemed unlimited. With the release of GPT-3.5 and ChatGPT in late 2022 we all experienced a massive change in AI capabilities.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Ethics of Generative AI</span>"
    ]
  },
  {
    "objectID": "ethics/genAI.html#what-is-generative-ai",
    "href": "ethics/genAI.html#what-is-generative-ai",
    "title": "31  Ethics of Generative AI",
    "section": "31.2 What is Generative AI?",
    "text": "31.2 What is Generative AI?\nGenerative Artificial Intelligence (GenAI) refers to artificial intelligence systems that are not explicitly programmed and are capable of producing novel content or data. You can say that GenAI systems can generate data of the same kind that was used for training. A GenAI image system generates new images based on an input image, a text system generates new text based on input text. However, GenAI systems now can handle input and output of different modality: generating images or video from text, for example.\nThe underlying technology of a GenAI system can be a generative adversarial network (GAN), a variational autoencoder (VAE), a diffusion-based system for image generation, or a generative pre-trained transformer (GPT). Whatever the technology, GenAI systems have some common traits that are relevant for our discussion. Our previous discussion is relevant because some of these traits connect back to the properties of large neural networks.\nThe “T” in GPT stands for Transformer, a neural network architecture designed for sequence-to-sequence learning: take one sequence, for example, a text prompt and generate another sequence based on it, for example, a poem. Or, translate a sequence of text from one language to another language.\nVaswani et al. (2017) introduced transformer architecture to overcome the shortcomings of sequence-to-sequence networks at the time: lack of contextual understanding, difficulties with longer sequences, limited opportunities to parallelize training algorithms. This is the technology behind GPT, the generative pre-trained transformer. We now know what the “G” and “T” stand for. How about the “P”, pre-trained?\nPrevious AI systems such as the convolutional neural networks of the previous section were trained in an approach called supervised learning. During training, the algorithm is presented with labeled data, identifying the correct value of the data point. An image with a cat is labeled “cat”, an image with a mailbox is labeled “mailbox”. The algorithm associates features of the image with the provided labels. Presented with a new image it evaluates the probabilities that its contents match the patterns it has previously seen. The predicted label is that for the category with the highest probability.\nA GPT system is not trained on labeled data. It learns in a self-supervised way, finding patterns and relationships in the data that lead to a foundation model, fundamental understanding of the data used in training. GPT-3.5, a large language model with 175 billion parameters, was trained on text data from Wikipedia, books, and other resources available on the internet through 2021. Based on what GPT 3.5 learned from that database in a self-supervised way, applications can be built on top of the foundation model. ChatGPT, for example, is a “question-answer” system built on the GPT models.\nI am using quotation marks here to describe ChatGPT as a “question-answer” system because it is not trained to produce answers. It is trained to generate coherent text. The system is optimized for fluency, not for accuracy. That is an important distinction. Responses from large language models are coherent, fluent, and sound authoritative. That does not mean they are factually correct. If you consider that generating output in sequence-to-sequence modeling means to choose the most likely next word or token given the sequence of tokens generated so far, the fact that the responses are grammatically correct is an astounding achievement. The systems do not know a right from a wrong answer or a plausible response from an implausible response without human intervention. Unfortunately, we fall into the trap of mistaking a well-worded response for a factual response.\n\nLike all neural networks, transformers and GenAI tools have random elements. For example, the starting values of the network parameters are usually chosen at random. During training there are other random mechanisms at work, the selection of randomly chosen batches of observations for gradient calculations, etc. Once the parameters are determined, the responses from the model should be deterministic, right? Not true. Large language models contain random elements during the inference phase, when the model generates content based on user input. This makes the responses non-deterministic. Ask the same question three times and you might get three different responses.\nWhile this seems troubling, it is considered an important property of generative models that increases novelty and serendipity. Even with the so-called temperature parameter dialed all the way down, a small amount of variability remains.\n\nWe spend a bit of ink on past approaches to AI, neural networks, and transformers to get to this point. It helps to inform the next topic of conversation about the ethics of AI in general, and of generative AI in particular.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Ethics of Generative AI</span>"
    ]
  },
  {
    "objectID": "ethics/genAI.html#ethical-considerations",
    "href": "ethics/genAI.html#ethical-considerations",
    "title": "31  Ethics of Generative AI",
    "section": "31.3 Ethical Considerations",
    "text": "31.3 Ethical Considerations\nIn this section we draw, among other sources, on a presentation by Scott Mutchler, formerly of Trilabyte, now Associate Professor of Practice in the Academy of Data Science at Virginia Tech. The presentation is available here.\nEthics is the systematic study of what constitutes good and bad conduct, including moral judgments and values. It examines questions such as:\n\nWhat actions are considered right or wrong?\nWhat makes a good life or society?\nHow should moral values guide individual and collective decision-making?\n\nEthical AI usage deals with defining and implementing good conduct that is generally considered to be good for both individuals and society as a whole. It is an emerging and an important field that no one involved with or touched by AI can ignore.\nWe do not discuss malfeasance, illegal behavior, and other intentionally unethical acts here. These are not ethical. Period. If you use AI to imitate a voice in order to deceive someone, it is clearly unethical. Using AI generated content to disinform is unethical because disinformation is unethical, regardless of the channel.\n\n\n\n\n\n\nMisinformation and Disinformation\n\n\n\nThe difference between misinformation and disinformation is important. In both cases incorrect information is communicated. When you say something that is not true, it is misinformation. If the goal is to deceive, it is disinformation.\nFor example, telling Bob that the party starts at 9 pm when it starts at 8 pm is misinformation if you got the facts wrong. If you tell Bob the party starts at 9 pm because you want him to show up late, you engage in disinformation.\n\n\nWhat we want to discuss here are the ways in which AI, in particular generative AI, has ethical implications that you need to be aware of. Harmful results can come from unintended consequences, habitual behavior, built-in biases, and so on.\n\nHarm\nThe legal definition of harm is to cause loss of or damage to a person’s right, property, or physical or mental well-being. The ethical definition of harm goes further: to limit someone’s opportunities and to deny them the possibility of a good life because of our actions or decisions. Perpetuating stereotypes or misallocating/withholding resources is harmful, and therefore unethical, if it limits opportunities; it might not be illegal.\nThe types of of harm associated with generative AI can be grouped in several categories.\n\nWillful Malicious Intent\n\nFraud\nViolence\nDisinformation\nMalware generation\nInappropriate content (sexually explicit, hate speech)\n\nImpact on Jobs\nImpact on the Environment\nBias\nHallucinations\nIntellectual Property Rights\nPrivacy\n\nWe are not going to dwell on willful malicious actions, these are obviously harmful and unethical.\n\n\nImpact on Jobs\nAll technical developments affect jobs. New technology might give us new tools to do our jobs better, it might create new jobs, and it might eliminate jobs. The greater the technological step forward, the greater typically the impact on how we live our lives and how we earn a living. The industrial revolution caused major job losses in the agricultural society, by replacing human and animal labor with machines and folks moving into the cities to take non-agricultural jobs. It created many more new jobs than it eliminated and increased employment overall. Hindsight is 20:20, most occupations in the post-industrial period were unimaginable at the time of the industrial revolution. Try explaining to a loom operator in 1840 what a software engineer does.\nEvery major technological advance is accompanied with hype about the impact on society, lives, and jobs. The rise of AI is no different. When it seemed that machines can perform tasks that were previously the sole domain of humans during the deep neural network area, fears initially were stoked about machines taking over humanity Terminator-style. That did not happen and anxiety was modulated into machines taking all our jobs. But that did not happen either and the story changed again from machines replacing us to machines augmenting what we do, making us better at what we do. Instead of AI image processing replacing the radiologist, they now turned into better radiologists assisted by AI to handle the routine MRIs so that they can focus their expertise on the difficult cases.\nThe hype and story line around generative AI is even worse, as algorithms are now able to create quality content that previously required extensive training: art, writing, video, code, etc.\nGoldman Sachs estimated that as many as 300 million full-time jobs could be lost or diminished globally by the rise of generative AI, with white-collar workers likely to be the most at risk.\nWe should ask some questions here:\n\nWhat is the relationship between the number of jobs created by generative AI versus the number of jobs lost due to GenAI?\nWhich occupations are impacted and how?\nWhich jobs are augmented and enhanced by generative AI instead of eliminated or diminished?\n\nIn order for a technology to completely eliminate a job, all its constituent activities must be replaced. If GenAI generates content that a digital marketer produces, it cannot replace the marketer unless her other tasks are accounted for or we redefine what it means to do digital marketing.\nQuestions 1. Think of occupations that could be eliminated entirely by GenAI. 2. Which parts of jobs are susceptible to be replaced by GenAI? 3. Can you imagine new jobs created by GenAI?\nMcKinsey and Company believe that by 2030 generative AI could account for automation of 30% of the hours worked today (Figure 31.3). Most affected are those working in STEM fields, education and training, creatives and arts management, and the legal profession. Do you agree?\n\n\n\n\n\n\nFigure 31.3: Impact of automation through generative AI according to McKinsey\n\n\n\nIt is widely believed that generative AI will make us more productive. You can now generate large amounts of text or code in seconds. Software developers can whip up programs much more quickly thanks to AI coding assistants. The conclusion is that the first jobs to be impacted are those where generative AI excels and those that have mundane, repetitive tasks. For example,\n\nData Entry\nAdministrative\nCustomer Service\nManufacturing\nRetail – Check Out\nLow Level Analysts\nEntry-Level Graphic Design\nTranslation\nCorporate Photography\n\nThat is the theory.\nDespite the advances in (generative) AI, interactions with AI customer service agents continue to be disappointing. AI is used in self checkout lines in retail stores. Instead of cashiers staffing the registers, employees are now monitoring the self checkout station to help customers, perform age checks, etc. It does not appear that AI eliminated any jobs, it is making shopping more odious.\nLockett (2024) discusses a survey by Intel of 6,000 users of AI PCs in Europe. Hoping that their productivity is greatly enhanced by AI-assisted computing, they found the opposite. Rather than saving time and boosting productivity, users of AI PCs were less productive, spending more on tasks than users of traditional, non-AI PCs. Ooops. The users spent more time figuring out how to best communicate with the AI and moderating and correcting the output from AI.\nAmazon’s walk-out grocery stores, where you can just pick an item of the shelf and walk out while AI takes care of the rest, are converting to self-scan stores. AI made too many mistakes which required many new employees to monitor the video feeds and verify most purchases. Instead of automating the shopping experience, and saving human resources, the system created jobs and was not economical.\nLabor cost is the most important cost factor in many companies. In startup companies the human resource expenses can be 80% of the total operating expenses. In larger companies, you might still spend 50–60% of the operating expenses on personnel. The pressure to reduce cost by reducing headcount will not go away.\nWill generative AI create new jobs? The one job that was talked about when ChatGPT hit the scene was the Prompt Engineer. How the AI acts and the way in which it responds depends on how it is prompted (how it is asked). While prompt engineering is a thing, the prompt engineer as a profession is not. Writing good prompts will be more done behind the scenes, by apps and agents that call the LLM API on your behalf.\nThe disconnect between expectation and reality is evident when the vast majority of company executives believe (generative) AI will boost productivity while the majority of their employees state that AI has increased their workload, spending more time on moderating AI output than doing the work themselves.\nThis is a common refrain for AI tools. They are great for ideation, brainstorming, drafting. Getting a polished end product from AI is more difficult. A particular issue with generative AI are hallucinations.\n\n\nImpact on the Environment\nThe impacts of generative AI on the environment are positive and negative. Proponents of GenAI cite greater efficiency and productivity due to AI which allows organizations to run better and that saves resources. In a cost-benefit analysis it seems that these perceived resource savings due to efficiency gains are far outweighed by the negative impact of GenAI on the environment.\nThe enthusiasm around GenAI is due to its potential benefits, many of them have not been realized as organizations are struggling to implement GenAI solutions. Concerns such as hallucinations, bias, and others discussed in this chapter are contributing factors. The negative impacts on the environment are very real, however.\nBashir et al. (2024) point out this imbalance between perceived potential good and real downsides:\n\nThis incomplete cost calculation promotes unchecked growth and a risk of unjustified techno-optimism with potential environmental consequences, including expanding demand for computing power, larger carbon footprints, shifts in patterns of electricity demand, and an accelerated depletion of natural resources. \n\nWhat are the reasons for the environmental impact of GenAI? Training these models requires immense compute resources. The models have billions of parameters and training is an iterative process during which the performance of the model slowly improves over iterations. In fact, parameters such as the learning rate are managed during the training process to make sure that the model does not learn too fast.\nUntil the rise of artificial neural network in the 2010s, training statistical and machine learning models relied primarily on traditional CPU-style chip architectures. The prevalent calculations in neural networks are operations on vectors of numerical data not unlike those encountered in processing graphical images. Graphical processing units (GPUs) were quickly adopted by the AI community to speed up the training and inference of neural networks. GPUs provide much greater levels of parallelism than CPU technology.\nThis created a massive demand for GPUs, one that propelled GPU maker NVIDIA into the position of one of the most valuable companies in the world. GPUs require a lot of electricity and they generate a lot of heat. Even a high-end personal computers equipped with many GPUs might require water cooling instead of fans to keep the machine from overheating. Data centers consume a lot of power and require a lot of water for cooling. The rise of GenAI has increased the demand for data centers and will make these trends only worse.\nBased on calculations of annual use of water for cooling systems, it is estimated that a session of questions and answers with GPT-3 (roughly 10 t0 50 responses) drives the consumption of a half-liter of fresh water (Berreby 2024).\nScientists have estimated that the power requirements of data centers in North America increased from 2,688 megawatts at the end of 2022 to 5,341 megawatts at the end of 2023, partly driven by the demands of generative AI (Zewe 2025). Note that this increase coincides with the release of GPT-3 at the end of 2022. Data centers are among the top 10 electricity consumers in the world, consuming more than entire nations. By 2026, data centers are expected to rank 5th in the world in electricity consumption, between Russia and Japan. Experts agree that it will not be possible to generate that much power from green technology, increasing demand for fossil fuel. Some large technology companies are considering nuclear energy to power their data centers.\nA 2019 study estimated the carbon footprint of training a transformer AI model (as in GPT) as 626,000 lbs of CO2. Not that this predates the large GPT models like GPT-3 and GPT-4. We can only assume that their carbon footprint is much greater. The 2019 number alone is staggering when compared to the carbon footprint of a mid-size car. Training one transformer model is equivalent to the carbon footprint of 5 cars over their entire lifetime.\nThis is just the training of the model. The usage of the models also requires large amounts of computing resources. A single ChatGPT query has been estimated to consume about five times more electricity than a simple web search (Zewe 2025).\nThe casual user sending prompts to ChatGPT is not aware of the environmental costs of their queries.\n\n\nHallucinations\nA hallucination in a GenAI response occurs when the AI perceives patterns that do not exist and generates output that is incorrect or even nonsensical. It is a nice way of saying that generative AI are “bullshit” machines. Studies have found a hallucination rate of large language model (LLM) as high as 10–20%.\nRecall that large language models are optimized for fluency and coherence of the response, not for accuracy. Therein lies a danger because a plausible, authoritative, and well-crafted response seems more accurate than gibberish. Even if the responses contain the same information. Given that sequence-to-sequence models chose the next word of the response based on the likelihood of words, it is surprising that LLMs perform as well as they do. One explanation for this phenomenon is the human feedback the systems received during training. Machine learning through reinforcement learning relies on a reward function that ranks the possible actions. The change in score in a game is an easy way to track possible moves a player can make. When generating text in response to a prompt it is more difficult to rank possible answers. Human evaluators were used in the training phase to rank different answers so the LLM can learn.\n\n\n\n\n\n\nTip\n\n\n\nA good hallucination check is to ask an LLM a question for which you know the answer. For example, ask it to write a short bio of yourself.\nWhen I tried this in 2023, ChatGPT produced a lot of incorrect information about me—a lot. For example, it stated I was the president of a professional society on Bayesian statistics. I had never been the president of a professional society and have not been a member of any Bayesian societies. When I tried the experiment again in January 2024, I received the following response:\n\n\n\n\n\n\nFigure 31.4: My January 2024 biography according to ChatGPT.\n\n\n\nThis is factually correct. Also, ChatGPT indicated the sources from which it compiled the biography. It also did not construct any new text but combined passages from different sources into a coherent write up. A click on the Sources icon on ChatGPT showed that the system relied on 16 resources across the internet to articulate its response.\n\n\nYou can affect the LLM response and thereby the extent of hallucinations by prompting the AI. Two prompts are important in this respect: the system prompt and the constitutional prompt. They precede the actual inquiry to the system. A system prompt is more general, it specifies for example the format of the response or how the AI solves math problems. The constitutional prompt tells the AI exactly how to act.\nYou can find the system prompts for Claude Sonnet here. Notice that they change over time. Below is an excerpt from the November 22, 2024 prompt. Notice that the system prompt explains when Claude’s response uses the term hallucination.\n\nClaude cannot open URLs, links, or videos. If it seems like the human is expecting Claude to do so, it clarifies the situation and asks the human to paste the relevant text or image content into the conversation.\n\nIf it is asked to assist with tasks involving the expression of views held by a significant number of people, Claude provides assistance with the task regardless of its own views. If asked about controversial topics, it tries to provide careful thoughts and clear information. Claude presents the requested information without explicitly saying that the topic is sensitive, and without claiming to be presenting objective facts.\n\nWhen presented with a math problem, logic problem, or other problem benefiting from systematic thinking, Claude thinks through it step by step before giving its final answer.\n\nIf Claude is asked about a very obscure person, object, or topic, i.e. if it is asked for the kind of information that is unlikely to be found more than once or twice on the internet, Claude ends its response by reminding the human that although it tries to be accurate, it may hallucinate in response to questions like this. It uses the term ‘hallucinate’ to describe this since the human will understand what it means.\n\nAn example of a constitutional prompt that presses the AI to distinguish fact from opinion, avoid speculation, and express uncertainty, follows:\n\nYou are an AI assistant committed to providing accurate and reliable information. Always express uncertainty when you’re not completely sure about something.\n\nClearly distinguish between facts and opinions. When referencing specific information, mention its general source. Be upfront about the limitations of your knowledge, including your knowledge cutoff date.\n\nAvoid speculation and making up information beyond your training data. If a query is ambiguous or you lack sufficient information, ask for clarification rather than making assumptions.\n\nIf you realize you’ve made an error, acknowledge it immediately and provide a correction. When discussing uncertain topics, use probabilistic language like “evidence suggests” rather than making absolute statements. Encourage users to verify important information from authoritative sources, especially for critical decisions.\n\nIf asked about topics outside your area of knowledge, clearly state that you cannot provide reliable information on that subject. By following these guidelines, you will help ensure that your responses are as accurate and reliable as possible, minimizing the risk of hallucinations or misinformation.\n\nHere are a few other ways to reduce the amount of hallucinations:\n\nCross-referencing with reliable external sources (web search), other LLMs\nExternal validation, for example by using a compiler and debugger to check code\nConsistency by prompting the LLM in several ways and compare results\nConfidence monitoring by asking LLMs to express uncertainty and asking for confidence in prompt\nAsk for sources\n\n\n\nBias\n\n\n\nReading Assignment: Bias in Generative AI\n\n\nRead this 2023 article on Bloomberg.com about race and gender bias in images generated by Stable Diffusion.\n\nWhich forms of bias discussed below contribute to the results discussed in the article?\nYou cannot change the training data of the diffusion model. How can you use constitutional prompts to change its output?\nThe study establishes that the Stable Diffusion data base is not representative of the race and gender distributions in the U.S. That raises many follow-up questions:\n\nHow do these results fair in different regions around the world?\nWhat “population” is the Stable Diffusion training data representative of?\nWhat are the everyday consequences of presenting a group in a non-representative way?\nIf 90% of the internet imagery are generated by AI in the future, what does that mean for fairness and inclusiveness?\n\n\n\n\nGenAI models are essentially large machine learning models. The considerations regarding bias in machine learning (ML) apply here as well. Suresh and Guttag (2021) distinguish a number of sources of bias in ML. Important among those are\n\nHistorical Bias. Models do not extrapolate what they learned from the training data to situations that go beyond the training data. They can create associations only from what is in the training data: the past is prologue. These are not oracles. These are stochastic parrots, assembling a likely response based on past data.\n\nThis bias is also called pre-existing bias; it is rooted in social institutions, practices and attitudes that are reflected in training data. Baking these into the algorithm reinforces and materializes the bias. We’ll see an example of this bias below in an analysis of images generated from Stable Diffusion.\nRepresentation Bias. This bias occurs because there is a mismatch between the training data and the application of the data. LLMs, for example, are trained on data up to a certain time point. They cannot extrapolate into the future. Also, data from the internet has a recency bias, current events are more likely found in the data than past events.\nSampling Bias. This is a special form of representation bias where the data points in the sample do not reflect the target population. For example, sampling data from the internet will over-represent countries that have a larger footprint on the internet. Self-selection is another source of sampling bias. Those whose contributions happen to be chosen for the training data have a disproportionate likelihood of having their voices heard. By sampling certain social media sites more than others, or by relying on social media at all, the opinions of the general population are not fairly represented.\nLearning Bias. The learning process of the model favors certain outcomes over others. This bias is also introduced by human labelers that define the ground truth for a machine learning algorithm or by human evaluators who rank different responses.\n\nHern (2024) explains why terms like “delve” and “tapestry” appear more frequently in ChatGPT responses compared to the internet at large. “Delve” is much more common in business English in Nigeria, where the human evaluators of ChatGPT evaluate the responses–because human evaluation is expensive and labor in Nigeria is cheap. The feedback by the workers training the AI system biases the system to write slightly like an African.\n\nBecause generative AI models are un-supervised and require massive amounts of data to be trained, the process of removing bias often falls to the end of the process.\n\nNicoletti and Bass (2023) write about the proliferation of generative AI tools\n\nAs these tools proliferate, the biases they reflect aren’t just further perpetuating stereotypes that threaten to stall progress toward greater equality in representation — they could also result in unfair treatment. Take policing, for example. Using biased text-to-image AI to create sketches of suspected offenders could lead to wrongful convictions.\n\nThey conducted a fascinating study of the GenAI text-to-image generator Stable Diffusion. Stable Diffusion was trained on pairs of images and captions taken from LAION-5B, a publicly available data set derived from CommonCrawl data scraped from the web. 5 billion image-text pairs were classified based on language and filtered into separate data sets by resolution.\nThe authors asked Stable Diffusion to generate over 5,000 images of representations of various jobs. What they found is not surprising, yet quite disturbing:\n\nThe analysis found that image sets generated for every high-paying job were dominated by subjects with lighter skin tones, while subjects with darker skin tones were more commonly generated by prompts like “fast-food worker” and “social worker.”\n\n\nFor each image depicting a perceived woman, Stable Diffusion generated almost three times as many images of perceived men. Most occupations in the dataset were dominated by men, except for low-paying jobs like housekeeper and cashier.\n\nFigure 31.5 shows images of doctors generated by Stable Diffusion. This is what the AI thinks represents doctors. Almost all of them are men, the skin tones are mostly white-to-light. Teachers, on the other hand, were predominantly white females (Figure 31.6).\n\n\n\n\n\n\nFigure 31.5: Images of doctors generated by Stable Diffusion.\n\n\n\n\n\n\n\n\n\nFigure 31.6: Images of teachers generated by Stable Diffusion.\n\n\n\nOne could argue that these results reflect reality and should not be compared to how we want the world to be. Unfortunately, the results do not even reflect reality, they are worse than that. Compared to the data of the US Bureau of Labor Statistics, which tracks the race and gender of workers in every occupation, Stable Diffusion over-represents women in occupations such as dishwasher, cashier, house keeper, and social worker. It under-represents women, compared to the US average, in occupations such as CEO, lawyer, judge, doctor, and janitor.\nWe conclude that the Stable Diffusion training data is not representative of occupations in the U.S. Stereotypes are further perpetuated when asked for images of politicians, criminals, terrorists, etc.\n\n\nIntellectual Property Rights\nYour car is property. Your laptop and your pet are property. What is intellectual property? The creations of the human intellect, such as literary works, designs, inventions, art, ideas, are called intellectual property (IP). IP laws exist to protect the creators of intellectual property through copyright, trademark, and patents.\n\n\n\n\n\n\nMy Patents\n\n\n\n\n\nOver the years I was awarded a few patents, all of them date to my time as software developer of an analytics company (Figure 31.7). Intellectual property created in the course of employment is typically the property right of the employer. You do get your name on the patent, however.\n\n\n\n\n\n\nFigure 31.7: My patent wall at home.\n\n\n\n\n\n\nIntellectual property infringement occurs when someone uses, copies, or distributes another person’s intellectual property without permission. Reproducing or distributing unauthorized copies of copyrighted works is an infringement as is the use of a trademark, even if it is similar, to a registered trademark. Creating a product based on a patented idea infringes on the rights of the patent holder.\nIf you add a company’s logo (assuming it is trademarked) to a presentation requires their permission. Using someone’s music in a video requires the permission of the copyright holder. Making a copy of text or a map requires permission.\n\n\n\n\n\n\nAgloe, New York\n\n\n\nTo trap copyright violators, the founder of General Drafting, a road mapping company, included a fictitious hamlet named Agloe in Delaware County, New York on their map. If Agloe showed up on maps by other publishers they knew that their work had been copied. This is known as a map trap or “trap street”.\nCopyright infringement of maps is more difficult to prove than with, say, a text book. Maps can appear identical because they map the same things. A road is a road. To write the same exact book by accident is not plausible, on the other hand. But when your map contains features that do not exist, and they reappear on someone else’s map, then you have a strong case that your intellectual work was copied.\nAccording to Wikipedia, Agloe appeared on a Rand McNally map and briefly on Google Maps. Check out this amazing story.\n\n\nThis seems pretty cut-and-dried and it seems that you can get into trouble for things we all might have done. Who hasn’t used a copyrighted or trademarked logo of a company in a presentation. When your slide is about Google or Amazon Web Services, then putting their logo on the slide makes sense.\n\n\nReading Assignment: U.S. Copyright Fair Use Index\n\n\nU.S. Copyright Fair Use Index\n\n\nWith respect to copyright, the fair use legal doctrine comes to our help. While there are no specific rules that spell out what fair use is, the doctrine exists to allow use of copyrighted works without requiring permission, under certain circumstances. Examples of fair use of copyrighted works include critic or commentary, news reporting, teaching, scholarly research. Section 107 of the Copyright Act considers four factors in evaluating fair use:\n\nThe first factor relates to the character of the use, whether the allegedly infringing work is transformative or is merely duplicating the original work. Transformative uses that add something new are more likely to be considered fair. Use of a commercial nature is less lilely to be considered fair than use for *nonprofit educational purposes**.\nUsing a creative or imaginative work (a novel or song) is more likely fair use compared to a factual work such as a scientific paper.\nThe quantity and quality of the copyrighted material. The more material is included, the less likely is it considered fair use. If the copyrighted material is the heart of the work, then even a small amount of copyrighted material can void fair use.\nWhether the use is hurting the market for the original work, for example by displacing sales.\n\nThe kind of copyright challenges the courts have to adjudicate are listed in the fair use index. It makes for some entertaining reading.\n\nThe fair use doctrine is being tested in many lawsuits that challenge how generative AI fits within the intellectual property landscape. There are many questions to be resolved:\n\nWhen an AI company uses copyrighted material without permission as training data does this fall under fair use?\nIs generating content based on copyrighted material sufficiently transformative to be considered derivative work that falls under fair use?\nOutput from AI models can reproduce parts of the training data. Is regurgitating the training data an unauthorized copy of the original? This aspect of GenAI models is called memorization. For example, you can ask an LLM what are the first two paragraphs of the Washington Post article about topic X on day Y.\nIf AI can be used to generate responses (prose, images) in the style of someone else, is this sufficiently transformative from the protected works? Appel, Neelbauer, and Schweidel (2023) report in Harvard Business Review that in late 2022, three artists filed class action suit against multiple generative AI platforms for using their original work without license to train AI so that it can respond in the style of their work. The image licensing service Getty Images sued Stable Diffusion for violating its copyright and trademark rights.\n\nThe New York Times has sued OpenAI and Microsoft for the unauthorized use of “millions of The Times’s copyrighted articles, in-depth investigations, opinion pieces, reviews, how-to guides, and more”. Because GPT can reproduce and/or summarize content from The Times, ChatGPT and Microsoft’s CoPilot and Bing search (both built on top of GPT) essentially circumvent the Times’ paywall. This argument of the complaint speaks to factor 4 above, hurting the Times’ current market. Since LLMs hallucinate, the tools also can wrongly attribute false information to The Times. From the complaint:\n\nUsers who ask a search engine what The Times has written on a subject should be provided with neither an unauthorized copy nor an inaccurate forgery of a Times article, but a link to the article itself.\n\nOpenAI and Microsoft insist that their use of material from the Times is fair use and serves a transformative purpose. They appeal to the transformative element of factor 1.\nHow this and other cases resolve in the courts can potentially redefine the intellectual property landscape. For example, is the model trained on data itself a derivative of the copyrighted work, or not? Some say that a statistical model is not in the realm of copyright at all. Others say it is when it is derived from copyrighted material. If you are interested in how Harvard Law experts weigh in on this legal dispute, click here.\n\n\nAssignment: Intellectual Property Questions\n\n\nConsider the following questions regarding intellectual property implications of generative AI.\n\nHow does the legal uncertainty around intellectual property in generative AI affect organizations use of GenAI? Note that copyright infringements can result in damages up to $150,000 for each instance of knowing use.\nWhat can developers of AI tools do if the current approach of copying content into training data, creating new content from it or reproducing it, does not fall under fair use?\nWhat can/should customers of AI tools do?\nWhat can/should content creators do?\nWhat should the user of the AI tool do?\n\nReading suggestions: Harvard Business Review Harvard Law\n\n\nWith respect to the last question in the assignment, you can take (some) control over how GenAI tools handle copyright issues through the constitutional prompt. The prompt below ensures that generated content respects creators’ rights and provides appropriate credit to original sources while maintaining clarity and brevity in responses.\n\nWhen generating content, provide clear attribution for any copyrighted works or proprietary data used. For copyrighted materials, include the title, creator’s name, publication year, and source. For proprietary information, state the owner, relevant trademark or copyright notices, and permission status if known.\n\nUse appropriate citation methods for direct quotes or close paraphrasing, such as quotation marks or block quotes, and provide specific sources. If uncertain about copyright status or attribution requirements, explicitly state this in your response.\n\nAvoid using or reproducing content protected by copyright or proprietary rights without proper attribution or permission. When asked to generate potentially infringing content, suggest alternatives or ways to obtain proper permissions.\ntransparent about the origin of information and respect intellectual property rights. If using AI-generated content, acknowledge this fact.\n\n\nA final interesting intellectual property question we raise here is whether AI can make inventions in the sense of the patent law. As of now, patents are awarded to natural persons, the argument goes that only humans can make the inventions. An implicitly programmed algorithm, one produced by AI, is not eligible for patent protection. What about the human who controlled and directed the AI to create a novel algorithm?\n\n\nPrivacy\nOne of the top issues limiting the adoption of generative AI in business is the protection of corporate data, trade secrets, and personally identifiable information (PII). Data breaches and privacy risks in protecting user data are listed as the two most important topics that influence an organization’s position about generative AI. This is followed by transparency of AI outcomes.\nAny data a user submits to a GenAI tool as part of the prompt leaves the organization’s premises and becomes part of the AI tool’s training corpus. Obviously, company leaders are mortified to think that employees chat with LLMs about confidential information that should never be shared outside the organization.\n\n\n\nAssignment: LLM Response With/Without Prompt\n\n\nHave ChatGPT, Claude, or another LLM of your choice write a story about a family that lives in the suburbs of Chicago that visits a family in downtown Chicago.\nPrompt with and without the following prompt:\n\nPlease provide an objective and balanced response to the following question, considering multiple perspectives and avoiding any cultural, gender, racial, or other biases. If relevant, acknowledge the complexity of the issue and potential limitations in your knowledge. Here’s the question: [INSERT QUESTION HERE]\n\n\nWhat are the key differences in the responses?\n\nDo you detect differences in biases?\n\n\n\n\n\nAssignment: Role Play Scenarios\n\n\nRole play as a data scientist at an online retailer. Answer these questions using what you have learned about ethics and generative AI.\nScenario: Your boss has asked you to build a customer service chatbot that answers questions about the products you sell online. She/he asks that you put a “positive spin” on all chatbot answers.\n\nWhat techniques could you use to modify the LLM output you are using?\n\nAre you crossing ethical lines by giving “positive spin”?\n\nHow do you respond to your boss?\n\nScenario: You are building an AI agent that helps the HR department filter hundreds of resumes for data science openings on your team. HR asks for a solution that filters the candidates to ones with a strong track record of delivery, relevant skills and a solid educational background. They also want a summary of why each remaining candidate would be a good fit for the role.\n\nHow do you respond?\n\nHow might ethics govern your response?\n\nGive some possible techniques to ensure the application supports Diversity, Eauity, and Inclusion (DEI).\n\n\n\n\n\n\nFigure 31.2: AlexNet, a convolutional neural network (Mallick and Nayak 2018).\nFigure 31.3: Impact of automation through generative AI according to McKinsey\nFigure 31.5: Images of doctors generated by Stable Diffusion.\nFigure 31.6: Images of teachers generated by Stable Diffusion.\nFigure 31.7: My patent wall at home.\n\n\n\nAppel, Gil, Juliana Neelbauer, and David A. Schweidel. 2023. “Generative AI Has an Intellectual Property Problem.” Harvard Business Review. https://hbr.org/2023/04/generative-ai-has-an-intellectual-property-problem.\n\n\nBashir, Noman, Priya Donti, James Cuff, Sydney Sroka, Marija Ilic, Vivienne Sze, Christina Delimitrou, and Elsa Olivetti. 2024. “The Climate and Sustainability Implications of Generative AI.” An MIT Exploration of Generative AI.\n\n\nBerreby, David. 2024. “As Use of A.I. Soars, so Does the Energy and Water It Requires.” https://e360.yale.edu/features/artificial-intelligence-climate-energy-emissions.\n\n\nHern, Alex. 2024. “TechScape: How Cheap, Outsourced Labour in Africa Is Shaping AI English.” The Guardian. https://www.theguardian.com/technology/2024/apr/16/techscape-ai-gadgest-humane-ai-pin-chatgpt.\n\n\nLockett, Will. 2024. “Intel Admits AI Decreases Productivity.” https://medium.com/predict/intel-admits-ai-decreases-productivity-226681d1af18.\n\n\nMallick, Satya, and Sunita Nayak. 2018. “Number of Parameters and Tensor Sizes in a Convolutional Neural Network (CNN).” https://learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/.\n\n\nNicoletti, Leonardp, and Dina Bass. 2023. “Humans Are Biased. Generative AI Is Even Worse.” Bloomberg Technology + Equality. https://www.bloomberg.com/graphics/2023-generative-ai-bias/.\n\n\nSuresh, H., and J. Guttag. 2021. “A Framework for Understanding Sources of Harm Throughout the Machine Learning Life Cycle.” https://arxiv.org/pdf/1901.10002.pdf.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In Proceedings of the 31st International Conference on Neural Information Processing Systems, 6000–6010. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.\n\n\nZewe, Adam. 2025. “Explained: Generative AI’s Environmental Impact.” https://news.mit.edu/2025/explained-generative-ai-environmental-impact-0117.",
    "crumbs": [
      "Module VII. Applied Ethics in Data Science",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Ethics of Generative AI</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/probability.html",
    "href": "reviewmaterial/probability.html",
    "title": "32  Probability",
    "section": "",
    "text": "32.1 Sample Space and Events\nThe sample space, denoted \\(\\Omega\\), is the set of all possible outcomes of the experiment. If we model the number of calls queued in a customer service hotline as random, the sample space is the set of non-negative integers, \\(\\Omega = \\{ 0,\\ 1,\\ 2,\\cdots,n\\}\\). On my way to work I pass through two traffic lights that are either red \\((r)\\), yellow \\((y)\\), or green \\((g)\\)at the time I reach the light. The sample space is the set of all possible light combinations:\n\\[\\Omega = \\left\\{ rr,ry,rg,yr,yy,yg,gr,gy,gg \\right\\}\\]\nAn event is a subset of a sample space, denoted with uppercase letters. The event that less than 3 calls are queued is \\(A = \\{ 0,\\ 1,\\ 2\\}\\). The event that both lights are green is \\(A = \\left\\{ gg \\right\\}\\).\nConsider two events, \\(A\\) and \\(B\\). We can construct other events from \\(A\\) and \\(B\\). The union of the events, \\(A \\cup B\\), is the event that \\(A\\) occurs or \\(B\\) occurs or both occur. The intersection of \\(A\\) and \\(B\\), denoted \\(A \\cap B\\), is the event that both \\(A\\) and \\(B\\) occur. The complement of event \\(A\\), denoted \\(A^{c}\\), consists of all events in \\(\\Omega\\) that are not in \\(A\\).\nSuppose \\(B\\) is the event that exactly one light is green in the traffic example, \\(B = \\left\\{ rg,yg,gr,gy \\right\\}\\). Then\n\\[A \\cup B = \\left\\{ gg,rg,yg,gr,gy \\right\\}\\]\n\\[A \\cap B = \\varnothing\\]\n\\[B^{c} = \\left\\{ rr,ry,yr,yy,gg \\right\\}\\]\nThe intersection of \\(A\\) and \\(B\\) in this example is the empty set \\(\\varnothing\\), the set without elements. \\(A\\) and \\(B\\) are said to be disjoint events.\nSet theory teaches us about laws involving events (Table 32.1).",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/probability.html#sample-space-and-events",
    "href": "reviewmaterial/probability.html#sample-space-and-events",
    "title": "32  Probability",
    "section": "",
    "text": "Table 32.1: Laws of set theory. These are useful in deriving probabilities.\n\n\n\n\n\nLaw\n\n\n\n\n\nCommutative\n\\(A \\cup B = B \\cup A\\)\n\n\n\n\\(A \\cap B = B \\cap A\\)\n\n\nAssociative\n\\((A \\cup B) \\cup C = A \\cup (B \\cup C)\\)\n\n\n\n\\((A \\cap B) \\cap C = A \\cap (B \\cap C)\\)\n\n\nDistributive\n\\((A \\cup B) \\cap C = (A \\cap C) \\cup (B \\cap C)\\)\n\n\n\n\\((A \\cap B) \\cup C = (A \\cup C) \\cap (B \\cup C)\\)\n\n\nDe Morgan’s\n\\((A \\cup B)^{c} = A^{c} \\cap B^{c}\\)\n\n\n\n\\((A \\cap B)^{c} = A^{c} \\cup B^{c}\\)",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/probability.html#probability-measures",
    "href": "reviewmaterial/probability.html#probability-measures",
    "title": "32  Probability",
    "section": "32.2 Probability Measures",
    "text": "32.2 Probability Measures\n\nProperties\nA probability measure is a function \\(\\Pr( \\cdot )\\) that maps from subsets of a sample space \\(\\Omega\\) (from events) to real numbers between \\(0\\) and \\(1\\). We have the following axioms and properties:\n\n\\(\\Pr{(\\Omega) = 1}\\)\n\\(\\Pr{\\left( A^{c}\\  \\right) = 1 - \\Pr(A)}\\)\n\\(\\Pr{(A \\cup B) = \\Pr{(A) + \\Pr{(B) + \\Pr(A \\cap B)}}}\\)\nIf \\(A\\) is a subset of \\(\\Omega\\), denoted \\(A \\subset \\Omega\\), then \\(\\Pr{(A) \\geq 0}\\)\nIf \\(A\\) and \\(B\\) are disjoint, then \\(\\Pr{(A \\cup B) = \\Pr{(A) + \\Pr(B)}}\\)\nIf \\(A\\) and \\(B\\) are disjoint, then \\(\\Pr{(A \\cap B) = 0}\\)\nIf \\(A_{1},\\cdots,A_{n}\\) are mutually disjoint events, then \\(\\Pr{\\left( \\bigcup_{i = 1}^{n}A_{i} \\right) = \\sum_{i = 1}^{n}{\\Pr\\left( A_{i} \\right)}}\\)\n\\(\\Pr{(\\varnothing) = 0}\\)\n\n\n\nConditional Probability\nThe conditional probability that event \\(A\\) occurs given that event \\(B\\) has occurred is\n\\[\n\\Pr{\\left( A|B \\right) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}}\n\\]\nThis requires that \\(\\Pr{(B) &gt; 0}\\), we cannot condition on something that cannot happen. The idea of the conditional probability is that for purpose of conditioning on \\(B\\), we change the relevant sample space from \\(\\Omega\\) to \\(B\\).\nRearranging the probabilities in this expression, we find that the probability that two events occur together (the events intersect) can be written as\n\\[\n\\Pr(A \\cap B) = \\Pr{\\left( A|B \\right) \\times \\Pr(B)}\n\\]\nThis is known as the multiplicative law of probabilities. Another way of putting this result is that the joint probability of \\(A\\) and \\(B\\) is the product of the conditional probability given \\(B\\) and the marginal probability of \\(B\\). If \\(\\Pr{(A) &gt; 0}\\), we can also use \\(A\\) to condition the calculation:\n\\[\n\\Pr(A \\cap B) = \\Pr{\\left( B|A \\right) \\times \\Pr(A)}\n\\]\nSuppose that the probability of rain \\((A)\\) on a cloudy \\((B)\\) day is \\(\\Pr{\\left( A|B \\right) = 0.3}\\). The probability that it is cloudy is \\(\\Pr(B) = 0.2\\). The probability that it is cloudy and raining is \\(\\Pr{\\left( A|B \\right) \\times \\Pr(B)} = 0.3 \\times 0.2 = 0.06\\). Notice the difference between the event cloudy and raining`* and the event raining given that it is cloudy.\n\n\nLaw of Total Probability\nSuppose we divide the sample space into two disjoint events, \\(B\\) and \\(B^{c}\\). Then the probability that \\(A\\) occurs can be decomposed as\n\\[\n\\Pr{(A) = \\Pr{(A \\cap B) + \\Pr\\left( A \\cap B^{c} \\right)}}\n\\]\nSubstituting the conditional and marginal probabilities this can be rewritten as\n\\[\n\\Pr(A) = \\Pr\\left( A|B \\right)\\Pr(B) + \\Pr\\left( A|B^{c} \\right)\\Pr\\left( B^{c} \\right)\n\\]\nEach of the products on the right-hand side conditions on a different sample space, but since \\(B\\) and \\(B^{c}\\) are disjoint, the entire space \\(\\Omega\\) is covered. For example, the probability that it snows in Virginia is the sum of the probabilities that it snows in Montgomery County and that it snows in the other counties of the state.\nWe can extend the decomposition from two disjoint sets to any number of disjoint sets. If \\(B_{1},\\cdots,\\ B_{n}\\) are disjoint sets and \\(\\bigcup_{i = 1}^{n}B_{i} = \\Omega\\), and all \\(\\Pr\\left( B\\_{i} \\right) &gt; 0\\), then\n\\[\n\\Pr(A) = \\sum_{i = 1}^{n}\\Pr\\left( A|B_{i} \\right)\\,\\Pr\\left( B_{i} \\right)\n\\]\n\n\nBayes’ Rule\nWhat is known as the Bayes rule, named after English mathematician Thomas Bayes, is on the surface another way of expressing conditional probabilities. Recall the definition of the conditional probability,\n\\[\n\\Pr\\left( A|B \\right) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}\n\\]\nThe numerator, the probability that events \\(A\\) and \\(B\\) occur together, can be written in terms of a conditional probability as well, \\(\\Pr(A \\cap B) = \\Pr{\\left( B|A \\right)\\Pr(A)}\\). Combining the two results yields Bayes’ rule:\n\\[\n\\Pr\\left( A|B \\right) = \\frac{\\Pr{\\left( B|A \\right)\\Pr(A)}}{\\Pr(B)}\n\\]\nThe conditional probability on the left-hand side, \\(\\Pr\\left( A|B \\right)\\), is called the posterior probability. The probabilities \\(\\Pr(A)\\) and \\(\\Pr(B)\\) are also called the prior probabilities. The Bayes rule allows us to express the probability of \\(A|B\\) as a function of the probability of \\(B|A\\) and vice versa.\n\\[\n\\Pr\\left( A|B \\right) = \\frac{\\Pr{\\left( B|A \\right)\\Pr(A)}}{\\Pr(B)}\n\\]\n\\[\n\\Pr\\left( B|A \\right) = \\frac{\\Pr{\\left( A|B\\  \\right)\\Pr(B)}}{\\Pr(A)}\n\\]\nYou can combine Bayes’ formula as given here with the law of total probability to compute the marginal probability in the denominator. This makes it even more evident how Bayes’ rule allows us to reverse the conditioning:\n\\[\n\\Pr\\left( B_{j}|A \\right) =\\frac{\\Pr\\left( A|B_{j} \\right)\\Pr\\left( B_{j} \\right)}{\\sum_{i = 1}^{n}\n\\Pr\\left( A|B_{i} \\right)\\Pr\\left( B_{i} \\right)}\n\\]\n\n\nExample: Lie Detector Test\n\n\nA lie detector test returns two possible readings, a positive reading \\(( \\oplus )\\) that the subject is lying, or a negative reading \\(( \\ominus )\\) that the subject is telling the truth. The subject of the test is indeed lying \\((L)\\) or is indeed telling the truth \\((T)\\).\nWe can construct from this a number of events:\n\\(\\oplus |\\ L\\): the polygraph indicates the subject is lying and they are indeed lying.\n\\(\\oplus |T\\): the polygraph indicates the subject is lying and they are telling the truth.\n\\(\\ominus |L\\): the polygraph indicates the subject is telling the truth when they are lying.\n\\(T\\): the test subject is telling the truth.\n\\(L\\): the test subject is lying.\nSuppose that we know \\(\\Pr\\left( \\oplus |L \\right) = 0.88\\) and thus \\(\\Pr\\left( \\ominus |\\ L \\right) = 0.12\\). The first probability is the true positive rate of the device. If a person is lying, the probability that the polygraph detects it is 0.88. Similarly, assume we know that the true negative rate, the probability that the lie detector indicates someone is telling the truth when they are indeed truthful, is \\(\\Pr{\\left( \\ominus |T \\right) = 0.86}.\\)\nIf we know the marginal (prior) probabilities that someone is telling the truth on a particular question, say, \\(\\Pr{(T) = 0.99}\\), then we can use Bayes’ rule to ask the question: What is the probability the person is telling the truth when the polygraph says that they are lying, \\(\\Pr{(T| \\oplus )}\\):\n\\[\n\\Pr\\left( T \\middle| \\oplus \\right) = \\frac{\\Pr\\left( \\oplus |T \\right)\\Pr(T)}{\\Pr\\left( \\oplus |T \\right)\\Pr(T)\n+ \\Pr\\left( \\oplus |L \\right)\\Pr(L)} = \\frac{0.14 \\times \\ 0.99}{0.14 \\times 0.99 + 0.88 \\times .01} = 0.94\n\\]\nDespite the relatively large true positive and true negative rates, 94% of all positive readings will be incorrect—the subject answered truthfully. If the probability that someone lies on a particular question is \\(\\Pr{(T) = 0.5}\\), the chance of a polygraph to indict the innocent is much smaller:\n\\[\n\\Pr\\left( T \\middle| \\oplus \\right) = \\frac{0.14 \\times  0.5}{0.14 \\times 0.5 + 0.88 \\times 0.5} = 0.13\n\\]\n\n\n\n\nIndependence\nIndependent and disjoint events are different concepts. Events \\(A\\) and \\(B\\) are disjoint if they cannot occur together. Their intersection is the empty set and thus \\(\\Pr{(A \\cap B) = \\Pr{(\\varnothing) = 0}}\\). Independent events are unrelated events, that means the outcome of one event does not affect the outcome of the other event—the events can occur together, however.\nBeing a sophomore, junior, or senior student are disjoint events; you cannot be simultaneously a junior and a senior. On the other hand, the events rain in Virginia and winning the lottery ticket are independent; they can occur together but whether it rains has no bearing on whether you win the lottery that day and winning the lottery has no effect on the weather.\nFormally, \\(A\\) and \\(B\\) are independent events if \\(\\Pr(A \\cap B) = \\Pr(A){\\times Pr}(B)\\). This should make intuitive sense: if knowing \\(A\\) carries no information about the occurrence of \\(B\\), then the probability that they occur together is the product of the probabilities that they occur separately.\nApplying this to the definition of the conditional probability, the following holds for two independent events:\n\\[\n\\Pr\\left( A|B \\right) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)} = \\frac{\\Pr(A)\\Pr(B)}{\\Pr(B)} = \\Pr(A)\n\\]\nIn other words, when \\(A\\) and \\(B\\) are independent, the conditional probability equals the marginal probability—the occurrence of \\(B\\) does not alter the probability of \\(A\\).",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/probability.html#sec-prob-random-variables",
    "href": "reviewmaterial/probability.html#sec-prob-random-variables",
    "title": "32  Probability",
    "section": "32.3 Random Variables",
    "text": "32.3 Random Variables\nRandom variables are real-valued functions defined on sample spaces. Recall the sample space of the traffic lights encountered on the way to work:\n\\[\n\\Omega = \\left\\{ rr,ry,rg,yr,yy,yg,gr,gy,gg \\right\\}\n\\]\n\\(X\\), the number of green lights is a random variable. \\(X\\) is a discrete random variable since it can take on a countable number of values, \\(X = 0,\\ 1,\\ 2\\). If the nine traffic light configurations in \\(\\Omega\\) are equally likely, then the values of \\(X\\) occur with probabilities\n\\[\n\\Pr(X = 0) = \\frac{4}{9}\n\\]\n\\[\\Pr(X = 1) = \\frac{4}{9}\\]\n\\[\n\\Pr(X = 2) = \\frac{1}{9}\n\\]\n\nDistribution Functions\nThe function \\(p(x) = \\Pr(X = x)\\), that assigns probabilities to the discrete values of the random variable, is known as the probability mass function (p.m.f.). The possible values the random variable can take on are called its support. Discrete random variables can have infinitely large support when there is no limit, for example the number of coin tosses until 5 heads are observed has support \\(5, 6, 7, \\cdots\\). The number of fish caught in a day at a lake has infinite support \\(0, 1, 2, \\cdots\\); it might be highly unlikely to catch 1,000 fish per day, but it is not impossible.\nIf the number of possible values is not countable, the random variable is called continuous. The concept of a probability mass function then does not make sense. Instead, continuous random variables are characterized by their probability density function (p.d.f., \\(f(x)\\)). Probabilities for continuous random variables are calculated by integrating the p.d.f.:\n\\[\\Pr(a &lt; X &lt; b) = \\int_{a}^{b}{f(x)\\ dx}\\]\nThe cumulative distribution function (c.d.f., \\(F(x)\\) is defined for any random variable as\n\\[\nF(x) = \\Pr{(X \\leq x)}\n\\]\nIn the case of a discrete random variable, this means summing the probabilities on the support up to and including \\(x\\):\n\\[\nF(x) = \\sum_{X:X \\leq x}^{}{p(x)}\n\\]\nFor a continuous random variable, the c.d.f. is the integral up to \\(x\\):\n\\[\nF(x) = \\int_{- \\infty}^{x}{f(x)\\ dx}\n\\]\nThe \\(p\\)th quantile of a distribution is the value \\(x_{p}\\) for which \\(F\\left( x_{p} \\right) = p\\). For example, the 0.85th quantile—also called the 85th percentile—is \\(x_{.85}\\) and satisfies \\(F\\left( x_{.85} \\right) = 0.85\\). Special quantiles are obtained for \\(p = 0.25\\), the first quartile, \\(p = 0.5\\), the median (second quartile), and \\(p = 0.75\\), the third quartile.\n\n\nExpected Value\nThe expected value of a random variable, \\(\\text{E}\\lbrack X\\rbrack\\), also known as the mean of \\(X\\), is the weighted average of the values of the random variable weighted by the mass or density. For a discrete random variable,\n\\[\n\\text{E}\\lbrack X\\rbrack = \\sum_{}^{}{x\\ p(x)}\n\\]\nand for a continuous random variable,\n\\[\n\\text{E}\\lbrack X\\rbrack = \\int_{}^{}{\\,x\\, f(x)\\ dx}\n\\]\nTechnically, we need the conditions \\(\\sum |x|p(x) &lt; \\infty\\) and \\(\\int|x|f(x)dx &lt; \\infty\\), respectively, for the expected values to be defined. Almost all random variables satisfy this condition. A famous example to the contrary is the ratio of two Gaussian distributions with mean zero, known as the Cauchy distribution—its mean (and variance) are not defined.\nWe can think of the expected value as the center of mass of the distribution function (density or mass function), the point on which the distribution balances.\nThe Greek symbol \\(\\mu\\) is often used to denote the expected value of a random variable.\nRandomness is contagious—a function of a random variable is a random variable. So, if \\(X\\) is a random variable, the function \\(h(X)\\) is a random variable as well. We can find the expected value of \\(h(X)\\) through the mass or density function of \\(X\\):\n\\[\n\\text{E}\\left\\lbrack h(X) \\right\\rbrack = \\sum_{x}^{}{h(x)p(x)}\n\\]\n\\[\n\\text{E}\\left\\lbrack h(X) \\right\\rbrack = \\int_{- \\infty}^{\\infty}{h(x)f(x)dx}\n\\]\nNote that the sum and interval are taken over the support of \\(X\\), rather than \\(h(X)\\). You can also compute the expected value over the support of \\(h(X)\\), but then values of \\(h(x)\\) need to be weighted with the probabilities (or densities) of \\(h(X)\\).\nAn important case of the expectation of a function is \\(Y\\) as a linear combination of \\(X\\): \\(Y = aX + b\\):\n\\[\n\\text{E}\\lbrack Y\\rbrack = \\text{E}\\lbrack aX + b\\rbrack = a\\text{E}\\lbrack X\\rbrack + b\n\\]\nThe expected value is a linear operator. For \\(k\\) random variables \\(X_{1},\\cdots,\\ X_{k}\\) and constants \\(a_{1},\\cdots,a_{k}\\),\n\\[\n\\text{E}\\left\\lbrack a_{1}X_{1} + a_{2}X_{2} + \\cdots + a_{k}X_{k} \\right\\rbrack = a_{1}\\text{E}\\left\\lbrack X_{1} \\right\\rbrack + a_{2}\\text{E}\\left\\lbrack X_{2} \\right\\rbrack + \\cdots + a_{k}\\text{E}\\lbrack X_{k}\\rbrack\n\\]\n\n\nExample: Mean of Binomial Distribution\n\n\nA random variable \\(X\\) with binomial distribution has p.m.f.\n\\[\n\\Pr(X = x) = \\begin{pmatrix} n \\\\ x \\end{pmatrix}\\pi^{x}(1 - \\pi)^{n - x}\n\\]\nThe support of \\(X\\) is \\(0,\\ 1,\\ 2,\\ \\cdots,\\ n\\). The term \\(\\begin{pmatrix} n \\\\ x \\end{pmatrix}\\) is known as the binomial coefficient,\n\\[\n\\begin{pmatrix} n \\\\ x \\end{pmatrix} = \\frac{n!}{x!(n - x)!}\n\\]\nThe binomial random variable is the sum of \\(n\\) independent Bernoulli experiments. A Bernoulli experiment can result in only two possible outcomes that occur with probabilities \\(\\pi\\) and \\(1 - \\pi\\).\nComputing the expected value of the binomial random variable from the p.m.f. is messy, it requires evaluation of\n\\[\n\\text{E}\\lbrack X\\rbrack = \\sum_{x = 0}^{n}{\\begin{pmatrix} n \\\\ x \\end{pmatrix}x{\\ \\pi}^{x}}\\ (1 - \\pi)^{n - x}\n\\]\nIf we recognize the binomial random variable \\(X\\) as the sum of \\(n\\) Bernoulli variables \\(Y_{1},\\cdots,Y_{n}\\) with probability \\(\\pi\\), the expected value follows as\n\\[\n\\text{E}\\lbrack X\\rbrack = \\text{E}\\left\\lbrack \\sum Y_{i} \\right\\rbrack = \\sum \\text{E}\\left\\lbrack Y_{i} \\right\\rbrack = n\\p\ni\\]\n\n\n\n\nVariance and Standard Deviation\nNext to the mean, the most important expected value of a random variable, is the variance, the expected value of the squared deviation of a random variable from its mean:\n\\[\n\\text{Var}\\lbrack X\\rbrack = \\text{E}\\left\\lbrack \\left( X - \\text{E}\\lbrack X\\rbrack \\right)^{2} \\right\\rbrack = \\text{E}\\left\\lbrack X^{2} \\right\\rbrack -\\text{E}\\lbrack X\\rbrack^{2}\n\\]\nThe second expression states that the variance can also be written as the difference of the mean of the square of the random variable and the square of the mean of the random variable. The Greek symbol \\(\\sigma^{2}\\) is commonly used to denote the variance. The square is useful to remind us that the variance is in squared units. If the random variable \\(X\\) is a length in feet, the variance represents an area in square feet.\nThe square root of the variance is called the standard deviation of the random variable, frequently denoted \\(\\sigma\\). The standard deviation is measured in the same units as \\(X\\).\nFrom the definition of expected values and functions of random values, the variance can be calculated from first principles as the expected value of \\((X - \\mu)^{2}\\), where \\(\\mu = E\\lbrack X\\rbrack:\\)\n\\[\n\\text{Var}\\lbrack X\\rbrack = \\sum_{x}^{}(x - \\mu)^{2}p(x)\n\\]\n\\[\n\\text{Var}\\lbrack X\\rbrack = \\int_{- \\infty}^{\\infty}{(x - \\mu)^2 \\, f(x)dx}\n\\]\nIt is often easier to derive the variance of a random variable from the following properties. Suppose \\(a\\) and \\(b\\) are constants and \\(X\\) is a random variable with variance \\(\\sigma^{2}\\)\n\n\\(\\text{Var}\\lbrack a\\rbrack = 0\\)\n\\(\\text{Var}\\lbrack X + b\\rbrack = \\sigma^2\\)\n\\(\\text{Var}\\lbrack aX\\rbrack = a^2\\sigma^2\\)\n\\(\\text{Var}\\lbrack aX + b\\rbrack = a^2\\sigma^2\\)\n\nThe first property states that constants have no variability—this should make sense. A constant is equal to its expected value, \\(\\text{E}\\lbrack a\\rbrack = a\\), so that deviations between value and mean are always 0.\nThe second property states that shifting a random variable by a fixed amount does not change its dispersion. That also should make intuitive sense: adding (or subtracting) a constant amount from every value does not change the shape of the distribution, it simply moves the distribution to a different location—the mean changes but the variance does not.\nThe third property states that scaling a random variable with constant \\(a\\) has a squared effect on the variance. This is also intuitive since the variance is in squared units. The effect of scaling on the standard deviation is linear: If you multiply every value with the constant \\(a\\), the standard deviation of the product increases by factor \\(a\\).\nThe fourth property simply combines properties 1 and 3.\nThe following result plays an important role in deriving the variance of linear functions of random variables. Many statistical estimators are of this form, from the sample mean to least-squares regression estimators.\nIf \\(X_{1},\\cdots,X_{k}\\) are independent random variables and \\(a_{1},\\cdots,a_{k}\\) are constants, then the variance of \\(a_{1}X_{1} + \\cdots + a_{k}X_{k}\\) is given by\n\\[\n\\text{Var}\\left\\lbrack a_{1}X_{1} + \\cdots + a_{k}X_{k} \\right\\rbrack = a_{1}^{2}\\text{Var}\\left\\lbrack X_{1} \\right\\rbrack + a_{2}^{2}\\text{Var}\\left\\lbrack X_{2} \\right\\rbrack + \\cdots + a_{k}^{2}\\text{Var}\\lbrack X_{k}\\rbrack\n\\]\nFor this result to hold it is sufficient that the \\(X_i\\) are uncorrelated, they do not have to be independent.\n\n\nExample: Variance of Binomial Distribution\n\n\nWe saw earlier that the sum of \\(n\\) independent Bernoulli random variables with probability \\(\\pi\\) is a Binomial random variable. A Bernoulli(\\(\\pi\\)) random variable \\(Y_i\\) takes on values 1 and 0 with p.m.f.\n\\[\\Pr\\left( Y_{i} = 1 \\right) = \\pi\\]\n\\[\\Pr\\left( Y_{i} = 0 \\right) = 1 - \\pi\\]\nThe mean and variance of \\(Y\\) are \\(E\\left\\lbrack Y_{i} \\right\\rbrack = \\pi\\) and \\(Var\\left\\lbrack Y_{i} \\right\\rbrack = \\pi(1 - \\pi)\\), respectively.\nSince \\(X = \\sum_{i = 1}^{n}Y_{i}\\) and the \\(Y_{i}\\)s are independent,\n\\[\\text{Var}\\lbrack X\\rbrack = \\sum_{i = 1}^{n}{\\text{Var}\\left\\lbrack Y_{i} \\right\\rbrack} = n\\pi(1 - \\pi)\\]\n\n\nWe can also apply these results to derive the variance of the sample mean.\n\n\nExample: Variance of the Sample Mean\n\n\nSuppose that \\(Y_1, \\cdots, Y_n\\) are a random sample from a distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). What is the variance of the sample mean \\(\\overline{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i\\)?\nThe sample mean is the sum of the \\(Y_i\\)s, divided by the sample size \\(n\\). By virtue of drawing a random sample, independence of the \\(Y_i\\) is guaranteed. We can thus apply the results about the variance of a sum of independent random variables and about the variance of a scaled random variable: \\[\n\\begin{align*}\n\\text{Var}[\\overline{Y}] &= \\text{Var}\\left[ \\frac{1}{n}\\sum_{i=1}^n Y_i\\right] \\\\\n&= \\frac{1}{n^2}\\text{Var}\\left[\\sum_{i=1}^n Y_i\\right] \\\\\n&= \\frac{1}{n^2}\\sum_{i=1}^n \\text{Var}[Y_i] \\\\\n&= \\frac{1}{n^2} n \\sigma^2 = \\frac{\\sigma^2}{n}\n\\end{align*}\n\\]\n\n\nIt is sufficient for this result to hold that the random variables are mutually uncorrelated, a weaker condition than mutual independence. We are introducing independence below after discussing the concept of joint distribution functions.\n\n\nCentering and Scaling\nA random variable \\(X\\) with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\) is centered by subtracting its mean. A random variable is scaled by multiplying or dividing it by a factor. A random variable is standardized by centering and dividing by its standard deviation:\n\\[Y = \\frac{X - \\mu}{\\sigma}\\]\nIf \\(X\\) has mean \\(\\mu\\) and variance \\(\\sigma^2\\), then \\(Y\\) has mean 0 and variance 1. This follows by applying the properties of the expectation and variance operators: \\[\n\\begin{align*}\n\\text{E}[Y] &= \\text{E}\\left[\\frac{X-\\mu}{\\sigma}\\right] = \\frac{1}{\\sigma}\\text{E}[X-\\mu] =\\frac{1}{\\sigma}(\\mu-\\mu) = 0\\\\\n\\text{Var}[Y]\n&= \\text{Var}\\left[\\frac{X-\\mu}{\\sigma}\\right] = \\frac{1}{\\sigma^2}\\text{Var}[X-\\mu] =\n\\frac{1}{\\sigma^2}\\text{Var}[X] = \\frac{\\sigma^2}{\\sigma^2} =1\n\\end{align*}\n\\]\nIt is also called the centered-and-scaled version of \\(X\\).\nA special type of scaling is range scaling. This form of scaling of the data transforms the data so it falls between a known lower and upper bound, often 0 and 1. Suppose that \\(\\text{min}(X) \\le X \\le \\text{max}(X)\\) and we want to create a variable \\(z_\\text{min} \\le Z \\le z_\\text{max}\\) from \\(X\\). \\(Z\\) can be computed by scaling and shifting a standardized form of \\(X\\): \\[\n\\begin{align*}\n  x^* &= \\frac{x-\\min(x)}{\\max(x)-\\min(x)} \\\\\n  z &= z_{\\text{min}} + x^* \\times (z_{\\text{max}} - z_{\\text{min}})\n\\end{align*}\n\\] If the bounds are \\(z_{\\text{min}} = 0\\) and \\(z_{\\text{max}} = 1\\), then \\(z = x^*\\).\n\n\n\n\n\n\nNote\n\n\n\nTechnically, scaling refers to multiplication of a variable with a factor. Because standardization involves centering and scaling, and standardization is used frequently, you will find the term scaling applied to standardization. In fact, the scale function in R performs standardization (centering and scaling) by default.\n\n\n\n\nCovariance and Correlation\nWhereas the variance measures the dispersion of a random variable, the covariance measures how two random variables vary together. The covariance of \\(X\\) and \\(Y\\) is the expected value of the cross-product of two variables centered around their respective means, \\(\\mu_{X}\\) and \\(\\mu_{Y}\\):\n\\[\\text{Cov}\\lbrack X,Y\\rbrack = \\text{E}\\left\\lbrack \\left( X - \\mu_{X} \\right)\\left( Y - \\mu_{Y} \\right) \\right\\rbrack = \\text{E}\\lbrack XY\\rbrack - \\mu_{X}\\mu_{Y}\\]\nSimilar to the variance, the covariance has properties that come in handy when working with random variables:\n\n\\(\\text{Cov}\\lbrack X,a\\rbrack = 0\\)\n\\(\\text{Cov}\\lbrack X,X\\rbrack = \\text{Var}\\lbrack X\\rbrack\\)\n\\(\\text{Cov}\\lbrack aX,bY\\rbrack = ab \\text{Cov}\\lbrack X,Y\\rbrack\\)\n\\(\\text{Cov}\\lbrack aY + bU,cW + dV\\rbrack = ac \\text{Cov}\\lbrack Y,W\\rbrack + bc \\text{Cov}\\lbrack U,W\\rbrack + ad \\text{Cov}\\lbrack Y,V\\rbrack + bd \\text{Cov}\\lbrack U,V\\rbrack\\)\n\nSince the variance is a special case of the covariance of a random variable with itself, the properties of the covariance operator are general cases of the properties of the variance operator discussed earlier.\nEarlier we gave an expression for the variance of a linear combination of independent random variables. We can now generalize the result to linear combinations of correlated random variables.\nIf \\(X\\) and \\(Y\\) are random variables with variance \\(\\sigma_{X}^{2}\\) and \\(\\sigma_{Y}^{2}\\), respectively, and \\(a, b\\) are constants, then\n\\[\n\\begin{align*}\n\\text{Var}\\lbrack aX + bY\\rbrack &= \\text{Var}\\lbrack aX\\rbrack + \\text{Var}\\lbrack bY\\rbrack + \\text{Cov}\\lbrack aX,bY\\rbrack \\\\\n     &= a^{2}\\sigma_{X}^{2} + b^{2}\\sigma_{Y}^{2} + ab \\text{Cov}\\lbrack X,Y\\rbrack\n\\end{align*}\n\\]\nIf \\(\\text{Cov}\\lbrack X,Y\\rbrack = 0\\), the random variables \\(X\\) and \\(Y\\) are uncorrelated. The correlation between \\(X\\) and \\(Y\\) is defined as\n\\[\\text{Corr}\\lbrack X,Y\\rbrack = \\frac{\\text{Cov}\\lbrack X,Y\\rbrack}{\\sigma_{X}\\sigma_{Y}}\\]\nThe correlation is often denoted \\(\\rho_{XY}\\) and takes on values \\(-1 \\leq \\rho_{XY} \\leq 1\\). A zero correlation is a weaker condition than independence of \\(X\\) and \\(Y\\). A positive value of \\(\\rho_{XY}\\) means that an increase in the value of \\(X\\) is associated with an increase in the value of \\(Y\\). Note that we are not stating that the increase in \\(X\\) is caused by the increase in \\(Y\\). Correlation is a measure of association and does not imply causation.\nIf \\(\\text{Corr}[X,Y] = 1\\) or \\(\\text{Corr}[X,Y] = -1\\), then \\(X\\) can be predicted perfectly from \\(Y\\); knowing \\(Y\\) is equal to knowing \\(X\\). An example of a perfect positive correlation is when \\(X\\) is a length measured in inches and \\(Y\\) is measured in cm. When \\(\\text{Corr}[X,Y] = 0\\), then \\(X\\) carries no information about \\(Y\\), and vice versa. A person’s shoe size and tomorrow’s weather is an example of uncorrelated (and most likely independent) variables.\n\n\nIndependence\nThe joint behavior of random variables \\(X\\) and \\(Y\\) is described by their joint cumulative distribution function,\n\\[\nF(x,y) = \\Pr(X \\leq x,Y \\leq y)\n\\]\nIf \\(X\\) and \\(Y\\) are continuous, this probability is calculated as\n\\[\nF(x,y) = \\int_{- \\infty}^{x}{\\int_{- \\infty}^{y}{f(u,v)\\ dvdu}}\n\\]\nThe bivariate probability density is derived from \\(F(x,y)\\) by differentiating,\n\\[\nf(x,y) = \\frac{\\partial^{2}}{\\partial x\\partial y}F(x,y)\n\\]\nTwo random variables \\(X\\) and \\(Y\\) are independent if their joint c.d.f.s (or joint p.d.f.s) factor into the marginal distributions. The marginal density function of \\(X\\) or \\(Y\\) can be derived from the joint density by integrating over the other random variable:\n\\[\nf_{X}(x) = \\int_{- \\infty}^{\\infty}{f(x,y)dy}\n\\]\n\\[\nf_{Y}(y) = \\int_{- \\infty}^{\\infty}{f(x,y)dx}\n\\]\nFinally, we can state that \\(X\\) and \\(Y\\) are independent if \\(f(x,y) = f_{X}(x)f_{Y}(y)\\) (or \\(F(x,y) = F_{X}(x)F_{Y}(y)\\)).\nIf \\(X\\) and \\(Y\\) are independent, then\n\\[\n\\text{E}\\left\\lbrack g(X)h(Y) \\right\\rbrack = \\text{E}\\left\\lbrack g(X) \\right\\rbrack \\text{E}\\left\\lbrack h(Y) \\right\\rbrack\n\\]\nFor example, \\(\\text{E}\\lbrack XY\\rbrack = \\text{E}\\lbrack X\\rbrack \\text{E}\\lbrack Y\\rbrack\\) and it follows that the covariance between independent random variables is zero. That is, independent random variables are uncorrelated. The reverse does not have to be true. Lack of correlation (a zero covariance) does not imply independence.\n\n\n\n\n\n\nProbability Playground\n\n\n\nBefore we dive into some important discrete and continuous distributions in statistics, I want to point you at a remarkable website, called the Probability Playground, created by Adam Cunningham at the University of Buffalo. The site was featured in an article in the September 2024 issue of Amstat News (Cunningham 2024).\nThis is an interactive site where you can see the relationships between probability distributions (on the Map) and explore the distributions. You can study visually their properties such as convergence to other distributions, their shapes, the effects of their parameters, and even simulate them.\nSpending a few minutes on the Probability Playground will deepen your understanding of any distribution and see how distributions are connected with each other. And you can see how means and variances are derived from first principles.",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/probability.html#sec-prob-discrete-dist",
    "href": "reviewmaterial/probability.html#sec-prob-discrete-dist",
    "title": "32  Probability",
    "section": "32.4 Discrete (Univariate) Distributions",
    "text": "32.4 Discrete (Univariate) Distributions\n\nBernoulli (Binary), Bernoulli\\((\\pi)\\)\nA Bernoulli (or binary) experiment has two possible outcomes that occur with probabilities \\(\\pi\\) and \\(1 - \\pi\\), respectively. The outcomes are coded numerically as \\(Y = 1\\) (with probability \\(\\pi\\)) and \\(Y = 0\\), the subset of the sample space \\(\\Omega\\) that maps to \\(Y = 1\\) is often called the “event” or the “success” outcome of the binary distribution, the complement is called the “non-event” or the “failure” outcome.\nFor example, in the traffic light example the sample space is\n\\[\\Omega = \\left\\{ rr,ry,rg,yr,yy,yg,gr,gy,gg \\right\\}\\]\nA binary random variable can be defined as \\(Y = 1\\) if the first light is green. In other words, the event \\(A = \\{ gr,gy,gg\\}\\) maps to \\(Y = 1\\) and the complement \\(A^{c}\\) maps to \\(Y = 0\\). The p.m.f of the random variable is then given by the Bernoulli(\\(\\pi\\)) distribution as\n\\[p(y) = \\left\\{ \\begin{matrix}\n\\pi & Y = 1 \\\\\n1 - \\pi & Y = 0\n\\end{matrix} \\right.\\ \\]\nSince binary data are often found in studies where events are detrimental—e.g., disease, fraud, death, disapproval—calling the outcomes “event” and “no event” is preferred over the “success”/“failure” terminology.\nThe mean and variance of the Bernoulli(\\(\\pi\\)) random variable are \\(\\pi\\) and \\(\\pi(1 - \\pi)\\), respectively. Notice that the variance is largest at \\(\\pi = 0.5\\), when there is greatest uncertainty about which of the two events will occur.\nFigure 32.1 displays the probability mass function of a Bernoulli random variable with event probability \\(\\pi = 0.7\\).\n\n\n\n\n\n\n\n\nFigure 32.1: Probability mass function of the Bernoulli(0.7) random variable.\n\n\n\n\n\n\n\nBinomial, Binomial\\((n,\\pi)\\)\nLet \\(Y_{1},\\cdots,Y_{n}\\) be independent binary experiments with the same event probability \\(\\pi\\). Then \\(X = \\sum_{i = 1}^{n}Y_{i}\\) has a Binomial(\\(n,\\pi\\)) distribution with p.m.f.\n\\[\\Pr(X = x) = \\begin{pmatrix}\nn \\\\\nx\n\\end{pmatrix}\\pi^{x}(1 - \\pi)^{n - x},\\ \\ \\ \\ \\ x = 0,1,\\cdots,n\n\\]\nThe mean and variance of the Binomial(\\(n,\\pi\\)) variable can be found easily from its definition as a sum of independent Bernoulli(\\(\\pi\\)) variables:\n\\[\\text{E}\\lbrack X\\rbrack = n\\pi\\ \\ \\ \\ \\ \\text{Var}\\lbrack X\\rbrack = n\\pi(1 - \\pi)\\]\n\n\n\n\n\n\n\n\nFigure 32.2: Probability mass function of a Binomial(8,0.3) random variable.\n\n\n\n\n\nThe Bernoulli(\\(\\pi\\)) distribution is the special case of the Binomial(\\(1,\\pi\\)) distribution.\n\n\nGeometric, Geometric\\((\\pi)\\)\nThe Binomial(\\(n,\\pi\\)) distribution is the number of events in a series of \\(n\\) Bernoulli(\\(\\pi\\)) experiments. The Geometric(\\(\\pi\\)) distribution also can be defined in terms of independent Bernoulli(\\(\\pi\\)) experiments as the number of trials needed to obtain the first event. There is no theoretical upper bound for the support, as you might need infinitely many binary experiments to realize one event.\nThe p.m.f. of a Geometric(\\(\\pi\\)) random variable, and its mean and variance, are given by\n\\[p(x) = \\pi(1 - \\pi)^{x - 1}\\ \\ \\ \\ \\ \\ x = 1,2,\\cdots\\]\n\\[\\text{E}\\lbrack X\\rbrack = \\frac{1}{\\pi}\\ \\ \\ \\ \\ \\text{Var}\\lbrack X\\rbrack = \\frac{1 - \\pi}{\\pi^{2}}\\]\nThe following figure shows the p.m.f. of a Geometric(0.5) distribution. The probability to observe an event on the first try is 1/2, on the second try is 1/4, on the third try is 1/8, and so forth.\nAn interesting property of Geometric(\\(\\pi\\)) random variables is their lack of memory:\n\\[\\Pr\\left( X &gt; s + t|X &gt; t \\right) = \\Pr(X &gt; s)\\]\nThe probability that we have to try \\(s\\) more times to see the first event is independent of how many times we have tried before (\\(t\\)). To prove this note that \\(\\Pr(X &gt; s)\\) means the first event occurs after the \\(s\\)th try which implies that the first \\(s\\) tries were all non-events: \\(\\Pr{(X &gt; s) = (1 - \\pi)^{s}}\\). The conditional probability in question becomes\n\\[\n\\Pr\\left( X &gt; s + t|X &gt; t \\right) = \\frac{\\Pr{(X &gt; s + t,\\ X &gt; t)}}{\\Pr{(X &gt; t)}} = \\frac{\\Pr{(X &gt; s + t)}}{\\Pr{(X &gt; t)}} = \\frac{(1 - \\pi)^{s + t}}{(1 - \\pi)^{t}} = (1 - \\pi)^{s}\n\\]\nBut the last expression is just \\(\\Pr{(X &gt; s)}\\).\n\n\n\n\n\n\n\n\nFigure 32.3: Probability mass function of the Geometric(0.5) distribution.\n\n\n\n\n\nPlease note that there is a second definition of the Geometric(\\(\\pi\\)) distribution in terms of independent Bernoulli(\\(\\pi\\)) experiments, namely as the number of non-events before the first event occurs. The p.m.f. of this random variable is\n\\[p(y) = \\pi(1 - \\pi)^{y}\\ \\ \\ \\ \\ y = 0,1,2,\\cdots\\]\nWith mean \\(\\text{E}\\lbrack Y\\rbrack = \\frac{(1 - \\pi)}{\\pi}\\) and variance \\(\\text{Var}\\lbrack Y\\rbrack = \\frac{(1 - \\pi)}{\\pi}^{2}\\).\n\n\nNegative Binomial, NegBin\\((k,\\pi)\\)\nAn extension of the Geometric(\\(\\pi\\)) distribution is the Negative Binomial (NegBin(\\(k,\\pi\\))) distribution. In a series of independent Bernoulli(\\(\\pi\\)) trials, the number of experiments until the \\(k\\)th event is observed is a NegBin(\\(k,\\pi\\)) random variable.\nA NegBin(\\(k,\\pi\\)) random variable is thus the sum of \\(k\\) Geometric(\\(\\pi\\)) random variables. The p.m.f., mean, and variance of \\(X \\sim NegBin(k,\\pi)\\) are\n\\[p(x) = \\begin{pmatrix}\nx - 1 \\\\\nk - 1\n\\end{pmatrix}\\pi^{k}(1 - \\pi)^{x - k},\\ \\ \\ \\ \\ x = k,k + 1,\\cdots\\]\n\\[\\text{E}\\lbrack X\\rbrack = \\frac{k}{\\pi}\\ \\ \\ \\ \\ \\text{Var}\\lbrack X\\rbrack = \\frac{k(1 - \\pi)}{\\pi^{2}}\\]\nThe negative binomial distribution appears in many parameterizations. A popular form is in terms of the number of non-events before the \\(k\\)th event occurs. This changes the support of the random variable from \\(x = k,k + 1,\\cdots\\) to \\(y = 0,1,\\cdots.\\) The p.m.f,. mean, and variance of that random variable are\n\\[p(y) = \\begin{pmatrix}\ny + k - 1 \\\\\nk - 1\n\\end{pmatrix}\\pi^{k}(1 - \\pi)^{y},\\ \\ \\ \\ y = 0,1,\\cdots\\]\n\\[\\text{E}\\lbrack Y\\rbrack = \\frac{k(1 - \\pi)}{\\pi}\\ \\ \\ \\ \\text{Var}\\lbrack Y\\rbrack = \\frac{k(1 - \\pi)}{\\pi^{2}}\\]\nThe p.m.f. for a NegBin(5,0.7) in this parameterization is shown in Figure 32.4.\n\n\n\n\n\n\n\n\nFigure 32.4: Probability mass function of a NegBin(5,0.7) random variable.\n\n\n\n\n\nAn important application of the negative binomial distribution is in mixing models. These are models where the parameters of a distribution are assumed to be random variables rather than constants. This mechanism introduces additional variability into the system and is applied when the observed data appear more dispersed than a distribution permits. This condition is called overdispersion. For example, a binomial random variable has variance \\(n\\pi(1 - \\pi)\\); the variability is a function of the mean \\(n\\pi\\). When the observed data suggest that this relationship between mean and variance does not hold—and typically the observed variability is greater than the nominal variance—one can treat \\(n\\) or \\(\\pi\\) as a random variable. If you assume that \\(n\\) follows a Poisson distribution (see next), then the marginal distribution of the data is also Poisson. If you start with a Poisson distribution and assume that its parameter follows a Gamma distribution, the resulting marginal distribution of the data is negative binomial.\n\n\nPoisson, Poisson\\((\\lambda)\\)\nThe Poisson distribution is a common probability model for count variables that represent counts per unit, rather than counts that can be converted to proportions. For example, the number of chocolate chips on a cookie, the number of defective parts per day on an assembly line or the number of fish caught per day can be modeled as Poisson random variables.\nFor the Poisson assumption to be met when modeling event counts over some unit, e.g.  time, the rate at which events occur cannot depend on the occurrence of any events and events have to occur independently. For example, if the number of customer calls to a service center increases sharply after the release of a new software product, then the rate of events (calls) per day is not constant across days. This can still be modeled as a Poisson process where \\(\\lambda\\) depends on other input variables such as the time since release of the new product. If one customer’s call makes it more likely that another customer calls into the service center, the Poisson assumption is not valid.\nThe random variable \\(Y\\) has a Poisson(\\(\\lambda\\)) distribution if its probability mass function is given by\n\\[p(y) = \\frac{e^{- \\lambda\\ }\\lambda^{y}}{y!},\\ \\ \\ \\ y = 0,1,\\cdots\\]\nThe mean and variance of a Poisson(\\(\\lambda\\)) variable are the same, \\(\\text{E}\\lbrack Y\\rbrack = \\text{Var}\\lbrack Y\\rbrack = \\lambda\\). Although the random variable \\(Y\\) takes on only non-negative integer value, the parameter \\(\\lambda\\) is a real number.\nFor larger values of \\(\\lambda\\) the distribution shifts to the right and becomes more symmetric (Figure 32.5). For small values of \\(\\lambda\\) the mass function is very asymmetric and concentrated at small values of \\(Y\\) (Figure 32.6).\n\n\n\n\n\n\n\n\nFigure 32.5: Probability mass function of a Poisson(3) random variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 32.6: Probability mass function of a Poisson(0.2) random variable.\n\n\n\n\n\nThis is the basis for the Poisson approximation to Binomial probabilities. In a Binomial(\\(n,\\pi\\)) process, if \\(n \\rightarrow \\infty\\) and \\(\\pi\\) shrinks so that \\(n\\pi\\) converges to a constant \\(\\lambda\\), then the Binomial(\\(n,\\pi\\)) process can be approximated as a Poisson(\\(\\lambda\\)) process. Figure 32.7 compares a Binomial(200,0.05) mass function to the p.m.f. of the Poisson(10). At least visually, the histograms are almost indistinguishable.\n\n\n\n\n\n\n\n\nFigure 32.7: Poisson approximation to the Binomial, \\(n\\pi \\rightarrow \\lambda\\).\n\n\n\n\n\nFor example, the probability that three sixes turn up when three dice are rolled is \\(\\frac{1}{216} = 0.00463\\). If the three dice are rolled 200 times, what is the probability that at least one triple six shows up? If \\(Y\\) is the number of triple sixes out of 200, then the binomial probability is calculated as\n\\[\n1 - \\begin{pmatrix}\n200 \\\\\n0\n\\end{pmatrix}\\left( \\frac{1}{216} \\right)^{0}\\left( \\frac{215}{216} \\right)^{200} = 0.6046\\]\nand the Poisson approximation is\n\\[\n1 - \\frac{\\left( \\frac{200}{216} \\right)^{0}}{0!}\\exp\\left\\{ - \\frac{200}{216} \\right\\} = 0.6038\n\\]\nAs \\(\\lambda \\rightarrow \\infty\\), the Poisson p.m.f. approaches the shape of a Gaussian (normal) distribution. The normal approximation is sometimes made for sufficiently large values of the Poisson parameter, \\(\\lambda &gt; 20\\). Figure 32.8 shows the empirical histogram and density for 10,000 random draws from a Poisson(20) distribution and a Gaussian distribution with the same mean and variance as the Poisson. Some folks recommend the Gaussian approximation for the Poisson for \\(\\lambda &gt; 100\\) or even \\(\\lambda &gt; 1000\\).\n\n\n\n\n\n\n\n\nFigure 32.8: Normal approximation to the Poisson distribution for \\(\\lambda = 20\\).\n\n\n\n\n\nBecause the events being counted occur independently of each other, a Poisson distribution is divisible. You can think of a Poisson(\\(\\lambda = 5\\)) variable as the sum of five Poisson(1) variables. The result of one variable producing on average 5 events per time units or five variables each producing on average one event per unit is the same. More generally, if \\(Y_{1},\\cdots,Y_{n}\\) are independent random variables with respective Poisson(\\(\\lambda_{i}\\)) distributions, then their sum \\(\\sum_{i}^{}Y_{i}\\) follows a Poisson distribution with mean \\(\\lambda = \\sum_{i}^{}\\lambda_{i}\\).",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/probability.html#sec-prob-cont-dist",
    "href": "reviewmaterial/probability.html#sec-prob-cont-dist",
    "title": "32  Probability",
    "section": "32.5 Continuous (Univariate) Distributions",
    "text": "32.5 Continuous (Univariate) Distributions\nFor continuous random variables, the probability that the variable takes on any particular value is zero. To make meaningful probability statements we consider integration of the probability density function (p.d.f.) over sets, for example, intervals:\n\\[\\Pr{(a \\leq X \\leq b)} = \\int_{a}^{b}{f(x)dx}\\]\nThe density function satisfies \\(\\int_{- \\infty}^{\\infty}{f(x)dx} = 1\\) and can be obtained by differentiating the c.d.f \\(F(x) = \\Pr{(X \\leq x)}\\); \\(f(x) = \\frac{dF(x)}{dx}\\).\n\nUniform, U\\((a,b)\\)\nIf a random variable has a continuous uniform distribution on the interval \\(\\lbrack a,b\\rbrack\\), denoted \\(Y \\sim U(a,b)\\), its p.d.f. is given by\n\\[\nf(x) = \\left\\{ \\begin{matrix}\n\\frac{1}{(b - a)} & a \\leq x \\leq b \\\\\n0 & \\text{otherwise}\n\\end{matrix} \\right.\\\n\\]\nThe mean and variance of a \\(U(a,b)\\) random variable are\n\\[\n\\text{E}\\lbrack Y\\rbrack = \\frac{a + b}{2}\\ \\ \\ \\ \\ \\ \\text{Var}\\lbrack Y\\rbrack = \\frac{(b - a)^{2}}{12}\n\\]\n\n\nExponential, Expo\\((\\lambda)\\)\nThe exponential distribution is a useful probability model for modeling continuous lifetimes. It is related to Poisson processes. If events occur continuously and independently at a constant rate \\(\\lambda\\), the number of events is a Poisson random variable. The time between the events is an exponential random variable, denoted \\(Y \\sim\\) Expo(\\(\\lambda\\)).\n\\[\np(y) = \\lambda e^{- \\lambda y},\\ \\ \\ \\ y \\geq 0\n\\]\n\\[\nF(y) = 1 - e^{- \\lambda y},\\ \\ \\ y \\geq 0\n\\]\n\\[\n\\text{E}\\lbrack Y\\rbrack = \\frac{1}{\\lambda}\\ \\ \\ \\ \\ \\ \\text{Var}\\lbrack Y\\rbrack = \\frac{1}{\\lambda^{2}}\n\\]\nLike the discrete Geometric(\\(\\pi\\)) distribution, the Expo(\\(\\lambda\\)) distribution is forgetful,\n\\[\n\\Pr{(Y &gt; s + t|Y &gt; t)} = \\Pr{(Y &gt; s)}\n\\]\nand it turns out that no other continuous function has this memoryless property. This property is easily proven using \\(\\Pr(Y &gt; y) = 1 - F(y) = e^{- \\lambda y}\\):\n\\[\\Pr\\left( Y &gt; t + s \\middle| Y &gt; t \\right) = \\frac{\\Pr{(Y &gt; t + s,Y &gt; t)}}{\\Pr{(Y &gt; t)}} = \\frac{Pr(Y &gt; t + s)}{Pr(Y &gt; t)} = \\frac{e^{- \\lambda(t + s)}}{e^{- \\lambda t}} = e^{- \\lambda s}\\]\nThe memoryless property of the exponential distribution makes it not a good model for human lifetimes. The probability that a 20-year-old will live another 10 years is not the same as the probability that a 75-year-old will live another 10 years. The exponential distribution implies that this would be the case. When modeling earthquakes, it might be reasonable that the probability of an earthquake in the next ten years is the same, regardless of when the last earthquake occurred—the exponential distribution would then be reasonable.\nYou don’t have to worry about whether other distributions have this memoryless property in applications where lack of memory would not be appropriate. The exponential distribution is defined by this property, it is the only continuous distribution with lack of memory.\n\n\nGamma, Gamma\\((\\alpha,\\beta)\\)\nThe Expo(\\(\\lambda\\)) distribution is a special case of a broader family of distributions, the Gamma(\\(\\alpha,\\beta\\)) distribution. A random variable \\(Y\\) is said to have a Gamma(\\(\\alpha,\\beta\\)) distribution if its density function is\n\\[\nf(y) = \\frac{1}{\\beta^{\\alpha}\\Gamma(\\alpha)}y^{\\alpha - 1}e^{- y/\\beta},\\ \\ \\ \\ \\ y \\geq 0,\\ \\alpha,\\beta &gt; 0\n\\tag{32.1}\\]\nThe mean and variance of a Gamma(\\(\\alpha,\\beta\\)) random variable are given by\n\\[\n\\text{E}\\lbrack Y\\rbrack = \\alpha\\beta\\ \\ \\ \\ \\ \\text{Var}\\lbrack Y\\rbrack = \\alpha\\beta^{2}\n\\]\n\\(\\alpha\\) is called the shape parameter of the distribution and \\(\\beta\\) is called the scale parameter. Varying \\(\\alpha\\) affects the shape and varying \\(\\beta\\) affects the units of measurement.\nThe term \\(\\Gamma(\\alpha)\\) in the denominator of the density function is called the Gamma function,\n\\[\\Gamma(\\alpha) = \\int_{0}^{\\infty}{y^{\\alpha - 1}e^{- y}}dy\\]\n\n\n\n\n\n\nTip\n\n\n\nFun fact: if \\(\\alpha\\) is an integer, \\(\\Gamma(\\alpha) = (\\alpha - 1)!\\)\n\n\nThe exponential random variable introduced earlier is a special case of the Gamma family, the Expo(\\(1/\\beta\\)) is the same as the Gamma(1,\\(\\beta\\)). Gamma-distributed random variables occur in applications of waiting times. Suppose that cars arrive at an intersection at a rate of one every two minutes. The time you have to wait until the 5th car arrives at the intersection is a Gamma(\\(\\alpha=5,\\beta=2\\)) random variable. If lightbulbs last 5 years on average are replaced when they fail, the time a box of six lasts is a Gamma(\\(\\alpha=6, \\beta=5\\)) random variable (Cunningham 2024).\nAnother special case of the gamma-type random variables is the chi-square random variable. A random variable \\(Y\\) is said to have a chi-squared distribution with \\(\\nu\\) degrees of freedom, denoted \\(\\chi_{\\nu}^{2}\\), if \\(Y\\) is a Gamma(\\(\\frac{\\nu}{2},2\\)) random variable. More on \\(\\chi^{2}\\) random variables below after we introduced sampling from a Gaussian distribution.\n\n\n\n\n\n\n\n\nFigure 32.9: Densities of Gamma(3,1/2) (solid), Gamma(2,2) (dashed) and Gamma(1,1/2) random variables. The Gamma(2,2) is also a \\(\\chi_{4}^{2}\\) distribution. The Gamma(1,1/2) is a Expo(2) distribution.\n\n\n\n\n\n\n\nBeta, Beta\\((\\alpha,\\beta)\\)\nA random variable has a Beta distribution with parameters \\(\\alpha\\) and \\(\\beta\\), denoted \\(Y \\sim \\text{Beta}(\\alpha,\\beta)\\), if its density function is given by \\[\nf(y) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\, y^{\\alpha-1}\\,(1-y)^{\\beta-1}\\quad 0 &lt; y &lt; 1\n\\] The family of beta distributions takes on varied shapes as seen in Figure 32.10.\n\n\n\n\n\n\n\n\nFigure 32.10: Densities of Beta(\\(\\alpha,\\beta\\)) distributions.\n\n\n\n\n\nThe ratio of Gamma functions is known as the Beta function and the density can also be written as \\(f(y) = y^{\\alpha-1}(1-y)^{(\\beta-1)} / B(\\alpha,\\beta)\\) where \\(B(\\alpha,\\beta) = \\Gamma(\\alpha)\\Gamma(\\beta)/\\Gamma(\\alpha+\\beta)\\).\nThe mean of a \\(\\text{Beta}(\\alpha,\\beta])\\) random variable is \\[\\text{E}[Y] = \\frac{\\alpha}{\\alpha+\\beta}\n\\] and the variance is \\[\n\\text{Var}[Y] = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2 (\\alpha+\\beta+1)} = \\text{E}[Y]\\frac{\\beta}{(\\alpha+\\beta)(\\alpha+\\beta+1)}\n\\]\nThe support of a Beta random variable is continuous on [0,1], which makes it an attractive candidate for modeling proportions, for example, the proportion of time a vehicle is in maintenance or the proportion of disposable income spent on rent. Cunningham (2024) gives the following examples of Beta distributions:\n\nFive numbers are chosen at random from the interval (0,1) and arranged in order. The middle number has a Beta(3,3) distribution.\nThe probability that an apple will be unblemished has a Beta(8,0.5) distribution.\nRelative humidity for Auckland, New Zealand for the first six months of 2022 has an approximate Beta(5.93, 1.78) distribution.\n\nThe Beta distribution can also be used for random variables that are defined on a different scale, \\(a &lt; Y &lt; b\\) by transforming to the [0,1] scale: \\(Y^* = (Y-a)/(b-a)\\).\nHere are some interesting properties and relationships for Beta random variables:\n\nThe \\(\\text{Beta}(1,1)\\) is a continuous uniform random variable on [0,1].\nIf \\(Y \\sim \\text{Beta}(\\alpha,\\beta)\\), then \\(1 - Y \\sim \\text{Beta}(\\beta,\\alpha)\\).\nThe relationship between Gamma and Beta functions hints at a relationship between Gamma and Beta random variables. Indeed, there is one. If \\(X \\sim \\text{Gamma}(\\alpha_1,\\beta)\\) and \\(Y \\sim \\text{Gamma}(\\alpha_2,\\beta)\\), and \\(X\\) and \\(Y\\) are independent, then the ratio \\(X/(X+Y)\\) follows a Beta\\((\\alpha_1,\\alpha_2)\\) distribution:\n\n\\[\n\\frac{X}{X+Y} \\sim \\text{Beta}(\\alpha_1,\\alpha_2)\n\\]\n\nIf \\(X\\) is an Expo(\\(\\lambda\\)) random variable then \\(e^{−X} \\sim \\text{Beta}(\\lambda,1)\\).\n\nSince \\(Y\\) is continuous, we can define the support of the Beta random variable as \\(0 \\le y \\le 1\\) or as \\(0 &lt; y &lt; 1\\). The probability that the continuous random variable takes on exactly the value 0 or 1 is zero. However, in practice you can observe proportions at the extreme of the support; the proportion of income spent on rent by a homeowner is zero.\n\n\nGaussian (Normal), G\\((\\mu,\\sigma^2)\\)\nThe Gaussian (or Normal) distribution is arguably the most important continuous distribution in all of probability and statistics. A random variable \\(Y\\) has a Gaussian distribution with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\) if its density function is\n\\[\nf(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left\\{ - \\frac{1}{2\\sigma^{2}}(y - \\mu)^{2} \\right\\},\\ \\ \\ \\ \\text{-}\\infty &lt; y &lt; \\infty\n\\]\nThe notation \\(Y \\sim G\\left( \\mu,\\sigma^{2} \\right)\\) or \\(Y \\sim N\\left( \\mu,\\sigma^{2} \\right)\\) is common.\nThe Gaussian distribution has the famous bell shape, symmetric about the mean \\(\\mu\\).\n\n\n\n\n\n\n\n\nFigure 32.11: Gaussian distributions. \\(G(4,1)\\) (solid), \\(G(5,4)\\) (dashed), and \\(G(8, {0.75}^{2})\\) (dotted).\n\n\n\n\n\nA special version is the standard Gaussian (standard normal) distribution \\(Z \\sim G(0,1)\\) with density\n\\[\nf(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{ - \\frac{1}{2}y^{2} \\right\\}\n\\]\nThe standard Gaussian is also referred to as the unit normal distribution.\nGaussian random variables have some interesting properties. For example, linear combinations of Gaussian random variables are Gaussian distributed. If \\(Y\\sim G\\left( \\mu,\\sigma^{2} \\right)\\), then \\(X = aY + b\\) has distribution \\(G(a\\mu + b,a^{2}\\sigma^{2})\\). As an example, if \\(Y\\sim G\\left( \\mu,\\sigma^{2} \\right)\\), then\n\\[Z = \\frac{Y - \\mu}{\\sigma}\\]\nhas a standard Gaussian distribution. You can express probabilities about \\(Y\\) in terms of probabilities about \\(Z\\):\n\\[\\Pr{(X \\leq x)} = \\Pr\\left( Z \\leq \\frac{x - \\mu}{\\sigma} \\right)\\]\nBecause linear functions of Gaussian random variables are Gaussian random variables, it is easy to establish the distribution of the sample mean \\(\\overline{Y} = \\frac{1}{n}\\sum_{i}^{}Y_{i}\\) in a random sample from a \\(G(\\mu,\\sigma^{2})\\) distribution. First, if we take a random sample from any distribution with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\), then the sample mean \\(\\overline{Y}\\) has mean and variance\n\\[\\text{E}\\left\\lbrack \\overline{Y} \\right\\rbrack = \\mu\\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{Var}\\left\\lbrack \\overline{Y} \\right\\rbrack = \\frac{\\sigma^{2}}{n}\\]\nThis follows from the linearity of the expectation operator and the independence of the observations in the random sample. If, in addition, the \\(Y_{i} \\sim G\\left( \\mu,\\sigma^{2} \\right)\\), then\n\\[\n\\overline{Y} \\sim G\\left( \\mu,\\frac{\\sigma^{2}}{n} \\right)\n\\]\nThe sample mean of a random sample from a Gaussian distribution also has a Gaussian distribution. Do we know anything about the distribution of \\(\\overline{Y}\\) if we randomly sample a non-Gaussian distribution? Yes, we do. That is the domain of the central limit theorem.\n\n\nCentral Limit Theorem\n\n\nLet \\(Y_{1},\\cdots,Y_{n}\\) be independent and identically distributed random variables with mean \\(\\mu\\) and variance \\(\\sigma^{2} &lt; \\infty.\\) The distribution of\n\\[\\frac{\\overline{Y} - \\mu}{\\sigma/\\sqrt{n}}\\]\nconverges to that of a standard Gaussian random variable as \\(n \\rightarrow \\infty\\).\n\n\nThese amazing properties contribute to the Gaussian distribution being the most important probability distribution. Most continuous attributes actually do not behave like Gaussian random variables at all, they often have a restricted range or are skewed or have heavier tails than a Gaussian random variable. There is really nothing normal about this distribution, which is one reason why we prefer the name Gaussian over Normal distribution.\n\n\n\n\n\n\nWhat is normal?\n\n\n\n\n\nWe avoid the use of the term normal to describe the Gaussian distribution for another reason: the connection of the concept of normality and this distribution to eugenics. The Belgian astronomer, statistician and mathematician Adolphe Quetelet (1796–1847) introduced the generalized notion of the normal. He studied the distribution of physical attributes and determined that the normal, the most representative, value of an attribute is its average. Discrepancies above and below the average were considered “errors”. Early applications of the Gaussian distribution were in the study of measurement errors. C.F. Gauss used the distribution to represent errors in the measurement of celestial bodies.\nPrior to Quetelet, the view of “norm” and “normality” was associated with carpentry. The carpenter square is also called the norm, and in normal construction everything is at right angles. With Quetelet, the middle of the distribution, the center, became the “new normal.” There is nothing objectionable so far.\nHowever, this view did not sit well with Francis Galton, who introduced the term eugenics. Galton replaced the term “error” with standard deviation and considered variability within a human population as potential for racial progress (Grue and Heiberg 2006). The bell shape of the normal distribution was not used to focus our attention on the average, as Quetelet did. Galton introduced quartiles to categorize the area under the normal into sections of greater and lesser genetic worth. That is not normal!",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/probability.html#functions-of-random-variables",
    "href": "reviewmaterial/probability.html#functions-of-random-variables",
    "title": "32  Probability",
    "section": "32.6 Functions of Random Variables",
    "text": "32.6 Functions of Random Variables\nIf random variable \\(X\\) has p.d.f. \\(f(x)\\), how can we find the density function of a transformation \\(g(X)\\) of \\(X\\)? Three methods are commonly employed, the method of distribution functions, transformations, and moment-generating functions. For a given problem, one often turns out to be simpler than the others, so it is good to know about all of them.\n\nMethod of Distribution Functions\nThis methods finds the cumulative distribution function by integrating the density of \\(X\\) over the region for which \\(Y = g(X) \\le y\\). The density of \\(Y\\) is then found by differentiation.\nWe just learned about the famous property of Gaussian random variables: linear combinations of Gaussian r.v.s are also Gaussian distributed. We can prove that by deriving the distribution of \\(Y = aX + b\\) (\\(a &gt; 0\\)) from the distribution of \\(X \\sim G(\\mu,\\sigma^2)\\). Using \\(F_y\\), \\(f_y\\) and \\(F_x\\), \\(f_x\\) to denote c.d.f. and p.d.f. for \\(Y\\) and \\(X\\), respectively, we can express \\(F_y\\) in terms of \\(F_x\\):\n\\[\n\\begin{align*}\nF_y(y) &= \\Pr(Y \\leq y) = \\Pr(aX+b \\leq y) \\\\\n  &= \\Pr\\left(X \\leq \\frac{y-b}{a}\\right) \\\\\n  &= F_x\\left(\\frac{y-b}{a}\\right)\n\\end{align*}\n\\] To find the density function \\(f_y\\) in terms of the density of \\(X\\), take derivatives with respect to \\(y\\) of the right hand side:\n\\[\nf_y(y) = \\frac{d\\,F_y(y)}{d \\, y} = \\frac{d}{d\\,y}F_x\\left(\\frac{y-b}{a}\\right) = \\frac{1}{a}f_x\\left(\\frac{y-b}{a}\\right)\n\\tag{32.2}\\]\nSo far, the Gaussian property of \\(X\\) has not come into play; Equation 32.2 applies to all distribution functions, provided that \\(F_x\\) can be differentiated. For the specific case where \\(X \\sim G(\\mu,\\sigma^2)\\), the p.d.f. of \\(X\\) is \\[\nf(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left\\{ -\\frac{1}{2} \\left( \\frac{x-\\mu}{\\sigma}\\right)^2\\right\\}\n\\] and \\(\\frac{1}{a} f_x((y-b)/a))\\) is \\[\n\\begin{align*}\nf_y(y) &= \\frac{1}{a\\sqrt{2\\pi}\\sigma} \\exp\\left\\{-\\frac{1}{2} \\left(\\frac{(y-b)/a - \\mu}{\\sigma}\\right)^2 \\right\\} \\\\\n     &= \\frac{1}{\\sqrt{2\\pi}a\\sigma} \\exp\\left\\{-\\frac{1}{2} \\left(\\frac{y-(a\\mu + b)}{a\\sigma}\\right)^2 \\right\\}\n\\end{align*}\n\\] This is the density of a \\(G(a\\mu + b,a^2\\sigma^2)\\) random variable.\n\nThis technique for deriving the density of a transformation of a random variable directly is useful to establish other famous relationships between random variables. For example, if \\(X \\sim G(0,1)\\), what is the distribution of \\(Y = X^2\\)? To solve this we start with a probability statement about \\(Y\\) and translate it into a probability statement about \\(X\\). (We use the notation \\(\\Phi(x)\\) and \\(\\phi(x)\\) to denote the standard Gaussian c.d.f and p.d.f, respectively).\n\\[\n\\begin{align*}\nF_y(y) = \\Pr(Y \\leq y) &= \\Pr(-\\sqrt{y} \\leq X \\leq \\sqrt{y}) \\\\\n&= \\Phi(\\sqrt{y}) - \\Phi(-\\sqrt{y})\n\\end{align*}\n\\] Taking derivatives with respect to \\(y\\) yields the density of \\(Y\\) in terms of the density of \\(X\\): \\[\n\\begin{align*}\nf_y(y) &= \\frac{1}{2}y^{-1/2}\\phi\\left(\\sqrt{y}\\right) + \\frac{1}{2}y^{-1/2}\\phi\\left(-\\sqrt{y}\\right)\\\\\n&= y^{-1/2}\\phi\\left(\\sqrt{y}\\right)\n\\end{align*}\n\\]\nPlugging into the formula for the standard Gaussian density, the density of \\(Y\\) becomes \\[\nf_y(y) = \\frac{y^{-1/2}}{\\sqrt{2\\pi}} \\, \\exp\\{-y/2\\}\n\\] Compare this expression to Equation 32.1, it turns out that \\(Y\\) has a Gamma density with parameters \\(\\alpha = 1/2\\), \\(\\beta=2\\). This special case is also known as a chi-square distribution with one degree of freedom. More on chi-square distributions in the review material on statistics.\n\n\nMethod of Transformations\nThis method of finding the distribution of \\(Y = g(X)\\), if the distribution of \\(X\\) is known is a variation of the method of distribution functions. If \\(g(X)\\) is an increasing or decreasing function of \\(X\\), in the sense that if \\(x_1 &lt; x_2\\) then \\(g(x_1) &lt; g(x_2)\\) and if \\(x_1 &gt; x_2\\) then \\(g(x_1) &gt; g(x_2)\\).\nIn that situation you can find the inverse function \\(X = g^{-1}(Y)\\). After calculating the derivative \\(dx/dy\\), the density of \\(Y\\) is found as \\[\nf_y(y) = f_x(y) \\left | \\frac{dx}{dy}  \\right |\n\\tag{32.3}\\]\nAn example will make the method of transformation clear.\n\n\nExample: Method of Transformation\n\n\nSuppose that \\(X\\) has density function \\[\nf_x(x) = \\left \\{\n\\begin{array}{ll}\n     2x & 0 \\leq x \\leq 1 \\\\\n     0  & \\text{otherwise}\n\\end{array}\\right .\n\\] What is the distribution of \\(Y = 3X - 1\\)?\nThe function \\(g(x) = 3x-1\\) increases montonically in \\(x\\) for \\(0 \\leq x \\leq 1\\). Inverting the relationship between \\(Y\\) and \\(X\\) yields \\[\nx = g^{-1}(y) = \\frac{y+1}{3}\n\\] and \\[\n\\frac{dx}{dy} = \\frac{1}{3}\n\\] Plugging into Equation 32.3 and modifying the support according to the transformation leads to \\[\nf_y(y) = 2x \\frac{dx}{dy} = 2\\left(\\frac{y+1}{3}\\right) \\frac{1}{3} = \\frac{2}{9}(y+1)\\qquad -1 \\leq y \\leq 2\n\\]\n\n\n\n\n\n\nCunningham, Adam. 2024. “Probability Playground.” Amstat News, 26–28.\n\n\nGrue, Lars, and Arvid Heiberg. 2006. “Notes on the History of Normality–Reflections on the Work of Quetelet and Galton.” Scandinavian Journal of Disability Research 8 (4): 232–46.",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/statistics.html",
    "href": "reviewmaterial/statistics.html",
    "title": "33  Statistics",
    "section": "",
    "text": "33.1 Introduction\nStatistics is the science of uncertainty. The name of the discipline hails from the German word “Statistik”, a quantity describing a state or country. Statistics deals with the collection of data, the generation of data through experiments, the analysis of data and the drawing of conclusions from data.\nHere is a strange statement: statistics is concerned with statistics. Seems kind of obvious, but there is a trick. The first “statistic” in this sentence refers to the discipline, the second to a quantity computed from data. The sample mean or the sample standard deviation are statistics, as are the coefficients of a regression model. Because data are inherently uncertain, either because they have been sampled from a population, are measured with error, represent naturally variable phenomena, or are the result of randomized manipulation, statistics computed from data are random variables. And variability is contagious. The distributional properties of the data impart a certain random behavior on the quantities calculated from statistics.\nStatisticians thus worry a great deal about which statistical quantities are best to be used in different situations. What are their properties based on a sample of size \\(n\\) and how do their properties change as the sample size increases? How do stochastic properties of the population being sampled transfer into properties of a statistic?",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/statistics.html#introduction",
    "href": "reviewmaterial/statistics.html#introduction",
    "title": "33  Statistics",
    "section": "",
    "text": "Parameters & Statistics\nUnknown quantities that describe the population we are interested in are called parameters. We sample elements of the population to collect data based on which we can compute statistics that estimate the paraemters (Figure 33.1).\n\n\n\n\n\n\nFigure 33.1: Sampling from as population to calculate statistics that estimate parameters of the population.\n\n\n\nThe statistics are random variables because the attribute sampled from the population is not constant, it has variability. Depending on which elements of the population make it into the sample, the values of the statistics will differ. The only ways around the variability in statistics would be to either\n\nobserve (measure) all elements of the population, a procedure known as a census\nobserve an attribute that does not vary in the population (\\(\\text{Var}[Y] = 0\\)).\n\nIn the first case there is no variability in the statistics because every repetition of the census yields the exact same data. In the second case there is no variability in the statistics because there is no variability in the values we observe. In situation 2 we might as well just sample a single observation and we know everything there is to know about the attribute in question.\n\n\nRandom Variables & Realizations\nWhen discussing statistics, the quantities computed from data, we must make a distinction between random quantities and their realizations. For example, suppose you draw \\(n\\) observations from a Gaussian distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). This is the same as saying that there is a population where attribute \\(Y\\) follows a \\(G(\\mu, \\sigma^2)\\) distribution and we are pulling a random sample of \\(n\\) elements of the population.\nWhat is the difference between the following quantities, \\[\n\\begin{align*}\n\\overline{Y} &= \\frac{1}{n} \\sum_{i=1}^n Y_i \\\\\n\\overline{y} &= \\frac{1}{n} \\sum_{i=1}^n y_i\n\\end{align*}\n\\]\nThe distributional properties of \\(Y\\) in the population can be expressed as \\(Y \\sim G(\\mu,\\sigma^2)\\). \\(Y_1\\) is the first random variable in our sample. \\(y_1\\) is the value we observe on the first draw, for example, \\(y_1 = 5.6\\).\nWhen we refer to the random variables, we use upper-case notation, when we refer to their realized (observed) values in a particular set of data we use lower-case notation. Both \\(\\overline{Y}\\) and \\(\\overline{y}\\) refer to the sample mean. \\(\\overline{Y}\\) is a random variable, \\(\\overline{y}\\) is the actual value of the sample mean in our data, it is a constant. \\(\\overline{Y}\\) has an expected value (the mean of the sample mean) and a variance that are functions of the mean and variance of the population from which the sample is drawn,\n\\[\n\\begin{align*}\n\\text{E}\\left[ \\overline{Y}\\right] &= \\mu \\\\\n\\text{Var} \\left[\\overline{Y}\\right] &= \\frac{\\sigma^2}{n}\n\\end{align*}\n\\]\nThe variance of \\(\\overline{y}\\) is zero, and its expected value is equal to the value we obtained for \\(\\overline{y}\\), because it is a constant:\n$$ \\[\\begin{align*}\n\\text{E}\\left[ \\overline{y}\\right] &= \\overline{y} \\\\\n\\text{Var} \\left[\\overline{y}\\right] &= 0\n\n\\end{align*}\\] $$\n\nIt should always be clear from context whether we are talking about the random properties of a statistic or its realized value. Unfortunately, notation is not always clear. In statistical models, for example, it is common to use Greek letters for parameters. These are unknown constants. Based on data we find estimators for the parameters. For example, in the simple linear regression model \\[\nY_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\] \\(\\beta_0\\) and \\(\\beta_1\\) are the intercept and slope parameters of the model. The ordinary least squares estimator of \\(\\beta_1\\) is \\[\n\\widehat{\\beta}_1 = \\frac{S_{xY}}{S_{xx}} = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})(Y_i - \\overline{Y})}{\\sum_{i=1}^n(x_i - \\overline{x})^2}\n\\tag{33.1}\\]\nThe input variable is considered fixed in regression models, not a random variable. That is why we use lower-case notation for \\(x_i\\) and \\(\\overline{x}\\) in Equation 33.1. With respect to the target variable we have two choices. Equation 33.1 uses upper-case notation, referring to the random variables. \\(\\widehat{\\beta}_1\\) in Equation 33.1 is a random variable and we can study the distributional properties of \\(\\widehat{\\beta}_1\\).\nThe expression for \\(\\widehat{\\beta}_1\\) in Equation 33.2 uses lower-case notation for \\(y_i\\) and \\(\\overline{y}\\), it is referring to the constants computed from a particular sample.\n\\[\n\\widehat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^n(x_i - \\overline{x})^2}\n\\tag{33.2}\\]\nThe quantity \\(\\widehat{\\beta}_1\\) in Equation 33.2 is the realized value of the regression slope, for example, 12.456. Unfortunately, the notation \\(\\widehat{\\beta}_1\\) does not tell us whether we are referring to the estimator of the slope—a random variable—or the estimate of the slope—a constant.\n\nTwo primary lines of inquiry in statistics are the description of data—through quantitative and visual summaries—and the drawing of conclusions from data. These lines of inquiry are called descriptive and inferential statistics. Descriptive statistics—the discipline—describes the features of a population (a distribution) based on the data in a sample. Inferential statistics—the discipline—draws conclusions about one or more populations.\nBoth lines of inquiry rely on statistics, quantities computed from data. The distinction lies in how the statistics are used. Descriptive statistics uses the sample mean to estimate the central tendency—the typical value—of a population. Inferential statistics uses the sample means in two groups to make conclude whether the means of the populations are different.",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/statistics.html#descriptive-statistics",
    "href": "reviewmaterial/statistics.html#descriptive-statistics",
    "title": "33  Statistics",
    "section": "33.2 Descriptive Statistics",
    "text": "33.2 Descriptive Statistics\nDescriptive statistics capture the feature of a distribution (a population) in numerical or graphical summaries. The most important features of a distribution are its central tendency (location) and its variability (dispersion). The central tendency captures where the typical values are located, the dispersion expresses how much the distribution spreads around the typical values.\n\nNumerical Summaries\nTable 33.1 shows important location statistics and Table 33.2 important dispersion statistics.\n\n\n\nTable 33.1: Important statistics measuring location attributes of a distribution. Sample mean, sample median, and sample mode are measures of the central tendency of a variable. \\(Y_{(k)}\\) denotes the value at the \\(k\\)th position when the values are arranged in ascending order; this is called the \\(k\\)th order statistic. The min is defined as the smallest non-missing values because NaNs often sort as smallest values in software packages.\n\n\n\n\n\n\n\n\n\n\n\nSample Statistic\nSymbol\nComputation\nNotes\n\n\n\n\nMin\n\n\\(Y_{(1)}\\)\nThe smallest non-missing value\n\n\nMax\n\n\\(Y_{(n)}\\)\nThe largest value\n\n\nMean\n\\(\\overline{Y}\\)\n\\(\\frac{1}{n}\\sum_{i=1}^n Y_i\\)\nMost important location measure, but can be affected by outliers\n\n\nMedian\nMed\n\\(\\left \\{ \\begin{array}{cc} Y_{\\left(\\frac{n+1}{2}\\right)} & n \\text{ is even} \\\\\n\\frac{1}{2} \\left( Y_{\\left(\\frac{n}{2} \\right)} + Y_{\\left(\\frac{n}{2}+1\\right)} \\right) & n\\text{ is odd} \\end{array}\\right .\\)\nHalf of the observations are smaller than the median; robust against outliers\n\n\nMode\nMode\n\nThe most frequent value; not useful when real numbers are unique\n\n\n1st Quartile\n\\(Q_1\\)\n\\(Y_{\\left(\\frac{1}{4}(n+1) \\right)}\\)\n25% of the observations are smaller than \\(Q_1\\)\n\n\n2nd Quartile\n\\(Q_2\\) = Med\nSee Median\n50% of the observations are smaller than \\(Q_2\\). This is the median\n\n\n3rd Quartile\n\\(Q_3\\)\n\\(Y_{\\left(\\frac{3}{4}(n+1) \\right)}\\)\n75% of the observations are smaller than \\(Q_3\\)\n\n\nX% Percentile\n\n\\(Y_{\\left(\\frac{X}{100}(n+1) \\right)}\\)\nFor example, 5% of the observations are larger than \\(P_{95}\\), the 95% percentile\n\n\n\n\n\n\nThe most important location measures are the sample mean \\(\\overline{Y}\\) and the median. The sample mean is the arithmetic average of the sample values. Because the sample mean can be affected by outliers—large values are pulling the sample mean up—the median is preferred in the presence of extreme observations. For example, when reporting central tendency of annual income, the median income is chosen over the arithmetic average so that extreme incomes like those of Elon Musk, Jeff Bezos, and Mark Zuckerberg do not distort the notion of the typical value.\n\n\n\nTable 33.2: Important statistics measuring dispersion (variability) of a variable.\n\n\n\n\n\n\n\n\n\n\n\nSample Statistic\nSymbol\nComputation\nNotes\n\n\n\n\nRange\n\\(R\\)\n\\(Y_{(n)} - Y_{(1)\\)\n\\(n\\)th order statistic (largest) minus first (smallest) order statistic\n\n\nInter-quartile Range\nIQR\n\\(Q_3 - Q_1\\)\nUsed in constructing box plots; covers the central 50% of the data\n\n\nStandard Deviation\n\\(S\\)\n\\(\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n\\left( Y_i - \\overline{Y}\\right)^2}\\)\nMost important dispersion measure; in the same units as the sample mean (the units of \\(Y\\))\n\n\nVariance\n\\(S^2\\)\n\\(\\frac{1}{n-1}\\sum_{i=1}^n \\left( Y_i - \\overline{Y} \\right)^2\\)\nImportant statistical measure of dispersion; in squared units of \\(Y\\)\n\n\nSkewness\n\\(g_1\\)\n\\(\\frac{\\frac{1}{n}\\sum_{i=1}^n(Y_i - \\overline{Y})^3}{S^3}\\)\nMeasures the symmetry of a distribution\n\n\nKurtosis\n\\(k\\)\n\\(\\frac{\\frac{1}{n}\\sum_{i=1}^n(Y_i - \\overline{Y})^4}{S^4}\\)\nMeasures the peakedness of a distribution\n\n\n\n\n\n\n\n\nGraphical Summaries\nWe view a population as a distribution of values. Data visualizations thus lend themselves to the description of populations. The more important visual descriptive summaries are the frequency distribution (histogram) and the boxplot.\n\nHistogram (Frequency Distribution)\nThe frequency distribution is constructed by binning the observed values into segments of equal width. Figure 33.2 shows the (absolute) frequency distribution of out-of-state tuition for private universities in 1995. Figure 33.3 displays the same data with a large number of frequency bins (30). Between 5 and 30 bins is typical for histograms. The number of bins should not be too small to obscure features of the distribution and it should not be too large to create artifacts in regions of low data density.\n\n\n\n\n\n\n\n\nFigure 33.2: Histograms for out-of-state tuition for private universities in 1995.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 33.3: Histograms with 30 breaks for out-of-state tuition for private universities in 1995.\n\n\n\n\n\nTo compare frequency distributions between groups histograms can be plotted side by side or overlaid. When the histograms overlap a side-by-side display is preferred because bars from different groups can obscure it each other. Adding transparency to the bars helps but can still create confusing displays (Figure 33.4).\n\n\n\n\n\n\n\n\nFigure 33.4: Overlaid histograms of out-of-state tuition for private and public universities in 1995.\n\n\n\n\n\nIn the side-by-side display attention needs to be paid to the scale of the horizontal and vertical axes. Figure 33.5 forces the axes to be the same in both histograms to facilitate a direct comparison. Alternatively to forcing the vertical axes to the same frequency range you can express the frequencies in relative terms (Figure 33.6).\n\n\n\n\n\n\n\n\nFigure 33.5: Histograms for out-of-state tuition for private and public universities in 1995.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 33.6: Relative frequency distributions for out-of-state tuition for private and public universities in 1995.\n\n\n\n\n\n\n\nBoxplot\nThe boxplot, also called the box-and-whisker plot, is a simple visualization of a distribution that combines information about location, dispersion, and extreme values. The standard box plot (Figure 33.7) comprises\n\na box that covers the inter-quartile range from \\(Q_1\\) to \\(Q_3\\).\na line through the box at the location of the median (\\(Q_2\\)).\nWhiskers that extend from the edge of the box to the largest and smallest observations within \\(1.5\\times \\text{IQR}\\).\nOutliers that fall outside the whiskers.\n\n\n\n\n\n\n\nFigure 33.7: Schema of a box plot.\n\n\n\nThe idea of the boxplot is that the box covers the central 50% of the observation, the whiskers extend to the vast majority of the data. Data points that fall outside the whiskers are declared outliers.\nNote, however, that outlying observations in boxplots does not necessarily indicate observations that are inconsistent with the data. A boxplot for a sample from a normal distribution will likely show some outliers by this definition (Figure 33.8). The probability to observe values outside of the whiskers for a \\(G(\\mu,\\sigma^2)\\) distribution is 0.00697. In a sample of \\(n=1000\\) you should expect to see 7 observations beyond the whiskers.\n\n\n\n\n\n\n\n\nFigure 33.8: Box plot of 300 observations from a \\(G(0,1)\\) distribution.\n\n\n\n\n\nBoxplots are great visual devices to compare the distribution for different attributes (Figure 33.9) or to compare the same attribute across different groups (Figure 33.10).\n\n\n\n\n\n\n\n\nFigure 33.9: Comparing distributions of attributes with boxplots.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 33.10: Comparing distributions of attributes with boxplots.",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/statistics.html#sec-prob-sampling-dist",
    "href": "reviewmaterial/statistics.html#sec-prob-sampling-dist",
    "title": "33  Statistics",
    "section": "33.3 Sampling Distributions",
    "text": "33.3 Sampling Distributions\nSampling distributions are probability distributions of statistics computed from random samples.\nChapter 32 discussed probability distributions in general. If one draws a random sample from, say, a \\(G(\\mu,\\sigma^2)\\) distribution, what are the distributional properties of statistics like \\(\\overline{Y}\\), \\(S^2\\), and so on?\nWe know that \\(\\overline{Y}\\) in a random sample from a distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) has mean \\(\\text{E}[\\overline{Y}] = \\mu\\) and variance \\(\\text{Var}[\\overline{Y}] = \\sigma^2/n\\). This can be verified based on properties of expected values in iid samples. We do not need to mess with any density or mass functions.\n\\[\n\\begin{align*}\n\\text{E}\\left[\\overline{Y}\\right] &= \\frac{1}{n}\\text{E}\\left[\\sum_{i=1}^n Y_i\\right]\n    = \\frac{1}{n} \\sum_{i=1}^n \\text{E}\\left[Y_i\\right] \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\mu = \\frac{n\\mu}{n} = \\mu\n\\end{align*}\n\\tag{33.3}\\]\n\\[\n\\begin{align*}\n\\text{Var}\\left[\\overline{Y} \\right] &= \\text{Var}\\left[ \\frac{1}{n}\\sum_{i=1}^n Y_i \\right]\\\\\n&= \\frac{1}{n^2} \\text{Var} \\left[ \\sum_{i=1}^n Y_i \\right]\\\\\n&= \\frac{1}{n^2} \\sum_{i=1}^n \\text{Var}\\left[ Y_i \\right]\\\\\n&= \\frac{1}{n^2} \\sum_{i=1}^n \\sigma^2 = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}\n\\end{align*}\n\\tag{33.4}\\]\nEquation 33.3 and Equation 33.4 apply to any population, as long as we draw a random sample. What else can we say about the distribution of \\(\\overline{Y}\\)?\n\nCentral Limit Theorem\nBy the central limit theorem, introduced at the end of the previous chapter, we can say that the distribution of \\[\n\\frac{\\overline{Y} - \\mu}{\\sigma/\\sqrt{n}}\n\\] converges to a \\(G(0,1)\\) distribution as \\(n\\) grows to infinity. Another way of saying this is that the distribution of \\(\\overline{Y}\\) converges to a \\(G(\\mu,\\sigma^2/n)\\) distribution.\nThis is a pretty amazing result. Regardless of the shape of the population you are sampling from, whether the attribute is continuous or discrete, if you draw a large enough sample, the sample mean will behave like a Gaussian random variable where the mean is the same as that of the population and the variance is \\(n\\)-times smaller than the population variance.\n\n\nExample: Distribution of Sample Mean of Bernoullis\n\n\nThe distribution furthest from a Gaussian distribution in some sense is the Bernoulli(\\(\\pi\\)) distribution. It is a discrete distribution with only two outcomes. Only for \\(\\pi=0.5\\) is the Bernoulli distribution symmetric Figure 33.11 shows the mass function of a Bernoulli distribution with \\(\\pi=0.7\\). How do you get from that to a continuous, symmetric Gaussian distribution?\n\n\n\n\n\n\n\n\nFigure 33.11: Probability mass function of the Bernoulli(0.7) random variable.\n\n\n\n\n\nLet’s study the distribution of the sample mean if we draw random samples of sizes \\(n_1 = 10\\), \\(n_2=100\\), \\(n_3 = 1000\\), and \\(n_4=10000\\) from a Bernoulli(0.7). In order to get a good estimate of the sampling distribution of \\(\\overline{Y}\\) the process needs to be repeated a sufficient number of times. We choose \\(m=2500\\) repetitions of the experiment to build up the distribution of \\(\\overline{Y}\\).\nFigure 33.12 shows the sampling distributions of \\(\\overline{Y}\\) for the four sample sizes. For \\(n=10\\), for example, a random sample of size 10 was drawn from a Bernoulli(0.7) and its sample mean was calculated. This was repeated another 2,499 times and the figure displays the estimated probability density of the 2,500 \\(\\overline{y}\\) values. A similar procedure for the other sample sizes leads to the other density estimates.\nWith \\(n=10\\), the distribution is far from Gaussian. The sample mean can take on only 10 possible values, \\(0, 0.1, 0.2, \\cdots, 1.0\\). As \\(n\\) increases the number of possible values of \\(\\overline{Y}\\) increases, the distribution becomes more continuous. It also becomes more symmetric and approaches the Gaussian distribution. For a sample of size \\(n=1000\\) the sample mean is very nearly Gaussian distributed and increasing the sample size to \\(n=10,000\\) does not appear to improve the quality of the Gaussian approximation much. However, it does increase the precision of \\(\\overline{Y}\\) as an estimator; the variance of the sampling distribution for \\(n=10,000\\) is less than the variance of the sampling distribution for \\(n=1,000\\). We can calculate the variance of the sample mean in this case based on Equation 33.4:\n\\[\n\\text{Var}\\left[\\overline{Y}\\right] = \\frac{0.7 \\times 0.3}{10000} = 0.000021\n\\]\n\n\n\n\n\n\n\n\nFigure 33.12: Sampling distribution of \\(\\overline{Y}\\) drawn from Bernoulli(0.7).\n\n\n\n\n\n\n\n\n\nSampling from a Gaussian\nThe central limit theorem specifies how the distribution of \\(\\overline{Y}\\) evolves as more and more samples are drawn. If the distribution we sample from is itself Gaussian, we do not need a large-sample approximation to describe the distribution of \\(\\overline{Y}\\).\nIn a random sample of size \\(n\\) from a \\(G(\\mu,\\sigma^2)\\) distribution, the distribution of \\(\\overline{Y}\\) follows a \\(G(\\mu,\\sigma^2/n)\\) distribution. In other words, the distribution of \\(\\overline{Y}\\) is known to be exactly Gaussian, regardless of the size of the sample. As long as we draw a random sample—that is, the random variables are iid.\nIn parallel to how the central limit theorem was stated above, we can say that if \\(Y_1, \\cdots, Y_n\\) are iid \\(G(\\mu,\\sigma^2)\\), then \\[\n\\frac{\\overline{Y}-\\mu}{\\sigma/\\sqrt{n}} \\sim G(0,1)\n\\tag{33.5}\\]\n\nThe Gaussian distribution is just one of several important distributions we encounter when working with statistics computed from Gaussian samples, The \\(t\\), \\(\\chi^{2}\\), and \\(F\\) distributions are other important sampling distributions. These distributions play an important role in inferential statistics, for example in computing confidence intervals or testing hypotheses about parameters. Names such as \\(t\\)-test, \\(F\\)-test, or chi-square test stem from the distribution of statistics used to carry out the procedure.\n\n\nChi-Square Distribution, \\(\\chi^2_\\nu\\)\nLet \\(Z_{1},\\cdots,Z_{k}\\) denote independent standard Gaussian random variables \\(G(0,1)\\). Then\n\\[\nX = Z_{1}^{2} + Z_{2}^{2} + \\cdots + Z_{k}^{2}\n\\]\nhas p.d.f.\n\\[\nf(x) = \\frac{1}{2^{\\frac{k}{2}}\\Gamma\\left( \\frac{k}{2} \\right)}x^{\\frac{k}{2}}e^{- x/2},\\ \\ \\ \\ \\ x \\geq 0\n\\]\nThis is known as the Chi-square distribution with \\(k\\) degrees of freedom, abbreviated \\(\\chi_{k}^{2}\\). The mean and variance of a \\(\\chi_{k}^{2}\\) random variable are \\(\\text{E}\\lbrack X\\rbrack = k\\) and \\(\\text{Var}\\lbrack X\\rbrack = 2k\\).\nThe degrees of freedom can be thought of as the number of independent pieces of information that contribute to the \\(\\chi^{2}\\) variable. Here, that is the number of independent \\(G(0,1)\\) variables. Since the \\(\\chi^{2}\\) variable is the sum of their squared values, the density shifts more to the right as the degrees of freedom increase (Figure 33.13).\n\n\n\n\n\n\n\n\nFigure 33.13: Probability density function of a \\(\\chi_4^2\\) and a \\(\\chi_8^2\\) random variable. \\(\\chi^2\\) variables are skewed to the right, the density shifts to the right with increasing degrees of freedom.\n\n\n\n\n\n\\(\\chi^{2}\\) distributions are important to capture the sample distributions of dispersion statistics. If \\(Y_{1},\\cdots,Y_{n}\\) are a random sample from a \\(G\\left( \\mu,\\sigma^{2} \\right)\\) distribution, and the sample variance is\n\\[\nS^2 = \\frac{1}{n-1}\\sum_{i=1}^n\\left( Y_i - \\overline{Y} \\right)^2\n\\] then the random variable \\[\n\\frac{(n-1)S^2}{\\sigma^2}\n\\] follows a \\(\\chi_{n-1}^2\\) distribution. It follows that \\(S^2\\) is an unbiased estimator of \\(\\sigma^2\\):\n\\[\n\\text{E}\\left\\lbrack \\frac{(n-1)S^2}{\\sigma^2} \\right\\rbrack = n - 1\n\\]\n\\[\n\\text{E}\\left\\lbrack S^{2} \\right\\rbrack = \\sigma^2\n\\]\n\n\nStudents’ t Distribution, \\(t_\\nu\\)\nIn the Gaussian case you can also show that \\(\\overline{Y}\\) and \\(S^2\\) are independent random variables. This is important because of another distributional result: if \\(Z \\sim G(0,1)\\) and \\(U \\sim \\chi_\\nu^2\\), and \\(Z\\) and \\(U\\) are independent, then the ratio\n\\[\nT = \\frac{Z}{\\sqrt{U/\\nu}}\n\\]\nhas a \\(t\\) distribution with \\(\\nu\\) degrees of freedom. In honor of Student, the pseudonym used by William Gosset for publishing statistical research while working at Guinness Breweries, the \\(t\\) distribution is also known as Student’s \\(t\\) distribution or Student’s distribution for short.\nAs with the \\(\\chi_\\nu^2\\) distributions, the shape of the \\(t_\\nu\\) distribution depends on the degrees of freedom \\(\\nu\\). However, the \\(t_\\nu\\) distributions are symmetric about zero and the degrees of freedom affect how heavy the tails are. As the degrees of freedom grow, the \\(t\\) density approaches that of the standard Gaussian distribution (Figure 33.14).\nWe can now combine the results about the sampling distributions of \\(\\overline{Y}\\) and \\(S^2\\) to derive the distribution of\n\\[\n\\frac{\\overline{Y} - \\mu}{S/\\sqrt{n}}\n\\tag{33.6}\\]\nWe know that \\(\\overline{Y} \\sim G\\left( \\mu,\\frac{\\sigma^2}{n} \\right)\\) and that \\(\\frac{(n-1)S^2}{\\sigma^2}\\) follows a \\(\\chi_{n-1}^2\\) distribution. Furthermore, \\(\\overline{Y}\\) and \\(S^2\\) are independent. Taking ratios as required by the definition of a \\(t\\) random variable yields\n\\[\n\\frac{\\frac{\\overline{Y} - \\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{S^2{/\\sigma}^2}} =\n\\frac{\\overline{Y} - \\mu}{S/\\sqrt{n}} \\sim t_{n - 1}\n\\]\nCompare Equation 33.5 to Equation 33.6. The difference is subtle, the dispersion statistic in the denominator. If the population variance \\(\\sigma^2\\) is known, we use Equation 33.5 and the distribution is Gaussian with mean 0 and variance 1. If \\(\\sigma^2\\) is unknown, we replace \\(\\sigma\\) in Equation 33.5 with an estimator based on the sample, the sample standard deviation \\(S\\). The result is a \\(t\\) distribution with \\(n-1\\) degrees of freedom. As \\(n\\) increases, the \\(t\\) distribution approaches a \\(G(0,1)\\) distribution because the estimator \\(S\\) approaches a constant. For any given value \\(n\\), the \\(t\\) distribution will be heavier in the tails than a \\(G(0,1)\\) distribution because there is more uncertainty in the system when the random variable \\(S\\) is used as an estimator of the constant \\(\\sigma\\) (Figure 33.14).\n\n\n\n\n\n\n\n\nFigure 33.14: Comparison of \\(G(0,1)\\) (dashed line) and \\(t\\) distributions with 2, 4, and 20 degrees of freedom. \\(t\\) distributions have heavier tails than the \\(G(0,1)\\).\n\n\n\n\n\n\n\nF Distribution, \\(F_{\\nu_1, \\nu_2}\\)\nIf \\(\\chi_1^2\\) and \\(\\chi_2^2\\) are independent chi-square random variables with \\(\\nu_1\\) and \\(\\nu_2\\) degrees of freedom, respectively, then the ratio\n\\[\nF = \\frac{\\chi_1^2/v_1}{\\chi_2^2/\\nu_2}\n\\]\nfollows an \\(F\\) distribution with \\(\\nu_1\\) numerator and \\(\\nu_2\\) denominator degrees of freedom (Figure 33.15). We denote this fact as \\(F\\sim F_{\\nu_1,\\nu_2}\\). The mean and variance of the \\(F\\) distribution are\n\\[\n\\begin{align*}\n\\text{E}\\lbrack F\\rbrack &= \\frac{\\nu_2}{\\nu_{2} - 2} \\\\\n\\text{Var}\\lbrack F\\rbrack &= \\frac{2\\nu_2^2 (\\nu_1 + \\nu_2 + 2)}{\\nu_1\\left( \\nu_2 - 2 \\right)^2\n(\\nu_2 - 4)}\n\\end{align*}\n\\]\nThe mean exists only if \\(\\nu_{2} &gt; 2\\) and the variance exists only if \\(\\nu_{2} &gt; 4\\).\n\n\n\n\n\n\n\n\nFigure 33.15: Density functions of an \\(F_{4,12}\\) and an \\(F_{12,20}\\) distribution.\n\n\n\n\n\nWhen sampling from a Gaussian distribution we established that \\(\\frac{(n-1)S^2}{\\sigma^2}\\) follows a \\(\\chi_{n-1}^{2}\\) distribution. Suppose we have two samples, one of size \\(n_1\\) from a \\(G(\\mu_1,\\sigma_1^2)\\) distribution and one of size \\(n_2\\) from a \\(G(\\mu_2,\\sigma_2^2)\\) distribution. If the estimators of the sample variances in the two samples are denoted \\(S_1^2\\) and \\(S_2^2\\), respectively, then the ratio\n\\[\n\\frac{S_1^2/\\sigma_1^2}{S_2^2/\\sigma_2^2}\n\\]\nhas an \\(F_{n_1-1,n_2-1}\\) distribution.\n\\(F\\) distributions play an important role in the analysis of variance, where the numerator and denominator are ratios of mean squares.\nRecall that the \\(T\\) random variable in the Gaussian case can be written as\n\\[\nT = \\frac{\\frac{\\overline{Y} - \\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{S^2{/\\sigma}^2}}\n\\]\nThe numerator is a \\(G(0,1)\\) variable, so squaring it yields a \\(\\chi_1^2\\) variable. The square of the denominator is a scaled \\(\\chi_{n-1}^2\\) variable, \\(S^2/\\sigma^2\\). Also, because \\(\\overline{Y}\\) and \\(S^2\\) are independent, the squares of the numerator and denominator are independent. It thus follows that\n\\[\nT^2 = \\frac{\\left( \\overline{Y} - \\mu \\right)^2}{S^2/n}\\]\nfollows an \\(F_{1,n - 1}\\) distribution.\nThis fact is used in software packages that report the result of an \\(F\\)-test instead of the result of a two-sided \\(t\\)-test. The two are equivalent and the \\(p\\)-values are the same.",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/statistics.html#inferential-statistics",
    "href": "reviewmaterial/statistics.html#inferential-statistics",
    "title": "33  Statistics",
    "section": "33.4 Inferential Statistics",
    "text": "33.4 Inferential Statistics\nThe process of statistical inference is to draw conclusions about the world based on data. The world is described in terms of populations and the probability distributions of attributes in the population. The data consists of observations drawn from those populations, random variables that follow the distribution laws that govern the populations. The goal is to infer properties of the populations based on the information captured in the data.\nSome of the statistics computed in inference procedures are the same used to describe populations (distributions). For example, the sample mean describes the central tendency of a distribution, it is also used to compute confidence intervals for the mean in the population. Statistical inference adds a further dimension to describing things; it investigates the properties of the statistics such as bias and variance.\n\nPoint Estimation\nA point estimate is a single number to estimate a parameter. The corresponding statistic is the point estimator. \\(\\overline{y} = 10.4\\) in a random sample is the point estimate for the mean \\(\\mu\\). \\(\\overline{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i\\) is the point estimator; a rule that tells how to calculate the estimate.\nImportant properties of a point estimator are the bias and variance. The bias is a measure of its accuracy. An unbiased estimator is an accurate estimator. The variance is a measure of the uncertainty associated with the estimator; it is the reciprocal of the estimator’s precision. A highly variable estimator is not precise—and vice versa.\nBias and variance are long-run properties of the estimator, they are expected values.\n\n\nDefinition: Bias, Variance, and Mean Squared Error\n\n\nThe bias of an estimator is the expected value of the difference between the estimator and the parameter it is estimating. If \\(\\theta\\) is the parameter of interest and \\(h(\\textbf{Y})\\) is an estimator—a function of \\(\\textbf{Y}=[Y_1,\\cdots,Y_n]^\\prime\\)—then \\[\n\\text{Bias}\\left[h(\\textbf{Y});\\theta\\right] = \\text{E}\\left[h(\\textbf{Y})-\\theta\\right] = \\text{E}\\left[h(\\textbf{Y})\\right]-\\theta\n\\] The variance of an estimator is the usual expected squared difference between the estimator and its mean: \\[\n\\text{Var}[h(\\textbf{Y})] = \\text{E}\\left[\\left(h(\\textbf{Y})-\\text{E}[h(\\textbf{Y})]\\right)^2\\right]\n\\] The mean squared error of estimator \\(h(\\textbf{Y})\\) for the parameter \\(\\theta\\) is \\[\n\\text{MSE}\\left[h(\\textbf{Y});\\theta\\right] = \\text{E}\\left[\\left(h(\\textbf{Y})-\\theta\\right)^2 \\right] = \\text{Var}[h(\\textbf{Y})] + \\text{Bias}\\left[h(\\textbf{Y});\\theta\\right]^2\n\\tag{33.7}\\]\n\n\nAn estimator with \\(\\text{Bias}[h(\\textbf{Y});\\theta] = 0\\) is said to be unbiased for \\(\\theta\\). An unbiased estimator that has the smallest variance among all unbiased estimators is said to be a uniform minimum variance unbiased estimator (UMVUE).\nEstimators with small bias and a sharply reduced variance—compared to the best unbiased estimator—can have a smaller mean squared error and can thus be preferred over an unbiased estimator.\n\n\nExample: Estimating the Parameter of an Exponential Distribution\n\n\nSuppose that \\(Y\\) has an exponential distribution with density \\[\nf(y) = \\frac{1}{\\theta} \\, e^{-y/\\theta}\\qquad \\text{for } y &gt; 0\n\\] This is a re-parameterization of the density in Section 32.5.2 where \\(\\theta = 1/\\)$. The mean and variance of \\(Y\\) are \\(\\text{E}[Y] = \\theta\\) and \\(\\text{Var}[Y] = \\theta^2\\). You draw a random sample of size \\(n=3\\). Consider the following estimators:\n\n\\(\\widehat{\\theta}_1 = Y_1\\)\n\\(\\widehat{\\theta}_2 = \\frac{Y1+Y2}{2}\\)\n\\(\\widehat{\\theta}_3 = \\frac{Y1+2Y2}{3}\\)\n\\(\\widehat{\\theta}_4 = \\overline{Y}\\)\n\\(\\widehat{\\theta}_5 = \\min\\{Y1,Y2,Y3\\}\\)\n\nWhich of these estimators are unbiased, which one has the smallest variance and mean squared error?\n\nIt is easy to establish that \\(\\widehat{\\theta}_1\\) through \\(\\widehat{\\theta}_4\\) are unbiased: \\[\n\\begin{align*}\n\\text{E}\\left[\\widehat{\\theta}_1\\right] &= \\text{E}[Y_1] = \\theta \\\\\n\\text{E}\\left[\\widehat{\\theta}_2\\right] &= \\frac{1}{2}(\\text{E}[Y_1]  + \\text{E}[Y_2])= \\frac{2\\theta}{2} = \\theta \\\\\n\\text{E}\\left[\\widehat{\\theta}_3\\right] &= \\frac{1}{3}(\\text{E}[Y_1]  + 2\\text{E}[Y_2])= \\frac{3\\theta}{3} = \\theta \\\\\n\\text{E}\\left[\\widehat{\\theta}_4\\right] &= \\frac{1}{3}(\\text{E}[Y_1]  + \\text{E}[Y_2] + \\text{E}[Y_3])= \\frac{3\\theta}{3} = \\theta \\\\\n\\end{align*}\n\\]\nThe variances of these estimators are \\[\n\\begin{align*}\n\\text{Var}\\left[\\widehat{\\theta}_1\\right] &= \\text{Var}[Y_1] = \\theta^2 \\\\\n\\text{Var}\\left[\\widehat{\\theta}_2\\right] &= \\frac{1}{4}(\\text{Var}[Y_1]  + \\text{Var}[Y_2])= \\frac{2}{4}\\theta^2 = \\frac{1}{2}\\theta^2 \\\\\n\\text{Var}\\left[\\widehat{\\theta}_3\\right] &= \\frac{1}{9}(\\text{Var}[Y_1]  + 4\\text{Var}[Y_2])= \\frac{5}{9}\\theta^2 \\\\\n\\text{Var}\\left[\\widehat{\\theta}_4\\right] &= \\frac{1}{3}\\theta^2\n\\end{align*}\n\\] The sample mean \\(\\widehat{\\theta}_4\\) is the most precise (smallest variance) estimator among the first four.\nTo find the properties of \\(\\widehat{\\theta}_5 = \\min\\{Y_1,Y_2,Y_3\\}\\) a bit more work is required. This is called the first order statistic, the smallest value in the sample. It is denoted as \\(Y_{(1)}\\). The largest value, the \\(n\\)th order statistic, is denoted \\(Y_{(n)}\\). You can show that the first order statistic in a random sample of size \\(n\\) has density function \\[\nf_{(1)}(y) = n\\left(1-F(y)\\right)^{n-1}f(y)\n\\] where \\(f(y)\\) and \\(F(y)\\) are the density and cumulative distribution function of \\(Y\\). For the case of a sample of size \\(n\\) from an exponential distribution we get \\[\nf_{(1)}(y) = n\\left(1-F(y)\\right)^{n-1}\\,\\frac{1}{\\theta}e^{-y/\\theta}\n\\] Plugging in \\(F(y) = 1 - e^{-y/\\theta}\\) we get \\[\n\\begin{align*}\nf_{(1)}(y) &= n\\left(e^{-y/\\theta}\\right)^{n-1}\\,\\frac{1}{\\theta}e^{-y/\\theta} \\\\\n&= \\frac{n}{\\theta}e^{yn/\\theta}\n\\end{align*}\n\\] The first order statistic from an exponential distribution with parameter \\(\\theta\\) has an exponential distribution with parameter \\(\\theta/n\\).\nWe can now evaluate the mean and variance of \\(\\widehat{\\theta}_5\\):\n\\[\n\\begin{align*}\n\\text{E}\\left[\\widehat{\\theta}_5\\right] &= \\frac{\\theta}{3} \\\\\n\\text{Var}\\left[\\widehat{\\theta}_5\\right] &= \\frac{\\theta^2}{9}\n\\end{align*}\n\\] The bias of \\(\\widehat{\\theta}_5\\) for \\(\\theta\\) is \\[\n\\text{Bias}\\left[\\widehat{\\theta}_5; \\theta\\right] = \\frac{\\theta}{3} - \\theta = -\\frac{2}{3}\\theta\n\\] and the mean squared error is \\[\n\\begin{align*}\n\\text{MSE}\\left[\\widehat{\\theta}_5; \\theta\\right] &= \\text{Var}\\left[\\widehat{\\theta}_5\\right] + \\text{Bias}\\left[\\widehat{\\theta}_5; \\theta\\right]^2 \\\\\n&= \\frac{1}{9}\\theta^2 + \\frac{4}{9}\\theta^2 = \\frac{5}{9}\\theta^2\n\\end{align*}\n\\]\nIn this case the biased estimator \\(\\widehat{\\theta}_5\\) has the same mean squared error as the unbiased estimator \\(\\widehat{\\theta}_3\\) but has a smaller mean squared error as the sample mean \\(\\widehat{\\theta}_4\\).\n\n\n\n\nInterval Estimation\nA confidence interval (CI) combines information on a point estimator with information about its precision to yield a range of values associated with a certain probability. For example, a 95% confidence for the mean of a population or a 99% confidence interval for the difference between two population means, e.g., comparing a placebo and a medical treatment.\nThe probability associated with the confidence interval is called the confidence level. The complement of the confidence level is frequently denoted \\(\\alpha\\), a nod to the relationship between confidence intervals and significance levels in hypothesis testing. The confidence level is then expressed as \\((1-\\alpha)\\times 100\\%\\).\n\nInterpretation\nBefore going into the derivation of confidence intervals, a word about the proper interpretation of the interval. First, a confidence interval refers to a parameter or a function of parameters, not a statistic. Second, a confidence interval is associated with a probability (or percentage); typically a large value such as 90%, 95%, 99%. The result of estimating a two-sided confidence interval is a lower and an upper confidence bound, these are real numbers. For example, a 95% confidence interval for the population mean might be 5.3 to 12.0. A one-sided confidence interval has either an upper bound or a lower bound.\nThe correct interpretation of a confidence interval is in terms of repetition of the sampling process, as a long-run frequency. If we were to repeat the sampling process over and over, in 95% of those repetitions the 95% confidence interval would include (cover) the true parameter. In 5% of those repetitions the parameter falls outside (is not covered by) the confidence interval.\nIt is correct to interpret a 95% confidence interval as a random interval that contains the parameter with probability 0.95. The following interpretations of a confidence interval are not correct, although they are common:\n\nThe probability that the interval [5.3, 12.0] covers the parameter is 95%.\nThis interpretation is incorrect because it assigns the probability to the confidence interval calculated from the specific sample.\nIt does not mean that 95% of the data fall within the interval.\nThe confidence interval makes a statement about a parameter, it does not describe the distribution of the data.\n\n\n\n\nExample\nFigure 33.16 displays the result of computing one hundred 95% confidence intervals for the mean of a distribution (population). Since the figure was obtained from a simulation, the value of the unknown mean is known; the mean is \\(\\mu = 4\\).\nUnder conceptual repetition of the sampling we expect 95% of the confidence intervals to cover the mean. In this particular simulation, computing 100 confidence intervals, 94 of them cover the value \\(\\mu = 4\\) (shown in black), and 6 of the intervals do not cover \\(\\mu=4\\) (shown in red). This is close to the expected, long-run frequency of 95% of intervals covering the parameter.\n\n\n\n\n\n\n\n\nFigure 33.16: One hundred 95% confidence intervals for the mean of a population. The true value of the unknown parameter is 4. Six of the 100 intervals do not cover the parameter, close to the expected nominal coverage rate of 95%.\n\n\n\n\n\n\n\nConstructing confidence intervals\nConstructing a confidence interval relies on a statistic and its distribution. The sampling distributions in Section 33.3 play an important role. If we can start from a pivot statistic, constructing confidence interval estimators is pretty easy.\n\n\nDefinition: Pivot Statistic\n\n\nPivot statistic: a statistic that involves the parameter of question and whose distribution is known and is free of the parameter.\n\n\nSuppose we want to construct a confidence interval for the mean of a \\(G(\\mu,\\sigma^2)\\) distribution based on the random sample \\(Y_1,\\cdots,Y_n\\). From Section 33.3.4 we know that the statistic \\[\nT = \\frac{\\overline{Y}-\\mu}{S/\\sqrt{n}} \\sim t_{n-1}\n\\] has a \\(t\\) distribution with \\(n-1\\) degree of freedom. \\(T\\) is a proper pivot statistic for calculating a confidence interval for \\(\\mu\\). It depends on \\(\\mu\\) and its distribution does not involve \\(\\mu\\); it does involve \\(n\\), however, but that is a known quantity (since we know the size of our sample).\nThe construction of a 95% confidence interval starts with a probability statement: \\[\n\\Pr\\left(t_{0.025,n-1}\\leq T\\leq t_{0.975,n-1}\\right) = 0.95\n\\]\nThis simply states that the probability a \\(T\\) random variable with \\(n-1\\) degrees of freedom falls between the 2.5% and the 97.5% percentiles is 95%. There are other ways of carving out 95% of the area under the density. The following probability is also true, but it is customary to work with symmetric percentiles, in particular when the pivot distribution is symmetric.\n\\[\n\\Pr\\left(t_{0.04,n-1}\\leq T\\leq t_{0.99,n-1}\\right) = 0.95\n\\]\nThe next step is to substitute the formula for the statistic for \\(T\\) and to isolate the parameter of interest inside the probability statement.\n\\[\\begin{align*}\n\\Pr\\left(t_{0.025,n-1}\\leq T\\leq t_{0.975,n-1}\\right) &= 0.95 \\\\\n\\Pr\\left(t_{0.025,n-1}\\leq \\frac{\\overline{Y}-\\mu}{S/\\sqrt{n}}\\leq t_{0.975,n-1}\\right) &= 0.95\\\\\n\\Pr\\left(t_{0.025,n-1}\\frac{S}{\\sqrt{n}}\\leq \\overline{Y}-\\mu\\leq t_{0.975,n-1}\\frac{S}{\\sqrt{n}}\\right) &= 0.95\\\\\n\n\\Pr\\left(\\overline{Y}-t_{0.025,n-1}\\frac{S}{\\sqrt{n}}\\leq \\mu\\leq \\overline{Y}+t_{0.975,n-1}\\frac{S}{\\sqrt{n}}\\right) &= 0.95\\\\\n\\end{align*}\\]\nThe 95% confidence interval for the mean of a \\(G(\\mu,\\sigma^2)\\) distribution, based on a random sample \\(Y_1,\\cdots,Y_n\\), is \\[\n\\overline{Y} \\pm t_{0.975,n-1}\\times\\frac{S}{\\sqrt{n}}\n\\] More generally, the \\((1-\\alpha)\\times 100\\)% confidence interval is \\[\n\\overline{Y} \\pm t_{\\alpha/2,n-1}\\times\\frac{S}{\\sqrt{n}}\n\\]\n\n\nConfidence intervals based on the CLT\nThe asymptotic distribution of a statistic can be used to construct confidence intervals when the sample size is sufficiently large. The Central Limit Theorem (CLT) plays an important role, it provides the distribution of the pivot statistic.\nSuppose \\(Y_1,\\cdots, Y_n\\) is a sample from a Bernoulli(\\(\\pi\\)) distribution and \\(n\\) is large enough to invoke the CLT. The distribution of the sample mean \\(\\overline{Y} = 1/n \\sum_{i=1}^n Y_i\\) converges to a Gaussian distribution with mean \\(\\pi\\) and variance \\(\\pi(1-\\pi)/n\\). The sample mean is an unbiased estimate of the event probability \\(\\pi\\).\nIf \\(z_\\alpha\\) denotes the \\(\\alpha\\) quantile of the \\(G(0,1)\\) distribution we can state that asymptotically (as \\(n\\rightarrow\\infty\\)) $$ (z_{/2} z_{1-/2}) -\n$$\nThe problem with this interval is that \\(\\pi\\) appears in the numerator and the denominator of the pivot statistic. Proceeding as in the case of the \\(t\\)-interval leads to $$ ( - + ) -\n$$\nThe structure of the interval for \\(\\pi\\) is very similar to that of the \\(t\\)-interval, except it uses quantiles from the \\(G(0,1)\\) distribution instead of quantiles from the \\(t_{n-1}\\): \\[\n\\overline{Y} \\pm \\frac{z_{\\alpha/2}}{\\sqrt{n}}\\sqrt{\\pi(1-\\pi)}\n\\] The term \\(\\sqrt{\\pi(1-\\pi)}\\) is the standard deviation in the population. Since \\(\\pi\\) is unknown, we replace it with an estimate, which happens to be \\(\\widehat{\\pi} = \\overline{y}\\). The approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval for the event probability of a Bernoulli distribution is \\[\n\\overline{Y} \\pm \\frac{z_{1-\\alpha/2}}{\\sqrt{n}}\\sqrt{\\widehat{\\pi}(1-\\widehat{\\pi})}\n=\n\\overline{Y} \\pm \\frac{z_{1-\\alpha/2}}{\\sqrt{n}}\\sqrt{\\overline{y}(1-\\overline{y})}\n\\]\n\n\nConfidence interval for \\(\\sigma^2\\)\nTo construct a confidence interval for the variance of a Gaussian distribution, based on a random sample, we use the pivot statistic \\[\n\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1}\n\\]\nWhen the distribution of the pivot statistic is symmetric, as in the case of the \\(t\\)-interval or the \\(z\\)-interval for the population mean, it is reasonable to use symmetric quantiles in interval construction. \\(chi\\)-square distributions are not symmetric so there is some choice how to place \\((1-\\alpha)\\) probability between upper and lower bounds. The convention is again to use quantiles with the same tail areas, but that does not necessarily yield the shortest confidence intervals.\nIf we accept the convention, the confidence interval for \\(\\sigma^2\\) is \\[\n\\Pr\\left( \\chi^2_{\\alpha/2} \\leq \\frac{(n-1)S^2}{\\sigma^2} \\leq \\chi^2_{1-\\alpha/2}\\right) = 1-\\alpha\n\\]\nRearranging terms and isolating \\(\\sigma^2\\) in the center of the probability statement: \\[\n\\Pr\\left( \\frac{(n-1)S^2}{\\chi^2_{1-\\alpha/2}} \\leq \\sigma^2 \\leq \\frac{(n-1)S^2}{\\chi^2_{\\alpha/2}}\\right) = 1-\\alpha\n\\]\n\n\nConfidence versus prediction interval\nA confidence interval is an interval estimate for a parameter. The preceding sections established confidence intervals for \\(\\mu\\), \\(\\pi\\), and \\(\\sigma^2\\).\nCan we also produce an interval estimate if the target of the interval is a random variable rather than a constant?\nThe procedure of estimating a random variable is called prediction. Unfortunately, the terminology can be confusing; in the context of regression and classification models we use the term prediction whether the target is a parameter (predicting the mean response) or a random variable (predicting a future observation).\nTo illustrate the difference between inference about parameters and inference about random variables let’s express the sample observations in this form: \\[\n\\begin{align*}\nY_i &= \\mu + \\epsilon_i \\qquad i=1,\\cdots,n\\\\\n\\epsilon_i &\\sim \\textit{iid } (0,\\sigma^2)\n\\end{align*}\n\\]\nThis simply states that \\(Y_1,\\cdots,Y_n\\) are iid draws from a distribution with mean \\(\\text{E}[Y_i] = \\mu\\) and variance \\(\\text{Var}[Y_i] = \\sigma^2\\). The properties of the error terms \\(\\epsilon_i\\) translate into stochastic properties of the \\(Y_i\\).\nThe obvious point estimator of \\(\\mu\\) is \\(\\overline{Y}\\) with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\). What is the best predictor of the random variable \\[\n\\mu + \\epsilon_0\n\\]\nwhere \\(\\epsilon_0\\) is a new observation, not contained in the sample. A good predictor \\(\\widetilde{Y}\\) minimizes the prediction error. If \\(\\widetilde{Y}\\) is unbiased, this means minimizing the variance \\[\n\\text{Var}[\\widetilde{Y}-\\mu-\\epsilon_0] = \\text{Var}[\\widetilde{Y}-\\epsilon_0]\n=\\text{Var}[\\widetilde{Y}]+\\text{Var}[\\epsilon_0] - 2\\text{Cov}[\\widetilde{Y},\\epsilon_0]\n\\] Since \\(\\epsilon_0\\) is a new observation the covariance term \\(\\text{Cov}[\\widetilde{Y},\\epsilon_0]\\) is zero. The prediction variance reduces to \\(\\text{Var}[\\widetilde{Y}]+\\sigma^2\\). If we choose \\(\\widetilde{Y} = \\overline{Y}\\), this prediction variance is \\[\n\\frac{\\sigma^2}{n}+\\sigma^2\n\\] If the errors \\(\\epsilon_i\\) are Gaussian distributed, and $^2 is known, a \\((1-\\alpha)\\times 100\\%)\\) prediction interval for \\(\\mu+\\epsilon_0\\) is \\[\n\\overline{Y} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\sigma^2}{n}+\\sigma^2}\n\\]\nThis interval is wider than the corresponding confidence interval \\[\n\\overline{Y} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\sigma^2}{n}}\n\\] because of the extra term \\(\\sigma^2\\) under the square root. It measures the irreducible error in the observations, the inherent variability in the population.\nPrediction intervals for random variables are wider than confidence intervals for parameters because they accommodate variability beyond the constant parameters. This phenomenon comes into play in regression analysis. Predicting the mean function at some value \\(x_0\\) is more precise than predicting the value of a new, unobserved, observation at \\(x_0\\). Confidence intervals might be preferred over prediction intervals because the former are more narrow. The desired interpretation of predictions often calls for the wider prediction intervals, however.",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/statistics.html#hypothesis-testing",
    "href": "reviewmaterial/statistics.html#hypothesis-testing",
    "title": "33  Statistics",
    "section": "33.5 Hypothesis Testing",
    "text": "33.5 Hypothesis Testing\n\nThe Procedure\nA hypothesis is the formulation of a theory. We want to judge whether the theory holds water. What value is statistics in solving this problem? Statistics passes judgment on hypotheses by comparing the theory against what the data tell us. The yardstick is whether observing the data is likely or unlikely if the hypothesis (the theory) is true. If we observe something that practically should not have happened if a theory holds then either the observation is bad or the theory is wrong.\n\nIngredients\nThe ingredients of a statistical hypothesis test are\n\nA null hypothesis formulated in terms of one or more parameters.\nAn alternative hypothesis that is the complement of the null hypothesis.\nA test statistic for which the sampling distribution under the null hypothesis can be derived.\nA decision rule that decides on the fate of the null hypothesis based on the (null) distribution and the realized value of the test statistic.\n\n\n\nExample\nAn example will make the process tangible. Suppose we are wondering whether a drug cures more than 75% of the patients suffering from a disease. To answer the question we draw a random sample of patients and administer the drug. The theory we wish to test is that the drug is effective for more than 75% of patients. This theory is translated into a statement about a population parameter:\nIf \\(\\pi\\) denotes the probability that a randomly selected patient is cured after taking the drug, then the null hypothesis of interest is \\(H: \\pi = 0.75\\) and the alternative hypothesis is \\(H_a: \\pi &gt; 0.75\\).\nThis is an example of a one-sided alternative hypothesis. We are only interested whether the probability of cure exceeds 0.75, not if the drug has an efficacy significantly less than the 0.75 threshold. A two-sided alternative hypothesis would be \\(H_a: \\pi \\neq 0.75\\).\nBased on the random sample of patients we can compute the sample proportion of cured patients, \\(\\widehat{\\pi} = \\overline{Y}\\), where \\(Y=1\\) if a patient is cured and \\(Y=0\\) otherwise. The Central Limit Theorem tells us that if \\(n\\) is sufficiently large, the distribution of \\(\\widehat{\\pi}\\) converges to a \\(G(\\pi,\\pi(1-\\pi)/n)\\) distribution. If the null hypothesis is true, the sampling distribution of \\(\\widehat{\\pi}\\) is a \\(G(0.75,0.75\\times 0.25/n)\\).\nSuppose the sample proportion is \\(\\widehat{\\pi} = 0.9\\). The data point toward the alternative hypothesis, the proportion is larger than 0.75. But \\(\\widehat{\\pi}\\) is a statistic with a distribution. Even if \\(\\pi = 0.75\\) there is a chance to observe a result as extreme or more extreme than \\(\\widehat{\\pi} = 0.9\\). The question is how unlikely is that outcome if the null hypothesis is true.\nIf, for example, \\(\\Pr(\\widehat{\\pi} &gt; 0.9 | \\pi = 0.75) = 0.2\\), that is, the probability is 0.2 to observe a sample proportion greater than 0.9 if the true event probability is 0.75, we cannot dismiss the null hypothesis as a plausible explanation for what we have observed. If, on the other hand, \\(\\Pr(\\widehat{\\pi} &gt; 0.9 | \\pi = 0.75) = 0.002\\), then we should very surprised to observe a sample proportion of 0.9 or greater.\n\n\nDecision rule\nThe decision rule of the statistical hypothesis test can be formulated in two equivalent ways. Both are based on the distribution of the test statistic under the null hypothesis.\n\nSpecify a probability level \\(\\alpha\\) that is sufficiently small that should an event occur with this probability (or less) we discount the possibility that the event is consistent with the null hypothesis. We adopt the alternative hypothesis as a plausible explanation of the observed data instead. The level \\(\\alpha\\) is called the significance level of the statistical test.\nWe compute the probability under the null hypothesis to observe a result as least as extreme as the one we obtained. This probability is called the \\(p\\)-value of the statistical test. If the \\(p\\)-value is less than the significance threshold \\(\\alpha\\) the null hypothesis is rejected.\n\nComputing a probability under the null hypothesis means that we consider the distribution of the test statistic assuming that the null hypothesis is true. Both decision approaches reject the null hypothesis as an explanation of the data if the observed event is highly unlikely under that distribution.\nThe difference in the two approaches is whether we decide on the null hypothesis based on comparing the value of the statistic to the rejection region under the null hypothesis or based on the \\(p\\)-value. The rejection region is the area under the null distribution that covers \\(\\alpha\\) probability. If the alternative is one-sided, the rejection region is in one tail of the distribution. If the alternative is two-sided, the rejection region can fall in separate tails of the distribution. (Some tests are implicitly two-sided and have a single rejection region, for example, \\(F\\) tests).\nIf you reject a test at the significance level \\(\\alpha\\) then you reject the test for all \\(p\\)-values less than \\(\\alpha\\).\nFigure 33.17 shows the \\(p\\)-value for a one-sided test where the test statistic takes on larger value if the null hypothesis is not true. The \\(p\\)-value corresponds to the shaded area, the probability of observing values of the test statistic a least as extreme as what we got. If the test would be two-sided and the test statistic would be inconsistent with \\(H\\) for small values as well, a second region on the left side of the distribution would have to be considered.\n\n\n\n\n\n\nFigure 33.17: One-sided hypothesis test.\n\n\n\nFigure 33.18 shows the decision procedure when we work with the significance level \\(\\alpha\\). The shaded region in the tail of the null distribution is the rejection region for this one-sided test. The area under the density of the region is exactly \\(\\alpha\\). If the test statistic falls into this region, the null hypothesis is rejected.\n\n\n\n\n\n\nFigure 33.18: One-sided hypothesis test.\n\n\n\n\nReturning to the previous example of testing \\(H:\\pi = 0.75\\) against \\(H_a:\\pi &gt; 0.75\\), recall that by the CLT and under the null hypothesis \\(\\widehat{\\pi} \\sim G(0.75,0.75\\times 0.25/n)\\). The probability to observe a particular sample proportion (or a more extreme result) under the null hypothesis is a function of \\(n\\), since the variance of the sample statistic depends on \\(n\\).\nIf the null hypothesis is very wrong then it should be easy to reject \\(H\\) based on a small sample size; the test statistic will be far in the tail of the null distribution. If the null hypothesis is only a bit wrong, we can still reject \\(H\\) when \\(n\\) is sufficiently large, because we can control the variance of \\(\\widehat{\\pi}\\) through the sample size. In other words, we can affect the dispersion of the null distribution through \\(n\\) and create a situation where even tiny deviations from the value under the null hypothesis are significant.\nThese considerations lead to an important point: there is a difference between statistical significance and practical importance. Just because a result is statistically significant—that is, it is not consistent with a null hypothesis, does not imply that we care about the magnitude of the detected difference. Suppose you compare two fertilizer treatments and reject the hypothesis that they produce the same crop yield per acre (\\(H:\\mu_1 = \\mu_2\\)). The observed difference between the yields is 0.0025 bushels per acre. While statistically significant, it is not practically relevant and nobody would launch a marketing campaign to promote the fertilizer with the slightly higher yield. If, however, the statistical significant difference is 25 bushels/acre we feel much different about it.\n\n\nConclusions\nThe conclusion of a statistical test is whether the null hypothesis can be rejected or is failed to be rejected. We never know whether the alternative hypothesis is true. If we knew that ahead of time there would be no statistical problem.\nThe test is performed under the assumption that \\(H\\) is true and the best we can do is reject or not reject that hypothesis. Failing to reject the null hypothesis does not make it true. We only assumed that it was true for the purpose of calculating probabilities. You should avoid statements such as “We accept the null hypothesis”.\n\n\n\nDerivation of a Statistical Test\nThe derivation of a statistical test is similar to the derivation of a confidence interval. It starts with a pivot statistic for which the distribution is known if the null hypothesis is true. Depending on whether the alternative hypothesis is one- or two-sided, we then calculate the probability to observe the value of the test statistic—or a more extreme result—under \\(H\\). If that \\(p\\)-value is sufficiently small, the null hypothesis is rejected.\n\n\nExample: Testing the Mean of a \\(G(\\mu,\\sigma^2)\\)\n\n\nYou are drawing a random sample of size \\(n=15\\) from a \\(G(\\mu,\\sigma^2)\\) distribution to test \\(H: \\mu = \\mu_0 = 4\\) against \\(H_a: \\mu \\neq 4\\). The observed value of the sample mean is \\(\\overline{y} = 5.7\\) and the sample standard deviation is \\(s=3.1\\).\nTo construct a test we use as the pivot statistic \\[\nT = \\frac{\\overline{Y}-\\mu}{S/\\sqrt{n}}\n\\] If the null hypothesis is true, then \\[\nT_0 = \\frac{\\overline{Y}-\\mu_0}{S/\\sqrt{n}} \\sim t_{n-1}\n\\] The \\(p\\)-value of the test is \\(2\\Pr(T_0 &gt; |t_0|)\\). The absolute value and the doubling of the probability is done to compute probabilities in both tail areas of the null distribution.\nThe value of the test statistic under the null hypothesis is \\[\nt_0 = \\frac{5.7-4}{3.1/\\sqrt{15}}=2.123\n\\] The probability \\(2\\Pr(t_14 &gt; 2.123) = 0.052\\) is the \\(p\\)-value of the test.\nIf the significance level at which we declare that events are inconsistent (they should not have happened because they are too unlikely) is \\(\\alpha = 0.05\\), then we would fail to reject the null hypothesis because \\(p &gt; 0.05\\). If the signifiance level is \\(\\alpha = 0.1\\), then we would reject \\(H\\).\n\n\n\n\nConnection to Confidence Intervals\nThere is a straightforward connection between testing a hypothesis and estimating a confidence interval. Both are based on pivot statistics and quantiles of their distribution. If the observed value of the pivot—under the null hypothesis—is extreme, the null hypothesis is rejected. Similarly, the confidence interval specifies the values of the parameters that are likely supported by the data.\nIn other words, if you have a two-sided \\((1-\\alpha)\\times 100\\%\\) confidence interval for parameter \\(\\theta\\), then the hypothesis \\(H:\\theta = 0\\) is rejected at the \\(\\alpha\\) significance level for all values outside of the confidence bounds. Similarly for one-sided confidence limits and one-sided hypothesis test.\nStarting with a null hypothesis \\(H:\\theta = \\theta_0\\), the hypothesis is rejected at the \\(\\alpha\\) significance level if a corresponding \\((1-\\alpha)\\times 100\\%\\) confidence interval covers \\(\\theta_0\\).",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/statistics.html#categorical-data",
    "href": "reviewmaterial/statistics.html#categorical-data",
    "title": "33  Statistics",
    "section": "33.6 Categorical Data",
    "text": "33.6 Categorical Data",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/statistics.html#simple-and-multiple-linear-regression",
    "href": "reviewmaterial/statistics.html#simple-and-multiple-linear-regression",
    "title": "33  Statistics",
    "section": "33.7 Simple and Multiple Linear Regression",
    "text": "33.7 Simple and Multiple Linear Regression\n\nOrdinary Least Squares Estimation\n\n\nPrediction and Confidence Intervals\n\n\nModel Diagnostics",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/statistics.html#time-series-analysis",
    "href": "reviewmaterial/statistics.html#time-series-analysis",
    "title": "33  Statistics",
    "section": "33.8 Time Series Analysis",
    "text": "33.8 Time Series Analysis\n\n\n\nFigure 33.12: Sampling distribution of \\(\\overline{Y}\\) drawn from Bernoulli(0.7).\nFigure 33.16: One hundred 95% confidence intervals for the mean of a population. The true value of the unknown parameter is 4. Six of the 100 intervals do not cover the parameter, close to the expected nominal coverage rate of 95%.",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/linalg.html",
    "href": "reviewmaterial/linalg.html",
    "title": "34  Linear Algebra",
    "section": "",
    "text": "34.1 Basics\nCommand of linear algebra is essential in data science, models and estimators are often expressed in terms of tensors, matrices, and vectors. Using scalar-based arithmetic becomes tedious very quickly as models become more complex. For example, the simple linear regression model and a straight line through the intercept model can be written as\n\\[Y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i}\\]\n\\[Y_{i} = \\beta_{1}x_{i} + \\epsilon_{i}\\]\nUsing scalar algebra, the estimates of the slope are quite different:\n\\[{\\widehat{\\beta}}_{1} = \\frac{\\left( \\sum_{i = 1}^{n}{\\left( y_{i} - \\overline{y} \\right)\\left( x_{i} - \\overline{x} \\right)} \\right)}{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}\\]\n\\[{\\widehat{\\beta}}_{1} = \\frac{\\left( \\sum_{i = 1}^{n}{y_{i}x_{i}} \\right)}{\\sum_{i = 1}^{n}x_{i}^{2}}\\]\nThe formulas get messier as we add another input variable to the model. Using matrix—vector notation, the estimator of all the regression coefficients takes the same form, regardless of the size of the model:\n\\[\\widehat{\\boldsymbol{\\beta}} = \\left( \\textbf{X}^{\\prime}\\textbf{X}\\right)^{- 1}\\textbf{X}^{\\prime}\\textbf{Y}\\]\nA scalar is a single real number, a vector is an array of scalars arranged in a single column (a column vector) or a row (a row vector). A matrix is a two-dimensional array of scalars, a tensor is a multi-dimensional array.\nThe order of a vector or matrix is specified as (rows x columns) and is sometimes used as a subscript for clarity. For example,\\(\\textbf{A}_{(3 \\times 5)}\\) denotes a matrix with 3 rows and 5 columns. It can be viewed as a concatenation} of five \\((3 \\times 1)\\) column vectors:\n\\[\\textbf{A}_{(3 \\times 5)}=\\begin{bmatrix}\n\\begin{matrix}\n1 \\\\\n1 \\\\\n1\n\\end{matrix} & \\begin{matrix}\n9.0 \\\\\n3.2 \\\\\n4.1\n\\end{matrix} & \\begin{matrix}\n\\begin{matrix}\n6.2 \\\\\n1.4 \\\\\n- 0.6\n\\end{matrix} & \\begin{matrix}\n1 \\\\\n0 \\\\\n0\n\\end{matrix} & \\begin{matrix}\n0 \\\\\n1 \\\\\n0\n\\end{matrix}\n\\end{matrix}\n\\end{bmatrix}\\]\n\\(\\textbf{a}_{1} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\ \\ \\ \\ \\ \\textbf{a}_{2} = \\begin{bmatrix} 9.0 \\\\ 3.2 \\\\ 4.1 \\end{bmatrix}\\ \\ \\ \\ \\textbf{a}_{3} = \\begin{bmatrix} 6.2 \\\\ 1.4 \\\\ - 0.6 \\end{bmatrix}\\ \\ \\ \\ \\ \\textbf{a}_{4} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\\ \\ \\ \\ \\ \\textbf{a}_{5} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}\\).\nA matrix with as many rows as columns is called a square matrix.\nBold symbols are common, lowercase for vectors and uppercase for matrices, but there are some exceptions. When dealing with vectors of random variables, bold uppercase notation is used for a vector of random variables and bold lowercase notation is used for a vector of the realized values. For example, if \\(Y_{1},\\cdots,Y_{n}\\) is a random sample of size \\(n\\), the vector of random variables is\n\\[\\textbf{Y}_{(n \\times 1)} = \\begin{bmatrix}\nY_{1} \\\\\n\\vdots \\\\\nY_{n}\n\\end{bmatrix}\\]\nand the vector of realized values is\n\\[\\textbf{y}_{(n \\times 1)} = \\begin{bmatrix}\ny_{1} \\\\\n\\vdots \\\\\ny_{n}\n\\end{bmatrix}\\]\nThe difference is significant because \\(\\textbf{Y}\\) is a random variable and \\(\\textbf{y}\\) is a vector of constants. \\(\\textbf{Y}\\) has a multi-variate distribution with mean and variance, \\(\\textbf{y}\\) is just a vector of numbers.\nWe follow the convention that all vectors are column vectors, so that \\(\\textbf{y}_{(n)}\\) serves as a shorthand for \\(\\textbf{y}_{(n \\times 1)}\\).\nThe typical element of a matrix is written as a scalar with subscripts that refer to rows and columns. For example, the statement\n\\[\\textbf{A}= \\left\\lbrack a_{ij} \\right\\rbrack\\]\nsays that matrix \\(\\textbf{A}\\) consists of the scalars \\(a_{ij}\\); for example, \\(a_{23}\\) is the scalar in row 2, column 3 of the matrix.",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/linalg.html#special-matrices",
    "href": "reviewmaterial/linalg.html#special-matrices",
    "title": "34  Linear Algebra",
    "section": "34.2 Special Matrices",
    "text": "34.2 Special Matrices\nA few special matrices, common in statistics and machine learning are\n\n\\(\\textbf{1}_{n} = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\), the unit vector of size \\(n\\); all its elements are 1.\n\\(\\textbf{0}_{n} = \\begin{bmatrix} 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\), the zero vector of size \\(n\\); all its elements are 0.\n\\(\\textbf{0}_{(n \\times k)} = \\begin{bmatrix} 0 & 0 & 0 \\\\ \\vdots & \\cdots & \\vdots \\\\ 0 & 0 & 0 \\end{bmatrix}\\), the zero matrix of order \\((n \\times k)\\). All its elements are 0.\n\\(\\textbf{J}_{(n \\times k)} = \\begin{bmatrix} 1 & 1 & 1 \\\\ \\vdots & \\cdots & \\vdots \\\\ 1 & 1 & 1 \\end{bmatrix}\\), the unit matrix of size \\((n \\times k)\\). All its elements are 1.\n\\(\\textbf{I}_{n} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & \\ddots & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\), the identity matrix of size \\((n \\times n)\\) with 1s on the diagonal and 0s elsewhere.\n\nIf the order of these matrices is obvious from the context, the subscripts tend to be omitted.",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/linalg.html#basic-operations-on-matrices-and-vectors",
    "href": "reviewmaterial/linalg.html#basic-operations-on-matrices-and-vectors",
    "title": "34  Linear Algebra",
    "section": "34.3 Basic Operations on Matrices and Vectors",
    "text": "34.3 Basic Operations on Matrices and Vectors\nThe basic operations on matrices and vectors are addition, subtraction, multiplication, transposition, and inversion. These are standard operations in manipulating matrix and vector equations. Decompositions such as Cholesky roots, eigenvalue and singular value decompositions are more advanced operations that are important in solving estimation problems in statistics.\n\nTranspose\nThe transpose of a matrix is obtained by exchanging rows and columns. If \\(a_{ij}\\) is the element in row \\(i\\), column \\(j\\) of matrix \\(\\textbf{A}\\), the transpose of \\(\\textbf{A}\\), denoted \\(\\textbf{A}^\\prime\\), has typical element \\(a_{ji}\\). In case of the \\((3\\  \\times 5)\\) matrix shown previously, its transpose is\n\\[\\textbf{A}^\\prime_{(5 \\times 3)} = \\begin{bmatrix}\n\\begin{matrix}\n1 \\\\\n9.0 \\\\\n\\begin{matrix}\n6.2 \\\\\n1 \\\\\n0\n\\end{matrix}\n\\end{matrix} & \\begin{matrix}\n1 \\\\\n3.2 \\\\\n\\begin{matrix}\n1.4 \\\\\n0 \\\\\n1\n\\end{matrix}\n\\end{matrix} & \\begin{matrix}\n1 \\\\\n4.1 \\\\\n\\begin{matrix}\n- 0.6 \\\\\n0 \\\\\n0\n\\end{matrix}\n\\end{matrix}\n\\end{bmatrix}\\]\nThe transpose of a column vector is a row vector:\n\\[\\textbf{a}^{\\prime} = \\begin{bmatrix}\na_{1} \\\\\n\\vdots \\\\\na_{n}\n\\end{bmatrix}^\\prime = \\left\\lbrack a_{1},\\cdots,a_{n} \\right\\rbrack\\]\nTransposing a transpose produces the original matrix, \\(\\left( \\textbf{A}^{\\prime} \\right)^{\\prime}\\ = \\textbf{A}\\).\nA matrix is symmetric if it is equal to its transpose, \\(\\textbf{A}^\\prime = \\textbf{A}\\). Symmetric matrices are square matrices (same numbers of rows and columns). The matrices \\(\\textbf{A}^\\prime\\textbf{A}\\) and \\(\\textbf{A}\\textbf{A}^\\prime\\) are always symmetric. A symmetric matrix whose off-diagonal elements are zero is called a diagonal matrix.\n\n\nAddition and Subtraction\nThe sum (difference) of two matrices is the matrix of the elementwise sums (differences) of their elements. These operations require that the matrices being summed or subtracted have the same order:\n\\[\\textbf{A}_{(n \\times k)} + \\textbf{B}_{(n \\times k)} = \\left\\lbrack a_{ij} + b_{ij} \\right\\rbrack\\]\n\\[\\textbf{A}_{(n \\times k)} - \\textbf{B}_{(n \\times k)} = \\left\\lbrack a_{ij} - b_{ij} \\right\\rbrack\\]\nSuppose, for example, that \\(\\textbf{A}= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\) and \\(\\textbf{B}=\\begin{bmatrix} - 1 & - 2 & - 3 \\\\ - 4 & - 5 & - 6 \\end{bmatrix}\\). Then,\n\\[\\textbf{A}+ \\textbf{B}= \\begin{bmatrix}\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\\]\n\\[\\textbf{A}- \\textbf{B}= \\begin{bmatrix}\n2 & 4 & 6 \\\\\n8 & 10 & 12\n\\end{bmatrix}\\]\nSince addition (subtraction) are elementwise operations, they can be combined with transposition:\n\\[\\left( \\textbf{A}+ \\textbf{B}\\right)^\\prime = \\textbf{A}^{\\prime} + \\textbf{B}^{\\prime}\\]\n\n\nMultiplication\nTwo matrices conform for addition (subtraction) if they have the same order, that is, the same number of rows and columns. Multiplication of matrices requires a different type of conformity; two matrices \\(\\textbf{A}\\) and \\(\\textbf{B}\\) can be multiplied as \\(\\text{AB}\\) (or \\(\\textbf{A}\\text{×}\\textbf{B}\\)), if the number of columns in \\(\\textbf{A}\\) equals the number of columns in \\(\\textbf{B}\\). We say that in the product \\(\\textbf{A}\\text{×}\\textbf{B}\\), \\(\\textbf{A}\\) is post-multiplied by \\(\\textbf{B}\\) or that \\(\\textbf{A}\\) is multiplied into \\(\\textbf{B}\\). The result of multiplying a \\((n \\times k)\\) matrix into a \\((k \\times p)\\) matrix is a \\((n \\times p)\\) matrix.\nBefore examining the typical elements in the result of multiplication, let’s look at a special case, the inner product of two \\((k \\times 1)\\) vectors \\(\\textbf{A}\\) and \\(\\textbf{B}\\), also called the dot product or the scalar product, is the result of multiplying the transpose of \\(\\textbf{A}\\) into \\(\\textbf{B}\\), a scalar value\n\\[\n\\textbf{A}^\\prime\\textbf{B}= \\left\\lbrack a_{1}, \\cdots,a_{k} \\right\\rbrack\n\\begin{bmatrix}\nb_{1} \\\\\n\\vdots \\\\\nb_{k}\n\\end{bmatrix}\n= a_{1}b_{1} + \\cdots a_{k}b_{k} = \\sum_{i = 1}^{k}{a_{i}b_{i}}\n\\]\nThe square root of the dot product of a vector with itself is the Euclidean \\({(L}_{2})\\) norm of the vector,\n\\[\n\\left| \\left| \\textbf{a}\\right| \\right| = \\sqrt{\\textbf{a}^\\prime\\textbf{a}} = \\sqrt{\\sum_{i = 1}^{k}a_{i}^{2}}\n\\]\nThe \\(L_{2}\\) norm plays an important role as a loss function in statistical models. The vector for which the norm is calculated is then often a vector of model errors.\nNow let’s return to the problem of multiplying the \\((n \\times k)\\) matrix \\(\\textbf{A}\\) into the \\((k \\times p)\\) matrix \\(\\textbf{B}\\) and introduce one more piece of notation: the \\(i\\)th row of \\(\\textbf{A}\\) is denoted \\(\\mathbf{\\alpha}_{i}\\) and the \\(j\\)th column of \\(\\textbf{B}\\) is denoted \\(\\textbf{B}_{j}\\). Now we can finally write the product \\(\\textbf{A}\\text{×}\\textbf{B}\\) as a matrix whose typical element is the inner product of \\(\\mathbf{\\alpha}_{i}\\) and \\(\\textbf{B}_{j}\\):\n\\[\n\\textbf{A}_{(n \\times k)} \\times \\textbf{B}_{(k \\times p)} = \\left\\lbrack \\boldsymbol{\\alpha}_{i}^\\prime\\ \\textbf{b}_{j} \\right\\rbrack_{(n \\times p)}\n\\]\nAs an example, let \\(\\textbf{A}= \\begin{bmatrix} 1 & 2 & 0 \\\\ 3 & 1 & - 3 \\\\ 4 & 1 & 2 \\end{bmatrix}\\) and \\(\\textbf{B}= \\begin{bmatrix} 1 & 0 \\\\ 2 & 3 \\\\ 2 & 1 \\end{bmatrix}\\). The product \\(\\textbf{A}\\times\\textbf{B}\\) is a \\((3 \\times 2)\\) matrix with elements\n\\[\n\\textbf{A}\\times\\textbf{B}= \\begin{bmatrix}\n1 \\times 1 + 2 \\times 2 + 0 \\times 2 & 1 \\times 0 + 2 \\times 3 + 0 \\times 1 \\\\\n3 \\times 1 + 1 \\times 2 - 3 \\times 2 & 3 \\times 0 + 1 \\times 3 - 3 \\times 1 \\\\\n4 \\times 1 + 1 \\times 2 + 2 \\times 2 & 4 \\times 0 + 1 \\times 3 + 2 \\times 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n5 & 6 \\\\\n- 1 & 0 \\\\\n10 & 5\n\\end{bmatrix}\n\\]\nHere are a few helpful rules for matrix multiplication:\n\n\\(c\\textbf{A}= \\left\\lbrack ca_{ij} \\right\\rbrack\\)\n\\(c\\left( \\textbf{A}+ \\textbf{B}\\right) = c\\textbf{A}+ c\\textbf{B}\\)\n\\(\\textbf{C}\\left( \\textbf{A}+ \\textbf{B}\\right) = \\textbf{C}\\textbf{A}+ \\textbf{C}\\textbf{B}\\)\n\\(\\left( \\textbf{A}\\textbf{B}\\right)\\textbf{C}= \\textbf{A}(\\textbf{B}\\textbf{C})\\)\n\\(\\left( \\textbf{A}+ \\textbf{B}\\right)\\left( \\textbf{C}+ \\mathbf{D} \\right) = \\textbf{A}\\textbf{C}+ \\textbf{A}\\mathbf{D} + \\mathbf{BC} + \\mathbf{BD}\\)\n\\(\\left( \\textbf{A}\\textbf{B}\\right)^\\prime = \\textbf{B}^\\prime\\textbf{A}^\\prime\\)\n\\(\\left( c\\textbf{A}\\right)^\\prime = c\\textbf{A}^\\prime\\)\n\n\n\nInversion and Rank\nIn scalar algebra, division and multiplication are inverse operations, dividing a non-zero scalar by itself yields the multiplicative identity: \\(\\frac{a}{a} = 1\\). What is the equivalent of this operation for matrices? First, inversion of a matrix does not reduce it to a scalar, the multiplicative identity for matrices is the identity matrix \\(\\textbf{I}\\), a diagonal matrix with 1s on the diagonal. Second, the inversion is only defined for square matrices. If \\(\\textbf{A}\\) is an \\((n \\times n)\\) matrix, the matrix \\(\\textbf{B}\\) for which\n\\[\n\\textbf{A}\\textbf{B}= \\textbf{I}\n\\]\nis called the inverse of \\(\\textbf{A}\\), denoted as \\(\\textbf{A}^{- 1}\\).\nInverse matrices do not have to exist, even for square matrices. If \\(\\textbf{A}\\) has an inverse matrix, then \\(\\textbf{A}\\) is called a non-singular matrix. In that case, \\(\\textbf{A}^{- 1}\\textbf{A}= \\textbf{A}\\textbf{A}^{- 1} = \\text{I}\\).\nFor the inverse of a square matrix to exist, for the matrix to be non-singular, the matrix must be of full rank. The rank of a matrix, denoted \\(r(\\textbf{A})\\), is the number of its linearly independent columns. What does that mean? Suppose we are dealing with a \\((n \\times k)\\) matrix \\(\\textbf{B}\\) and its column vectors are \\(\\textbf{B}_{1},\\cdots,\\textbf{B}_{k}\\). A linear combination of the columns of \\(\\textbf{B}\\) is\n\\[\nc_{1}\\textbf{b}_{1} + c_{2}\\textbf{b}_{2} + \\cdots + c_{k}\\textbf{b}_{k} = q\n\\]\nIf you can find a set of scalars \\(c_{1},\\cdots,c_{k}\\) such that \\(q = 0\\), then the columns of \\(\\textbf{B}\\) are linearly dependent. If the only set of scalars that yields \\(q = 0\\) is\n\\[c_{1} = c_{2} = \\cdots = c_{k} = 0\\]\nthen the columns of \\(\\textbf{B}\\) are not linearly dependent and the rank of \\(\\textbf{B}\\) is \\(k\\).\nHere are a few more useful results about the rank of a matrix:\n\n\\(r\\left( \\textbf{A}\\right) = r\\left( \\textbf{A}^\\prime \\right) = r\\left( \\textbf{A}^\\prime\\textbf{A}\\right) = r\\left( \\textbf{A}\\textbf{A}^{\\prime} \\right)\\)\n\\(r\\left( \\textbf{A}\\textbf{B}\\right) \\leq \\min\\left\\{ r\\left( \\textbf{A}\\right),r\\left( \\textbf{B}\\right) \\right\\}\\)\n\\(r\\left( \\textbf{A}+ \\textbf{B}\\right) \\leq r\\left( \\textbf{A}\\right) + r(\\textbf{B})\\)\n\nThe first two results are particularly important in statistical models. In models with linear structures, it is common to collect the \\(p\\) input variables in a linear model, including the intercept as a column of ones, into a matrix \\(\\textbf{X}_{(n\\  \\times p + 1)}\\):\n\\[\n\\textbf{X}_{(n\\  \\times p + 1)} =\n\\begin{bmatrix}\n1 & x_{11} & \\begin{matrix}\n\\cdots & x_{1p}\n\\end{matrix} \\\\\n\\vdots & \\vdots &\n\\begin{matrix}\n\\ddots & \\vdots\n\\end{matrix} \\\\\n1 & x_{n1} & \\begin{matrix}\n\\cdots & x_{np}\n\\end{matrix}\n\\end{bmatrix}\n\\]\nSuppose we want to solve the linear system \\(\\textbf{Y}= \\textbf{X}\\textbf{c}\\) for \\(\\textbf{c}\\). Start by pre-multiplying both sides of the equation with the transpose of \\(\\textbf{X}\\):\n\\[\n\\textbf{X}^{\\prime}\\textbf{Y}= \\textbf{X}^{\\prime}\\textbf{X}\\textbf{c}\n\\]\nIf we had an inverse of \\(\\textbf{X}^\\prime\\textbf{X}\\), then we can now pre-multiply both sides with that inverse and isolate \\(\\text{c}\\):\n\\[\n\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{\\mathbf{- 1}}\\textbf{X}^{\\prime}\\textbf{Y}= \\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\textbf{X}^\\prime\\textbf{X}\\textbf{c}= \\textbf{I}\\textbf{c}= \\textbf{c}\n\\]\nWe have a solution to the system, namely \\({\\textbf{c}=\\left( \\textbf{X}^\\prime\\textbf{X}\\right)}^{- 1}\\textbf{X}^{\\prime}\\textbf{Y}\\), only if the inverse \\(\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\) exists. And that requires this \\((p + 1) \\times (p + 1)\\) matrix is of full rank \\(r\\left( \\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1} \\right) = p + 1\\). This, in turn is equivalent to saying that \\(\\textbf{X}\\) has full rank \\(p + 1\\) because of property (i).\nHere are some useful results about inverse matrices:\n\n\\(\\left( \\textbf{A}^{- 1} \\right)^\\prime = \\left( \\textbf{A}^\\prime \\right)^{- 1}\\)\n\\(\\left( \\textbf{A}^{- 1} \\right)^{- 1} = \\textbf{A}\\)\n\\(\\left( \\textbf{A}\\textbf{B}\\right)^{- 1} = \\textbf{B}^{-1}\\textbf{A}^{-1}\\)\n\\(r\\left( \\textbf{A}^{- 1} \\right) = r(\\textbf{A})\\)\n\nIf the matrix \\(\\textbf{X}\\) is of less than full rank, it is called a rank-deficient matrix. Can we still solve the linear system \\(\\textbf{Y}= \\textbf{X}\\textbf{c}\\)? Not by using a (regular) inverse matrix, but there is a way out, by using a generalized inverse matrix. If a matrix \\(\\textbf{A}^{-}\\) can be found that satisfies\n\\[\n\\textbf{A}\\textbf{A}^{-}\\textbf{A}= \\textbf{A}\n\\]\nthen it is called the generalized inverse (or pseudo-inverse or g-inverse) of \\(\\textbf{A}\\). Suppose we can find such a generalized inverse \\(\\left( \\textbf{X}^{\\prime}\\textbf{X}\\right)^{-}\\)f or \\(\\textbf{X}^\\prime\\textbf{X}\\). What if we use that in the solution of the linear system,\n\\[\n\\textbf{c}= {\\left( \\textbf{X}^{\\prime}\\textbf{X}\\right)^{-}\\textbf{X}}^{\\prime}\\textbf{Y}\n\\]\nUnfortunately, whereas regular inverses are unique, there are (infinitely) many generalized inverses that satisfy the condition \\((\\textbf{X}^\\prime\\textbf{X})\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-}\\textbf{X}^\\prime\\textbf{X}= \\textbf{X}^\\prime\\textbf{X}\\). So, there will be infinitely many possible solutions to the linear system. Fortunately, it turns out that generalized inverses have some nice properties, for example, \\(\\textbf{X}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-}\\textbf{X}\\) is invariant to the choice of the generalized inverse. Even if the solution \\(\\textbf{c}\\) is not unique, \\(\\textbf{X}\\textbf{c}\\) is unique. This result is important in linear models with rank-deficient design matrices, a condition that is common when the model contains classification variables. While the parameter estimates in such a model are not unique, because we need to use a generalized inverse to derive the estimates, the predicted values are the same, no matter which generalized inverse we choose.\n\n\nDeterminant\nThe rank reduces a matrix to a single scalar value, the number of linearly independent columns of the matrix. Another value that reduces a square matrix to a single scalar is the determinant, written as \\(det(\\textbf{A})\\) or \\(|\\textbf{A}|\\). The determinant has a geometric interpretation which is not that relevant for our discussion. What matters more is that the determinant appears frequently in expressions of multivariate probability distributions and knowing how to manipulate the determinants.\n\n\\(|\\textbf{A}| = |\\textbf{A}^\\prime|\\)\n\\(|\\textbf{I}| = 1\\)\n\\(\\left| c\\textbf{A}\\right| = c^{n}\\mathbf{|A}\\mathbf{|}\\)\nIf \\(\\textbf{A}\\) is singular, then \\(\\left| \\textbf{A}\\right| = 0\\)\nIf each element of a row (column) of \\(\\textbf{A}\\) is zero, then \\(\\left| \\textbf{A}\\right| = 0\\)\nIf two rows (column) of \\(\\textbf{A}\\) are identical, then \\(\\left| \\textbf{A}\\right| = 0\\)\n\\(\\left| \\textbf{A}\\textbf{B}\\right| = \\left| \\textbf{A}\\right|\\ \\left| \\textbf{B}\\right|\\)\n\\(\\left| \\textbf{A}^{- 1} \\right| = 1/|\\textbf{A}|\\)\nIf \\(\\textbf{A}\\) is a triangular matrix, then \\(|\\textbf{A}| = \\prod_{i = 1}^{n}a_{ii}\\)\nIf \\(\\textbf{A}\\) is a diagonal matrix, then \\(|\\textbf{A}| = \\prod_{i = 1}^{n}a_{ii}\\)\n\n\n\nTrace\nThe trace operator, \\(tr(\\textbf{A})\\), applies only to square matrices. The trace of an \\(\\textbf{A}_{(n \\times n)}\\) matrix is the sum of its diagonal elements:\n\\[\ntr\\left( \\textbf{A}\\right) = \\sum_{i = 1}^{n}a_{ii}\n\\]\nThe trace plays an important role in statistics in determining expected values of quadratic forms of random variables, for example, sums of squares in linear models. An important property of the trace is its invariance under cyclic permutations,\n\\[\ntr\\left( \\mathbf{ABC} \\right) = tr\\left( \\mathbf{BCA} \\right) = tr(\\mathbf{CAB})\n\\]\nprovided the matrices conform to multiplication.\nSome other useful properties of the trace are\n\n\\(tr\\left( \\textbf{A}+ \\textbf{B}\\right) = tr\\left( \\textbf{A}\\right) + tr\\left( \\textbf{B}\\right)\\)\n\\(tr\\left( \\textbf{A}\\right) = tr\\left( \\textbf{A}^\\prime \\right)\\)\n\\(\\textbf{Y}^\\prime\\text{Ay} = tr\\left( \\textbf{Y}^\\prime\\text{Ay} \\right)\\)\n\\(tr\\left( c\\textbf{A}\\right) = c \\times tr\\left( \\textbf{A}\\right)\\)\n\\(tr\\left( \\textbf{A}\\right) = r(\\textbf{A})\\) if \\(\\textbf{A}\\) is symmetric and idempotent (\\(\\textbf{A}\\textbf{A}= \\textbf{A}\\) and \\(\\textbf{A}= \\textbf{A}^\\prime\\))",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/linalg.html#random-vectors",
    "href": "reviewmaterial/linalg.html#random-vectors",
    "title": "34  Linear Algebra",
    "section": "34.4 Random Vectors",
    "text": "34.4 Random Vectors\nIf the elements of a vector are random variables, the vector object itself is a random variable. You can think of random vectors as a convenient mechanism to collect random variables. Suppose we draw a random sample \\(Y_{1},\\cdots,Y_{n}\\), then we can collect the \\(n\\) random variables in a single random vector\n$$ =\n\\[\\begin{bmatrix}\nY_{1} \\\\\n\\vdots \\\\\nY_{n}\n\\end{bmatrix}\\]\n$$\n\nExpected Value\nSince each \\(Y_{i}\\) has a probability distribution, a mean (expected value) \\(\\text{E}\\left\\lbrack Y_{i} \\right\\rbrack\\), a variance \\(\\text{Var}\\left\\lbrack Y_{i} \\right\\rbrack\\), and so forth, the same applies to their collection. The expected value (mean) of a random vector is the vector of the expected values of its elements:\n\\[\n\\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack = \\begin{bmatrix}\n\\text{E}\\left\\lbrack Y_{1} \\right\\rbrack \\\\\n\\vdots \\\\\n\\text{E}\\left\\lbrack Y_{n} \\right\\rbrack\n\\end{bmatrix}\n\\]\nSuppose that \\(\\textbf{A},\\ \\textbf{B},\\ \\textbf{c}\\) are matrices and vectors of constants, respectively, and that \\(\\textbf{Y}\\) and \\(\\mathbf{U}\\) are random vectors. The following are useful expectation operations in this situations:\n\n\\(\\text{E}\\left\\lbrack \\textbf{A}\\right\\rbrack = \\textbf{A}\\)\n\\(\\text{E}\\left\\lbrack \\mathbf{AYB} + \\mathbf{C} \\right\\rbrack = \\textbf{A}\\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack\\textbf{B}+ \\textbf{C}\\)\n\\(\\text{E}\\left\\lbrack \\mathbf{AY} + \\mathbf{c} \\right\\rbrack = \\textbf{A}\\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack + \\textbf{c}\\)\n\\(\\text{E}\\left\\lbrack \\mathbf{AY} + \\mathbf{BU} \\right\\rbrack = \\textbf{A}\\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack + \\textbf{B}\\ \\text{E}\\lbrack\\mathbf{U}\\rbrack\\)\n\n\n\nCovariance Matrix\nWhile the distribution of \\(Y_{i}\\) is univariate, \\(\\textbf{Y}\\) has a multivariate (\\(n\\)-variate) distribution. The mean of the distribution is represented by a vector. The variance of the distribution is represented by a matrix, the variance-covariance matrix, a special case of a covariance matrix.\nThe covariance matrix between random vectors \\(\\textbf{Y}_{(k \\times 1)}\\) and \\(\\mathbf{U}_{(p \\times 1)}\\) is a \\((k \\times p)\\) matrix whose typical elements are the covariances between the elements of \\(\\textbf{Y}\\) and \\(\\mathbf{U}\\):\n\\[\n\\text{Cov}\\left\\lbrack \\textbf{Y},\\mathbf{U} \\right\\rbrack = \\left\\lbrack \\text{Cov}(Y_{i},U_{j}) \\right\\rbrack\n\\]\nThe covariance matrix can be written in terms of expected values of \\(\\textbf{Y}\\), \\(\\mathbf{U}\\), and \\(\\textbf{Y}\\mathbf{U}^\\prime\\)\n\\[\n\\text{Cov}\\left\\lbrack \\textbf{Y},\\mathbf{U} \\right\\rbrack = \\text{E}\\left\\lbrack \\left( \\textbf{Y}- \\text{E}\\lbrack\\textbf{Y}\\rbrack \\right)\\left( \\mathbf{U} - \\text{E}\\left\\lbrack \\mathbf{U} \\right\\rbrack \\right)^\\prime \\right\\rbrack = \\text{E}\\left\\lbrack \\textbf{Y}\\mathbf{U}^\\prime \\right\\rbrack - \\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack \\text{E}\\left\\lbrack \\mathbf{U} \\right\\rbrack^{\\prime}\n\\]\nSome useful rules to manipulate covariance matrices are:\n\n\\(\\text{Cov}\\left\\lbrack \\mathbf{AY},\\mathbf{U} \\right\\rbrack = \\textbf{A}\\text{Cov}\\lbrack\\textbf{Y},\\mathbf{U}\\rbrack\\)\n\\(\\text{Cov}\\left\\lbrack \\textbf{Y},\\mathbf{BU} \\right\\rbrack = \\text{Cov}\\left\\lbrack \\textbf{Y},\\mathbf{U} \\right\\rbrack\\textbf{B}^\\prime\\)\n\\(\\text{Cov}\\left\\lbrack \\mathbf{AY},\\mathbf{BU} \\right\\rbrack = \\textbf{A}\\text{Cov}\\left\\lbrack \\textbf{Y},\\mathbf{U} \\right\\rbrack\\ \\textbf{B}^\\prime\\)\n\\(\\text{Cov}\\left\\lbrack a\\textbf{Y}+ b\\mathbf{U},c\\mathbf{W} + d\\textbf{V}\\right\\rbrack = ac\\text{Cov}\\left\\lbrack \\textbf{Y},\\mathbf{W} \\right\\rbrack + bc\\text{Cov}\\left\\lbrack \\mathbf{U},\\mathbf{W} \\right\\rbrack + ad\\text{Cov}\\left\\lbrack \\textbf{Y},\\textbf{V}\\right\\rbrack + bd\\text{Cov}\\lbrack\\mathbf{U},\\textbf{V}\\rbrack\\)\n\n\n\nVariance-covariance Matrix\nThe variance-covariance matrix (or variance matrix for short) of a random vector \\(\\textbf{Y}\\) is the covariance matrix of \\(\\textbf{Y}\\) with itself.\n\\[\n\\text{Var}\\left\\lbrack \\textbf{Y}\\right\\rbrack = \\text{Cov}\\left\\lbrack \\textbf{Y},\\textbf{Y}\\right\\rbrack = \\text{E}\\left\\lbrack \\left( \\textbf{Y}- \\text{E}\\lbrack\\textbf{Y}\\rbrack \\right)\\left( \\textbf{Y}-\\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack \\right)^\\prime \\right\\rbrack = \\text{E}\\left\\lbrack \\textbf{Y}\\textbf{Y}^\\prime \\right\\rbrack - \\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack \\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack^{\\prime}\n\\]\nThe diagonal entries of the variance-covariance matrix contain the variances of the \\(Y_{i}\\). The off-diagonal cells contain the covariances \\(\\text{Cov}\\left\\lbrack Y_{i},Y_{j} \\right\\rbrack\\). If the variance matrix is diagonal, the elements of random vector \\(\\textbf{Y}\\) are uncorrelated. Two random vectors \\(\\textbf{Y}\\) and \\(\\mathbf{U}\\) are uncorrelated if their variance matrix is block-diagonal:\n\\[\n\\text{Var}\\begin{bmatrix}\n\\textbf{Y}_{1} \\\\\n\\textbf{Y}_{2}\n\\end{bmatrix} = \\begin{bmatrix}\n\\text{Var}\\lbrack\\textbf{Y}_{2}\\rbrack & \\textbf{0}\\\\\n\\textbf{0}& \\text{Var}\\lbrack\\textbf{Y}_{1}\\rbrack\n\\end{bmatrix}\n\\]\nA very special variance-covariance matrix in statistical models is the scaled identity matrix, \\(\\sigma^{2}\\textbf{I}\\). This is the variance matrix of uncorrelated observations drawn from the same distribution—a common assumption for the error terms in models.\nThe rules for working with covariances extend to working with variance matrices:\n\n\\(\\text{Var}\\left\\lbrack \\mathbf{AY} \\right\\rbrack = \\textbf{A}\\ \\text{Var}\\left\\lbrack \\textbf{Y}\\right\\rbrack\\textbf{A}^\\prime\\)\n\\(\\text{Var}\\left\\lbrack \\textbf{Y}+ \\textbf{A}\\right\\rbrack = \\text{Var}\\lbrack\\textbf{Y}\\rbrack\\)\n\\(\\text{Var}\\left\\lbrack \\textbf{A}^\\prime\\textbf{Y}\\right\\rbrack = \\textbf{A}^\\prime\\text{Var}\\left\\lbrack \\textbf{Y}\\right\\rbrack\\textbf{A}\\)\n\\(\\text{Var}\\left\\lbrack a\\textbf{Y}\\right\\rbrack = a^{2}\\text{Var}\\lbrack\\textbf{Y}\\rbrack\\)\n\\(\\text{Var}\\left\\lbrack a\\textbf{Y}+ b\\mathbf{U} \\right\\rbrack = a^{2}\\text{Var}\\left\\lbrack \\textbf{Y}\\right\\rbrack + b^{2}\\text{Var}\\left\\lbrack \\mathbf{U} \\right\\rbrack + 2ab\\ \\text{Cov}\\lbrack\\textbf{Y},\\mathbf{U}\\rbrack\\)\n\nFinally, an important result about expected values of quadratic forms, heavily used to in decomposing variability is\n\\[\n\\text{E}\\left\\lbrack \\textbf{Y}^\\prime\\mathbf{AY} \\right\\rbrack = tr\\left( \\textbf{A}\\text{Var}\\left\\lbrack \\textbf{Y}\\right\\rbrack \\right) + \\text{E}\\left\\lbrack \\textbf{Y}\\right\\rbrack^\\prime\\textbf{A}\\ \\text{E}\\lbrack\\textbf{Y}\\rbrack\n\\]",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/linalg.html#sec-matrix-differentiation",
    "href": "reviewmaterial/linalg.html#sec-matrix-differentiation",
    "title": "34  Linear Algebra",
    "section": "34.5 Matrix Differentiation",
    "text": "34.5 Matrix Differentiation\nEstimation of parameters in statistical models often requires minimization or maximization of an objective function. For example, the ordinary least squares (OLS) principle finds the OLS estimator as the function of the data that minimizes the error sum of squares of the model. Maximum likelihood finds estimators of the parameters as the functions of the data that maximizes the joint likelihood (the joint distribution function) of the data.\nThe parameters of the models appear as elements of vectors and matrices. Finding estimators of the parameters thus requires calculus on vectors and matrices. Consider matrix \\(\\textbf{A}\\), whose elements depend on a scalar parameter \\(\\theta\\), \\(\\textbf{A}= \\left\\lbrack a_{ij}(\\theta) \\right\\rbrack\\). The derivative of \\(\\textbf{A}\\) with respect to \\(\\theta\\) is the matrix of the derivatives of the typical elements \\(a_{ij}(\\theta)\\) with respect to \\(\\theta\\). We write this formally as\n\\[\n\\frac{\\partial\\textbf{A}}{\\partial\\theta} = \\left\\lbrack \\frac{\\partial a_{ij}(\\theta)}{\\partial\\theta} \\right\\rbrack\n\\]\nThe derivative of a function \\(f(\\boldsymbol{\\theta})\\) with respect to the vector \\(\\boldsymbol{\\theta}_{(p \\times 1)}\\) is the vector of the partial derivatives of the function\n\\[\n\\frac{\\partial f\\left( \\boldsymbol{\\theta}\\right)}{\\partial\\boldsymbol{\\theta}} =\n\\begin{bmatrix}\n\\frac{\\partial f\\left( \\boldsymbol{\\theta}\\right)}{\\partial\\theta_{1}} \\\\\n\\vdots \\\\\n\\frac{\\partial f\\left( \\boldsymbol{\\theta}\\right)}{\\partial\\theta_{p}}\n\\end{bmatrix}\n\\]\nHere are some useful results from vector and matrix calculus where \\(\\textbf{A}\\) and \\(\\textbf{B}\\) are functions of \\(\\theta\\) and vector \\(\\textbf{X}\\) does not depend on \\(\\theta\\):\n\n\\(\\frac{{\\partial ln}\\left| \\textbf{A}\\right|}{\\partial\\theta} = \\frac{1}{\\left| \\textbf{A}\\right|}\\frac{\\partial\\left| \\textbf{A}\\right|}{\\partial\\theta} = tr\\left( \\textbf{A}^{- 1}\\frac{\\partial\\textbf{A}}{\\partial\\theta} \\right)\\)\n\\(\\frac{\\partial\\textbf{A}^{- 1}}{\\partial\\theta} = - \\textbf{A}^{- 1}\\frac{\\partial\\textbf{A}}{\\partial\\theta\\ }\\textbf{A}^{- 1}\\)\n\\(\\frac{\\partial tr\\left( \\mathbf{AB} \\right)}{\\partial\\theta} = tr\\left( \\frac{\\mathbf{\\partial}\\textbf{A}}{\\partial\\theta}\\textbf{B}\\right) + tr\\left( \\textbf{A}\\frac{\\mathbf{\\partial}\\textbf{B}}{\\partial\\theta} \\right)\\)\n\\(\\frac{\\partial\\textbf{X}^\\prime\\textbf{A}^{- 1}\\textbf{X}}{\\partial\\theta} = - \\textbf{X}^\\prime\\textbf{A}^{- 1}\\frac{\\partial\\textbf{A}}{\\partial\\theta}\\textbf{A}^{- 1}\\textbf{X}\\)\n\\(\\frac{\\partial\\textbf{X}^{\\prime}\\mathbf{Ax}}{\\partial\\textbf{X}} = 2\\mathbf{Ax}\\)\n\\(\\frac{\\partial\\textbf{X}^\\prime\\textbf{A}}{\\partial\\textbf{X}} = \\frac{\\partial\\textbf{A}^\\prime\\textbf{X}}{\\partial\\textbf{X}} = \\textbf{A}\\)",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/linalg.html#idempotent-matrices",
    "href": "reviewmaterial/linalg.html#idempotent-matrices",
    "title": "34  Linear Algebra",
    "section": "34.6 Idempotent Matrices",
    "text": "34.6 Idempotent Matrices\nThe class of (symmetric) idempotent matrices play an important role in statistical estimation. Idempotent matrices are projection matrices, that means they map a vector from a space to a sub-space.\n\nProjections\nFor example, suppose we want to find a solution for \\(\\boldsymbol{\\beta}\\) in the linear model \\(\\textbf{Y}_{(n \\times 1)} = \\textbf{X}\\boldsymbol{\\beta}_{(p + 1 \\times 1)} + \\mathbf{\\epsilon}\\). The vector \\(\\textbf{Y}\\) is a vector in \\(n\\)-dimensional space \\(\\mathbb{R}^{n}\\) and the model places a restriction on the predicted values \\(\\widehat{\\textbf{Y}} = \\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\): the predicted values are confined to a \\((p + 1)\\)-dimensional sub-space of \\(\\mathbb{R}^{n}\\). Regardless of how we choose the estimator \\(\\widehat{\\boldsymbol{\\beta}}\\), we are dealing with projecting vector \\(\\textbf{Y}\\) onto a sub-space of \\(\\mathbb{R}^{n}\\).\nWe can thus think of the problem of finding the best estimator in this model as the problem of finding the best projection onto the space generated by the columns of \\(\\textbf{X}\\). In that case, why not choose the projection that minimizes the distance between the observed values \\(\\textbf{Y}\\) and the predicted values \\(\\widehat{\\textbf{Y}}\\). This is achieved by projecting \\(\\textbf{Y}\\) perpendicular (orthogonal) onto the sub-space generated by \\(\\textbf{X}\\). In other words, our solution is the vector \\(\\widehat{\\boldsymbol{\\beta}}\\) that satisfies\n\\[\n\\left( \\textbf{Y}- \\textbf{X}\\widehat{\\boldsymbol{\\beta}} \\right)^\\prime\\textbf{X}\\widehat{\\boldsymbol{\\beta}} = 0\n\\]\nMultiplying out and rearranging terms yields\n\\[\n\\widehat{\\boldsymbol{\\beta}}^{\\prime}\\textbf{X}^{\\prime}\\textbf{Y}= \\widehat{\\boldsymbol{\\beta}}^{\\prime}\\textbf{X}^\\prime\\textbf{X}\\widehat{\\boldsymbol{\\beta}}\n\\]\nwhich implies that\n\\[\n\\textbf{X}^{\\prime}\\textbf{Y}= \\textbf{X}^\\prime\\textbf{X}\\widehat{\\boldsymbol{\\beta}}\n\\]\nIf \\(\\textbf{X}\\) is of full column rank, then \\(\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}\\) exists and we can solve:\n\\[\n\\widehat{\\boldsymbol{\\beta}}=\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}\\textbf{X}^{\\prime}\\textbf{Y}\n\\]\n\n\nThe “Hat” Matrix\nThe ordinary least squares estimator is the orthogonal projection of \\(\\textbf{Y}\\) onto the sub-space created by the columns of \\(\\textbf{X}\\). To see the projection matrix at work, compute the predicted values, \\(\\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\):\n\\[\n\\textbf{X}\\widehat{\\boldsymbol{\\beta}}=\\textbf{X}\\left( \\textbf{X}^{\\prime}\\textbf{X}\\right)^{- 1}\\textbf{X}^{\\prime}\\textbf{Y}= \\widehat{\\textbf{Y}}\n\\]\nThe matrix \\(\\textbf{X}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}\\textbf{X}^{\\prime}\\) has a very special role in regression analysis, it is often called the “hat” matrix and denoted \\(\\textbf{H}\\), because pre-multiplying \\(\\textbf{Y}\\) with \\(\\textbf{H}\\) puts the hats on \\(\\textbf{Y}\\):\n\\[\\textbf{H}\\textbf{Y}= \\widehat{\\textbf{Y}}\\]\nLet’s verify that \\(\\textbf{H}\\) is indeed a projection matrix, which requires that \\(\\textbf{H}\\textbf{H}= \\textbf{H}\\):\n\\[\n\\textbf{H}\\textbf{H}= \\textbf{X}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}\\textbf{X}^{\\prime} \\times \\textbf{X}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\textbf{X}^\\prime=\\textbf{X}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}\\textbf{X}^{\\prime} = \\textbf{H}\n\\]\nMatrices with the property that \\(\\textbf{H}\\textbf{H}= \\textbf{H}\\) are called idempotent matrices, these are projection matrices. If, in addition, \\(\\textbf{H}\\) is symmetric, \\(\\textbf{H}^\\prime = \\textbf{H}\\), the matrix is called symmetric idempotent—these are orthogonal projection matrices. (An idempotent matrix that is not symmetric is called an oblique projector.)\nThe hat matrix in the regression model is a symmetric idempotent matrix.\nHere are some results about (symmetric) idempotent matrices that come in handy when working out the properties of estimators in regression-type models:\n\nProjection matrices are typically not of full rank. If an \\((n \\times n)\\) idempotent matrix is of rank \\(n\\), then it is the identity matrix \\(\\textbf{I}\\).\nIf \\(\\textbf{A}\\) is (symmetric) idempotent, then \\(\\textbf{I}- \\textbf{A}\\) is (symmetric) idempotent.\nIf \\(\\mathbf{P}\\) is non-singular, then \\(\\mathbf{PA}\\mathbf{P}^{-1}\\) is an idempotent matrix.\n\nYou can use these properties to show that in the linear regression model with uncorrelated errors and equal variance the variance matrix of the model residuals \\(\\textbf{Y}- \\widehat{\\textbf{Y}}\\) is\n\\[\n\\text{Var}\\left\\lbrack \\textbf{Y}- \\widehat{\\textbf{Y}} \\right\\rbrack = \\sigma^{2}(\\textbf{I}- \\textbf{H})\n\\]\n\n\nA Special Case\nConsider the special case where \\(\\textbf{X}= \\textbf{1}_{n}\\), a column vector of ones. The corresponding linear model is \\(\\textbf{Y}= \\textbf{1}\\beta + \\boldsymbol{\\epsilon}\\), an intercept-only model. The hat matrix for this model is\n\\[\n\\textbf{1}\\left( \\textbf{1}^{\\prime}\\textbf{1}\\right)^{- 1}\\textbf{1}^\\prime = \\frac{1}{n}\\textbf{1}\\textbf{1}^\\prime = \\frac{1}{n}\\textbf{J}= \\begin{bmatrix}\n\\frac{1}{n} & \\cdots & \\frac{1}{n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{1}{n} & \\cdots & \\frac{1}{n}\n\\end{bmatrix}\n\\] a matrix filled with \\(\\frac{1}{n}\\). The projection of \\(\\textbf{Y}\\) onto the space generated by \\(\\textbf{1}_{n}\\) is\n\\[\n\\frac{1}{n}\\mathbf{JY} = \\begin{bmatrix}\n\\overline{Y} \\\\\n\\vdots \\\\\n\\overline{Y}\n\\end{bmatrix}\n\\]\nThe predicted values are all the same, the sample mean. In other words, \\(\\beta = \\overline{Y}\\). Since the projector is idempotent, deriving the variance of the predicted value in the iid case is simple:\n\\[\n\\text{Var}\\left\\lbrack \\textbf{H}\\textbf{Y}\\right\\rbrack = \\textbf{H}\\text{Var}\\left\\lbrack \\textbf{Y}\\right\\rbrack\\textbf{H}^\\prime = \\sigma^{2}\\textbf{H}\\textbf{H}^{\\prime} = \\sigma^{2}\\textbf{H}= \\sigma^{2}\\frac{1}{n}\\textbf{J}=\\begin{bmatrix}\n\\frac{\\sigma^{2}}{n} & \\cdots & \\frac{\\sigma^{2}}{n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\sigma^{2}}{n} & \\cdots & \\frac{\\sigma^{2}}{n}\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/linalg.html#sec-multi-gauss",
    "href": "reviewmaterial/linalg.html#sec-multi-gauss",
    "title": "34  Linear Algebra",
    "section": "34.7 Multivariate Gaussian Distribution",
    "text": "34.7 Multivariate Gaussian Distribution\n\nDefinition\nA scalar random variable \\(Y\\) has a Gaussian distribution function with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\) if its probability density function is given by\n\\[\nf(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left\\{ - \\frac{1}{2\\sigma^{2}}(y - \\mu)^{2} \\right\\}\n\\]\nWe also say that \\(Y\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\) but prefer the name Gaussian over Normal for the reasons given in Section 32.5. The shorthand expressions \\(Y \\sim G(\\mu,\\sigma^{2})\\) or \\(Y \\sim N(\\mu,\\sigma^{2})\\) are common.\nThe generalization from a scalar random variable \\(Y\\) to a random vector \\(\\textbf{Y}_{(n \\times 1)}\\) with a multivariate Gaussian distribution is as follows. \\(\\textbf{Y}_{(n \\times 1)}\\) has a multivariate Gaussian (normal) distribution with mean \\(\\boldsymbol{\\mu}\\) and variance matrix \\(\\textbf{V}\\), if its density is given by\n\\[\nf\\left( \\textbf{Y}\\right)=\\frac{\\left| \\textbf{V}\\right|^{- 1/2}}{(2\\pi)^{\\frac{n}{2}}}\\exp\\left\\{ - \\frac{1}{2}\\left( \\textbf{Y}- \\boldsymbol{\\mu}\\right)^\\prime\\textbf{V}^{- 1}\\left( \\textbf{Y}- \\boldsymbol{\\mu}\\right) \\right\\}\n\\]\nThis is denoted with the shorthand \\(\\textbf{Y}\\sim G_{n}(\\boldsymbol{\\mu},\\textbf{V})\\) or \\(\\textbf{Y}\\sim N_{n}(\\boldsymbol{\\mu},\\textbf{V}\\)\\). If the dimension of the distribution is clear from context, the subscript \\(n\\) can be omitted. A special case is the standard multivariate Gaussian distribution with mean \\(\\textbf{0}\\) and variance matrix \\(\\textbf{I}\\) :\n\\[\nf\\left( \\textbf{Y}\\right)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}}\\exp\\left\\{ - \\frac{1}{2}\\textbf{Y}^{\\prime}\\textbf{Y}\\right\\} = \\frac{1}{(2\\pi)^{\\frac{n}{2}}}\\exp\\left\\{ - \\frac{1}{2}\\sum_{i}^{n}y_{i}^{2} \\right\\}\n\\]\nBut this is just the product of the \\(n\\) univariate densities of \\(G(0,1)\\) random variables:\n\\[\nf\\left( \\textbf{Y}\\right) = f\\left( y_{1} \\right) \\times \\cdots \\times f\\left( y_{n} \\right)\n\\]\nwhere\n\\[\nf\\left( y_{i} \\right) = \\frac{1}{(2\\pi)^{1/2}}\\exp\\left\\{ - \\frac{1}{2}y^{2} \\right\\}\n\\]\nIf the variance matrix is diagonal—that is, the \\(Y_{i}\\) are uncorrelated—the multivariate normal distribution is the product of the univariate distributions. The random variables are independent.\n\n\nProperties\nGaussian distributions have amazing (magical) properties.\n\nLinear combinations are Gaussian\nFor example, a linear combination of Gaussian random variables also follows a Gaussian distribution. Formally, this can be expressed as follows: if \\(\\textbf{Y}\\sim G_{n}\\left( \\boldsymbol{\\mu},\\textbf{V}\\right)\\) and \\(\\textbf{A}\\) and \\(\\textbf{B}\\) are a matrix and vector of constants (not random variables), respectively, then \\(\\mathbf{AY} + \\textbf{B}\\) follows a \\(G(\\textbf{A}\\boldsymbol{\\mu},\\mathbf{AVA})\\) distribution.\nA special case of this result is that if \\(\\textbf{Y}\\sim G_{n}\\left( \\boldsymbol{\\mu},\\textbf{V}\\right)\\), \\(\\textbf{Y}- \\boldsymbol{\\mu}\\) has a \\(G\\left( 0,\\textbf{V}\\right)\\) distribution.\nBecause a linear function of a Gaussian random variable is Gaussian distributed, you can define all multivariate Gaussian distributions as linear transformations of the standard multivariate Gaussian distribution. If \\(\\textbf{Z}\\sim G_{n}(\\textbf{0},\\textbf{I})\\), and \\(\\textbf{V}= \\textbf{C}^\\prime\\textbf{C}\\), then \\(\\textbf{Y}= \\mathbf{C}^\\prime\\textbf{Z}+ \\boldsymbol{\\mu}\\) has a \\(G(\\boldsymbol{\\mu},\\textbf{V})\\) distribution.\n\n\nZero covariance implies independence\nAnother unusual property of Gaussian random variables is that if they are uncorrelated, they are also stochastically independent. We derived this above for the special case of \\(\\textbf{Y}\\sim G(\\textbf{0},\\sigma^{2}\\textbf{I})\\).\nYou cannot in general conclude that random variables are independent based on their lack of correlation. For Gaussian random variables you can. This result can be extended to Gaussian random vectors. Suppose \\(\\textbf{Y}_{(n \\times 1)} \\sim G(\\boldsymbol{\\mu},\\ \\textbf{V})\\) is partitioned into two sub-vectors of size \\(s\\) and \\(k\\), where \\(n = s + k\\). Then we can similarly partition the mean vector and variance matrix:\n\\[\n\\textbf{Y}_{(n \\times 1)} = \\begin{bmatrix}\n\\textbf{Y}_{1(s \\times 1)} \\\\\n\\textbf{Y}_{2(k \\times 1)}\n\\end{bmatrix},\\qquad \\boldsymbol{\\mu}= \\begin{bmatrix}\n\\boldsymbol{\\mu}_{1} \\\\\n\\boldsymbol{\\mu}_{2}\n\\end{bmatrix},\\qquad \\textbf{V}= \\begin{bmatrix}\n\\textbf{V}_{11} & \\textbf{V}_{12} \\\\\n\\textbf{V}_{21} & \\textbf{V}_{22}\n\\end{bmatrix}\n\\]\nIf \\(\\textbf{V}_{12} = \\textbf{0}\\), then \\(\\textbf{Y}_{1}\\) and \\(\\textbf{Y}_{2}\\) are independent. Also, each partition is Gaussian distributed, for example, \\(\\textbf{Y}_{1} \\sim G(\\boldsymbol{\\mu}_{1},\\ \\textbf{V}_{11})\\). We call the distribution of \\(\\textbf{Y}_{1}\\) the marginal distribution.\nIt follows immediately that each element of \\(\\textbf{Y}\\) follows a (univariate) Gaussian distribution, \\(Y_{i} \\sim G(\\mu_{i},V_{ii})\\)—all marginal univariate distributions are Gaussian.\n\n\nConditionals are Gaussian\nThe conditional distribution of \\(\\textbf{Y}_{1}\\) given \\(\\textbf{Y}_{2}\\) is also a Gaussian distribution, specifically:\n\\[\n\\textbf{Y}_{1}|\\textbf{Y}_{2} \\sim G\\left( \\boldsymbol{\\mu}_{1}\\mathbf{+}\\textbf{V}_{12}\\textbf{V}_{22}^{- 1}\\left( \\textbf{Y}_{2} - \\boldsymbol{\\mu}_{2} \\right),\\ \\textbf{V}_{11} - \\textbf{V}_{12}\\textbf{V}_{22}^{- 1}\\textbf{V}_{12}^\\prime \\right)\n\\]\nThis result plays an important role when predicting Gaussian random variables, for example in mixed models.\nNotice that the variance matrix of the conditional distribution does not depend on the particular value \\(\\textbf{Y}_{2} = \\textbf{Y}_{2}\\) on which the distribution is conditioned. However, the mean of the conditional distribution does depend on \\(\\textbf{Y}_{2}\\) unless \\(\\textbf{V}_{12} = \\textbf{0}\\), a condition established earlier for independence of \\(\\textbf{Y}_{1}\\) and \\(\\textbf{Y}_{2}\\).\n\n\n\nMaximum Likelihood Estimator\nSuppose that we want to find an estimator for \\(\\boldsymbol{\\beta}\\) in the linear model \\(\\textbf{Y}= \\textbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\). If the model errors follow a Gaussian distribution with mean \\(\\textbf{0}\\) and variance \\(\\textbf{V}\\), then \\(\\textbf{Y}\\) follows a Gaussian distribution because it is a linear function of \\(\\boldsymbol{\\epsilon}\\). The probability density function of \\(\\textbf{Y}\\) is\n\\[\nf\\left( \\textbf{Y}\\right)=\\frac{\\left| \\textbf{V}\\right|^{- 1/2}}{(2\\pi)^{\\frac{n}{2}}}\\exp\\left\\{ - \\frac{1}{2}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)^\\prime\\textbf{V}^{- 1}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right) \\right\\}\n\\]\nThis joint distribution of the data can be used to derive the maximum likelihood estimator (MLE) of \\(\\boldsymbol{\\beta}\\). Maximum likelihood estimation considers this as a function of \\(\\boldsymbol{\\beta}\\) rather than a function of \\(\\textbf{Y}.\\) Maximizing this likelihood function \\(\\mathcal{l(}\\boldsymbol{\\beta};\\textbf{Y})\\) is equivalent to maximizing its logarithm and working on the log scale is much simpler. The log-likelihood fu1nction for this problem is given by\n\\[\nln\\mathcal{\\{ l}\\left( \\boldsymbol{\\beta};\\textbf{Y}\\right\\} = l\\left( \\boldsymbol{\\beta};\\textbf{Y}\\right) = - \\frac{1}{2}\\ln\\left( \\left| \\textbf{V}\\right| \\right) - \\frac{n}{2}\\ln(2\\pi) - \\frac{1}{2}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)^\\prime\\textbf{V}^{- 1}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)\n\\]\nFinding the maximum of this function with respect to \\(\\boldsymbol{\\beta}\\) is equivalent to minimizing the quadratic form \\[\n\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)^\\prime\\textbf{V}^{-1}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)\n\\] with respect to \\(\\boldsymbol{\\beta}\\). Applying the results about matrix differentiation from above we obtain\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial\\boldsymbol{\\beta}}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)^\\prime\\textbf{V}^{- 1}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right) &= \\frac{\\partial}{\\partial\\boldsymbol{\\beta}}\\left\\{ \\textbf{Y}^\\prime\\textbf{V}^{- 1}\\textbf{Y}- 2\\textbf{Y}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\beta}^\\prime\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\boldsymbol{\\beta}\\right\\} \\\\\n\n&= - 2\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}+ 2\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\boldsymbol{\\beta}\n\\end{align*}\\]\nThe derivative is zero when \\(\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}= \\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\boldsymbol{\\beta}\\).\nIf \\(\\textbf{X}\\) is of full column rank, then \\(\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\) is non-singular and its inverse exists. Pre-multiplying both sides of the equation with that inverse yields the solution\n\\[\\begin{align*}\n\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}&= \\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\widehat{\\boldsymbol{\\beta}} \\\\\n\n\\left( \\textbf{X}^{\\prime}\\textbf{V}^{- 1}\\textbf{X}\\right)^{- 1}\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}&= {\\left( \\textbf{X}^{\\prime}\\textbf{V}^{- 1}\\textbf{X}\\right)^{- 1}\\textbf{X}}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\\\\n\n\\left( \\textbf{X}^{\\prime}\\textbf{V}^{- 1}\\textbf{X}\\right)^{- 1}\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}&= \\widehat{\\boldsymbol{\\beta}}\n\\end{align*}\\]\nThe maximum likelihood estimator of \\(\\boldsymbol{\\beta}\\) is the generalized least squares estimator\n\\[\n\\widehat{\\boldsymbol{\\beta}}=\\left( \\textbf{X}^{\\prime}\\textbf{V}^{- 1}\\textbf{X}\\right)^{- 1}\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}\n\\]\nA special case arises when the model errors \\(\\boldsymbol{\\epsilon}\\) are uncorrelated. Since the errors are Gaussian distributed, we know that the errors are then independent. The variance matrix \\(\\textbf{V}\\) is then a diagonal matrix\n\\[\n\\textbf{V}= \\begin{bmatrix}\n\\sigma_{1}^{2} & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\sigma_{n}^{2}\n\\end{bmatrix}\n\\]\nA further special case arises when the diagonal entries are all the same,\n\\[\n\\textbf{V}= \\begin{bmatrix}\n\\sigma^{2}\\  & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\sigma^{2}\n\\end{bmatrix} = \\sigma^{2}\\textbf{I}\n\\]\nWe can write the error distribution in this case as \\(\\boldsymbol{\\epsilon}\\sim G\\left(\\textbf{0},\\sigma^{2}\\textbf{I}\\right)\\) and the model for \\(\\textbf{Y}\\) as \\(\\textbf{Y}\\sim G\\left( \\textbf{X}\\boldsymbol{\\beta},\\sigma^{2}\\textbf{I}\\right)\\). This is known as the iid case, the errors are independent and identically distributed. You will encounter the iid assumption a lot in statistical modeling and in machine learning, because it allows you to write multivariate joint distributions as products of univariate distributions and this greatly simplifies matters.\nUnder the iid assumption for the Gaussian linear model we can substitute \\(\\sigma^{2}\\textbf{I}\\) for \\(\\textbf{V}\\) in the formula for \\(\\widehat{\\boldsymbol{\\beta}}\\) and obtain\n\\[{\\widehat{\\boldsymbol{\\beta}}=\\left( \\textbf{X}^{\\prime}\\textbf{X}\\right)}^{- 1}\\textbf{X}^\\prime\\textbf{Y}\\]\nthe ordinary least squares estimator. Notice that \\(\\sigma^{2}\\) cancels out of the formula; the value of the OLS estimator does not depend on the intrinsic variability of the data. However, the variability of the OLS estimator does depend on \\(\\sigma^{2}\\) (and on \\(\\textbf{X}\\)).",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/linalg.html#sherman-morrison-woodbury-formula",
    "href": "reviewmaterial/linalg.html#sherman-morrison-woodbury-formula",
    "title": "34  Linear Algebra",
    "section": "34.8 Sherman, Morrison, Woodbury Formula",
    "text": "34.8 Sherman, Morrison, Woodbury Formula\nThis remarkable formula is at the heart of many regression-type diagnostics and cross-validation techniques. A version of this formula was first given by Gauss in 1821. Around 1950, it appeared in several papers by Sherman and Morrison, and Woodbury.\nSuppose we are in a full-rank linear modeling context with design matrix \\(\\textbf{X}_{(n \\times p + 1)}\\), so that the inverse \\(\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\) exists. In diagnosing the quality of a model, we are interested in measuring the prediction error for the \\(i\\)th observation as if the data point had not contributed to the analysis. This is an example of a leave-one-out estimate: remove an observation from the data, redo the analysis, and measure how well the quantity of interest can be computed for the withheld observation.\nIf you do this in turn for all \\(n\\) observations, you must fit the model \\(n + 1\\) times, an overall fit to the training data with \\(n\\) observations, and \\(n\\) additional fits with training data sets of size \\(n - 1\\), leaving out each observation in turn. The computationally expensive part of fitting the linear model is building the cross-product matrix \\(\\textbf{X}^\\prime\\textbf{X}\\) and computing its inverse \\(\\left( \\textbf{X}^pX \\right)^{-1}\\).\nThe Sherman-Morrison-Woodbury formula allows us to compute the inverse of the cross-product matrix based on \\(\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}\\) as if the \\(i\\)th observation had been removed.\nDenote as \\(\\textbf{X}_{-i}\\) the design matrix with the \\(i\\)th observation removed. Then\n\\[\n\\left( \\textbf{X}_{-i}^\\prime\\textbf{X}_{-i} \\right)^{- 1} = \\left( \\textbf{X}^\\prime\\textbf{X}- \\textbf{X}_{i}\\textbf{X}_{i}^{\\prime} \\right)^{-1}\\  = \\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1} + \\frac{\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-1}{\\textbf{X}_{i}\\textbf{X}_{i}^{\\prime}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)}^{- 1}}{1 - \\textbf{X}_{i}^{\\prime}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{-1}\\textbf{X}_{i}}\n\\]\nBecause of this remarkable result, leave-one-out statistics can be calculated easily—without retraining any models—based on the fit to the full training data alone. Note that the quantity in the denominator of the right-hand side is the diagonal value of \\(\\textbf{I}- \\textbf{H}\\), where \\(\\textbf{H}\\) is the hat matrix. If \\(h_{ii}\\) denotes the diagonal values of \\(\\textbf{H}\\), we can write the update formula as\n\\[\n\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1} + \\frac{\\left( \\textbf{X}^\\prime\\textbf{X}\\right)^{- 1}{\\textbf{X}_{i}\\textbf{X}_{i}^{\\prime}\\left( \\textbf{X}^\\prime\\textbf{X}\\right)}^{- 1}}{1 - h_{ii}}\n\\]\nThe leverage values \\(h_{ii}\\) play an important role in the computation of residual, influence, and case-deletion diagnostics in linear models.",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/estimation.html",
    "href": "reviewmaterial/estimation.html",
    "title": "35  Estimation",
    "section": "",
    "text": "35.1 Introduction\nStatistical models contain two types of unknown quantities, parameters and hyperparameters. Parameters describe the distributional properties of the data; they are part of the mean function or the variance-covariance structure of the model. They are estimated from the data.\nThis chapter is not concerned with tuning hyperparameters but with the principles we use to estimate parameters of a model’s mean function from data—that is, estimating the internal parameters of the model.\nFinding parameter estimates can be expressed as a numerical problem: find the values that minimize some metric of discrepancy between data and the model. The discrepancy can be some measure of loss such as squared error between observed and predicted target values \\[\\left( y_i - \\widehat{f}_i(\\textbf{x};\\boldsymbol{\\theta}) \\right)^2\\] or the misclassification error \\[\nI(y_i \\ne \\widehat{f}_i(\\textbf{x};\\boldsymbol{\\theta}))\n\\] and the loss for the entire data set is summed over all observations, for example \\[\n\\ell(\\textbf{y};\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\left( y_i - \\widehat{y}_i \\right)^2\n\\]\nThe solution to the minimization problem \\[\n\\mathop{\\mathrm{arg\\,min}}_\\boldsymbol{\\theta}\\, \\ell(\\textbf{y};\\boldsymbol{\\theta})\n\\] is the estimator \\(\\widehat{\\boldsymbol{\\theta}}\\) of the parameters \\(\\boldsymbol{\\theta}\\).\nIn some situations, this minimization problem has a closed-form solution that can be computed directly. In other cases we have to rely on numerical procedures to find a solution iteratively or approximations to simplify a complex or intractable problem. The solution \\(\\widehat{\\boldsymbol{\\theta}}\\) is unique for some problems and might be one of many solutions, not all equally good.\nExpressing parameter estimation as a general minimization problem does not reveal the foundations of important principles in parameter estimation, in particular, least squares and maximum likelihood estimation. We introduce these principles based on geometric and probabilistic considerations, the relationship to function minimization will be obvious.",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/estimation.html#introduction",
    "href": "reviewmaterial/estimation.html#introduction",
    "title": "35  Estimation",
    "section": "",
    "text": "Note\n\n\n\nAll optimization tasks will be presented as minimization problem. Finding the maximum of a function can be turned into a minimization of its negative value.",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/estimation.html#least-squares-estimation",
    "href": "reviewmaterial/estimation.html#least-squares-estimation",
    "title": "35  Estimation",
    "section": "35.2 Least Squares Estimation",
    "text": "35.2 Least Squares Estimation\nLeast squares estimation is arguably one of the most important estimation principles and rests on a geometric concept. Suppose we have a model with additive errors, \\(\\textbf{Y}= \\mathbf{f}(\\textbf{x}; \\boldsymbol{\\theta}) + \\boldsymbol{\\epsilon}\\). The function \\(\\mathbf{f}()\\) is an \\((n \\times 1)\\) vector of the mean function evaluations, the \\(i\\)th value is \\(f(\\textbf{x}_i;\\boldsymbol{\\theta})\\). The least squares estimate \\(\\widehat{\\boldsymbol{\\theta}}\\) of \\(\\boldsymbol{\\theta}\\) is the value that is closest to \\(\\textbf{Y}\\) among all possible values \\(\\tilde{\\boldsymbol{\\theta}}\\).\n\nOrdinary Least Squares (OLS)\nConsider the identity\n\\[\n\\textbf{Y}= \\bf (\\textbf{x};\\tilde{\\boldsymbol{\\theta}}) + \\left(\\textbf{Y}- \\bf(\\textbf{x};\\tilde{\\boldsymbol{\\theta}}) \\right)\n\\]\nthat expresses the observed data \\(\\textbf{Y}\\) as the sum of fitted values and residuals.\nBy the Pythagorean theorem, the solution \\(\\widehat{\\boldsymbol{\\theta}}\\) with the smallest vector of residuals is the orthogonal projection of \\(\\textbf{Y}\\) onto \\(\\bf\\), which implies that \\[\n\\left(\\textbf{Y}- \\bf(\\textbf{x};\\widehat{\\boldsymbol{\\theta}}) \\right)^\\prime \\bf(\\textbf{x};\\widehat{\\boldsymbol{\\theta}}) = \\textbf{0}\n\\] When \\(\\bf(\\textbf{x};\\boldsymbol{\\theta})\\) is linear in the parameters, it is common to denote the coefficients (parameters) as \\(\\boldsymbol{\\beta}\\) instead of the more generic \\(\\boldsymbol{\\theta}\\). We thus write the standard linear (regression) as \\[\n\\textbf{Y}= \\textbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\n\\] and the orthogonality criterion becomes \\[\n\\left(\\textbf{Y}- \\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\right)^\\prime \\textbf{X}\\widehat{\\boldsymbol{\\beta}} = \\textbf{0}\n\\] It is easy to show that this implies \\[\n\\textbf{X}^\\prime\\textbf{X}\\widehat{\\boldsymbol{\\beta}} = \\textbf{X}^\\prime\\textbf{Y}\n\\] and if the cross-product matrix \\(\\textbf{X}^\\prime\\textbf{X}\\) is of full rank, the (ordinary) least squares estimator is \\[\n\\widehat{\\boldsymbol{\\beta}} = (\\textbf{X}^\\prime\\textbf{X})^{-1}\\textbf{X}^\\prime\\textbf{Y}\n\\] Expressed as the problem of minimizing a loss function, ordinary least squares (OLS) is approached as follows. Suppose that we measure loss as squared-error loss \\((y_i - \\widehat{y}_i)^2\\). In the linear model where \\(\\widehat{y}_i = \\textbf{x}_i^\\prime \\widehat{\\boldsymbol{\\beta}}\\), the loss function is the residual (error) sum of squares \\[\n\\text{SSE}(\\boldsymbol{\\beta}) = \\boldsymbol{\\epsilon}^\\prime\\boldsymbol{\\epsilon}= \\left( \\textbf{Y}-\\textbf{X}^\\prime\\boldsymbol{\\beta}\\right)^\\prime \\left(\\textbf{Y}-\\textbf{X}^\\prime\\boldsymbol{\\beta}\\right)\n\\] and the minimization problem is \\[\n\\mathop{\\mathrm{arg\\,min}}_\\boldsymbol{\\beta}\\, \\text{SSE}(\\boldsymbol{\\beta})\n\\]\nTo find the minimum we set to zero the derivative of \\(\\text{SSE}(\\boldsymbol{\\beta})\\) with respect to \\(\\boldsymbol{\\beta}\\). Expanding the residual sum of squares yields \\[\nSSE(\\boldsymbol{\\beta}) = \\textbf{Y}^\\prime\\textbf{Y}- 2\\boldsymbol{\\beta}^\\prime\\textbf{X}^\\prime\\textbf{Y}+ \\boldsymbol{\\beta}^\\prime\\textbf{X}^\\prime\\textbf{X}\\boldsymbol{\\beta}\n\\]\nThe derivative (Section 34.5) with respect to \\(\\boldsymbol{\\beta}\\) is \\[\n\\frac{\\partial\\,\\text{SSE}(\\boldsymbol{\\beta})}{\\partial\\boldsymbol{\\beta}} = -2\\textbf{X}^\\prime\\textbf{Y}+ 2\\textbf{X}^\\prime\\textbf{X}\\boldsymbol{\\beta}\n\\]\nSetting the derivative to zero and solving yields the normal equations as above: \\[\n\\textbf{X}^\\prime\\textbf{X}\\widehat{\\boldsymbol{\\beta}} = \\textbf{X}^\\prime\\textbf{Y}\n\\] and the OLS estimator \\[\n\\widehat{\\boldsymbol{\\beta}} = (\\textbf{X}^\\prime\\textbf{X})^{-1}\\textbf{X}^\\prime\\textbf{Y}\n\\] The vector of fitted values, \\(\\widehat{\\textbf{y}}\\) is obtained by pre-multiplying with the data matrix\n\\[\\begin{align*}\n    \\widehat{\\textbf{Y}} &= \\textbf{X}^\\prime\\widehat{\\boldsymbol{\\beta}} = (\\textbf{X}^\\prime\\textbf{X})^{-1}\\textbf{X}^\\prime\\textbf{Y}\\\\\n                  &= \\textbf{H}\\textbf{Y}\n\\end{align*}\\]\nThe matrix \\(\\textbf{H}\\) is called the “Hat” matrix, because pre-multiplying \\(\\textbf{Y}\\) with \\(\\textbf{H}\\) “puts hats on the \\(y\\)s” (Section 34.6.2). The last equation shows that the OLS estimator is a linear estimator, \\(\\widehat{y}_i\\) is a linear combination of all \\(y_i\\), the weights of the linear combination are given by the entries of the hat matrix.\nThe statistical properties of the OLS estimator depend on the nature of the random error process. The most common assumption is that the \\(\\epsilon_i\\) have zero mean and are iid, independently and identically distributed, formally, \\(\\boldsymbol{\\epsilon}\\sim (\\textbf{0},\\sigma^2\\textbf{I})\\).\n\n\n\n\n\n\nNote\n\n\n\nThe statement \\(\\boldsymbol{\\epsilon}\\sim (\\textbf{0},\\sigma^2\\textbf{I})\\) is technically weaker than stating independence, it implies zero correlations among the observations. Independent random variables are uncorrelated, but the reverse is not necessarily true. You can conclude independence from lack of correlations for Gaussian (normal) random variables, but not generally.\n\n\nIn the iid case, the OLS estimator is unbiased, \\(\\text{E}[\\widehat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\) and has variance \\(\\text{Var}[\\widehat{\\boldsymbol{\\beta}}] = \\sigma^2(\\textbf{X}^\\prime\\textbf{X})^{-1}\\). In fact, it is a BLUE (best linear unbiased estimator) in this situation. No other unbiased estimator has smaller variance than the OLS estimator. However, it is possible that other estimators have smaller mean squared error than the OLS estimator, if the introduction of bias is more than offset by a reduction of the variance of the estimator. This is important in high-dimensional problems where the number of predictors (\\(p\\)) is large. As \\(p\\) increases, the OLS estimator becomes more unstable, especially if the predictors are highly related to each other (a condition known as multi-collinearity). The values of \\(\\widehat{\\boldsymbol{\\beta}}\\) then have a tendency to vary widely. Estimators that limit the variability of the model coefficients through regularization techniques, such as Ridge or Lasso regression, can have considerably lower variance at the expense of some bias, leading to better mean squared error.\n\n\n\n\n\n\nNote\n\n\n\nA Gaussian distribution (normality) assumption is not a requirement of the linear model or of least squares estimation. The OLS estimator has desirable properties even if the errors are not normally distributed. However, making statements about the significance of the \\(\\beta_j\\) requires additional assumptions such as \\(\\boldsymbol{\\epsilon}\\sim G(\\textbf{0},\\sigma^2\\textbf{I})\\).\nWhen \\(\\boldsymbol{\\epsilon}\\sim G(\\textbf{0},\\sigma^2\\textbf{I})\\), the OLS estimator is not only BLUE but a minimum variance unbiased estimator (MVUE), best among all unbiased estimators, not only those estimators linear in \\(\\textbf{Y}\\).\n\n\n\nLeast squares from scratch\nTo understand statistical computing, it is a good idea to implement some algorithms from scratch. That also helps to identify the numbers reported by statistical software. Here we implement the OLS estimator from scratch in R using the fitness data set. The data comprise measurements of aerobic capacity and other attributes on 31 men involved in a physical fitness course at N.C. State University.\nAerobic capacity is the ability of the heart and lungs to provide the body with oxygen. It is a measure of fitness and expressed as the oxygen intake in ml per kg body weight per minute. Measuring aerobic capacity is expensive and time consuming compared to attributes such as age, weight, and pulse. The question is whether aerobic capacity can be predicted from the easily measurable attributes. If so, a predictive equation can reduce time and effort to assess aerobic capacity.\nThe variables are\n\nAge: age in years\nWeight: weight in kg\nOxygen: oxygen intake rate (ml per kg body weight per minute)\nRunTime: time to run 1.5 miles (minutes)\nRestPulse: heart rate while resting\nRunPulse: heart rate while running (same time Oxygen rate measured)\nMaxPulse: maximum heart rate recorded while running\n\nThe linear model we have in mind is \\[\n\\text{Oxygen}_i = \\beta_0 + \\beta_1\\text{Age}_i + \\beta_2\\text{Weight}_i + \\beta_3\\text{RunTime}_i + \\beta_4\\text{RestPulse}_i + \\beta_5\\text{RunPulse}_i + \\beta_6\\text{MaxPulse}_i + \\epsilon_i\n\\] and the \\(\\epsilon_i\\) are assumed zero-mean random variables with common variance \\(\\sigma^2\\).\nThe following code loads the data from the ads DuckDB database.\n\nlibrary(\"duckdb\")\ncon &lt;- dbConnect(duckdb(),dbdir = \"../ads5064.ddb\",read_only=TRUE)\nfit &lt;- dbGetQuery(con, \"SELECT * FROM fitness\")\n\nhead(fit)\n\n  Age Weight Oxygen RunTime RestPulse RunPulse MaxPulse\n1  44  89.47 44.609   11.37        62      178      182\n2  40  75.07 45.313   10.07        62      185      185\n3  44  85.84 54.297    8.65        45      156      168\n4  42  68.15 59.571    8.17        40      166      172\n5  38  89.02 49.874    9.22        55      178      180\n6  47  77.45 44.811   11.63        58      176      176\n\n\nThe target variable for the linear model is Oxygen, the remaining variables are inputs to the regression. The next statements create the \\(\\textbf{y}\\) vector and the \\(\\textbf{X}\\) matrix for the model. Note that the first column of \\(\\textbf{X}\\) is a vector of ones, representing the “input” for the intercept \\(\\beta_0\\).\n\ny &lt;- as.matrix(fit[,which(names(fit)==\"Oxygen\")])\nX &lt;- as.matrix(cbind(Intcpt=rep(1,nrow(fit)), \n                     fit[,which(names(fit)!=\"Oxygen\")]))\nhead(X)\n\n     Intcpt Age Weight RunTime RestPulse RunPulse MaxPulse\n[1,]      1  44  89.47   11.37        62      178      182\n[2,]      1  40  75.07   10.07        62      185      185\n[3,]      1  44  85.84    8.65        45      156      168\n[4,]      1  42  68.15    8.17        40      166      172\n[5,]      1  38  89.02    9.22        55      178      180\n[6,]      1  47  77.45   11.63        58      176      176\n\n\nNext we are building the \\(\\textbf{X}^\\prime\\textbf{X}\\) matrix and compute its inverse, \\((\\textbf{X}^\\prime\\textbf{X})^{-1}\\), with the solve() function. t() transposes a matrix and %*% indicates that we are performing matrix multiplication rather than elementwise multiplication.\n\nXpX &lt;- t(X) %*% X\nXpXInv &lt;- solve(XpX)\n\nWe can verify that XpxInv is indeed the inverse of XpX by multiplying the two. This should yield the identity matrix\n\nround(XpX %*% XpXInv,3)\n\n          Intcpt Age Weight RunTime RestPulse RunPulse MaxPulse\nIntcpt         1   0      0       0         0        0        0\nAge            0   1      0       0         0        0        0\nWeight         0   0      1       0         0        0        0\nRunTime        0   0      0       1         0        0        0\nRestPulse      0   0      0       0         1        0        0\nRunPulse       0   0      0       0         0        1        0\nMaxPulse       0   0      0       0         0        0        1\n\n\nNext we compute the OLS estimate of \\(\\boldsymbol{\\beta}\\) and the predicted values \\(\\widehat{\\textbf{y}} = \\textbf{X}\\widehat{\\boldsymbol{\\beta}}\\).\n\nbeta_hat &lt;- XpXInv %*% t(X) %*% y\nbeta_hat\n\n                  [,1]\nIntcpt    102.93447948\nAge        -0.22697380\nWeight     -0.07417741\nRunTime    -2.62865282\nRestPulse  -0.02153364\nRunPulse   -0.36962776\nMaxPulse    0.30321713\n\ny_hat &lt;- X %*% beta_hat\n\nThe estimate of the intercept is \\(\\widehat{\\beta}_0\\) = 102.9344795, the estimate of the coefficient for Age is \\(\\widehat{\\beta}_1\\) = -0.2269738 and so on.\nThe residuals \\(\\widehat{\\boldsymbol{\\epsilon}} = \\textbf{y}- \\widehat{\\textbf{y}}\\), the error sum of squares\n\\[\n\\text{SSE} = (\\textbf{y}- \\widehat{\\textbf{y}} )^\\prime (\\textbf{y}- \\widehat{\\textbf{y}}) = \\sum_{i=1}^n \\left(y_i - \\widehat{y}_i\\right)^2\n\\] and the estimate of the residual variance \\[\n\\widehat{\\sigma}^2 = \\frac{1}{n-r(\\textbf{X})} \\, \\text{SSE}\n\\]\nare computed as\n\nlibrary(\"Matrix\")\n\nresiduals &lt;- y - y_hat\nSSE &lt;- sum(residuals^2)\nn &lt;- nrow(fit)\nrankX &lt;- rankMatrix(XpX)[1]\nsigma2_hat &lt;- SSE/(n - rankX)\n\nSSE\n\n[1] 128.8379\n\nsigma2_hat\n\n[1] 5.368247\n\n\nWe used the rankMatrix function in the Matrix package to compute the rank of \\(\\textbf{X}\\), which is identical to the rank of \\(\\textbf{X}^\\prime\\textbf{X}\\). With these quantities available, the variance-covariance matrix of \\(\\widehat{\\boldsymbol{\\beta}}\\), \\[\n\\text{Var}[\\widehat{\\boldsymbol{\\beta}}] = \\sigma^2 (\\textbf{X}^\\prime\\textbf{X})^{-1}\n\\] can be estimated by substituting \\(\\widehat{\\sigma}^2\\). The standard errors of the regression coefficient estimates are the square roots of the diagonal values of this matrix.\n\nVar_beta_hat &lt;- sigma2_hat * XpXInv\nse_beta_hat &lt;- sqrt(diag(Var_beta_hat))\nse_beta_hat\n\n     Intcpt         Age      Weight     RunTime   RestPulse    RunPulse \n12.40325810  0.09983747  0.05459316  0.38456220  0.06605428  0.11985294 \n   MaxPulse \n 0.13649519 \n\n\nNow let’s compare our results to the output from the lm() function in R.\n\nlinmod &lt;- lm(Oxygen ~ ., data=fit)\nsummary(linmod)\n\n\nCall:\nlm(formula = Oxygen ~ ., data = fit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.4026 -0.8991  0.0706  1.0496  5.3847 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 102.93448   12.40326   8.299 1.64e-08 ***\nAge          -0.22697    0.09984  -2.273  0.03224 *  \nWeight       -0.07418    0.05459  -1.359  0.18687    \nRunTime      -2.62865    0.38456  -6.835 4.54e-07 ***\nRestPulse    -0.02153    0.06605  -0.326  0.74725    \nRunPulse     -0.36963    0.11985  -3.084  0.00508 ** \nMaxPulse      0.30322    0.13650   2.221  0.03601 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.317 on 24 degrees of freedom\nMultiple R-squared:  0.8487,    Adjusted R-squared:  0.8108 \nF-statistic: 22.43 on 6 and 24 DF,  p-value: 9.715e-09\n\n\nBased on the quantities calculated earlier, the following code reproduces the lm summary.\n\ntvals &lt;- beta_hat/se_beta_hat\npvals &lt;- 2*(1-pt(abs(tvals),n-rankX))\nresult &lt;- cbind(beta_hat, se_beta_hat, tvals, pvals)\ncolnames(result) &lt;- c(\"Estimate\", \"Std. Error\", \"t value\", \"Pr(&gt;|t|)\")\nround(result,5)\n\n           Estimate Std. Error  t value Pr(&gt;|t|)\nIntcpt    102.93448   12.40326  8.29899  0.00000\nAge        -0.22697    0.09984 -2.27343  0.03224\nWeight     -0.07418    0.05459 -1.35873  0.18687\nRunTime    -2.62865    0.38456 -6.83544  0.00000\nRestPulse  -0.02153    0.06605 -0.32600  0.74725\nRunPulse   -0.36963    0.11985 -3.08401  0.00508\nMaxPulse    0.30322    0.13650  2.22145  0.03601\n\ncat(\"\\nResidual standard error: \", sqrt(sigma2_hat),\" on \", n-rankX, \"degrees of freedom\\n\")\n\n\nResidual standard error:  2.316948  on  24 degrees of freedom\n\nSST &lt;- sum( (y -mean(y))^2 )\ncat(\"Multiple R-squared: \", 1-SSE/SST, \n    \"Adjusted R-squared: \", 1 - (SSE/SST)*(n-1)/(n-rankX), \"\\n\")\n\nMultiple R-squared:  0.8486719 Adjusted R-squared:  0.8108399 \n\nFstat &lt;- ((SST-SSE)/(rankX-1)) / (SSE/(n-rankX))\ncat(\"F-statistic: \", Fstat, \"on \", \n    rankX-1, \"and\", n-rankX, \"DF, p-value:\", 1-pf(Fstat,rankX-1,n-rankX))\n\nF-statistic:  22.43263 on  6 and 24 DF, p-value: 9.715305e-09\n\n\n\n\n\nWeighted and Generalized Least Squares\nAnother interesting feature of the OLS estimator is that it does not depend on the variability of the model errors. Whether \\(\\sigma^2\\) is large or small, the OLS estimator is only a function of \\(\\textbf{Y}\\) and \\(\\textbf{X}\\). However, the error variance affects the variability of \\(\\widehat{\\boldsymbol{\\beta}}\\).\nWhen the error distribution is more complex than the iid case, the variance and covariances of the errors must be taken into account for least squares estimators to retain their optimality. The first case is that of uncorrelated errors that have unequal variances, a situation known as heteroscedasticity. The variance-covariance matrix of the \\(\\boldsymbol{\\epsilon}\\) is then a diagonal matrix. Let’s call the inverse of the variance-covariance matrix \\(\\textbf{W}\\). \\(\\textbf{W}\\) is a \\((n \\times n)\\) d iagonal matrix with \\(1/\\sigma^2_i\\), the inverse of the variance of the \\(i\\)th observation, in the \\(i\\)th diagonal cell. The weighted least squares estimator \\[\n\\widehat{\\boldsymbol{\\beta}}_{WLS} = (\\textbf{X}^\\prime\\textbf{W}\\textbf{X})^{-1}\\textbf{X}^\\prime\\textbf{W}\\textbf{Y}\n\\] is the optimal estimator in this situation.\n\n\n\n\n\n\nTip\n\n\n\nA weighted analysis is the correct approach when the weights are inversely proportional to the variance of the observations. This makes sense if we think of the weights as expressing how strongly the analysis should depend on a particular observation. A larger variance means that we are less certain about the observed value and thus should give the observation less weight.\n\n\nIn the weighted model the variance-covariance matrix of the errors are diagonal, the observations have unequal variances but are uncorrelated. If the errors are correlated, the variance-covariance matrix is not diagonal. Suppose that \\(\\boldsymbol{\\epsilon}\\sim (\\textbf{0}, \\textbf{V})\\), the optimal least squares estimator is the generalized least squares estimator \\[\n\\widehat{\\boldsymbol{\\beta}}_{GLS} = (\\textbf{X}^\\prime\\textbf{V}^{-1}\\textbf{X})^{-1}\\textbf{X}^\\prime\\textbf{V}^{-1}\\textbf{Y}\n\\]\nThis seems like a small change from the weighted case, replacing \\(\\textbf{W}\\) with \\(\\textbf{V}\\). So what is the big deal? In weighted analyses the weights are often known, at least up to a multiple. For example, when the variability of the target variable increases proportionally with one of the inputs, \\(x_2\\) say, then \\(\\textbf{W}\\) is essentially known. In situations where we apply GLS estimation, \\(\\textbf{V}\\) is often not known and depends itself on parameters. The overall model then composes a model for the mean function that depends on \\(\\boldsymbol{\\beta}\\) and a model for the error structure that depends on \\(\\boldsymbol{\\theta}\\):\n\\[\\begin{align*}\n    \\textbf{Y}&= \\textbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\\\\n    \\boldsymbol{\\epsilon}& \\sim (\\textbf{0}, \\textbf{V}(\\boldsymbol{\\theta}))\n\\end{align*}\\]\nand both sets of parameters must be derived from the data. This is somewhat of a cat-and-mouse game. You need to know \\(\\boldsymbol{\\beta}\\) to estimate \\(\\boldsymbol{\\theta}\\) and the estimates of \\(\\boldsymbol{\\theta}\\) depend on \\(\\boldsymbol{\\beta}\\). This tension is resolved by the estimated generalized least squares principle. Given an estimate of \\(\\boldsymbol{\\theta}\\), you compute the estimated GLS estimator \\[\n\\widehat{\\boldsymbol{\\beta}}_{EGLS} =  (\\textbf{X}^\\prime\\textbf{V}(\\widehat{\\boldsymbol{\\theta}})^{-1}\\textbf{X})^{-1}\\textbf{X}^\\prime\\textbf{V}(\\widehat{\\boldsymbol{\\theta}})^{-1}\\textbf{Y}\n\\]\nWith an updated estimate of \\(\\boldsymbol{\\beta}\\) you use a different estimation principle to compute an updated estimate of \\(\\boldsymbol{\\theta}\\). This is the principle behind restricted maximum likelihood, a likelihood-based estimation principle important for mixed models.\n\n\nNonlinear Least Squares\nThe linear structure of the model \\(\\textbf{Y}= \\textbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\) leads to a closed form solution of the least squares problem \\[\n\\widehat{\\boldsymbol{\\beta}} = (\\textbf{X}^\\prime\\textbf{X})^{-1}\\textbf{X}^\\prime\\textbf{Y}\n\\]\nWhen the model is nonlinear in the parameters, \\(\\textbf{Y}= \\bf(\\textbf{x};\\boldsymbol{\\theta}) + \\boldsymbol{\\epsilon}\\), finding the solution that minimizes\n\\[\n\\text{SSE} = \\left(\\textbf{Y}- \\bf(\\textbf{x};\\boldsymbol{\\theta})\\right)^\\prime \\left(\\textbf{Y}- \\bf(\\textbf{x};\\boldsymbol{\\theta})\\right)\n\\tag{35.1}\\]\nis not so straightforward, it requires an iterative approach. Starting from some initial guess for \\(\\boldsymbol{\\theta}\\), call it \\(\\widehat{\\boldsymbol{\\theta}}^{(0)}\\), we iteratively update the guess until we have arrived at step \\(t\\) at \\(\\widehat{\\boldsymbol{\\theta}}^{(t)}\\) such that \\[\n\\frac{\\partial \\,\\text{SSE}}{\\partial\\boldsymbol{\\theta}} \\lvert_{\\widehat{\\boldsymbol{\\theta}}^{(t)}} = \\textbf{0}\n\\] The left hand side of the previous expression is read as the derivative of SSE with respect to \\(\\boldsymbol{\\theta}\\), evaluated at \\(\\widehat{\\boldsymbol{\\theta}}^{t}\\).\nCommon iterative approaches to solve this optimization problem involve the Gauss-Newton and Newton-Raphson algorithms. We introduce the Gauss-Newton method here. The basic idea is that we can approximate the nonlinear mean function with a linear version that depends on some current values for \\(\\boldsymbol{\\theta}\\). Linear least squares can be applied to the linearized form to compute an update of the estimate for \\(\\boldsymbol{\\theta}\\). With the updated estimate the approximation can be refined and another least squares step is performed. This sequence is repeated until some convergence criterion is met.\nWe start by approximating \\(\\bf(\\textbf{x};\\boldsymbol{\\theta})\\) with a first-order Taylor series about \\(\\boldsymbol{\\theta}^{(0)}\\) \\[\n    \\bf(\\textbf{x}; \\boldsymbol{\\theta}) \\approx \\bf(\\textbf{x};\\boldsymbol{\\theta}^{(0)}) + \\textbf{F}^{(0)}(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{(0)})\n\\] where \\(\\textbf{F}^{(0)}\\) is a matrix of derivatives of \\(\\bf(\\textbf{x};\\boldsymbol{\\theta})\\) evaluated at the value \\(\\boldsymbol{\\theta}^{(0)}\\). The residual \\(\\textbf{y}- \\bf(\\textbf{x};\\boldsymbol{\\theta})\\) can now be approximated as \\(\\textbf{r}(\\boldsymbol{\\theta}^{(0)}) = \\textbf{y}- \\bf(\\textbf{x};\\boldsymbol{\\theta}^{(0)}) - \\textbf{F}^{(0)}(\\boldsymbol{\\theta}- \\boldsymbol{\\theta}^{(0)})\\). Substitute this expression into Equation 35.1 we get an approximate error sums of squares\n\\[\n\\text{SSE} \\approx \\textbf{r}(\\boldsymbol{\\theta}^{(0)})^\\prime \\textbf{r}(\\boldsymbol{\\theta}^{(0)}) - 2 \\textbf{r}(\\boldsymbol{\\theta}^{(0)})\\textbf{F}^{(0)}(\\boldsymbol{\\theta}- \\boldsymbol{\\theta}^{(0)}) +\n(\\boldsymbol{\\theta}- \\boldsymbol{\\theta}^{(0)})^\\prime \\textbf{F}^{(0)\\prime}\\textbf{F}^{(0)} (\\boldsymbol{\\theta}- \\boldsymbol{\\theta}^{(0)})\n\\]\nTaking derivatives with respect to \\(\\boldsymbol{\\theta}\\) and setting to zero results in the following condition \\[\n(\\boldsymbol{\\theta}- \\boldsymbol{\\theta}^{(0)}) = \\left(\\textbf{F}^{(0)\\prime}\\textbf{F}^{(0)}\\right)^{-1}\\textbf{F}^{(0)\\prime}\\textbf{r}(\\boldsymbol{\\theta}^{(0)})\n\\] This is a really interesting expression. Imagine replacing on the right hand side \\(\\textbf{F}^{(0)}\\) with \\(\\textbf{X}\\) and \\(\\textbf{r}(\\boldsymbol{\\theta}^{(0)})\\) with \\(\\textbf{y}\\). The right hand side is an ordinary least squares solution in a linear model where the \\(x\\)-matrix is given by the derivatives of the nonlinear model and the target variable is the difference between the actual \\(y\\)-values and the approximated mean function. The left hand side of the equation is the difference between the parameter estimate and our current guess. This suggests the following iterative updates \\[\n    \\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} + \\left(\\textbf{F}^{(t)\\prime}\\textbf{F}^{(t)}\\right)^{-1}\\textbf{F}^{(t)\\prime}\\textbf{r}(\\boldsymbol{\\theta}^{(t)})\n\\]\nand is known as the Gauss-Newton algorithm.\n\nGauss-Newton from scratch\nJust like with the OLS problem, we are going to implement a nonlinear least squares solution from scratch, based on the linear algebra presented in this section. The nonlinear model chosen for this exercise has two parameters, \\(\\boldsymbol{\\theta}= [\\theta_1, \\theta_2]\\), one input variable \\(x\\), and mean function \\[\n    f(x; \\boldsymbol{\\theta}) = 1 - \\theta_1 \\exp \\{-x^{\\theta_2}\\}\n\\] We fit this model to a tiny data set with just five observations\n\ny &lt;- c(0.1, 0.4, 0.6, 0.9)\nx &lt;- c(0.2, 0.5, 0.7, 1.8)\n\nThe derivatives of the mean function with respect to the parameters are given by\n\\[\\begin{align*}\n  \\frac{\\partial f(x;\\boldsymbol{\\theta})}{\\partial \\theta_1} &= -\\exp\\{-x^{\\theta_2}\\} \\\\\n  \\frac{\\partial f(x;\\boldsymbol{\\theta})}{\\partial \\theta_2} &= \\theta_1 \\log(x) x^{\\theta_2} \\exp\\{-x^{\\theta_2}\\}\n\\end{align*}\\]\nand we can write a simple R function to compute the derivative matrix \\(\\textbf{F}\\) based on a current estimate of \\(\\boldsymbol{\\theta}\\). The function getres computes \\(\\textbf{r}(\\boldsymbol{\\theta}^{(t)})\\).\n\ngetF &lt;- function(x, theta) {\n    exterm &lt;- exp(-x^theta[2])\n    der1 &lt;- -exterm\n    der2 &lt;- theta[1] * log(x) * (x^theta[2]) * exterm\n    return(cbind(der1,der2))\n}\n\ngetres &lt;- function(y,x,theta) {\n    fx &lt;- 1 - theta[1] * exp(-x^theta[2])\n    return(y - fx)\n}\n\nNow all we need is starting values \\(\\boldsymbol{\\theta}^{(0)}\\) and a loop that iterates until the estimation routine has converged. It is a good idea to not take a full Gauss-Newton step in the updates since there is no guarantee that SSE is lower at iterate \\(t+1\\) than at iterate \\(t\\). We thus multiply the update to the current value by the hyperparameter \\(\\alpha &lt; 1\\). In machine learning, this step size is known as the learning rate. A full implementation of the Gauss-Newton algorithm could determine \\(\\alpha\\) dynamically at each iteration through a line search algorithm.\n\nmaxiter &lt;- 50          # the max number of iterations\nalpha &lt;- 0.5           # the learning rate\ntheta &lt;- c(1, 1.3)     # the starting values\ntol &lt;- 1e-6            # the convergence tolerance\n\nfor (iter in 1:maxiter) {\n    X &lt;- getF(x,theta)\n    r &lt;- getres(y,x,theta)\n    \n    # The linear least squares update\n    new_theta &lt;- theta + alpha * (solve(t(X) %*% X) %*% t(X) %*% r)\n\n    # Now we check a convergence criterion. We take the maximum relative\n    # change in the parameter estimates. If that is less than some tolerance\n    # the algorithm is considered converged.\n    crit &lt;- max(abs(new_theta-theta)/abs(theta))\n    if (crit &lt; tol) {\n        cat( \"Algorithm converged after\", iter,\" iterations! SSE =\", sum(r^2), \"\\n\" )\n        print (new_theta)\n        break\n    } else {\n        theta &lt;- new_theta\n        print (crit)\n    }\n}\n\n[1] 0.03047302\n[1] 0.01661031\n[1] 0.008692942\n[1] 0.004446789\n[1] 0.002248043\n[1] 0.001129919\n[1] 0.000566354\n[1] 0.0002835054\n[1] 0.0001418301\n[1] 7.093331e-05\n[1] 3.547098e-05\n[1] 1.773652e-05\n[1] 8.868505e-06\n[1] 4.434311e-06\n[1] 2.21717e-06\n[1] 1.108588e-06\nAlgorithm converged after 17  iterations! SSE = 0.01569114 \n          [,1]\nder1 0.9366938\nder2 1.2791919\n\n\nAfter 17 iterations with a learning rate (step size) of \\(\\alpha=0.5\\) the algorithm converged on parameter estimates \\(\\widehat{\\theta}_1\\) = 0.9366 and \\(\\widehat{\\theta}_2\\) = 1.2791.\nWe can validate these results with the nls function from the nls2 package.\n\nlibrary(nls2)\n\nLoading required package: proto\n\nnonlin_data &lt;- data.frame(cbind(y=y,x=x))\nf_x &lt;- y ~ 1 - theta1 * exp(-x^theta2)\nnls(f_x, \n    start=list(theta1=1, theta2=1.3), \n    data=nonlin_data)\n\nNonlinear regression model\n  model: y ~ 1 - theta1 * exp(-x^theta2)\n   data: nonlin_data\ntheta1 theta2 \n0.9367 1.2792 \n residual sum-of-squares: 0.01569\n\nNumber of iterations to convergence: 15 \nAchieved convergence tolerance: 6.109e-06",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "reviewmaterial/estimation.html#maximum-likelihood-estimation",
    "href": "reviewmaterial/estimation.html#maximum-likelihood-estimation",
    "title": "35  Estimation",
    "section": "35.3 Maximum Likelihood Estimation",
    "text": "35.3 Maximum Likelihood Estimation\nMaximum likelihood estimation (MLE) is an intuitive and important estimation principle in statistics. It is based on the distributional properties of the data, hence it applies to stochastic data modeling. If the observed data \\(\\textbf{y}\\) are the realization of a data-generating random mechanism, then it makes sense to examine the probability distribution of the data and choose as parameter estimates those values that make it most likely to have observed the data. In other words, we use the probability distribution to find the most likely explanation for the data–hence the name maximum likelihood.\nMaking progress with MLE requires that we know the joint distribution of the random vector \\(\\textbf{Y}\\), an \\(n\\)-dimensional distribution. The distribution depends on a vector \\(\\boldsymbol{\\theta}\\) of unknown parameters and we denote it as \\(f(\\textbf{y}; \\boldsymbol{\\theta})\\).\n\n\n\n\n\n\nNote\n\n\n\nWhenever you calculate a maximum likelihood estimator, you are making assumptions about the distribution of the data. If software packages report MLEs, check the documentation regarding distributional assumptions.\n\n\nWhen the observations are independent, the joint density is the product of the individual densities, \\[\nf(\\textbf{y}; \\boldsymbol{\\theta}) = \\prod_{i=1}^n \\, f(y_i; \\boldsymbol{\\theta})\n\\]\nFor example, the joint mass function of \\(n\\) iid Bernoulli(\\(\\pi\\)) random variables is \\[\n    f(\\textbf{y}; \\pi) = \\prod_{i=1}^n \\, \\pi^{y_i} \\, (1-\\pi)^{1-y_i}\n\\]\nThe likelihood function is the joint density or mass function of the data, but we interpret it as a function of the parameters evaluated at the data, whereas the density (mass) function is a function of the data evaluated at the parameter values. The log-likelihood function is the natural log of the likelihood function, denoted \\(\\mathcal{l}(\\boldsymbol{\\theta}; \\textbf{y})\\). The parameters that maximize the likelihood function also maximize the log of the function. Logarithms are much easier to work with since they turn products into sums and exponents into multipliers.\n\n\nExample: Likelihood Function for Poisson Data\n\n\nIf \\(Y_1, \\cdots, Y_n\\) are a random sample from a Poisson(\\(\\lambda\\)) distribution, the log-likelihood function is\n\\[\n    \\mathcal{l}(\\lambda; \\textbf{y}) = \\sum_{i=1}^n \\left( y_i \\log(\\lambda) - \\lambda - \\log(y_i!) \\right ) =\n    \\log(\\lambda)\\sum_{i=1}^n y_i - n\\lambda - \\sum_{i=1}^n \\log(y_i!)\n\\] Setting the derivative with respect to \\(\\lambda\\) to zero yields \\[\n    \\frac{1}{\\lambda}\\sum_{i=1}^n y_i = n\n\\] The MLE of \\(\\lambda\\) is \\(\\widehat{\\lambda} = \\overline{y}\\), the sample mean.\nSuppose that \\(n=4\\) and we observe \\(\\textbf{y}= [3, 4, 2, 2]\\). The sample mean is \\(\\overline{y} = 2.75\\). The following R code computes the log-likelihood function \\(\\mathcal{l}(\\lambda; \\textbf{y})\\) for different values of \\(\\lambda\\). The log-likelihood function has a maximum at \\(\\overline{y} = 2.75\\).\n\ny &lt;- c(3, 4, 2, 2)\nn &lt;- length(y)\nsumy &lt;- sum(y)\nsumlogfac &lt;- sum(log(factorial(y)))\nlambda &lt;- seq(0.1, 5, 0.1)\nloglike &lt;- log(lambda)*sumy - n*lambda - sumlogfac\n\nplot(lambda,loglike,type=\"l\",bty=\"l\",lwd=1.5,\n     xlab=expression(lambda),\n     ylab=\"log likelihood\")\nabline(v=mean(y),lty=\"dotted\",lwd=1.5,col=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n\nExample: MLE for iid Bernoulli Experiments\n\n\nTo find the maximum likelihood estimator of \\(\\pi\\) in \\(n\\) iid Bernoulli(\\(\\pi\\)) experiments, we need to find the value \\(\\widehat{\\pi}\\) that maximizes the log-likelihood function\n\\[\\begin{align*}\n    \\mathcal{l}(\\pi; \\textbf{y}) &= \\log\\left(\\prod_{i=1}^n \\, \\pi^{y_i} \\, (1-\\pi)^{1-y_i}\\right) \\\\\n                          &= \\sum_{i=1}^n \\, y_i\\log(\\pi) + (1-y_i)\\log(1-\\pi)\n\\end{align*}\\]\n\\(\\mathcal{l}(\\pi; \\textbf{y}) = \\sum_i y_i\\log(\\pi) + (1-y_i)\\log(1-\\pi)\\). The derivative with respect to \\(\\pi\\) is\n\\[\\begin{align*}\n    \\frac{\\partial \\mathcal{l}(\\pi; \\textbf{y})}{\\partial \\pi} &= \\frac{1}{\\pi}\\sum_{i=1}^n y_i - \\frac{1}{1-\\pi}\\sum_{i=1}^n(1-y_i) \\\\\n    &= \\frac{1}{\\pi} n\\overline{y} - \\frac{1}{1-\\pi}(n - n\\overline{y})\n\\end{align*}\\]\nSetting the derivative to zero and rearranging terms we get\n\\[\\begin{align*}\n    \\frac{1-\\widehat{\\pi}}{\\widehat{\\pi}} &= \\frac{n-n\\overline{y}}{n\\overline{y}} \\\\\n    \\frac{1}{\\widehat{\\pi}} &= \\frac{n - n\\overline{y}}{n\\overline{y}} + 1 \\\\\n    \\widehat{\\pi} &= \\overline{y}\n\\end{align*}\\]\nThe MLE of \\(\\pi\\) is the sample mean.\n\n\nMaximum likelihood estimation is popular because it is an intuitive principle if we accept a random data-generating mechanism. MLEs have very appealing properties, for example, they are invariant estimators. If \\(\\widehat{\\theta}\\) is the MLE of \\(\\theta\\), then \\(g(\\widehat{\\theta})\\) is the maximum likelihood estimator of \\(g(\\theta)\\).\n\n\nExample: MLEs of Confidence Intervals\n\n\nIn generalized linear models, a linear predictor is related to a transformation of the mean through the link function \\[\n    g(\\mu) = \\eta = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_p x_p\n\\] For example, if the data are Poisson random variables, \\(g(\\cdot)\\) is typically the log function (the log link). The coefficient estimates have a linear interpretation on the logarithmic scale. Suppose \\(\\boldsymbol{\\beta}\\) is estimated by maximum likelihood and is used to construct a 95% confidence interval \\([\\widehat{\\eta}_{.025},\\widehat{\\eta}_{.975}]\\) for \\(\\eta\\).\nYou can transform from \\(\\eta\\) to \\(\\mu\\) by inverting the link function, \\(\\mu = g^{-1}(\\mu)\\). Thus, \\[\n    \\left[ \\exp\\{\\widehat{\\eta}_{.025}\\}, \\exp\\{\\widehat{\\eta}_{.975}\\} \\right]\n\\] is a 95% confidence interval for \\(\\mu\\).\n\n\nFor a finite sample size, MLEs are not necessarily optimal estimators but they have appealing properties as the sample size grows. As \\(n \\rightarrow \\infty\\), maximum likelihood estimators\n\nare consistent, that means they converge in probability to the true value\nare normally distributed\nare efficient in that no other estimator has an asymptotically smaller mean squared error\n\n\nLinear Model with Gaussian Errors\nSuppose we want to find an estimator for \\(\\boldsymbol{\\beta}\\) in the linear model \\(\\textbf{Y}= \\textbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\) where the model errors follow a Gaussian distribution with mean \\(\\textbf{0}\\) and variance \\(\\textbf{V}\\). \\(\\textbf{Y}\\) then follows a Gaussian distribution because it is a linear function of \\(\\boldsymbol{\\epsilon}\\) (see Section 34.7). The probability density function of \\(\\textbf{Y}\\) is\n\\[\nf\\left( \\textbf{Y}\\right)=\\frac{\\left| \\textbf{V}\\right|^{- 1/2}}{(2\\pi)^{\\frac{n}{2}}}\\exp\\left\\{ - \\frac{1}{2}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)^\\prime\\textbf{V}^{- 1}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right) \\right\\}\n\\]\nThis joint distribution of the data can be used to derive the maximum likelihood estimator (MLE) of \\(\\boldsymbol{\\beta}\\). Maximum likelihood estimation considers this as a function of \\(\\boldsymbol{\\beta}\\) rather than a function of \\(\\textbf{Y}.\\) Maximizing this likelihood function \\(\\mathcal{l}(\\boldsymbol{\\beta};\\textbf{Y})\\) is equivalent to maximizing its logarithm. The log-likelihood function for this problem is given by\n\\[\n\\log\\mathcal{l}\\left( \\boldsymbol{\\beta};\\textbf{Y}\\right\\} = l\\left( \\boldsymbol{\\beta};\\textbf{Y}\\right) = - \\frac{1}{2}\\log\\left( \\left| \\textbf{V}\\right| \\right) - \\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)^\\prime\\textbf{V}^{- 1}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)\n\\]\nFinding the maximum of this function with respect to \\(\\boldsymbol{\\beta}\\) is equivalent to minimizing the quadratic form \\[\n\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)^\\prime\\textbf{V}^{-1}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)\n\\] with respect to \\(\\boldsymbol{\\beta}\\). Applying the results about matrix differentiation in Section 34.5 leads to\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial\\boldsymbol{\\beta}}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right)^\\prime\\textbf{V}^{- 1}\\left( \\textbf{Y}- \\textbf{X}\\boldsymbol{\\beta}\\right) &= \\frac{\\partial}{\\partial\\boldsymbol{\\beta}}\\left\\{ \\textbf{Y}^\\prime\\textbf{V}^{- 1}\\textbf{Y}- 2\\textbf{Y}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\beta}^\\prime\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\boldsymbol{\\beta}\\right\\} \\\\\n\n  &= - 2\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}+ 2\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\boldsymbol{\\beta}\n\\end{align*}\\]\nThe derivative is zero when \\(\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}= \\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\boldsymbol{\\beta}\\).\nIf \\(\\textbf{X}\\) is of full column rank, then \\(\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\) is non-singular and its inverse exists. Pre-multiplying both sides of the equation with that inverse yields the solution\n\\[\\begin{align*}\n    \\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}&= \\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\widehat{\\boldsymbol{\\beta}} \\\\\n\n    \\left( \\textbf{X}^{\\prime}\\textbf{V}^{- 1}\\textbf{X}\\right)^{- 1}\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}&= {\\left( \\textbf{X}^{\\prime}\\textbf{V}^{- 1}\\textbf{X}\\right)^{- 1}\\textbf{X}}^\\prime\\textbf{V}^{- 1}\\textbf{X}\\widehat{\\boldsymbol{\\beta}} \\\\\n\n    \\left( \\textbf{X}^{\\prime}\\textbf{V}^{- 1}\\textbf{X}\\right)^{- 1}\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}&= \\widehat{\\boldsymbol{\\beta}}\n\\end{align*}\\]\nThe maximum likelihood estimator of \\(\\boldsymbol{\\beta}\\) is the generalized least squares estimator\n\\[\\widehat{\\boldsymbol{\\beta}}_{GLS} = \\left( \\textbf{X}^{\\prime}\\textbf{V}^{- 1}\\textbf{X}\\right)^{- 1}\\textbf{X}^\\prime\\textbf{V}^{- 1}\\textbf{Y}\\]\nA special case arises when the model errors \\(\\boldsymbol{\\epsilon}\\) are uncorrelated and the variance matrix \\(\\textbf{V}\\) is a diagonal matrix:\n\\[\n\\textbf{V}= \\begin{bmatrix}\n\\sigma_{1}^{2} & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\sigma_{n}^{2}\n\\end{bmatrix}\n\\]\nSince the errors are Gaussian distributed, we know that the errors are then independent. The MLE of \\(\\boldsymbol{\\beta}\\) is the weighted least squares estimator\n\\[\n\\widehat{\\boldsymbol{\\beta}}_{WLS} = \\left(\\textbf{X}^\\prime\\textbf{W}\\textbf{X}\\right)^{-1}\\textbf{X}^\\prime\\textbf{W}\\textbf{Y}\n\\] where \\(\\textbf{W}= \\textbf{V}^{-1}\\).\nA further special case arises when the diagonal entries are all the same,\n\\[\\textbf{V}= \\begin{bmatrix}\n\\sigma^{2}\\  & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\sigma^{2}\n\\end{bmatrix} = \\sigma^{2}\\textbf{I}\\]\nWe can write the error distribution in this case as \\(\\boldsymbol{\\epsilon}\\sim G\\left(\\textbf{0},\\sigma^{2}\\textbf{I}\\right)\\) and the model for \\(\\textbf{Y}\\) as \\(\\textbf{Y}\\sim G\\left( \\textbf{X}\\boldsymbol{\\beta},\\sigma^{2}\\textbf{I}\\right)\\).\nUnder this iid assumption for the Gaussian linear model we can substitute \\(\\sigma^{2}\\textbf{I}\\) for \\(\\textbf{V}\\) in the formula for \\(\\widehat{\\boldsymbol{\\beta}}\\). The maximum likelihood estimator for \\(\\boldsymbol{\\beta}\\) is the ordinary least squares estimator:\n\\[\\widehat{\\boldsymbol{\\beta}}_{OLS} = \\left( \\textbf{X}^{\\prime}\\textbf{X}\\right)^{- 1}\\textbf{X}^\\prime\\textbf{Y}\\]\nNotice that \\(\\sigma^{2}\\) cancels out of the formula; the value of the OLS estimator does not depend on the inherent variability of the data. However, the variability of the OLS estimator does depend on \\(\\sigma^{2}\\) (and on \\(\\textbf{X}\\)).\nNow that we have found the MLE of \\(\\boldsymbol{\\beta}\\), the parameters in the mean function, we can also derive the MLE of the parameters in the variance-covariance structure of the multi-variate Gaussian. Let’s do this for the iid case, \\(\\boldsymbol{\\epsilon}\\sim G(\\textbf{0},\\sigma^2\\textbf{I})\\). The joint density of the data can then be written as \\[\n    f(\\textbf{Y}) = \\prod_{i=1}^n (2 \\pi\\sigma^2)^{-1/2} \\exp\\left\\{-\\frac{1}{2\\sigma^2} (y_i - \\textbf{x}_i^\\prime\\boldsymbol{\\beta})^2\\right\\}\n\\] Taking logs and arranging terms, the log-likelihood function for \\(\\sigma^2\\) is \\[\n\\mathcal{l}(\\sigma^2; \\textbf{y}) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i - \\textbf{x}_i\\prime\\boldsymbol{\\beta})^2\n\\]\nTaking the derivative of \\(\\mathcal{l}(\\sigma^2; \\textbf{y})\\) with respect to \\(\\sigma^2\\), setting it to zero and arranging terms results in \\[\n    \\frac{1}{\\sigma^4}\\sum_{i=1}^n(y_i - \\textbf{x}_i^\\prime\\boldsymbol{\\beta})^2 = \\frac{n}{\\sigma^2}\n\\] The MLE of \\(\\sigma^2\\) in this case is \\[\n    \\widehat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n(y_i - \\textbf{x}_i^\\prime\\boldsymbol{\\beta})^2\n\\] The MLE of \\(\\sigma^2\\) looks similar to the estimator we used in the OLS case, \\[\n    \\frac{1}{n-r(\\textbf{X})} \\sum_{i=1}^n(y_i - \\textbf{x}_i^\\prime \\widehat{\\boldsymbol{\\beta}})^2 = \\frac{1}{n-r(\\textbf{X})} \\, \\text{SSE}\n\\] with two important differences. The divisor in the MLE is \\(n\\) instead of \\(n-r(\\textbf{X})\\) and the MLE uses \\(\\boldsymbol{\\beta}\\) rather than the OLS estimator \\(\\widehat{\\boldsymbol{\\beta}}\\) in the sum-of-squares term. In practice, we would substitute the MLE for \\(\\boldsymbol{\\beta}\\) to compute \\(\\widehat{\\sigma}^2\\), so that the least squares-based estimator and the maximum likelihood estimator differ only in the divisor. Consequently, they cannot be both unbiased estimators of \\(\\sigma^2\\). Which should we choose?\nWe can think of the divisor \\(n-r(\\textbf{X})\\) as accounting for the actual degrees of freedom, the amount of information if you will, in the estimator. Since we are using an estimate of \\(\\boldsymbol{\\beta}\\) to compute SSE, we have “used up” information in the amount of the rank of \\(\\textbf{X}\\). This is the same rationale that computes the estimate of the sample variance as \\[\n    s^2 = \\frac{1}{n-1} \\sum_{i=1}^n \\left(y_i - \\overline{y}\\right)^2\n\\] because we are using the sample mean \\(\\overline{y}\\) in the computation rather than the true mean. Once the sample mean is known, only \\((n-1)\\) of the \\(y_i\\) can be chosen freely, the value of the last one is determined.\nThe reason the MLE divides by \\(n\\) instead of \\(n-r(\\textbf{X})\\) is that it uses \\(\\boldsymbol{\\beta}\\) and does not need to account for information already “used up” in the estimation of the mean function. If you substitute \\(\\widehat{\\boldsymbol{\\beta}}\\) for \\(\\boldsymbol{\\beta}\\), the MLE of \\(\\sigma^2\\) is a biased estimator.",
    "crumbs": [
      "Module VIII. Review Topics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "From the project part\nA Very Short History Of Data Science by Gil Press, Forbes, 2013\nThe History of Data Science and Pioneers You Should Know\nData Science 101: Life Cycle of a Data Science Project, Abraham Musa, Medium, 2021\nSolving the Last Mile Problem for Data Science Project Success, Bill Waid, Forbes, 2019\nThe Ultimate Guide to Deploying Machine Learning Models, Luigi Patruno, 2020\nMeet Airbnb’s official party pooper, who reduced partying by 5% in two years, CNBC, Sept. 19, 2023",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "references.html#from-the-data-part",
    "href": "references.html#from-the-data-part",
    "title": "References",
    "section": "From the data part",
    "text": "From the data part\nJunk Charts Trifecta Checkup: The Definite Guide, by Kaiser Fung.\nFundamentals of Data Visualization, by Claus O. Wilke, O’Reilly Media, 2019.\nThe science behind data visualization, by Graham Odds on Creative Blog, August 8, 2013\nDesigning Against Bias in Machine Learning and AI by David Corliss, AMSTAT News, September 2023\nBig data ethics and 10 controversial data science experiments, by Sabrina Dominquez, Data Science Dojo, May 2018\nUniversity lecturer slams ‘sexist’ Google Translate as gender neutral languages are translated into English, Daily Mail.com, March 24, 2021\nRegression to the mean: what it is and how to deal with it, by Barnett, A.G., van der Pols, J.C., and Dobson, A.J., International Journal of Epidemiology, 34(1), 2005.\nCausality, 2nd ed., Judea Pearl, Cambridge University Press, 2009\nMatrices with Applications in Statistics, 2nd ed., Franklin A. Graybill, Wadsworth International Group, Belmont, CA., 1983\nTheory and Application of the Linear Model, Franklin A. Graybill, Duxbury Press, North Scituate, Massachusetts, 1976\nAdjustments of an inverse matrix corresponding to changes in the elements of a given column or a given row of the original matrix. Sherman, J., and Morrison, W.J. Ann. Math. Stat., 20, 621, 1949\nInverting modified matrices. Woodbury, M., Memorandum No. 42, Statistical Research Group, Princeton University, 1950.\nMathematical Statistics and Data Analysis, 2nd ed. John A. Rice, Duxbury Press, Belmont, CA, 1995",
    "crumbs": [
      "References"
    ]
  }
]